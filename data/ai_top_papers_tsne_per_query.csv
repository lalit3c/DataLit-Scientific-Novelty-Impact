x,y,type,query_id,title,cosine_distance,color
-1.3498629,-1.6547129,query,1023605,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",0.0,#1f77b4
-0.89595294,-2.1210892,neighbor,1023605,Deep Residual Learning for Image Recognition,0.035079240798950195,#1f77b4
-0.61572814,-0.6989851,neighbor,1023605,Resnet in Resnet: Generalizing Residual Architectures,0.053925931453704834,#1f77b4
-1.9939054,1.0484678,neighbor,1023605,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.05463743209838867,#1f77b4
-1.8473693,2.5630648,neighbor,1023605,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),0.05596816539764404,#1f77b4
-1.0902696,-6.073379,neighbor,1023605,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.05626720190048218,#1f77b4
-1.5140371,-7.7203064,neighbor,1023605,Rethinking the Inception Architecture for Computer Vision,0.0583270788192749,#1f77b4
-0.20174128,-5.6266255,neighbor,1023605,Going deeper with convolutions,0.05852055549621582,#1f77b4
1.3903649,-1.2223586,neighbor,1023605,How far can we go without convolution: Improving fully-connected networks,0.06118839979171753,#1f77b4
3.7938225,5.954392,neighbor,1023605,Training Deeper Convolutional Networks with Deep Supervision,0.06167030334472656,#1f77b4
-0.034834944,-3.8181148,neighbor,1023605,Striving for Simplicity: The All Convolutional Net,0.06362664699554443,#1f77b4
-0.7515129,4.25567,neighbor,1023605,Natural Neural Networks,0.06421548128128052,#1f77b4
-2.2150242,-6.290137,neighbor,1023605,Enhanced image classification with a fast-learning shallow convolutional neural network,0.06446999311447144,#1f77b4
4.1678987,3.7250423,neighbor,1023605,Training Very Deep Networks,0.06484806537628174,#1f77b4
-9.376998,-3.672431,neighbor,1023605,Visualizing and Understanding Convolutional Networks,0.06586199998855591,#1f77b4
-4.506999,-1.2192875,neighbor,1023605,Network In Network,0.0659719705581665,#1f77b4
-2.5356038,-7.079779,neighbor,1023605,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.0668216347694397,#1f77b4
-6.647387,12.176146,neighbor,1023605,Recurrent Neural Network Regularization,0.06790649890899658,#1f77b4
-5.761441,-1.8787433,neighbor,1023605,Deep Epitomic Convolutional Neural Networks,0.06820434331893921,#1f77b4
-1.2189938,8.047578,neighbor,1023605,Learning Compact Convolutional Neural Networks with Nested Dropout,0.06827545166015625,#1f77b4
4.5563855,10.620403,neighbor,1023605,Discriminative Recurrent Sparse Auto-Encoders,0.06834918260574341,#1f77b4
1.7762979,-6.2389874,neighbor,1023605,Large-Scale Deep Learning on the YFCC100M Dataset,0.06857144832611084,#1f77b4
0.14396794,-6.870338,neighbor,1023605,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.06898516416549683,#1f77b4
0.74432206,2.3173265,neighbor,1023605,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.06908625364303589,#1f77b4
6.641121,7.6400523,neighbor,1023605,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.06913429498672485,#1f77b4
-4.403148,-0.08820136,neighbor,1023605,All you need is a good init,0.06981724500656128,#1f77b4
-1.565635,1.6423806,neighbor,1023605,Empirical Evaluation of Rectified Activations in Convolutional Network,0.06996023654937744,#1f77b4
-2.1579938,11.092434,neighbor,1023605,RandomOut: Using a convolutional gradient norm to rescue convolutional filters,0.07031065225601196,#1f77b4
4.158379,0.7460951,neighbor,1023605,Learning both Weights and Connections for Efficient Neural Network,0.07049274444580078,#1f77b4
-0.33247972,9.506671,neighbor,1023605,Improving neural networks by preventing co-adaptation of feature detectors,0.07108688354492188,#1f77b4
-7.0683975,13.899538,neighbor,1023605,A Clockwork RNN,0.07122617959976196,#1f77b4
0.43809986,6.173787,neighbor,1023605,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,0.07165247201919556,#1f77b4
4.1672316,3.9170456,neighbor,1023605,Highway Networks,0.0717766284942627,#1f77b4
-9.340308,-3.90702,neighbor,1023605,Visualizing and Comparing Convolutional Neural Networks,0.07191473245620728,#1f77b4
5.477475,-3.1307147,neighbor,1023605,SimNets: A Generalization of Convolutional Networks,0.07234269380569458,#1f77b4
5.28319,5.2609463,neighbor,1023605,Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks,0.07251328229904175,#1f77b4
1.0004303,7.3268886,neighbor,1023605,Committees of Deep Feedforward Networks Trained with Few Data,0.07317173480987549,#1f77b4
2.811097,3.3027258,neighbor,1023605,FitNets: Hints for Thin Deep Nets,0.07327759265899658,#1f77b4
2.9623554,-1.2932019,neighbor,1023605,Deep Fried Convnets,0.0734373927116394,#1f77b4
4.3480053,1.9114063,neighbor,1023605,Learning Neural Network Architectures using Backpropagation,0.07366335391998291,#1f77b4
-2.587397,2.2901635,neighbor,1023605,Expressiveness of Rectifier Networks,0.07380557060241699,#1f77b4
2.9342232,7.973219,neighbor,1023605,Mediated experts for deep convolutional networks,0.07452869415283203,#1f77b4
-2.29211,-9.901591,neighbor,1023605,Fast R-CNN,0.07461130619049072,#1f77b4
3.7576082,6.386312,neighbor,1023605,Deeply-Supervised Nets,0.07461929321289062,#1f77b4
9.568781,-2.9572437,neighbor,1023605,Towards Open Set Deep Networks,0.07463085651397705,#1f77b4
-2.6302416,6.2424226,neighbor,1023605,Maxout Networks,0.07488632202148438,#1f77b4
-5.2240686,13.364976,neighbor,1023605,Benchmarking of LSTM Networks,0.075009286403656,#1f77b4
-5.2562456,-3.8569133,neighbor,1023605,MatConvNet: Convolutional Neural Networks for MATLAB,0.07524174451828003,#1f77b4
-5.3469768,-9.221142,neighbor,1023605,Do Convnets Learn Correspondence?,0.07574677467346191,#1f77b4
-3.0877686,-10.538954,neighbor,1023605,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.07584553956985474,#1f77b4
-4.8966208,-5.7025094,neighbor,1023605,Feature Representation in Convolutional Neural Networks,0.07592004537582397,#1f77b4
1.1167372,-11.106671,neighbor,1023605,Training CNNs with Low-Rank Filters for Efficient Image Classification,0.07599538564682007,#1f77b4
1.2821633,5.1545186,neighbor,1023605,GradNets: Dynamic Interpolation Between Neural Architectures,0.07608616352081299,#1f77b4
10.01521,-5.474576,neighbor,1023605,Confusing Deep Convolution Networks by Relabelling,0.07610136270523071,#1f77b4
1.7785171,-8.418836,neighbor,1023605,Convolutional neural networks at constrained time cost,0.07625991106033325,#1f77b4
-6.517868,2.4032214,neighbor,1023605,Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification,0.07636898756027222,#1f77b4
-9.603074,-8.863064,neighbor,1023605,How transferable are features in deep neural networks?,0.07681292295455933,#1f77b4
-2.1119423,9.15667,neighbor,1023605,Efficient batchwise dropout training using submatrices,0.0769008994102478,#1f77b4
-5.655482,3.7290087,neighbor,1023605,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",0.07692265510559082,#1f77b4
3.4961395,-8.0169325,neighbor,1023605,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.07725876569747925,#1f77b4
3.1581864,11.680631,neighbor,1023605,Scheduled denoising autoencoders,0.07735574245452881,#1f77b4
7.6615667,5.556044,neighbor,1023605,Do Deep Nets Really Need to be Deep?,0.07736992835998535,#1f77b4
-9.914893,7.7294307,neighbor,1023605,Recurrent Models of Visual Attention,0.07759642601013184,#1f77b4
-3.7046237,-5.8625636,neighbor,1023605,A Taxonomy of Deep Convolutional Neural Nets for Computer Vision,0.07765328884124756,#1f77b4
2.179722,-11.014401,neighbor,1023605,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.07787257432937622,#1f77b4
5.4655933,-11.541026,neighbor,1023605,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.07805955410003662,#1f77b4
2.3699636,12.425484,neighbor,1023605,Gradual Training Method for Denoising Auto Encoders,0.07806670665740967,#1f77b4
10.007801,-5.146254,neighbor,1023605,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,0.07816451787948608,#1f77b4
-9.544182,8.332071,neighbor,1023605,Learning Wake-Sleep Recurrent Attention Models,0.07832199335098267,#1f77b4
-2.839568,7.5730567,neighbor,1023605,Shakeout: A New Regularized Deep Neural Network Training Scheme,0.07842046022415161,#1f77b4
-4.6615295,-4.8074007,neighbor,1023605,An Introduction to Convolutional Neural Networks,0.0784255862236023,#1f77b4
2.396593,12.406409,neighbor,1023605,Gradual training of deep denoising auto encoders,0.07842600345611572,#1f77b4
5.5323424,-12.045588,neighbor,1023605,Multi-column deep neural networks for image classification,0.0784372091293335,#1f77b4
7.070825,3.8240542,neighbor,1023605,Piecewise Linear Multilayer Perceptrons and Dropout,0.07859665155410767,#1f77b4
0.06365322,-8.281697,neighbor,1023605,Shoot to Know What: An Application of Deep Networks on Mobile Devices,0.07863247394561768,#1f77b4
-4.8689375,4.7967324,neighbor,1023605,Fractional Max-Pooling,0.07867932319641113,#1f77b4
-3.2714794,5.5916324,neighbor,1023605,Improving Deep Neural Networks with Probabilistic Maxout Units,0.07884657382965088,#1f77b4
-6.156099,4.208088,neighbor,1023605,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.07885175943374634,#1f77b4
-5.854904,-11.195816,neighbor,1023605,DAG-Recurrent Neural Networks for Scene Labeling,0.07898259162902832,#1f77b4
-6.94831,12.865784,neighbor,1023605,A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,0.07898968458175659,#1f77b4
5.558167,-2.9443016,neighbor,1023605,Deep SimNets,0.07909184694290161,#1f77b4
3.095345,-2.0920753,neighbor,1023605,Binarized Neural Networks,0.07917100191116333,#1f77b4
-8.533497,-9.597236,neighbor,1023605,From generic to specific deep representations for visual recognition,0.07920145988464355,#1f77b4
1.858516,-4.774639,neighbor,1023605,Towards Good Practices for Very Deep Two-Stream ConvNets,0.07925635576248169,#1f77b4
5.7589345,2.445598,neighbor,1023605,Representation Benefits of Deep Feedforward Networks,0.07948976755142212,#1f77b4
6.4593024,8.2487335,neighbor,1023605,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.0803377628326416,#1f77b4
-7.490739,12.343206,neighbor,1023605,Regularization and nonlinearities for neural language models: when are they needed?,0.08038127422332764,#1f77b4
-2.8476539,-4.1654377,neighbor,1023605,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.08064925670623779,#1f77b4
-1.5950861,-4.209274,neighbor,1023605,A HMAX with LLC for visual recognition,0.08081650733947754,#1f77b4
-2.084001,8.084397,neighbor,1023605,Fundamental differences between Dropout and Weight Decay in Deep Networks,0.08105099201202393,#1f77b4
-5.8549266,14.846028,neighbor,1023605,rnn : Recurrent Library for Torch,0.08108532428741455,#1f77b4
-3.0874615,9.015605,neighbor,1023605,Parallel Dither and Dropout for Regularising Deep Neural Networks,0.08132940530776978,#1f77b4
5.695991,-4.8853807,neighbor,1023605,Deep Clustered Convolutional Kernels,0.08135330677032471,#1f77b4
-4.674831,6.345328,neighbor,1023605,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.08138138055801392,#1f77b4
3.3268912,-7.41158,neighbor,1023605,Caffe: Convolutional Architecture for Fast Feature Embedding,0.08140891790390015,#1f77b4
-7.9372168,-10.125773,neighbor,1023605,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.08153676986694336,#1f77b4
-6.90222,-7.062073,neighbor,1023605,On Deep Representation Learning from Noisy Web Images,0.08160156011581421,#1f77b4
-4.1654615,-6.676086,neighbor,1023605,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.0816228985786438,#1f77b4
-3.9639041,-11.176548,neighbor,1023605,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.08168685436248779,#1f77b4
1.2956219,-11.60393,neighbor,1023605,Convolutional neural networks with low-rank regularization,0.08214408159255981,#1f77b4
4.692013,-8.517217,neighbor,1023605,PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions,0.08223360776901245,#1f77b4
3.35933,2.1580422,query,10328909,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.0,#1f77b4
2.2978783,2.5517404,neighbor,10328909,"Scalable, High-Quality Object Detection",0.03039991855621338,#1f77b4
-3.1393354,-2.664607,neighbor,10328909,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.03327500820159912,#1f77b4
2.558732,3.6424115,neighbor,10328909,What Makes for Effective Detection Proposals?,0.03361690044403076,#1f77b4
4.0286098,1.9368951,neighbor,10328909,R-CNN minus R,0.035881757736206055,#1f77b4
7.0656996,2.4327865,neighbor,10328909,Fast R-CNN,0.03640216588973999,#1f77b4
-3.609128,-2.6591623,neighbor,10328909,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.03685420751571655,#1f77b4
6.8169227,0.9170752,neighbor,10328909,Object Detection Networks on Convolutional Feature Maps,0.043154001235961914,#1f77b4
2.1313124,-2.3135624,neighbor,10328909,Scalable Object Detection Using Deep Neural Networks,0.0443577766418457,#1f77b4
0.67318875,-3.8414805,neighbor,10328909,Boosting Convolutional Features for Robust Object Proposals,0.044617652893066406,#1f77b4
2.481015,4.1386995,neighbor,10328909,"How good are detection proposals, really?",0.04561054706573486,#1f77b4
6.2822995,1.8051411,neighbor,10328909,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.04760557413101196,#1f77b4
-1.3083951,-3.9230094,neighbor,10328909,Mid-level Elements for Object Detection,0.04883217811584473,#1f77b4
1.479818,2.643184,neighbor,10328909,DeepBox: Learning Objectness with Convolutional Networks,0.05042010545730591,#1f77b4
1.4054129,0.5072682,neighbor,10328909,1-HKUST: Object Detection in ILSVRC 2014,0.05149108171463013,#1f77b4
3.4063063,-0.14823982,neighbor,10328909,Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction,0.051625072956085205,#1f77b4
-4.9100347,-2.2871733,neighbor,10328909,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.05404871702194214,#1f77b4
0.006918559,-5.099812,neighbor,10328909,Regionlets for Generic Object Detection,0.05406057834625244,#1f77b4
3.8704822,4.087402,neighbor,10328909,Object Proposal Generation Using Two-Stage Cascade SVMs,0.05457568168640137,#1f77b4
-2.0961678,3.5601792,neighbor,10328909,Self-taught object localization with deep networks,0.05469691753387451,#1f77b4
5.2884235,-1.5787259,neighbor,10328909,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.054782748222351074,#1f77b4
-1.0032432,2.6083355,neighbor,10328909,Visual chunking: A list prediction framework for region-based object detection,0.05540496110916138,#1f77b4
5.122104,-1.7584689,neighbor,10328909,DeepID-Net: Deformable deep convolutional neural networks for object detection,0.055886149406433105,#1f77b4
-5.8310328,1.5051072,neighbor,10328909,Part-Based R-CNNs for Fine-Grained Category Detection,0.056247174739837646,#1f77b4
0.71541524,-5.8081822,neighbor,10328909,Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection,0.05663257837295532,#1f77b4
-3.8360043,-1.322487,neighbor,10328909,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.05681353807449341,#1f77b4
4.314439,-2.5789335,neighbor,10328909,Generic Object Detection with Dense Neural Patterns and Regionlets,0.058247148990631104,#1f77b4
-7.145349,-0.9498719,neighbor,10328909,Hypercolumns for object segmentation and fine-grained localization,0.0592116117477417,#1f77b4
0.4722719,-9.82094,neighbor,10328909,BING: Binarized normed gradients for objectness estimation at 300fps,0.05982083082199097,#1f77b4
-7.17943,-3.5306056,neighbor,10328909,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.060926198959350586,#1f77b4
-0.9144352,-4.7207866,neighbor,10328909,Max-Margin Object Detection,0.06263130903244019,#1f77b4
2.672494,6.036684,neighbor,10328909,An active search strategy for efficient object class detection,0.06317096948623657,#1f77b4
1.5613115,4.6738296,neighbor,10328909,Object-Proposal Evaluation Protocol is ‘Gameable’,0.06397837400436401,#1f77b4
-2.6107728,7.0053287,neighbor,10328909,On learning to localize objects with minimal supervision,0.06567233800888062,#1f77b4
-4.752887,-0.26871955,neighbor,10328909,Deformable part models are convolutional neural networks,0.06598448753356934,#1f77b4
1.3809,-4.95006,neighbor,10328909,The Fastest Deformable Part Model for Object Detection,0.06613540649414062,#1f77b4
3.9298587,-1.8753929,neighbor,10328909,Deep learning for class-generic object detection,0.06624716520309448,#1f77b4
-1.8204721,-1.5302944,neighbor,10328909,Detect2Rank: Combining Object Detectors Using Learning to Rank,0.06628060340881348,#1f77b4
9.1709795,2.4481664,neighbor,10328909,Going deeper with convolutions,0.0667007565498352,#1f77b4
8.057876,0.5679776,neighbor,10328909,Do Convnets Learn Correspondence?,0.06680524349212646,#1f77b4
4.576483,-6.6378107,neighbor,10328909,Object-centric Sampling for Fine-grained Image Classification,0.06725466251373291,#1f77b4
-2.084142,-5.6856227,neighbor,10328909,Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection,0.06790131330490112,#1f77b4
1.5372728,-10.753558,neighbor,10328909,Measuring the Objectness of Image Windows,0.06813985109329224,#1f77b4
-1.9898831,4.8831043,neighbor,10328909,Weakly Supervised Object Localization with Multi-Fold Multiple Instance Learning,0.06833261251449585,#1f77b4
-1.3568357,-2.0561905,neighbor,10328909,Ensemble of exemplar-SVMs for object detection and beyond,0.06913208961486816,#1f77b4
-1.8688937,1.4044514,neighbor,10328909,Unsupervised Visual Representation Learning by Context Prediction,0.06922322511672974,#1f77b4
8.909009,3.1111019,neighbor,10328909,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.06946247816085815,#1f77b4
-6.722781,-9.762832,neighbor,10328909,3D object class detection in the wild,0.07007694244384766,#1f77b4
-3.3720083,-3.9990695,neighbor,10328909,Deep Joint Task Learning for Generic Object Extraction,0.07026249170303345,#1f77b4
-5.8594146,-4.9449606,neighbor,10328909,Semantic segmentation using regions and parts,0.0706108808517456,#1f77b4
5.823704,-4.2102346,neighbor,10328909,Object Detectors Emerge in Deep Scene CNNs,0.07132548093795776,#1f77b4
-3.736549,4.0660396,neighbor,10328909,Looking out of the window: object localization by joint analysis of all windows in the image,0.07186347246170044,#1f77b4
-9.248872,-2.5650055,neighbor,10328909,Fully convolutional networks for semantic segmentation,0.07214027643203735,#1f77b4
5.705419,-0.12199815,neighbor,10328909,Do More Dropouts in Pool5 Feature Maps for Better Object Detection,0.07227379083633423,#1f77b4
7.660673,-1.9249527,neighbor,10328909,LSDA: Large Scale Detection through Adaptation,0.07230937480926514,#1f77b4
-8.637534,-2.87369,neighbor,10328909,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.07246118783950806,#1f77b4
-5.4986987,-3.4813395,neighbor,10328909,Layered object detection for multi-class segmentation,0.07246500253677368,#1f77b4
1.5375084,0.13645141,neighbor,10328909,ImageNet Large Scale Visual Recognition Challenge,0.07263660430908203,#1f77b4
0.4556254,-2.361529,neighbor,10328909,Context-Aware Semi-Local Feature Detector,0.07286828756332397,#1f77b4
-2.7222471,-9.891152,neighbor,10328909,A novel method for object localization in digital images,0.07317674160003662,#1f77b4
-0.5484462,-1.1137252,neighbor,10328909,Context Forest for efficient object detection with large mixture models,0.07336986064910889,#1f77b4
10.52244,2.7806792,neighbor,10328909,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.0736282467842102,#1f77b4
10.788664,1.7662218,neighbor,10328909,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.07363706827163696,#1f77b4
-3.3890998,-10.319441,neighbor,10328909,Fast concurrent object localization and recognition,0.07376879453659058,#1f77b4
2.8789723,-11.339899,neighbor,10328909,Salient object detection: A survey,0.0740654468536377,#1f77b4
-6.6097255,-5.8943667,neighbor,10328909,Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation,0.07416313886642456,#1f77b4
-6.6592793,-9.740517,neighbor,10328909,Data-driven 3D Voxel Patterns for object category recognition,0.07429778575897217,#1f77b4
-1.9910209,7.272005,neighbor,10328909,Watch and learn: Semi-supervised learning of object detectors from videos,0.0746503472328186,#1f77b4
8.495258,-5.2648993,neighbor,10328909,Transferring Rich Feature Hierarchies for Robust Visual Tracking,0.07477957010269165,#1f77b4
7.401842,6.778035,neighbor,10328909,Taking a deeper look at pedestrians,0.0757783055305481,#1f77b4
-9.22249,-2.0992799,neighbor,10328909,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.07601755857467651,#1f77b4
2.8398652,-11.204114,neighbor,10328909,Salient Object Detection: A Benchmark,0.07649725675582886,#1f77b4
-10.624713,4.0143766,neighbor,10328909,R-CNNs for Pose Estimation and Action Detection,0.07671141624450684,#1f77b4
10.479749,-0.853214,neighbor,10328909,Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection,0.07678967714309692,#1f77b4
-5.318418,-6.8576374,neighbor,10328909,Object category detection by incorporating mid-level grouping cues,0.07699549198150635,#1f77b4
-4.307053,8.123295,neighbor,10328909,Detection of Partially Visible Objects,0.07708579301834106,#1f77b4
-4.843979,-5.2032146,neighbor,10328909,Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts,0.07766163349151611,#1f77b4
-4.763326,1.3615122,neighbor,10328909,Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks,0.07768845558166504,#1f77b4
10.227906,3.1410348,neighbor,10328909,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.07780396938323975,#1f77b4
7.8646584,6.341922,neighbor,10328909,Convolutional Channel Features,0.07780724763870239,#1f77b4
2.4896715,-12.243485,neighbor,10328909,Salient Object Detection via Augmented Hypotheses,0.07801401615142822,#1f77b4
11.5805235,1.1628885,neighbor,10328909,Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification,0.07802611589431763,#1f77b4
4.16214,-12.69015,neighbor,10328909,Robust Object Recognition via Visual Pathway Feedback,0.07805132865905762,#1f77b4
-2.3836138,-10.176032,neighbor,10328909,Efficient Subwindow Search: A Branch and Bound Framework for Object Localization,0.0788043737411499,#1f77b4
2.089714,-7.47813,neighbor,10328909,Is the Game worth the Candle? - Evaluation of OpenCL for Object Detection Algorithm Optimization,0.07899975776672363,#1f77b4
11.199945,3.764508,neighbor,10328909,Multi-scale Recognition with DAG-CNNs,0.07918417453765869,#1f77b4
-11.0159235,3.6766126,neighbor,10328909,Efficient object localization using Convolutional Networks,0.07924544811248779,#1f77b4
-1.4629314,0.36061755,neighbor,10328909,An empirical study of context in object detection,0.07957816123962402,#1f77b4
3.0228026,-12.399422,neighbor,10328909,Salient Object Detection via Objectness Proposals,0.07961177825927734,#1f77b4
-9.997784,4.5439515,neighbor,10328909,Finding action tubes,0.07966715097427368,#1f77b4
-1.207365,5.094459,neighbor,10328909,Detector discovery in the wild: Joint multiple instance and representation learning,0.07966792583465576,#1f77b4
0.81718725,-7.252301,neighbor,10328909,Computation of Rotation Local Invariant Features using the Integral Image for Real Time Object Detection,0.07985925674438477,#1f77b4
-0.8493835,-6.650925,neighbor,10328909,Shared Random Ferns for Efficient Detection of Multiple Categories,0.0801386833190918,#1f77b4
9.461893,4.2447686,neighbor,10328909,Caffe: Convolutional Architecture for Fast Feature Embedding,0.08054512739181519,#1f77b4
-4.3365903,-10.629075,neighbor,10328909,Object Detection in Real Images,0.08061856031417847,#1f77b4
-2.8570547,6.0587296,neighbor,10328909,Weakly-supervised Discovery of Visual Pattern Configurations,0.08069801330566406,#1f77b4
-9.350245,-3.6395285,neighbor,10328909,Convolutional feature masking for joint object and stuff segmentation,0.08078855276107788,#1f77b4
-6.4333987,1.3933864,neighbor,10328909,Orientational Spatial Part Modeling for Fine-Grained Visual Categorization,0.080921471118927,#1f77b4
-5.946743,3.0223982,neighbor,10328909,Attention for Fine-Grained Categorization,0.08097898960113525,#1f77b4
3.179811,-4.8993373,neighbor,10328909,Training a multi-exit cascade with linear asymmetric classification for efficient object detection,0.08098989725112915,#1f77b4
-7.1481733,-4.7478833,neighbor,10328909,Semantic contours from inverse detectors,0.0810742974281311,#1f77b4
-0.98111284,-1.5387813,query,1033682,Generative Adversarial Networks,0.0,#1f77b4
-0.41239354,4.8108377,neighbor,1033682,Generative Adversarial Networks (GANs),0.014911651611328125,#1f77b4
3.2365382,3.6066363,neighbor,1033682,A Survey on the Progression and Performance of Generative Adversarial Networks,0.015390932559967041,#1f77b4
3.5957997,-0.717424,neighbor,1033682,Emerging Applications of Generative Adversarial Networks,0.01709771156311035,#1f77b4
3.8714504,2.2530346,neighbor,1033682,A Detailed Study on Generative Adversarial Networks,0.017276525497436523,#1f77b4
3.1494343,-2.887412,neighbor,1033682,Generative Adversarial Networks and Adversarial Autoencoders: Tutorial and Survey,0.018620073795318604,#1f77b4
5.6435733,4.4202204,neighbor,1033682,A Comprehensive Qualitative and Quantitative Review of Current Research in GANs,0.0193750262260437,#1f77b4
1.1605268,0.18106937,neighbor,1033682,NIPS 2016 Tutorial: Generative Adversarial Networks,0.02010244131088257,#1f77b4
-1.0630896,-3.3546073,neighbor,1033682,Training Generative Adversarial Networks in One Stage,0.02013528347015381,#1f77b4
4.8884354,-0.6638152,neighbor,1033682,"Generative Adversarial Networks (GANs): An Overview of Theoretical Model, Evaluation Metrics, and Recent Developments",0.020212531089782715,#1f77b4
6.076805,-0.9282514,neighbor,1033682,Comparison on Generative Adversarial Networks –A Study,0.020278215408325195,#1f77b4
0.6965768,6.923678,neighbor,1033682,Regularization Methods for Generative Adversarial Networks: An Overview of Recent Studies,0.02119523286819458,#1f77b4
1.5770177,-4.6150756,neighbor,1033682,Generative Adversarial Networks: A Survey and Taxonomy,0.022252976894378662,#1f77b4
-0.83780396,5.301717,neighbor,1033682,Challenges and Corresponding Solutions of Generative Adversarial Networks (GANs): A Survey Study,0.022899210453033447,#1f77b4
1.7151867,-4.7112875,neighbor,1033682,Generative Adversarial Networks in Computer Vision,0.022930502891540527,#1f77b4
1.0064243,1.6408087,neighbor,1033682,Generative Adversarial Networks : A Survey,0.022942841053009033,#1f77b4
5.5150437,2.8095703,neighbor,1033682,Recent Advance On Generative Adversarial Networks,0.022990167140960693,#1f77b4
4.9107404,1.1669563,neighbor,1033682,A Primer on Generative Adversarial Networks,0.023100554943084717,#1f77b4
4.503353,0.6371884,neighbor,1033682,Generative Adversarial Networks: Outline and its Use Cases,0.02350318431854248,#1f77b4
4.3595366,4.73887,neighbor,1033682,Loss Functions of Generative Adversarial Networks (GANs): Opportunities and Challenges,0.023698031902313232,#1f77b4
5.0084662,3.4659653,neighbor,1033682,Recent Progress on Generative Adversarial Networks (GANs): A Survey,0.023870229721069336,#1f77b4
-5.2445354,-5.345689,neighbor,1033682,Why are Generative Adversarial Networks so Fascinating and Annoying?,0.024027228355407715,#1f77b4
-7.1556363,1.0282611,neighbor,1033682,A Guided Learning Approach for Generative Adversarial Networks,0.024496138095855713,#1f77b4
5.5677767,1.6581124,neighbor,1033682,A survey on generative adversarial networks and their variants methods,0.024639368057250977,#1f77b4
9.133858,-0.0032845277,neighbor,1033682,Generative Adversarial Networks (GANs): What it can generate and What it cannot?,0.025057077407836914,#1f77b4
7.131531,-3.5416696,neighbor,1033682,The research process of generative adversarial networks,0.025341689586639404,#1f77b4
6.123261,-1.9330028,neighbor,1033682,Review on Generative Adversarial Networks: Focusing on Computer Vision and Its Applications,0.025488078594207764,#1f77b4
-8.587056,-3.8450015,neighbor,1033682,An Empirical Study of Generative Models with Encoders,0.02556508779525757,#1f77b4
-1.6337295,6.3292704,neighbor,1033682,"A Survey on Generative Adversarial Networks: Variants, Applications, and Training",0.025692224502563477,#1f77b4
7.449674,-0.3771231,neighbor,1033682,A Survey of Generative Adversarial Networks,0.025794684886932373,#1f77b4
-6.842385,2.3663406,neighbor,1033682,HGAN: Hybrid generative adversarial network,0.02591496706008911,#1f77b4
-3.0128772,0.3918265,neighbor,1033682,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,0.026343703269958496,#1f77b4
-2.2105737,-7.3764887,neighbor,1033682,Generative Adversarial Stacked Autoencoders,0.026377439498901367,#1f77b4
-1.4323214,-0.835253,neighbor,1033682,MAGAN: Margin Adaptation for Generative Adversarial Networks,0.02653294801712036,#1f77b4
1.0646284,7.5783796,neighbor,1033682,A Systematic Survey of Regularization and Normalization in GANs,0.02679389715194702,#1f77b4
-6.5542192,-4.244348,neighbor,1033682,Optimizing Latent Distributions for Non-Adversarial Generative Networks,0.02692270278930664,#1f77b4
-4.3000474,8.579796,neighbor,1033682,VARGAN: variance enforcing network enhanced GAN,0.026969432830810547,#1f77b4
-4.8822827,-1.4244249,neighbor,1033682,Generative Adversarial Network Training is a Continual Learning Problem,0.027029693126678467,#1f77b4
-8.13833,-5.2720494,neighbor,1033682,IVE-GAN: Invariant Encoding Generative Adversarial Networks,0.027432560920715332,#1f77b4
2.5411725,0.34935617,neighbor,1033682,Training Generative Adversarial Networks With Weights,0.02743631601333618,#1f77b4
7.5156775,0.9904471,neighbor,1033682,Recent Advancement in Generative Adversarial Network Models,0.027494311332702637,#1f77b4
5.5713315,-3.338404,neighbor,1033682,Overview of Generative Adversarial Networks,0.027574539184570312,#1f77b4
-2.3998299,-7.8956966,neighbor,1033682,Stacked Generative Adversarial Networks,0.02759838104248047,#1f77b4
1.4494276,8.463171,neighbor,1033682,"The GAN Landscape: Losses, Architectures, Regularization, and Normalization",0.027633845806121826,#1f77b4
-5.2413864,1.896177,neighbor,1033682,CAGAN: Consistent Adversarial Training Enhanced GANs,0.02786654233932495,#1f77b4
2.0301185,3.7008204,neighbor,1033682,FusedProp: Towards Efficient Training of Generative Adversarial Networks,0.02787935733795166,#1f77b4
1.4413528,-10.574075,neighbor,1033682,Training Generative Networks Using Random Discriminators,0.02799898386001587,#1f77b4
-6.4493265,8.340301,neighbor,1033682,Prescribed Generative Adversarial Networks,0.028076529502868652,#1f77b4
5.86186,-5.951732,neighbor,1033682,FCC-GAN: A Fully Connected and Convolutional Net Architecture for GANs,0.02819240093231201,#1f77b4
8.178159,0.71377563,neighbor,1033682,A Review: Generative Adversarial Networks,0.028721153736114502,#1f77b4
-8.906225,-5.1987247,neighbor,1033682,Inverting the Generator of a Generative Adversarial Network,0.02884805202484131,#1f77b4
9.429966,2.1738987,neighbor,1033682,Generative adversarial networks: introduction and outlook,0.028857290744781494,#1f77b4
-4.8925004,-3.26121,neighbor,1033682,Generative Multi-Adversarial Networks,0.028864383697509766,#1f77b4
-7.9011292,4.4472613,neighbor,1033682,Dual Discriminator Generative Adversarial Nets,0.028880059719085693,#1f77b4
-4.7687454,3.5417633,neighbor,1033682,Robust generative adversarial network,0.02891099452972412,#1f77b4
6.9377947,3.3122566,neighbor,1033682,Understanding Trending Variants of Generative Adversarial Networks,0.02899843454360962,#1f77b4
-6.3483586,-5.739064,neighbor,1033682,Unregularized Auto-Encoder with Generative Adversarial Networks for Image Generation,0.028999030590057373,#1f77b4
-8.9694395,-7.8365736,neighbor,1033682,Autoencoding Generative Adversarial Networks,0.02903878688812256,#1f77b4
9.342139,7.03465,neighbor,1033682,Improved evolution of generative adversarial networks,0.02909231185913086,#1f77b4
-6.632781,4.333237,neighbor,1033682,TWGAN: Twin Discriminator Generative Adversarial Networks,0.029125213623046875,#1f77b4
0.9846928,-7.773581,neighbor,1033682,ChainGAN: A sequential approach to GANs,0.029147565364837646,#1f77b4
4.080278,-4.6300807,neighbor,1033682,Generative Adversarial Networks: An Overview,0.02941054105758667,#1f77b4
-3.9319859,9.600141,neighbor,1033682,Prb-GAN: A Probabilistic Framework for GAN Modelling,0.029423952102661133,#1f77b4
-7.2524805,-3.4624283,neighbor,1033682,Optimizing the Latent Space of Generative Networks,0.029500961303710938,#1f77b4
-8.485885,3.5766656,neighbor,1033682,Multi-Generator Generative Adversarial Nets,0.029501736164093018,#1f77b4
4.967617,9.493903,neighbor,1033682,Least Squares Generative Adversarial Networks,0.029565393924713135,#1f77b4
-6.7355065,6.9935284,neighbor,1033682,Towards Understanding the Dynamics of Generative Adversarial Networks,0.029581725597381592,#1f77b4
-5.251227,-10.492706,neighbor,1033682,Using generative adversarial networks to synthesize artificial financial datasets,0.029671788215637207,#1f77b4
8.259984,-2.4593482,neighbor,1033682,Review on Generative Adversarial Networks,0.029777884483337402,#1f77b4
0.1769534,0.894711,neighbor,1033682,Generative adversarial networks,0.029943108558654785,#1f77b4
0.54884607,-5.5987544,neighbor,1033682,InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning,0.030025839805603027,#1f77b4
-4.1801353,3.7269104,neighbor,1033682,Improving Generalization and Stability of Generative Adversarial Networks,0.03018498420715332,#1f77b4
-6.25303,-6.425806,neighbor,1033682,Generative Adversarial Networks with Decoder-Encoder Output Noise,0.030214548110961914,#1f77b4
-2.1170888,-9.092731,neighbor,1033682,Robust Conditional Generative Adversarial Networks,0.030263304710388184,#1f77b4
7.1673794,-4.0271854,neighbor,1033682,The generative adversarial networks and its application in machine vision,0.030326247215270996,#1f77b4
8.595261,8.036098,neighbor,1033682,Evolutionary Generative Adversarial Networks with Crossover Based Knowledge Distillation,0.03033381700515747,#1f77b4
-2.5938048,6.5848384,neighbor,1033682,Unbalanced GANs: Pre-training the Generator of Generative Adversarial Network using Variational Autoencoder,0.03045475482940674,#1f77b4
-2.9761543,0.99080914,neighbor,1033682,Regularizing Generative Adversarial Networks under Limited Data,0.030545413494110107,#1f77b4
-8.818424,-1.8771269,neighbor,1033682,Generative Adversarial Parallelization,0.03056114912033081,#1f77b4
-7.653297,0.34938475,neighbor,1033682,Improved Techniques for Training GANs,0.030564546585083008,#1f77b4
6.7395473,4.9612575,neighbor,1033682,Spectrum of Advancements and Developments in Multidisciplinary Domains for Generative Adversarial Networks (GANs),0.03062838315963745,#1f77b4
-3.7215233,-2.9515834,neighbor,1033682,Semi-supervised learning based on generative adversarial network: a comparison between good GAN and bad GAN approach,0.030644893646240234,#1f77b4
-5.4208074,-7.5246587,neighbor,1033682,Utilizing Amari-Alpha Divergence to Stabilize the Training of Generative Adversarial Networks,0.030737102031707764,#1f77b4
8.677344,7.796231,neighbor,1033682,Evolutionary Generative Adversarial Networks,0.03118455410003662,#1f77b4
8.003964,1.7100425,neighbor,1033682,Recent Advances in Generative Adversarial Networks: An Analysis along with its outlook,0.031202256679534912,#1f77b4
-9.14395,1.7288218,neighbor,1033682,Lessons Learned From the Training of GANs on Artificial Datasets,0.031230568885803223,#1f77b4
-8.922701,4.9431143,neighbor,1033682,Learning Generative Adversarial Networks from Multiple Data Sources,0.03138476610183716,#1f77b4
3.5493612,6.414668,neighbor,1033682,Quantitatively Evaluating GANs With Divergences Proposed for Training,0.03139150142669678,#1f77b4
-0.12145181,-3.0662882,neighbor,1033682,Decoupled Learning for Conditional Adversarial Networks,0.03154188394546509,#1f77b4
-11.057454,-2.7508633,neighbor,1033682,Energy-based Generative Adversarial Network,0.031561076641082764,#1f77b4
-5.4720826,4.8601313,neighbor,1033682,Tempered Adversarial Networks,0.03158837556838989,#1f77b4
-0.89889365,-5.1462603,neighbor,1033682,"Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs",0.031607747077941895,#1f77b4
-4.2242913,7.279018,neighbor,1033682,Generative Adversarial Networks Based on Denoising and Reconstruction Regularization,0.03164905309677124,#1f77b4
-6.851323,-5.125364,neighbor,1033682,Generative Latent Flow: A Framework for Non-adversarial Image Generation,0.03168827295303345,#1f77b4
-0.10861793,3.061206,neighbor,1033682,Tensorizing Generative Adversarial Nets,0.031815409660339355,#1f77b4
-2.112557,-5.0166106,neighbor,1033682,Large Scale Adversarial Representation Learning,0.031877100467681885,#1f77b4
-2.0230176,-2.4138956,neighbor,1033682,GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks,0.0319286584854126,#1f77b4
1.8982271,4.4987173,neighbor,1033682,Improved Training of Generative Adversarial Networks Using Decision Forests,0.031941771507263184,#1f77b4
-0.76218,8.062432,neighbor,1033682,JR-GAN: Jacobian Regularization for Generative Adversarial Networks,0.03195369243621826,#1f77b4
1.250622,-1.5319469,neighbor,1033682,The Six Fronts of the Generative Adversarial Networks,0.031962811946868896,#1f77b4
4.119972,-7.648494,neighbor,1033682,Adversarial Transfer Learning,0.03203243017196655,#1f77b4
8.988407,1.300175,query,1055111,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",0.0,#1f77b4
8.578597,2.2425864,neighbor,1055111,Show and tell: A neural image caption generator,0.03510737419128418,#1f77b4
5.539554,2.815337,neighbor,1055111,From captions to visual concepts and back,0.047330498695373535,#1f77b4
7.2333636,0.49775216,neighbor,1055111,Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN),0.04871714115142822,#1f77b4
6.8675275,1.8891407,neighbor,1055111,Learning a Recurrent Visual Representation for Image Caption Generation,0.05823981761932373,#1f77b4
3.7515502,1.3622043,neighbor,1055111,Nonparametric Method for Data-driven Image Captioning,0.0607951283454895,#1f77b4
7.632767,3.3026812,neighbor,1055111,Translating Videos to Natural Language Using Deep Recurrent Neural Networks,0.06205374002456665,#1f77b4
7.482024,-0.13426058,neighbor,1055111,Explain Images with Multimodal Recurrent Neural Networks,0.06332075595855713,#1f77b4
5.788342,4.181628,neighbor,1055111,Deep visual-semantic alignments for generating image descriptions,0.06708937883377075,#1f77b4
9.273302,4.1351085,neighbor,1055111,Simple Image Description Generator via a Linear Phrase-Based Approach,0.06881147623062134,#1f77b4
-6.9742227,-7.7069826,neighbor,1055111,Multiple Object Recognition with Visual Attention,0.06894391775131226,#1f77b4
-6.464865,-8.192576,neighbor,1055111,Recurrent Models of Visual Attention,0.0712968111038208,#1f77b4
12.264965,-0.8016167,neighbor,1055111,Reading Text in the Wild with Convolutional Neural Networks,0.0724901556968689,#1f77b4
8.789037,7.8736367,neighbor,1055111,"See No Evil, Say No Evil: Description Generation from Densely Labeled Images",0.07301056385040283,#1f77b4
4.432088,2.557124,neighbor,1055111,Domain-Specific Image Captioning,0.07367897033691406,#1f77b4
9.082679,13.908198,neighbor,1055111,Towards a Visual Turing Challenge,0.07455891370773315,#1f77b4
2.356333,2.0289972,neighbor,1055111,Data-driven image captioning with meta-class based retrieval,0.07657754421234131,#1f77b4
-12.120602,-3.7079732,neighbor,1055111,Visual-Semantic Scene Understanding by Sharing Labels in a Context Network,0.078482985496521,#1f77b4
8.822865,5.529288,neighbor,1055111,TreeTalk: Composition and Compression of Trees for Image Descriptions,0.0791063904762268,#1f77b4
10.050475,5.9925985,neighbor,1055111,Choosing Linguistics over Vision to Describe Images,0.07982289791107178,#1f77b4
-1.848595,-7.9750066,neighbor,1055111,Video (language) modeling: a baseline for generative models of natural videos,0.08073467016220093,#1f77b4
12.882498,-1.9323779,neighbor,1055111,Deep Structured Output Learning for Unconstrained Text Recognition,0.08257049322128296,#1f77b4
12.518409,9.369761,neighbor,1055111,Learning Distributions over Logical Forms for Referring Expression Generation,0.08313554525375366,#1f77b4
4.122377,5.2362633,neighbor,1055111,Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework,0.0835692286491394,#1f77b4
6.1809273,-1.4411104,neighbor,1055111,Long-term recurrent convolutional networks for visual recognition and description,0.08382529020309448,#1f77b4
9.340704,13.320811,neighbor,1055111,Hard to Cheat: A Turing Test based on Answering Questions about Images,0.08541536331176758,#1f77b4
-2.280879,-3.6912825,neighbor,1055111,Towards semantic embedding in visual vocabulary,0.08557158708572388,#1f77b4
7.531451,7.889286,neighbor,1055111,DISCO: Describing Images Using Scene Contexts and Objects,0.08573246002197266,#1f77b4
4.9417224,-1.5693487,neighbor,1055111,Co-regularized deep representations for video summarization,0.08672749996185303,#1f77b4
13.169366,-1.3841617,neighbor,1055111,Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition,0.0877007246017456,#1f77b4
7.2609677,9.89361,neighbor,1055111,ImageSpirit,0.0877528190612793,#1f77b4
-11.319595,-4.334958,neighbor,1055111,Associative Embeddings for Large-Scale Knowledge Transfer with Self-Assessment,0.08839738368988037,#1f77b4
-11.760225,-10.992373,neighbor,1055111,Exploring Invariances in Deep Convolutional Neural Networks Using Synthetic Images,0.08890080451965332,#1f77b4
7.250857,6.2418814,neighbor,1055111,"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics",0.08913451433181763,#1f77b4
10.432735,-5.1817837,neighbor,1055111,Recurrent Neural Network Regularization,0.0904431939125061,#1f77b4
-5.808576,-11.397237,neighbor,1055111,Zero-Shot Learning Through Cross-Modal Transfer,0.09098601341247559,#1f77b4
-3.0398948,3.7378511,neighbor,1055111,Efficient Media Retrieval from Non-Cooperative Queries,0.09109258651733398,#1f77b4
-1.9292089,0.92193365,neighbor,1055111,"Start from Scratch: Towards Automatically Identifying, Modeling, and Naming Visual Attributes",0.09128975868225098,#1f77b4
-7.320401,-10.640702,neighbor,1055111,Visual Transfer Learning: Informal Introduction and Literature Overview,0.09138375520706177,#1f77b4
2.9281647,6.803131,neighbor,1055111,"Discriminative Training: Learning to Describe Video with Sentences, from Video Described with Sentences",0.09193408489227295,#1f77b4
1.5174514,1.0335683,neighbor,1055111,Weakly supervised construction of a repository of iconic images,0.09212899208068848,#1f77b4
1.1392217,6.4743323,neighbor,1055111,VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events,0.09213852882385254,#1f77b4
-12.108102,-12.298261,neighbor,1055111,Learning to generate chairs with convolutional neural networks,0.0925702452659607,#1f77b4
8.709845,-1.4161026,neighbor,1055111,"Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network",0.09283250570297241,#1f77b4
5.4703684,5.1322575,neighbor,1055111,Deep Fragment Embeddings for Bidirectional Image Sentence Mapping,0.09298563003540039,#1f77b4
-5.102452,0.14836933,neighbor,1055111,Recognizing Image Style,0.09317123889923096,#1f77b4
11.903263,8.511863,neighbor,1055111,Information Structure Prediction for Visual-world Referring Expressions,0.09322690963745117,#1f77b4
-5.8753467,3.6423135,neighbor,1055111,Image search by graph-based label propagation with image representation from DNN,0.09355264902114868,#1f77b4
-7.390714,0.32446122,neighbor,1055111,TagProp: Discriminative metric learning in nearest neighbor models for image auto-annotation,0.09368038177490234,#1f77b4
-5.133134,-0.8796173,neighbor,1055111,The Rijksmuseum Challenge: Museum-Centered Visual Recognition,0.09470540285110474,#1f77b4
12.2122,3.8040924,neighbor,1055111,What makes an Image Iconic? A Fine-Grained Case Study,0.09528428316116333,#1f77b4
3.766863,7.367922,neighbor,1055111,A Faster Method for Tracking and Scoring Videos Corresponding to Sentences,0.09546393156051636,#1f77b4
10.630715,7.2912555,neighbor,1055111,Image Description using Visual Dependency Representations,0.09558349847793579,#1f77b4
-3.2496452,-2.905543,neighbor,1055111,Visual Word Selection without Re-Coding and Re-Pooling,0.09567582607269287,#1f77b4
-11.096077,-7.392196,neighbor,1055111,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.09608280658721924,#1f77b4
-12.073236,-10.434135,neighbor,1055111,Learning Deep Object Detectors from 3D Models,0.0961257815361023,#1f77b4
-10.031689,-14.122117,neighbor,1055111,Learning Generative Models with Visual Attention,0.09667080640792847,#1f77b4
11.290259,-4.8122416,neighbor,1055111,Recurrent Continuous Translation Models,0.09667569398880005,#1f77b4
13.067684,10.340622,neighbor,1055111,ReferItGame: Referring to Objects in Photographs of Natural Scenes,0.09692364931106567,#1f77b4
14.816689,5.2064886,neighbor,1055111,Exploiting Language Models for Visual Recognition,0.09821867942810059,#1f77b4
-3.6262753,3.807851,neighbor,1055111,Multi-platform image search using tag enrichment,0.09823977947235107,#1f77b4
0.96762055,-5.8655877,neighbor,1055111,A revisit of Generative Model for Automatic Image Annotation using Markov Random Fields,0.09833216667175293,#1f77b4
-11.328338,-1.0049233,neighbor,1055111,Fusing object detection and region appearance for image-text alignment,0.09836483001708984,#1f77b4
-4.0901184,-3.0007792,neighbor,1055111,Fisher and VLAD with FLAIR,0.09886318445205688,#1f77b4
14.212736,5.6076126,neighbor,1055111,Improving Multi-Modal Representations Using Image Dispersion: Why Less is Sometimes More,0.09905737638473511,#1f77b4
5.643563,6.4002643,neighbor,1055111,Grounded Compositional Semantics for Finding and Describing Images with Sentences,0.09909075498580933,#1f77b4
-8.020654,-12.030486,neighbor,1055111,Learning invariance through imitation,0.09913307428359985,#1f77b4
-2.427177,-9.986364,neighbor,1055111,The Patch Transform,0.09932231903076172,#1f77b4
-12.267493,-8.835978,neighbor,1055111,Deformable part models are convolutional neural networks,0.09938007593154907,#1f77b4
3.5780685,10.883416,neighbor,1055111,Generating Image Captions using Topic Focused Multi-document Summarization,0.0995568037033081,#1f77b4
-9.043443,-7.3845277,neighbor,1055111,Attention for Fine-Grained Categorization,0.09960377216339111,#1f77b4
0.07804941,6.877628,neighbor,1055111,Zero-Example Event Search using MultiModal Pseudo Relevance Feedback,0.09979861974716187,#1f77b4
-1.8158966,0.08785089,neighbor,1055111,Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice,0.09984761476516724,#1f77b4
-10.125155,-6.611849,neighbor,1055111,ImageNet Large Scale Visual Recognition Challenge,0.09990823268890381,#1f77b4
-3.0691717,-7.3723865,neighbor,1055111,The Shape-Time Random Field for Semantic Video Labeling,0.09990829229354858,#1f77b4
10.232863,13.513474,neighbor,1055111,A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input,0.09997701644897461,#1f77b4
3.830803,-5.7527065,neighbor,1055111,Visual Contextual Advertising: Bringing Textual Advertisements to Images,0.10005778074264526,#1f77b4
-10.414504,-13.186551,neighbor,1055111,Deep Lambertian Networks,0.10011214017868042,#1f77b4
3.4278955,-0.18415116,neighbor,1055111,A Classification Based Framework for Concept Summarization,0.10021120309829712,#1f77b4
-7.369786,3.3001637,neighbor,1055111,Efficient On-the-fly Category Retrieval Using ConvNets and GPUs,0.10021799802780151,#1f77b4
8.121765,15.067522,neighbor,1055111,Learning to see like children: proof of concept,0.10029292106628418,#1f77b4
12.141554,6.490965,neighbor,1055111,From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,0.10047048330307007,#1f77b4
-0.7313429,-4.616532,neighbor,1055111,Learning Deep Structured Models,0.10063278675079346,#1f77b4
2.3615768,-4.418473,neighbor,1055111,Mining Associated Text and Images with Dual-Wing Harmoniums,0.10073840618133545,#1f77b4
-0.46920094,3.282423,neighbor,1055111,T-IRS: textual query based image retrieval system for consumer photos,0.10078507661819458,#1f77b4
-7.979281,3.4894574,neighbor,1055111,Neural Codes for Image Retrieval,0.10091888904571533,#1f77b4
-7.3313227,-5.4691944,neighbor,1055111,Salient Object Detection: A Benchmark,0.10099542140960693,#1f77b4
-10.139186,-4.8111534,neighbor,1055111,Self-taught object localization with deep networks,0.10100829601287842,#1f77b4
-9.208357,-3.3978975,neighbor,1055111,Looking out of the window: object localization by joint analysis of all windows in the image,0.10101354122161865,#1f77b4
2.4781725,8.117325,neighbor,1055111,Grounding Action Descriptions in Videos,0.10124015808105469,#1f77b4
9.120198,9.202457,neighbor,1055111,A system that learns to describe objects in visual scenes,0.10125744342803955,#1f77b4
-8.800261,-6.001385,neighbor,1055111,Scalable Object Detection Using Deep Neural Networks,0.1013498306274414,#1f77b4
-8.9810505,-4.40287,neighbor,1055111,Visual chunking: A list prediction framework for region-based object detection,0.10138380527496338,#1f77b4
11.767617,-5.320442,neighbor,1055111,Neural Machine Translation by Jointly Learning to Align and Translate,0.10157120227813721,#1f77b4
-13.59511,-6.237731,neighbor,1055111,Compact Part-Based Image Representations: Extremal Competition and Overgeneralization,0.10171985626220703,#1f77b4
2.0621517,-2.2630527,neighbor,1055111,Jointly Aligning and Segmenting Multiple Web Photo Streams for the Inference of Collective Photo Storylines,0.10173988342285156,#1f77b4
-12.197009,-5.518505,neighbor,1055111,Self-informed neural network structure learning,0.10178351402282715,#1f77b4
-3.1306283,5.3554583,neighbor,1055111,Towards the next plateau: innovative multimedia research beyond trecvid,0.10189574956893921,#1f77b4
-11.28804,-1.7185162,neighbor,1055111,Object Recognition by Scene Alignment,0.10191577672958374,#1f77b4
3.2817898,-4.798903,neighbor,1055111,VELDA: Relating an Image Tweet's Text and Images,0.10208743810653687,#1f77b4
-6.4819727,-5.3592353,neighbor,1055111,Predicting human gaze beyond pixels.,0.10222893953323364,#1f77b4
7.9341354,-0.16713548,query,13029170,“Why Should I Trust You?”: Explaining the Predictions of Any Classifier,0.0,#aec7e8
7.463008,-0.24414814,neighbor,13029170,How to Explain Individual Classification Decisions,0.053439319133758545,#aec7e8
9.435987,0.3954149,neighbor,13029170,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,0.07281279563903809,#aec7e8
-4.234075,-0.8484223,neighbor,13029170,Predictions as Statements and Decisions,0.0751543641090393,#aec7e8
11.355357,-3.5595212,neighbor,13029170,100% Classification Accuracy Considered Harmful: The Normalized Information Transfer Factor Explains the Accuracy Paradox,0.07707148790359497,#aec7e8
-6.4739323,-0.26894212,neighbor,13029170,Accurate and interpretable regression trees using oracle coaching,0.077095627784729,#aec7e8
5.5778966,1.9441202,neighbor,13029170,On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation,0.07773202657699585,#aec7e8
-1.9541333,0.13778776,neighbor,13029170,"Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation",0.07865840196609497,#aec7e8
5.4571114,-11.167696,neighbor,13029170,Evaluating computational models of explanation using human judgments,0.08025097846984863,#aec7e8
-3.1112468,-2.468733,neighbor,13029170,Large-scale probabilistic predictors with and without guarantees of validity,0.08110523223876953,#aec7e8
9.778153,-9.976101,neighbor,13029170,Learning to Learn: Algorithmic Inspirations from Human Problem Solving,0.08164942264556885,#aec7e8
2.0637012,2.4889991,neighbor,13029170,Introduction to Machine Learning,0.08402895927429199,#aec7e8
-3.1114671,1.0199007,neighbor,13029170,Predicting accurate probabilities with a ranking loss,0.084084153175354,#aec7e8
-3.230012,-3.3589957,neighbor,13029170,Model-based machine learning,0.08433830738067627,#aec7e8
6.5951624,-7.15145,neighbor,13029170,Correction: Computational Fact Checking from Knowledge Networks,0.08445650339126587,#aec7e8
0.7529756,8.786436,neighbor,13029170,Towards Shockingly Easy Structured Classification: A Search-based Probabilistic Online Learning Framework,0.08456182479858398,#aec7e8
0.55519557,-7.6749544,neighbor,13029170,"Algorithmic statistics, prediction and machine learning",0.08523154258728027,#aec7e8
-5.5996084,-3.8057842,neighbor,13029170,Methods and Models for Interpretable Linear Classification,0.08566731214523315,#aec7e8
-2.7146535,2.2954447,neighbor,13029170,Ranking and combining multiple predictors without labeled data,0.08600938320159912,#aec7e8
6.928073,2.3522122,neighbor,13029170,Efficiently explaining the predictions of a probabilistic radial basis function classification network,0.08703690767288208,#aec7e8
-3.8452635,5.2372427,neighbor,13029170,Reinforced Decision Trees,0.08712071180343628,#aec7e8
1.7836707,-0.86328286,neighbor,13029170,Value of Information Based on Decision Robustness,0.08735167980194092,#aec7e8
0.2674771,-2.305639,neighbor,13029170,Parsimonious Naive Bayes,0.08748209476470947,#aec7e8
7.129438,8.282436,neighbor,13029170,Large-Margin Multi-Label Causal Feature Learning,0.08779418468475342,#aec7e8
-8.887658,1.1322498,neighbor,13029170,Confidence in predictions from random tree ensembles,0.08832305669784546,#aec7e8
-2.0577695,6.1765914,neighbor,13029170,The offset tree for learning with partial labels,0.08839946985244751,#aec7e8
5.837157,-5.744991,neighbor,13029170,From machine learning to machine reasoning,0.08858978748321533,#aec7e8
7.654431,-3.3968165,neighbor,13029170,Reading to Learn: Constructing Features from Semantic Abstracts,0.08868938684463501,#aec7e8
13.292949,2.8813078,neighbor,13029170,Toward harnessing user feedback for machine learning,0.08933019638061523,#aec7e8
11.017492,-2.190279,neighbor,13029170,Machine Learning that Matters,0.08975827693939209,#aec7e8
-6.226358,3.5559182,neighbor,13029170,Feedback Detection for Live Statistical Predictors,0.08984094858169556,#aec7e8
1.7392077,-5.41167,neighbor,13029170,The mysterious optimality of Naive Bayes: Estimation of the probability in the system of “classifiers”,0.08994448184967041,#aec7e8
-2.6570082,-8.229196,neighbor,13029170,"Statistical Model Building, Machine Learning, and the Ah-Ha Moment",0.09013044834136963,#aec7e8
-4.543452,1.250568,neighbor,13029170,The Feature Importance Ranking Measure,0.0902019739151001,#aec7e8
-9.50383,-1.6045376,neighbor,13029170,A Very Simple Safe-Bayesian Random Forest,0.09023046493530273,#aec7e8
-10.75547,-1.9080983,neighbor,13029170,Narrowing the Gap: Random Forests In Theory and In Practice,0.09030085802078247,#aec7e8
12.457439,-2.532031,neighbor,13029170,Why Nitpicking Works: Evidence for Occam’s Razor in Error Correctors,0.09031343460083008,#aec7e8
-7.9499855,-3.6989915,neighbor,13029170,Interpretable Selection and Visualization of Features and Interactions Using Bayesian Forests,0.09094494581222534,#aec7e8
-1.938361,7.569796,neighbor,13029170,Quantity Makes Quality: Learning with Partial Views,0.09104776382446289,#aec7e8
-0.6154579,0.8005196,neighbor,13029170,An empirical comparison of supervised learning algorithms,0.09110087156295776,#aec7e8
0.33920357,6.632033,neighbor,13029170,Patterns for Learning with Side Information,0.09116005897521973,#aec7e8
7.3261,-4.9157715,neighbor,13029170,Learning Question Classifiers,0.09118813276290894,#aec7e8
-0.79453737,2.4276695,neighbor,13029170,Scalable Semi-Supervised Classifier Aggregation,0.09136652946472168,#aec7e8
-2.6708198,-9.77486,neighbor,13029170,Using New Models to Analyze Complex Regularities of the World,0.09152597188949585,#aec7e8
5.0844307,-1.8784806,neighbor,13029170,SpeedMachines: Anytime Structured Prediction,0.09161198139190674,#aec7e8
4.6615896,-14.446454,neighbor,13029170,A Generative Computational Model for Human Hide and Seek Behavior,0.09162068367004395,#aec7e8
-5.1811557,-6.770603,neighbor,13029170,Statistical mechanics of learning: a variational approach for real data.,0.09162139892578125,#aec7e8
8.191698,6.242026,neighbor,13029170,Taxonomy grounded aggregation of classifiers with different label sets,0.09191292524337769,#aec7e8
1.998066,8.517748,neighbor,13029170,Piecewise training for structured prediction,0.09192973375320435,#aec7e8
10.372988,-9.099563,neighbor,13029170,Teaching Classification Boundaries to Humans,0.09197258949279785,#aec7e8
8.720864,-10.122348,neighbor,13029170,Sampling Assumptions in Inductive Generalization,0.09197384119033813,#aec7e8
2.501238,5.1100636,neighbor,13029170,A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation,0.09201091527938843,#aec7e8
12.739487,-9.3373375,neighbor,13029170,Using graphical models to infer multiple visual classification features.,0.09204721450805664,#aec7e8
5.9968457,-4.9643636,neighbor,13029170,Hard to Cheat: A Turing Test based on Answering Questions about Images,0.09243988990783691,#aec7e8
0.50662684,-0.619394,neighbor,13029170,Locally Weighted Naive Bayes,0.09252357482910156,#aec7e8
4.6638627,3.1397831,neighbor,13029170,The Attentive Perceptron,0.09260743856430054,#aec7e8
10.796538,-6.0095115,neighbor,13029170,Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education,0.09267604351043701,#aec7e8
0.20797846,-1.2398627,neighbor,13029170,Bayesian Model Averaging Naive Bayes (BMA-NB): Averaging over an Exponential Number of Feature Models in Linear Time,0.09267628192901611,#aec7e8
10.356364,1.1570455,neighbor,13029170,The Rating Game: Sentiment Rating Reproducibility from Text,0.09274131059646606,#aec7e8
12.214589,4.06261,neighbor,13029170,End-User Feature Labeling via Locally Weighted Logistic Regression,0.0928720235824585,#aec7e8
5.424986,-11.9245205,neighbor,13029170,"Predictive mind, cognition, and chess",0.09307289123535156,#aec7e8
3.87913,10.18597,neighbor,13029170,DART: Dropouts meet Multiple Additive Regression Trees,0.09321993589401245,#aec7e8
-2.1756697,-1.4465714,neighbor,13029170,Obtaining Calibrated Probabilities from Boosting,0.09330308437347412,#aec7e8
0.03898542,-6.947322,neighbor,13029170,"Information, learning and falsification",0.09337592124938965,#aec7e8
1.6241393,7.1981225,neighbor,13029170,Structured Learning via Logistic Regression,0.09349709749221802,#aec7e8
-8.599241,-1.9838467,neighbor,13029170,Information Forests,0.0935293436050415,#aec7e8
-8.477278,1.7817143,neighbor,13029170,Cross-conformal predictors,0.09363728761672974,#aec7e8
3.093145,3.2548668,neighbor,13029170,The Margitron: A Generalised Perceptron with Margin,0.09366518259048462,#aec7e8
11.066322,-9.184317,neighbor,13029170,Becoming the expert - interactive multi-class machine teaching,0.09366852045059204,#aec7e8
12.2598295,3.9576366,neighbor,13029170,End-user feature labeling: a locally-weighted regression approach,0.09370213747024536,#aec7e8
5.3607354,6.193366,neighbor,13029170,On Dataless Hierarchical Text Classification,0.09381729364395142,#aec7e8
-0.6444717,-6.7224617,neighbor,13029170,Falsification and Future Performance,0.09389001131057739,#aec7e8
-11.116059,-2.5066137,neighbor,13029170,Analysis of a Random Forests Model,0.09393084049224854,#aec7e8
2.968874,8.791238,neighbor,13029170,Structure Regularization for Structured Prediction,0.09412282705307007,#aec7e8
-0.28573117,2.889836,neighbor,13029170,Semisupervised Classifier Evaluation and Recalibration,0.09412407875061035,#aec7e8
0.5247617,5.0220704,neighbor,13029170,On Information Regularization,0.09421461820602417,#aec7e8
-11.182467,-3.9612215,neighbor,13029170,Interpreting random forest classification models using a feature contribution method,0.09424155950546265,#aec7e8
-2.589589,7.3029695,neighbor,13029170,Efficient Learning with Partially Observed Attributes,0.09436780214309692,#aec7e8
7.3334885,-7.1096745,neighbor,13029170,Learning Plausible Inferences from Semantic Web Knowledge by Combining Analogical Generalization with Structured Logistic Regression,0.09443718194961548,#aec7e8
-5.079183,-8.697245,neighbor,13029170,Thurstonian Boltzmann Machines: Learning from Multiple Inequalities,0.09455394744873047,#aec7e8
-0.9025085,4.9689565,neighbor,13029170,No Free Lunch versus Occam's Razor in Supervised Learning,0.09458673000335693,#aec7e8
8.208814,7.1115947,neighbor,13029170,Learning structured prediction models for interactive image labeling,0.09459793567657471,#aec7e8
4.3421617,-0.862892,neighbor,13029170,Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of Pruning Classifiers,0.09467417001724243,#aec7e8
-6.7223673,-4.3884573,neighbor,13029170,Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model,0.09478849172592163,#aec7e8
5.4740906,-14.548599,neighbor,13029170,Computationally modeling interpersonal trust,0.0947999358177185,#aec7e8
2.8375556,7.899403,neighbor,13029170,Efficient Decomposed Learning for Structured Prediction,0.09481728076934814,#aec7e8
-4.933836,5.6905165,neighbor,13029170,Optimal Behavior is Easier to Learn than the Truth,0.09487468004226685,#aec7e8
-10.800682,1.4060746,neighbor,13029170,DECISION TREES DO NOT GENERALIZE TO NEW VARIATIONS,0.09496486186981201,#aec7e8
1.8363616,-11.676106,neighbor,13029170,Using cognitive models to combine probability estimates,0.09511107206344604,#aec7e8
-2.7070165,-6.809508,neighbor,13029170,Predictive Hypothesis Identification,0.0951126217842102,#aec7e8
-9.4725685,-0.5183738,neighbor,13029170,Random Composite Forests,0.0951656699180603,#aec7e8
-1.8710917,-4.9622927,neighbor,13029170,Robust Probabilistic Inference,0.09517687559127808,#aec7e8
6.1906986,-13.836465,neighbor,13029170,Binding lies,0.09526598453521729,#aec7e8
4.613871,7.099453,neighbor,13029170,Normalized Hierarchical SVM,0.09530895948410034,#aec7e8
1.054604,-3.1638224,neighbor,13029170,"Inducing Interpretable Voting Classifiers without Trading Accuracy for Simplicity: Theoretical Results, Approximation Algorithms, and Experiments",0.09544527530670166,#aec7e8
2.643322,-8.070824,neighbor,13029170,The SP theory of intelligence: benefits and applications,0.09573113918304443,#aec7e8
3.3697011,1.8479801,neighbor,13029170,How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation,0.09575903415679932,#aec7e8
-3.1703787,-5.58066,neighbor,13029170,Inferential Models for Linear Regression,0.09600645303726196,#aec7e8
-1.9376218,-8.559352,neighbor,13029170,Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author),0.09604573249816895,#aec7e8
-2.0806181,-11.247156,neighbor,13029170,Discovering Structure in High-Dimensional Data Through Correlation Explanation,0.09604930877685547,#aec7e8
2.4002926,-6.5623775,neighbor,13029170,Open Problems in Universal Induction & Intelligence,0.09605848789215088,#aec7e8
8.688523,1.3046632,query,13740328,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.0,#ff7f0e
7.276795,1.8158507,neighbor,13740328,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.05583512783050537,#ff7f0e
6.0915976,0.653049,neighbor,13740328,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.056415557861328125,#ff7f0e
-3.89488,-7.8440623,neighbor,13740328,Maxout Networks,0.06012582778930664,#ff7f0e
6.8652387,9.405513,neighbor,13740328,From generic to specific deep representations for visual recognition,0.06062114238739014,#ff7f0e
5.3342595,-1.5309614,neighbor,13740328,Striving for Simplicity: The All Convolutional Net,0.061045169830322266,#ff7f0e
3.9748454,-3.6538656,neighbor,13740328,Deep Epitomic Convolutional Neural Networks,0.061625123023986816,#ff7f0e
-1.3083361,-10.745033,neighbor,13740328,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.062217772006988525,#ff7f0e
2.8174684,-5.6899333,neighbor,13740328,SimNets: A Generalization of Convolutional Networks,0.06251394748687744,#ff7f0e
5.4092326,-0.49258,neighbor,13740328,Going deeper with convolutions,0.06441932916641235,#ff7f0e
-3.1638618,-8.676843,neighbor,13740328,Improving Deep Neural Networks with Probabilistic Maxout Units,0.06561189889907837,#ff7f0e
-2.387254,4.3158865,neighbor,13740328,Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification,0.06705290079116821,#ff7f0e
2.906288,4.4908767,neighbor,13740328,Learnable Pooling Regions for Image Classification,0.06734126806259155,#ff7f0e
-10.358093,-0.2745646,neighbor,13740328,Scheduled denoising autoencoders,0.06751108169555664,#ff7f0e
8.053251,13.823032,neighbor,13740328,Do Convnets Learn Correspondence?,0.06752556562423706,#ff7f0e
-6.8125134,6.4467354,neighbor,13740328,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.06770914793014526,#ff7f0e
4.526737,-5.868134,neighbor,13740328,MatConvNet: Convolutional Neural Networks for MATLAB,0.06796008348464966,#ff7f0e
-5.5233536,-4.0545197,neighbor,13740328,Deeply-Supervised Nets,0.068692147731781,#ff7f0e
-3.5446846,-1.6192367,neighbor,13740328,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.06911081075668335,#ff7f0e
5.2733555,-9.216152,neighbor,13740328,Deep Fried Convnets,0.07072997093200684,#ff7f0e
8.231471,8.930074,neighbor,13740328,How transferable are features in deep neural networks?,0.07082557678222656,#ff7f0e
2.6566107,-3.7150202,neighbor,13740328,Network In Network,0.07085412740707397,#ff7f0e
0.04273703,-12.024757,neighbor,13740328,On the Number of Linear Regions of Deep Neural Networks,0.0709807276725769,#ff7f0e
5.5833683,2.1316078,neighbor,13740328,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.07125282287597656,#ff7f0e
10.939189,6.3109245,neighbor,13740328,Deformation Models for Image Recognition,0.07136571407318115,#ff7f0e
8.503819,-1.9482682,neighbor,13740328,Visualizing and Understanding Convolutional Networks,0.0715518593788147,#ff7f0e
5.9535255,10.064925,neighbor,13740328,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.071635901927948,#ff7f0e
-3.5444014,-5.9451537,neighbor,13740328,Piecewise Linear Multilayer Perceptrons and Dropout,0.071766197681427,#ff7f0e
-10.651181,-2.3761005,neighbor,13740328,Switched linear encoding with rectified linear autoencoders,0.07200413942337036,#ff7f0e
9.531261,-8.662735,neighbor,13740328,Efficient and accurate approximations of nonlinear convolutional networks,0.07203269004821777,#ff7f0e
-0.2067668,4.811751,neighbor,13740328,Differentiable Pooling for Hierarchical Feature Learning,0.07213783264160156,#ff7f0e
-2.490598,8.497494,neighbor,13740328,Convolutional Kernel Networks,0.07241642475128174,#ff7f0e
-4.9639955,8.001781,neighbor,13740328,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.07296586036682129,#ff7f0e
-5.306663,-6.852187,neighbor,13740328,Learning Compact Convolutional Neural Networks with Nested Dropout,0.07330352067947388,#ff7f0e
6.7908697,-5.270398,neighbor,13740328,Caffe: Convolutional Architecture for Fast Feature Embedding,0.07387685775756836,#ff7f0e
-5.4644327,7.429941,neighbor,13740328,Unsupervised feature learning by augmenting single images,0.07396483421325684,#ff7f0e
-1.080049,-9.080066,neighbor,13740328,Fractional Max-Pooling,0.07499969005584717,#ff7f0e
3.6766977,3.4110065,neighbor,13740328,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.07543545961380005,#ff7f0e
14.483525,-5.301187,neighbor,13740328,Multi-column deep neural networks for image classification,0.07544726133346558,#ff7f0e
-1.0220884,-5.627587,neighbor,13740328,Quadratic Features and Deep Architectures for Chunking,0.07564067840576172,#ff7f0e
15.323963,-0.7347095,neighbor,13740328,Large-Margin kNN Classification Using a Deep Encoder Network,0.07585376501083374,#ff7f0e
-8.604193,-8.245165,neighbor,13740328,Improving neural networks by preventing co-adaptation of feature detectors,0.07611691951751709,#ff7f0e
-1.380403,-2.913337,neighbor,13740328,FitNets: Hints for Thin Deep Nets,0.07626986503601074,#ff7f0e
-1.8257332,-8.062212,neighbor,13740328,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.07646214962005615,#ff7f0e
-9.18067,-2.7731538,neighbor,13740328,Discriminative Recurrent Sparse Auto-Encoders,0.076687753200531,#ff7f0e
-4.9192324,-1.8489027,neighbor,13740328,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.07691025733947754,#ff7f0e
12.111669,2.617102,neighbor,13740328,Denoising autoencoder with modulated lateral connections learns invariant representations of natural images,0.07692742347717285,#ff7f0e
2.6839466,-11.926914,neighbor,13740328,Understanding Deep Architectures using a Recursive Convolutional Network,0.07716357707977295,#ff7f0e
-2.2359662,3.3271804,neighbor,13740328,Sparsity-Regularized HMAX for Visual Recognition,0.07733440399169922,#ff7f0e
-8.314736,0.30628222,neighbor,13740328,An introduction to deep learning,0.0778888463973999,#ff7f0e
-3.2626815,-3.8441117,neighbor,13740328,Do Deep Nets Really Need to be Deep?,0.07799804210662842,#ff7f0e
-8.002531,-5.163921,neighbor,13740328,Unsupervised Pretraining Encourages Moderate-Sparseness,0.07803928852081299,#ff7f0e
-2.679498,9.506028,neighbor,13740328,Learning Invariant Representations with Local Transformations,0.07812362909317017,#ff7f0e
-5.8028207,-1.7865515,neighbor,13740328,Deep learning,0.07842636108398438,#ff7f0e
12.258583,-4.9697256,neighbor,13740328,Committees of Deep Feedforward Networks Trained with Few Data,0.07885324954986572,#ff7f0e
9.121142,-1.504343,neighbor,13740328,Visualizing and Comparing Convolutional Neural Networks,0.07892334461212158,#ff7f0e
-1.2887056,2.155448,neighbor,13740328,Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition,0.07928675413131714,#ff7f0e
-11.258818,-4.0819616,neighbor,13740328,Saturating Auto-Encoders,0.07932150363922119,#ff7f0e
-5.9291034,-8.054987,neighbor,13740328,Dropout Rademacher complexity of deep neural networks,0.07961404323577881,#ff7f0e
2.1739862,11.2328415,neighbor,13740328,Fine-grained object recognition with Gnostic Fields,0.07969892024993896,#ff7f0e
1.2673098,5.4251695,neighbor,13740328,Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models,0.07995498180389404,#ff7f0e
-11.997517,-8.148423,neighbor,13740328,Learning from Noisy Labels with Deep Neural Networks,0.08049345016479492,#ff7f0e
6.477815,-7.675879,neighbor,13740328,Fast Training of Convolutional Networks through FFTs,0.08053809404373169,#ff7f0e
-9.535546,-0.21279763,neighbor,13740328,Gradual training of deep denoising auto encoders,0.08100181818008423,#ff7f0e
8.147239,-9.9827585,neighbor,13740328,Convolutional neural networks at constrained time cost,0.08168703317642212,#ff7f0e
4.1329784,-9.75042,neighbor,13740328,Low precision storage for deep learning,0.08176076412200928,#ff7f0e
-6.7826605,9.586781,neighbor,13740328,Deep Representation Learning with Target Coding,0.08201217651367188,#ff7f0e
-8.705223,-9.74916,neighbor,13740328,Recurrent Neural Network Regularization,0.08228331804275513,#ff7f0e
-16.108402,-4.507139,neighbor,13740328,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,0.08260053396224976,#ff7f0e
4.061029,11.262233,neighbor,13740328,ImageNet Large Scale Visual Recognition Challenge,0.08291280269622803,#ff7f0e
15.126104,-3.8323116,neighbor,13740328,"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",0.08305084705352783,#ff7f0e
-10.805857,-9.780237,neighbor,13740328,Dropout Training as Adaptive Regularization,0.08308053016662598,#ff7f0e
8.90663,5.472618,neighbor,13740328,Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection,0.0835079550743103,#ff7f0e
6.4502535,13.567654,neighbor,13740328,Associative Embeddings for Large-Scale Knowledge Transfer with Self-Assessment,0.08364403247833252,#ff7f0e
15.994259,-3.4701483,neighbor,13740328,"Fast, simple and accurate handwritten digit classification using extreme learning machines with shaped input-weights",0.08384555578231812,#ff7f0e
4.944731,5.8654785,neighbor,13740328,The treasure beneath convolutional layers: Cross-convolutional-layer pooling for image classification,0.0841560959815979,#ff7f0e
11.459342,7.62353,neighbor,13740328,Domain Adaptive Neural Networks for Object Recognition,0.08446955680847168,#ff7f0e
-5.497374,-10.530926,neighbor,13740328,Big Neural Networks Waste Capacity,0.08447957038879395,#ff7f0e
7.083767,6.0830755,neighbor,13740328,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.08458459377288818,#ff7f0e
4.8470907,8.059348,neighbor,13740328,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.08470219373703003,#ff7f0e
-15.321192,-3.6107569,neighbor,13740328,Intriguing properties of neural networks,0.08489906787872314,#ff7f0e
-7.193158,-2.39323,neighbor,13740328,Feature Weight Tuning for Recursive Neural Networks,0.08491379022598267,#ff7f0e
-5.1860332,3.2532167,neighbor,13740328,Generative Deep Deconvolutional Learning,0.08504056930541992,#ff7f0e
-6.525898,0.60715467,neighbor,13740328,Joint Training of Deep Boltzmann Machines,0.08535867929458618,#ff7f0e
2.8261397,-9.000265,neighbor,13740328,Memory Bounded Deep Convolutional Networks,0.08538168668746948,#ff7f0e
-15.011926,-2.5458715,neighbor,13740328,Towards Deep Neural Network Architectures Robust to Adversarial Examples,0.08552831411361694,#ff7f0e
3.9086099,0.5254459,neighbor,13740328,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.08607691526412964,#ff7f0e
-8.377221,2.126027,neighbor,13740328,How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation,0.08634454011917114,#ff7f0e
4.951782,13.817398,neighbor,13740328,Self-informed neural network structure learning,0.08648747205734253,#ff7f0e
-2.4111338,-12.603971,neighbor,13740328,Signal recovery from Pooling Representations,0.08650362491607666,#ff7f0e
5.6552978,4.587699,neighbor,13740328,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.0865364670753479,#ff7f0e
8.641749,-7.8365808,neighbor,13740328,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.08655089139938354,#ff7f0e
-11.187917,-10.554111,neighbor,13740328,Dropout Training for Support Vector Machines,0.08658605813980103,#ff7f0e
7.957388,-4.842888,neighbor,13740328,Theano-based Large-Scale Visual Recognition with Multiple GPUs,0.08662784099578857,#ff7f0e
2.6668873,2.5744495,neighbor,13740328,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.08666324615478516,#ff7f0e
3.570527,13.519392,neighbor,13740328,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.08684414625167847,#ff7f0e
5.675188,-11.802291,neighbor,13740328,Scalable stacking and learning for building deep architectures,0.08691442012786865,#ff7f0e
3.3065581,10.44238,neighbor,13740328,Deep learning for class-generic object detection,0.08712387084960938,#ff7f0e
-11.543737,2.0995708,neighbor,13740328,Two SVDs produce more focal deep learning representations,0.08728355169296265,#ff7f0e
13.779671,-4.2033944,neighbor,13740328,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.08740788698196411,#ff7f0e
8.442273,11.258324,neighbor,13740328,Visual Transfer Learning: Informal Introduction and Literature Overview,0.08744913339614868,#ff7f0e
1.441592,4.18541,query,14124313,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.0,#ff7f0e
0.3917605,3.882502,neighbor,14124313,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.03635364770889282,#ff7f0e
1.7665012,3.0440583,neighbor,14124313,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.0446736216545105,#ff7f0e
-1.2489667,2.503317,neighbor,14124313,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.04624485969543457,#ff7f0e
-6.8506737,-2.7676373,neighbor,14124313,From generic to specific deep representations for visual recognition,0.05031907558441162,#ff7f0e
1.3253881,6.316832,neighbor,14124313,Caffe: Convolutional Architecture for Fast Feature Embedding,0.052002906799316406,#ff7f0e
3.8871245,5.107193,neighbor,14124313,Deep Epitomic Convolutional Neural Networks,0.05342686176300049,#ff7f0e
-15.070766,-2.96331,neighbor,14124313,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.054712116718292236,#ff7f0e
3.495514,8.035855,neighbor,14124313,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.056486308574676514,#ff7f0e
5.515289,5.054078,neighbor,14124313,Network In Network,0.058759450912475586,#ff7f0e
-11.321447,-4.4754767,neighbor,14124313,ImageNet Large Scale Visual Recognition Challenge,0.059088706970214844,#ff7f0e
11.840085,7.976797,neighbor,14124313,Multi-column deep neural networks for image classification,0.05913490056991577,#ff7f0e
-3.3564987,1.9599545,neighbor,14124313,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.060332417488098145,#ff7f0e
12.815224,-1.1692787,neighbor,14124313,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.06050652265548706,#ff7f0e
-8.1734705,-0.8425442,neighbor,14124313,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.06082838773727417,#ff7f0e
2.3489802,8.313434,neighbor,14124313,Fast Training of Convolutional Networks through FFTs,0.061078548431396484,#ff7f0e
-1.2273922,1.1598897,neighbor,14124313,Learnable Pooling Regions for Image Classification,0.061304330825805664,#ff7f0e
11.796782,-2.1281185,neighbor,14124313,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.061512768268585205,#ff7f0e
6.505496,3.163978,neighbor,14124313,Understanding Deep Architectures using a Recursive Convolutional Network,0.06198543310165405,#ff7f0e
-3.256676,4.611823,neighbor,14124313,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.06237626075744629,#ff7f0e
4.4775405,3.0334208,neighbor,14124313,Visualizing and Understanding Convolutional Networks,0.06268870830535889,#ff7f0e
-14.348738,-3.9715812,neighbor,14124313,Scalable Object Detection Using Deep Neural Networks,0.06283682584762573,#ff7f0e
12.288377,-2.955897,neighbor,14124313,Deep learning,0.0629686713218689,#ff7f0e
-12.529678,-3.467742,neighbor,14124313,Deep learning for class-generic object detection,0.06437534093856812,#ff7f0e
-3.6768157,-6.327413,neighbor,14124313,Building high-level features using large scale unsupervised learning,0.0659056305885315,#ff7f0e
-2.8512814,-1.62359,neighbor,14124313,Convolutional Kernel Networks,0.06680119037628174,#ff7f0e
5.7093596,-0.16086473,neighbor,14124313,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.06847387552261353,#ff7f0e
-7.6526566,-3.4700449,neighbor,14124313,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.06862848997116089,#ff7f0e
10.430608,8.357336,neighbor,14124313,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.06879609823226929,#ff7f0e
11.387828,8.943522,neighbor,14124313,"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",0.06976306438446045,#ff7f0e
-11.436586,-6.489568,neighbor,14124313,Fine-grained object recognition with Gnostic Fields,0.06997203826904297,#ff7f0e
11.323864,0.06501092,neighbor,14124313,Do Deep Nets Really Need to be Deep?,0.07101893424987793,#ff7f0e
3.6602218,-8.034574,neighbor,14124313,Recurrent Models of Visual Attention,0.07143700122833252,#ff7f0e
1.4508289,-2.114264,neighbor,14124313,Differentiable Pooling for Hierarchical Feature Learning,0.07162469625473022,#ff7f0e
6.0734487,-2.2009869,neighbor,14124313,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.07213681936264038,#ff7f0e
10.201286,6.347336,neighbor,14124313,Committees of Deep Feedforward Networks Trained with Few Data,0.07248353958129883,#ff7f0e
-7.275427,-7.566092,neighbor,14124313,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.07267254590988159,#ff7f0e
0.3137824,-5.3254743,neighbor,14124313,Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification,0.07290011644363403,#ff7f0e
8.49894,-4.245642,neighbor,14124313,Scheduled denoising autoencoders,0.07397383451461792,#ff7f0e
-15.043667,-4.608734,neighbor,14124313,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.07415682077407837,#ff7f0e
-2.9959216,-3.3998046,neighbor,14124313,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.07480806112289429,#ff7f0e
-1.4643623,-6.5582075,neighbor,14124313,Unsupervised Feature Learning by Deep Sparse Coding,0.07503217458724976,#ff7f0e
-8.838816,5.151081,neighbor,14124313,Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification,0.07605218887329102,#ff7f0e
-10.089946,3.9805415,neighbor,14124313,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.07610934972763062,#ff7f0e
0.6209048,-0.35330307,neighbor,14124313,Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models,0.07612484693527222,#ff7f0e
-3.2115488,7.2230153,neighbor,14124313,Fast image scanning with deep max-pooling convolutional neural networks,0.07633143663406372,#ff7f0e
8.173592,-5.819785,neighbor,14124313,Discriminative Recurrent Sparse Auto-Encoders,0.07635176181793213,#ff7f0e
9.669801,-0.6354299,neighbor,14124313,Piecewise Linear Multilayer Perceptrons and Dropout,0.0767827033996582,#ff7f0e
-0.11516772,-10.48444,neighbor,14124313,Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition,0.0768967866897583,#ff7f0e
7.992215,-1.2232763,neighbor,14124313,Maxout Networks,0.0774385929107666,#ff7f0e
9.977095,-3.4541032,neighbor,14124313,An introduction to deep learning,0.07750052213668823,#ff7f0e
-10.651606,0.18715967,neighbor,14124313,Efficient On-the-fly Category Retrieval Using ConvNets and GPUs,0.07761245965957642,#ff7f0e
7.2653623,-1.6456999,neighbor,14124313,Improving Deep Neural Networks with Probabilistic Maxout Units,0.0784752368927002,#ff7f0e
8.870455,0.48976383,neighbor,14124313,Dropout Rademacher complexity of deep neural networks,0.07919275760650635,#ff7f0e
-14.44819,-6.5598197,neighbor,14124313,LSDA: Large Scale Detection through Adaptation,0.0793944001197815,#ff7f0e
-2.153736,-12.202643,neighbor,14124313,Complexity of Representation and Inference in Compositional Models with Part Sharing,0.08030027151107788,#ff7f0e
7.01299,9.19818,neighbor,14124313,Scalable stacking and learning for building deep architectures,0.0805349349975586,#ff7f0e
1.4225942,-4.330537,neighbor,14124313,Deconvolutional networks,0.08081561326980591,#ff7f0e
6.666675,1.0656174,neighbor,14124313,On the Number of Linear Regions of Deep Neural Networks,0.081432044506073,#ff7f0e
0.00961238,-7.406405,neighbor,14124313,Sparsity-Regularized HMAX for Visual Recognition,0.08147013187408447,#ff7f0e
-17.331717,-5.5838223,neighbor,14124313,Recurrent Convolutional Neural Networks for Scene Parsing,0.0815773606300354,#ff7f0e
-8.054697,3.2170599,neighbor,14124313,Local Naive Bayes Nearest Neighbor for image classification,0.08163195848464966,#ff7f0e
7.2977357,5.156129,neighbor,14124313,An Analysis of the Connections Between Layers of Deep Neural Networks,0.08176535367965698,#ff7f0e
15.743884,5.952557,neighbor,14124313,Large-Margin kNN Classification Using a Deep Encoder Network,0.08186757564544678,#ff7f0e
5.628119,9.984276,neighbor,14124313,Multi-GPU Training of ConvNets,0.08221220970153809,#ff7f0e
-12.745469,-5.5239415,neighbor,14124313,Part-Based R-CNNs for Fine-Grained Category Detection,0.08242970705032349,#ff7f0e
13.173962,2.2252164,neighbor,14124313,Learning from Noisy Labels with Deep Neural Networks,0.08247381448745728,#ff7f0e
0.5849494,12.785149,neighbor,14124313,Two SVDs produce more focal deep learning representations,0.08277928829193115,#ff7f0e
-9.330153,0.00023656967,neighbor,14124313,Neural Codes for Image Retrieval,0.0829935073852539,#ff7f0e
-10.88917,4.533023,neighbor,14124313,Deep Attribute Networks,0.08392727375030518,#ff7f0e
9.46513,2.1690257,neighbor,14124313,Big Neural Networks Waste Capacity,0.08429825305938721,#ff7f0e
4.750465,9.457943,neighbor,14124313,One weird trick for parallelizing convolutional neural networks,0.08474433422088623,#ff7f0e
11.69524,3.0718062,neighbor,14124313,Improving neural networks by preventing co-adaptation of feature detectors,0.08480685949325562,#ff7f0e
9.36183,-9.075053,neighbor,14124313,Feature Graph Architectures,0.085502028465271,#ff7f0e
13.160684,-3.5670054,neighbor,14124313,Deep belief networks,0.08574742078781128,#ff7f0e
-3.8869317,-2.9822013,neighbor,14124313,Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks,0.08600080013275146,#ff7f0e
-4.015768,-8.838604,neighbor,14124313,Sparse Output Coding for Large-Scale Visual Recognition,0.08611822128295898,#ff7f0e
-3.458845,5.706003,neighbor,14124313,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.08657747507095337,#ff7f0e
12.465264,-5.5494576,neighbor,14124313,Deep Learning of Representations: Looking Forward,0.08664286136627197,#ff7f0e
8.408526,3.7473855,neighbor,14124313,Predicting Parameters in Deep Learning,0.08686763048171997,#ff7f0e
-2.6253295,0.8269621,neighbor,14124313,Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences,0.08694726228713989,#ff7f0e
-10.102725,-3.2204854,neighbor,14124313,Zero-shot Learning with Deep Neural Networks for Object Recognition,0.08695286512374878,#ff7f0e
-5.452396,2.0908146,neighbor,14124313,Visual Objects Classification with Sliding Spatial Pyramid Matching,0.08705264329910278,#ff7f0e
4.5767612,-9.806657,neighbor,14124313,Two-Stream Convolutional Networks for Action Recognition in Videos,0.08746248483657837,#ff7f0e
-13.211974,-2.34065,neighbor,14124313,Generic Object Detection with Dense Neural Patterns and Regionlets,0.08756732940673828,#ff7f0e
10.926293,-5.3418536,neighbor,14124313,Avoiding pathologies in very deep networks,0.0878022313117981,#ff7f0e
1.2480915,-7.8172755,neighbor,14124313,Efficient Learning of Sparse Invariant Representations,0.08780670166015625,#ff7f0e
-2.2955399,-8.234452,neighbor,14124313,Large-Scale Feature Learning With Spike-and-Slab Sparse Coding,0.08793890476226807,#ff7f0e
-10.22698,-8.892456,neighbor,14124313,Fisher and VLAD with FLAIR,0.08825820684432983,#ff7f0e
-9.06975,-5.007509,neighbor,14124313,Associative Embeddings for Large-Scale Knowledge Transfer with Self-Assessment,0.08872348070144653,#ff7f0e
-6.619032,1.1071508,neighbor,14124313,Feature combination with Multi-Kernel Learning for fine-grained visual classification,0.08915066719055176,#ff7f0e
-0.82251245,9.294682,neighbor,14124313,Invariant Scattering Convolution Networks,0.08922797441482544,#ff7f0e
4.7807717,15.949845,neighbor,14124313,Neocortical frame-free vision sensing and processing through scalable Spiking ConvNet hardware,0.08936434984207153,#ff7f0e
17.272152,4.046901,neighbor,14124313,Parallel multiclass stochastic gradient descent algorithms for classifying million images with very-high-dimensional signatures into thousands classes,0.09037744998931885,#ff7f0e
1.146817,-9.179242,neighbor,14124313,Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning?,0.09092956781387329,#ff7f0e
-16.96555,-4.099661,neighbor,14124313,Learning Rich Features from RGB-D Images for Object Detection and Segmentation,0.09098440408706665,#ff7f0e
15.241317,-2.752272,neighbor,14124313,Big Data Deep Learning: Challenges and Perspectives,0.09127885103225708,#ff7f0e
14.047743,8.19194,neighbor,14124313,A Probabilistic WKL Rule for Incremental Feature Learning and Pattern Recognition,0.09155058860778809,#ff7f0e
5.1848216,15.792791,neighbor,14124313,On scalable spiking convnet hardware for cortex-like visual sensory processing systems,0.09160727262496948,#ff7f0e
-2.4290066,-3.9705176,neighbor,14124313,Unsupervised feature learning by augmenting single images,0.09171617031097412,#ff7f0e
16.709534,5.054739,neighbor,14124313,Deep Multiple Kernel Learning,0.09190553426742554,#ff7f0e
-6.9201145,-2.505888,query,15019293,Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization,0.0,#ff7f0e
-6.2174525,-1.3250726,neighbor,15019293,Generating Visual Explanations,0.04887819290161133,#ff7f0e
2.9773078,8.61509,neighbor,15019293,Towards Transparent AI Systems: Interpreting Visual Question Answering Models,0.05045723915100098,#ff7f0e
7.3697777,5.4567137,neighbor,15019293,Analyzing the Behavior of Visual Question Answering Models,0.05202770233154297,#ff7f0e
6.331785,5.163169,neighbor,15019293,"Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering",0.05254995822906494,#ff7f0e
-11.615402,-10.008685,neighbor,15019293,Visual Concept Recognition and Localization via Iterative Introspection,0.05657219886779785,#ff7f0e
7.3376746,0.49116436,neighbor,15019293,Ask Your Neurons: A Deep Learning Approach to Visual Question Answering,0.05701965093612671,#ff7f0e
1.1969013,4.350015,neighbor,15019293,What Value Do Explicit High Level Concepts Have in Vision to Language Problems?,0.0582926869392395,#ff7f0e
6.679688,1.2308754,neighbor,15019293,Tutorial on Answering Questions about Images with Deep Learning,0.05985438823699951,#ff7f0e
4.9983935,4.1129613,neighbor,15019293,Visual7W: Grounded Question Answering in Images,0.05992859601974487,#ff7f0e
10.8968115,8.548879,neighbor,15019293,Learning to Compose Neural Networks for Question Answering,0.06031394004821777,#ff7f0e
1.9145851,4.497904,neighbor,15019293,Image Captioning and Visual Question Answering Based on Attributes and External Knowledge,0.06059670448303223,#ff7f0e
5.6598935,5.9708533,neighbor,15019293,Hierarchical Question-Image Co-Attention for Visual Question Answering,0.06066548824310303,#ff7f0e
9.731319,2.629171,neighbor,15019293,Mean Box Pooling: A Rich Image Representation and Output Embedding for the Visual Madlibs Task,0.06081134080886841,#ff7f0e
4.2583685,7.806999,neighbor,15019293,Human Attention in Visual Question Answering: Do Humans and Deep Networks look at the same regions?,0.061142027378082275,#ff7f0e
-7.761778,5.1556396,neighbor,15019293,Attention Correctness in Neural Image Captioning,0.06122100353240967,#ff7f0e
-6.0142775,0.97750294,neighbor,15019293,Learning to generalize to new compositions in image understanding,0.06176888942718506,#ff7f0e
9.034936,3.410712,neighbor,15019293,Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding,0.062083899974823,#ff7f0e
-8.898267,2.6906345,neighbor,15019293,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",0.06280279159545898,#ff7f0e
3.2492917,4.9808235,neighbor,15019293,Exploring Models and Data for Image Question Answering,0.06289136409759521,#ff7f0e
6.116212,6.35425,neighbor,15019293,ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering,0.06374919414520264,#ff7f0e
-14.909812,-9.468707,neighbor,15019293,Visualizing and Comparing Convolutional Neural Networks,0.06489056348800659,#ff7f0e
5.2564044,4.268701,neighbor,15019293,Graph-Structured Representations for Visual Question Answering,0.065726637840271,#ff7f0e
-7.28459,1.5981668,neighbor,15019293,Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge,0.06604212522506714,#ff7f0e
-10.743423,-0.7068145,neighbor,15019293,Learning Wake-Sleep Recurrent Attention Models,0.06606346368789673,#ff7f0e
5.3243976,7.0533442,neighbor,15019293,Stacked Attention Networks for Image Question Answering,0.0663185715675354,#ff7f0e
7.8753796,4.89463,neighbor,15019293,Simple Baseline for Visual Question Answering,0.06700414419174194,#ff7f0e
3.3567746,4.239197,neighbor,15019293,Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge from External Sources,0.06725817918777466,#ff7f0e
10.732117,8.68631,neighbor,15019293,Neural Module Networks,0.06774789094924927,#ff7f0e
10.063949,7.256485,neighbor,15019293,Dynamic Memory Networks for Visual and Textual Question Answering,0.06837189197540283,#ff7f0e
7.307935,4.1386786,neighbor,15019293,Compositional Memory for Visual Question Answering,0.06850045919418335,#ff7f0e
4.2611775,3.3085742,neighbor,15019293,Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions,0.06927061080932617,#ff7f0e
5.09809,-1.4024948,neighbor,15019293,Towards a Visual Turing Challenge,0.06942248344421387,#ff7f0e
8.942525,5.622565,neighbor,15019293,Training Recurrent Answering Units with Joint Loss Minimization for VQA,0.0694504976272583,#ff7f0e
-0.8198211,2.362251,neighbor,15019293,Visual Madlibs: Fill in the blank Image Generation and Question Answering,0.06948322057723999,#ff7f0e
-13.214063,-7.3858275,neighbor,15019293,Deep Residual Learning for Image Recognition,0.06968510150909424,#ff7f0e
-12.184572,2.7464635,neighbor,15019293,DeepGaze II: Reading fixations from deep features trained on object recognition,0.06991136074066162,#ff7f0e
12.142567,3.9539196,neighbor,15019293,Gated-Attention Readers for Text Comprehension,0.06995648145675659,#ff7f0e
9.594326,5.081941,neighbor,15019293,Multimodal Residual Learning for Visual QA,0.06999748945236206,#ff7f0e
-6.8349724,5.6584554,neighbor,15019293,DenseCap: Fully Convolutional Localization Networks for Dense Captioning,0.07051998376846313,#ff7f0e
-8.725296,-7.7478848,neighbor,15019293,Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians,0.07076531648635864,#ff7f0e
7.396335,0.006276427,neighbor,15019293,Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images,0.0708116888999939,#ff7f0e
-6.5381036,3.0296643,neighbor,15019293,Image Caption Generation with Text-Conditional Semantic Attention,0.07105100154876709,#ff7f0e
-15.4481,-9.105556,neighbor,15019293,Visualizing and Understanding Convolutional Networks,0.07113540172576904,#ff7f0e
-7.105354,3.4588192,neighbor,15019293,Guiding Long-Short Term Memory for Image Caption Generation,0.07143741846084595,#ff7f0e
2.1104882,2.634717,neighbor,15019293,DualNet: Domain-invariant network for visual question answering,0.07210195064544678,#ff7f0e
-7.838992,1.758757,neighbor,15019293,Show and tell: A neural image caption generator,0.07267206907272339,#ff7f0e
-8.185479,3.8758404,neighbor,15019293,Image Representations and New Domains in Neural Image Captioning,0.07325375080108643,#ff7f0e
1.3583157,5.7982945,neighbor,15019293,Leveraging Visual Question Answering for Image-Caption Ranking,0.07359480857849121,#ff7f0e
-12.138987,-3.1647916,neighbor,15019293,Symmetries and control in generative neural nets,0.07370495796203613,#ff7f0e
-8.286492,-10.126222,neighbor,15019293,Top-Down Learning for Structured Labeling with Convolutional Pseudoprior,0.07417505979537964,#ff7f0e
-6.439027,3.910145,neighbor,15019293,Image Captioning with Deep Bidirectional LSTMs,0.07434475421905518,#ff7f0e
5.23573,9.860255,neighbor,15019293,Neural Self Talk: Image Understanding via Continuous Questioning and Answering,0.07437717914581299,#ff7f0e
-12.969284,-1.5723637,neighbor,15019293,"Variational Autoencoder for Deep Learning of Images, Labels and Captions",0.07451456785202026,#ff7f0e
4.7424874,-1.1901776,neighbor,15019293,Hard to Cheat: A Turing Test based on Answering Questions about Images,0.07455915212631226,#ff7f0e
3.9434202,0.10996306,neighbor,15019293,Measuring Machine Intelligence Through Visual Question Answering,0.07466435432434082,#ff7f0e
-1.8110095,2.9225447,neighbor,15019293,Generating Natural Questions About an Image,0.07473611831665039,#ff7f0e
7.3416767,7.177713,neighbor,15019293,Learning to Answer Questions from Image Using Convolutional Neural Network,0.07523810863494873,#ff7f0e
-10.732555,-9.210558,neighbor,15019293,Do Convnets Learn Correspondence?,0.07540905475616455,#ff7f0e
-14.628227,-6.387891,neighbor,15019293,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,0.07589435577392578,#ff7f0e
-10.742051,-8.40352,neighbor,15019293,Learning Deep Features for Discriminative Localization,0.07608389854431152,#ff7f0e
4.464031,2.65009,neighbor,15019293,The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA),0.07613855600357056,#ff7f0e
-9.937759,-7.4688745,neighbor,15019293,Top-Down Neural Attention by Excitation Backprop,0.07654297351837158,#ff7f0e
-12.03253,-10.2171755,neighbor,15019293,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.07669085264205933,#ff7f0e
-6.2908187,6.5200753,neighbor,15019293,Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for Locally Robust Captioning,0.07705885171890259,#ff7f0e
-10.874008,-0.35169148,neighbor,15019293,DRAW: A Recurrent Neural Network For Image Generation,0.07707226276397705,#ff7f0e
-8.742448,5.319788,neighbor,15019293,Technical Report: Image Captioning with Semantically Similar Images,0.07708460092544556,#ff7f0e
-7.740896,-6.888159,neighbor,15019293,On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation,0.0771755576133728,#ff7f0e
-12.1566305,-1.1765251,neighbor,15019293,Conditional Image Generation with PixelCNN Decoders,0.07720625400543213,#ff7f0e
3.508414,2.5124242,neighbor,15019293,VQA: Visual Question Answering,0.07724624872207642,#ff7f0e
0.11342659,1.4356714,neighbor,15019293,Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations,0.07764512300491333,#ff7f0e
9.076352,9.437058,neighbor,15019293,Visual Cortex Inspired CNN Model for Feature Construction in Text Analysis,0.07779628038406372,#ff7f0e
6.3782234,2.5736675,neighbor,15019293,Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question,0.07781946659088135,#ff7f0e
-11.373405,3.433596,neighbor,15019293,Seeing with Humans: Gaze-Assisted Neural Image Captioning,0.07787477970123291,#ff7f0e
-10.681871,-11.283212,neighbor,15019293,Self-informed neural network structure learning,0.07807940244674683,#ff7f0e
-9.324959,6.1102347,neighbor,15019293,Oracle Performance for Visual Captioning,0.0781087875366211,#ff7f0e
-10.694964,-10.8250675,neighbor,15019293,Associative Embeddings for Large-Scale Knowledge Transfer with Self-Assessment,0.07814294099807739,#ff7f0e
-11.220949,-5.9204226,neighbor,15019293,Book Review: Deep Learning,0.07884764671325684,#ff7f0e
3.2315598,6.453352,neighbor,15019293,Where to Look: Focus Regions for Visual Question Answering,0.07888901233673096,#ff7f0e
-5.537064,2.5475316,neighbor,15019293,Learning a Recurrent Visual Representation for Image Caption Generation,0.07895404100418091,#ff7f0e
-11.838379,-8.615607,neighbor,15019293,Webly Supervised Learning of Convolutional Networks,0.07929092645645142,#ff7f0e
-16.169748,-8.786343,neighbor,15019293,A New Method to Visualize Deep Neural Networks,0.07943540811538696,#ff7f0e
-8.268195,-1.7455782,neighbor,15019293,Colorful Image Colorization,0.07951843738555908,#ff7f0e
-13.385078,-10.704547,neighbor,15019293,Understanding Intra-Class Knowledge Inside CNN,0.07988882064819336,#ff7f0e
-13.422163,-6.789132,neighbor,15019293,DisturbLabel: Regularizing CNN on the Loss Layer,0.08016002178192139,#ff7f0e
-13.2319975,-2.993919,neighbor,15019293,Scoring and Classifying with Gated Auto-Encoders,0.08028668165206909,#ff7f0e
-2.1399055,5.5251193,neighbor,15019293,Learning language through pictures,0.08042305707931519,#ff7f0e
-5.13388,5.073602,neighbor,15019293,Image Captioning with Semantic Attention,0.08051884174346924,#ff7f0e
-6.3452644,-5.652449,neighbor,15019293,“Why Should I Trust You?”: Explaining the Predictions of Any Classifier,0.0806131362915039,#ff7f0e
-7.4977107,2.4552274,neighbor,15019293,Cross-Lingual Image Caption Generation,0.08106571435928345,#ff7f0e
-11.9631605,-2.3185358,neighbor,15019293,Improved Techniques for Training GANs,0.08127808570861816,#ff7f0e
7.5283904,8.10598,neighbor,15019293,Image Question Answering Using Convolutional Neural Network with Dynamic Parameter Prediction,0.08134913444519043,#ff7f0e
-16.366207,-10.031575,neighbor,15019293,Visualizing Deep Convolutional Neural Networks Using Natural Pre-images,0.08140051364898682,#ff7f0e
11.703749,3.9923527,neighbor,15019293,Attentive Pooling Networks,0.08146673440933228,#ff7f0e
-7.9309154,-10.024672,neighbor,15019293,Local Perturb-and-MAP for Structured Prediction,0.08149975538253784,#ff7f0e
-11.771631,-5.373568,neighbor,15019293,Towards deep compositional networks,0.0815885066986084,#ff7f0e
4.469229,5.9087625,neighbor,15019293,A Focused Dynamic Attention Model for Visual Question Answering,0.08163034915924072,#ff7f0e
-13.711639,-7.754253,neighbor,15019293,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",0.0817597508430481,#ff7f0e
-4.062587,3.7950852,neighbor,15019293,Aligning where to see and what to tell: image caption with region-based attention and scene factorization,0.08201855421066284,#ff7f0e
-4.80216,2.4389203,neighbor,15019293,The Long-Short Story of Movie Description,0.08202731609344482,#ff7f0e
-6.787516,-6.2242537,neighbor,15019293,Explaining Predictions of Non-Linear Classifiers in NLP,0.0821523666381836,#ff7f0e
4.731433,-5.7716765,query,15798713,Conducting Meta-Analyses in R with the metafor Package,0.0,#ffbb78
9.52205,-1.4049579,neighbor,15798713,A Handbook of Statistical Analyses Using R,0.047710537910461426,#ffbb78
5.3269377,-5.647182,neighbor,15798713,Metaplot: A Novel Stata Graph for Assessing Heterogeneity at a Glance,0.05244410037994385,#ffbb78
10.256781,-1.9296365,neighbor,15798713,reporttools: R Functions to Generate LaTeX Tables of Descriptive Statistics,0.058178603649139404,#ffbb78
4.2203097,-6.483301,neighbor,15798713,Assessment of regression-based methods to adjust for publication bias through a comprehensive simulation study,0.05925452709197998,#ffbb78
-3.3664567,1.4709063,neighbor,15798713,Reviewers,0.06268340349197388,#ffbb78
12.300241,-0.682579,neighbor,15798713,CGIwithR: Facilities for processing web forms using R,0.06341922283172607,#ffbb78
-1.693144,3.4093542,neighbor,15798713,Information for Authors,0.06466484069824219,#ffbb78
-3.2217712,-0.2082837,neighbor,15798713,Acknowledgement of Reviewers,0.06539082527160645,#ffbb78
-3.4173312,1.2958828,neighbor,15798713,Reviewers,0.0655553936958313,#ffbb78
10.672924,-0.46128598,neighbor,15798713,Caching and Distributing Statistical Analyses in R,0.06679081916809082,#ffbb78
-5.819426,0.07422264,neighbor,15798713,Acknowledgement of Reviewers,0.06706732511520386,#ffbb78
-2.7864683,0.7708542,neighbor,15798713,Reviewers,0.06753396987915039,#ffbb78
9.434084,-0.059348248,neighbor,15798713,Data Manipulation with R,0.0692206621170044,#ffbb78
-1.1255449,6.25807,neighbor,15798713,Keyword index,0.06933122873306274,#ffbb78
-4.0229716,-0.44481766,neighbor,15798713,Guest Reviewers,0.0693846344947815,#ffbb78
11.733018,-1.4101413,neighbor,15798713,Quality Control for Statistical Graphics: The graphicsQC Package for R,0.07104712724685669,#ffbb78
-2.4142168,1.0989476,neighbor,15798713,Reviewers,0.07115262746810913,#ffbb78
-4.2971754,0.28784704,neighbor,15798713,Thanks to Reviewers,0.07198727130889893,#ffbb78
-8.843554,-9.686252,neighbor,15798713,Retraction,0.07227486371994019,#ffbb78
11.6680565,-0.036838606,neighbor,15798713,The R Commander: A Basic-Statistics Graphical User Interface to R,0.07292568683624268,#ffbb78
10.058648,1.8562392,neighbor,15798713,Toward a Common Framework for Statistical Analysis and Development,0.07293063402175903,#ffbb78
7.1495214,-2.8816404,neighbor,15798713,Review of the Statistical Techniques in Medical Sciences,0.0732085108757019,#ffbb78
11.101639,-5.17341,neighbor,15798713,glmulti: An R Package for Easy Automated Model Selection with (Generalized) Linear Models,0.07325178384780884,#ffbb78
-4.6994596,5.4081583,neighbor,15798713,Sponsors,0.07344406843185425,#ffbb78
-8.74799,-5.5019193,neighbor,15798713,Correction,0.07352906465530396,#ffbb78
-4.2955847,-8.52662,neighbor,15798713,Erratum,0.07357406616210938,#ffbb78
-4.2955847,-8.52662,neighbor,15798713,Erratum,0.07357406616210938,#ffbb78
10.882048,-4.2251887,neighbor,15798713,Effect Displays in R for Generalised Linear Models,0.0737953782081604,#ffbb78
-6.120831,-10.854686,neighbor,15798713,Erratum,0.07440590858459473,#ffbb78
-6.521226,-11.439008,neighbor,15798713,Erratum,0.07445651292800903,#ffbb78
11.28199,0.8726372,neighbor,15798713,RinRuby: Accessing the R Interpreter from Pure Ruby,0.0746728777885437,#ffbb78
-7.723708,-9.149216,neighbor,15798713,Erratum,0.0748932957649231,#ffbb78
-0.024173938,7.406515,neighbor,15798713,Table of Contents,0.07579284906387329,#ffbb78
-0.023825565,7.4062533,neighbor,15798713,Table of contents,0.07579284906387329,#ffbb78
-5.0804915,1.1506171,neighbor,15798713,List of Reviewers,0.07652997970581055,#ffbb78
-6.111756,-8.912877,neighbor,15798713,Erratum,0.07680493593215942,#ffbb78
1.9660891,-5.2470145,neighbor,15798713,"Characteristics of Meta-Analyses Reported in Mental Retardation, Learning Disabilities, and Emotional and Behavioral Disorders",0.07704883813858032,#ffbb78
-6.4746933,4.8252196,neighbor,15798713,List of participants,0.07710117101669312,#ffbb78
-5.360111,-9.550623,neighbor,15798713,Erratum,0.07738739252090454,#ffbb78
-4.80416,-1.5056331,neighbor,15798713,Acknowledgment to reviewers,0.07749617099761963,#ffbb78
-4.5188036,6.1531386,neighbor,15798713,Sponsors,0.07777369022369385,#ffbb78
12.198915,-4.446107,neighbor,15798713,FactoMineR: An R Package for Multivariate Analysis,0.0779145359992981,#ffbb78
-7.5427384,3.773119,neighbor,15798713,LIST OF PARTICIPANTS,0.07823222875595093,#ffbb78
13.818565,-3.2772784,neighbor,15798713,Quasi-variances in Xlisp-Stat and on the web,0.07830220460891724,#ffbb78
-8.584331,5.759948,neighbor,15798713,Content,0.07839345932006836,#ffbb78
-7.3763847,-10.845758,neighbor,15798713,Erratum,0.07857918739318848,#ffbb78
-5.2077894,9.123437,neighbor,15798713,Committees,0.07893747091293335,#ffbb78
8.910036,0.37826675,neighbor,15798713,Robust Statistical Methods With R,0.07917261123657227,#ffbb78
-6.8184156,-10.207812,neighbor,15798713,Erratum,0.07932263612747192,#ffbb78
9.806559,-5.347685,neighbor,15798713,The R Package geepack for Generalized Estimating Equations,0.07938498258590698,#ffbb78
2.8870773,8.635246,neighbor,15798713,Author Index,0.07939594984054565,#ffbb78
2.8867612,8.635218,neighbor,15798713,Author Index,0.07939594984054565,#ffbb78
-4.6138177,10.195463,neighbor,15798713,Program Committee,0.07948696613311768,#ffbb78
3.0106454,2.7762804,neighbor,15798713,Reviewers,0.07951158285140991,#ffbb78
-7.1582236,4.1760945,neighbor,15798713,List of Participants,0.07951170206069946,#ffbb78
8.629655,-3.1903074,neighbor,15798713,A Visual Basic Software for Computing Fisher\'s Exact Probability,0.0796893835067749,#ffbb78
6.6173058,-1.0382158,neighbor,15798713,"Research and statistics: distribution, variability, and statistical significance.",0.07973694801330566,#ffbb78
11.290157,-3.0187385,neighbor,15798713,ggplot2: Elegant Graphics for Data Analysis,0.07980775833129883,#ffbb78
-6.143609,9.299808,neighbor,15798713,Committees,0.08010143041610718,#ffbb78
-6.143609,9.299808,neighbor,15798713,Committees,0.08010143041610718,#ffbb78
-1.0536978,3.8863032,neighbor,15798713,About the Authors,0.08012169599533081,#ffbb78
11.690793,-3.0302417,neighbor,15798713,Beanplot: A Boxplot Alternative for Visual Comparison of Distributions,0.08030098676681519,#ffbb78
-0.6148699,8.976824,neighbor,15798713,Foreword,0.08031421899795532,#ffbb78
-5.928699,-9.948831,neighbor,15798713,Erratum,0.0804702639579773,#ffbb78
-7.303609,-5.8123155,neighbor,15798713,Correction,0.08050334453582764,#ffbb78
-2.0767822,7.774627,neighbor,15798713,Preface,0.0806722640991211,#ffbb78
-2.5352616,6.399931,neighbor,15798713,Poster,0.08075958490371704,#ffbb78
-8.312247,2.7586734,neighbor,15798713,How to Summarize Single-Participant Research: Ideas and Applications,0.08113527297973633,#ffbb78
-7.6623063,-10.239208,neighbor,15798713,Erratum,0.08143413066864014,#ffbb78
-6.223667,8.429328,neighbor,15798713,Committees,0.08153456449508667,#ffbb78
-7.874484,-5.149705,neighbor,15798713,Correction,0.08179813623428345,#ffbb78
12.786454,1.3929957,neighbor,15798713,Using R via PHP for Teaching Purposes: R-php,0.08193439245223999,#ffbb78
-3.8372717,6.661398,neighbor,15798713,Acknowledgments,0.08207064867019653,#ffbb78
-2.5484486,-1.6816442,neighbor,15798713,Reviewers,0.08213210105895996,#ffbb78
0.11658217,-0.07417724,neighbor,15798713,Reviews,0.08215713500976562,#ffbb78
-4.3946223,3.9815419,neighbor,15798713,Acknowledgements,0.08219611644744873,#ffbb78
-2.5308762,-4.352083,neighbor,15798713,In this issue,0.08220529556274414,#ffbb78
-5.36985,8.240909,neighbor,15798713,Committees,0.08229368925094604,#ffbb78
-5.2756066,-6.195439,neighbor,15798713,Correction,0.08247959613800049,#ffbb78
13.564282,-0.5355416,neighbor,15798713,R2WinBUGS: A Package for Running WinBUGS from R,0.08251321315765381,#ffbb78
-4.9828196,-10.790826,neighbor,15798713,Erratum,0.08264583349227905,#ffbb78
0.3059837,-0.39375287,neighbor,15798713,REVIEWS,0.08298385143280029,#ffbb78
-5.08681,5.108032,neighbor,15798713,Sponsors,0.08305883407592773,#ffbb78
-0.078393616,1.5076504,neighbor,15798713,REVIEWS,0.08308446407318115,#ffbb78
-0.12956677,-4.1714234,neighbor,15798713,Critical Reviews in Psychiatry (3rd edn),0.08321529626846313,#ffbb78
-4.6875567,8.570875,neighbor,15798713,Committees,0.08337342739105225,#ffbb78
1.7799224,-0.325381,neighbor,15798713,Editors’ Introduction,0.08344346284866333,#ffbb78
-1.8999447,10.675749,neighbor,15798713,MP users guide,0.08354854583740234,#ffbb78
-7.5193458,-5.667637,neighbor,15798713,Correction,0.08364599943161011,#ffbb78
-2.740507,-8.557264,neighbor,15798713,The Medical Algorithms Project,0.08369892835617065,#ffbb78
-2.447835,7.72825,neighbor,15798713,Preface,0.08370012044906616,#ffbb78
-5.123712,10.736419,neighbor,15798713,Program Committee,0.08370625972747803,#ffbb78
-6.9217534,-9.462347,neighbor,15798713,Erratum,0.08390206098556519,#ffbb78
2.6894867,2.8814068,neighbor,15798713,Reviewers,0.08391988277435303,#ffbb78
-2.774483,9.006179,neighbor,15798713,List of participants,0.0841255784034729,#ffbb78
0.09320395,-2.1950133,neighbor,15798713,Editorial,0.08420801162719727,#ffbb78
8.033894,-1.1812133,neighbor,15798713,Statistical Methods for Communication Science,0.08427274227142334,#ffbb78
0.93456954,3.7759502,neighbor,15798713,Summary and recommendations,0.08430612087249756,#ffbb78
3.2692902,3.1908805,neighbor,15798713,Reviewers,0.08436405658721924,#ffbb78
-2.6834908,-4.3048043,neighbor,15798713,In This Issue,0.0844259262084961,#ffbb78
-4.1051354,-1.1316427,query,16120223,MICE: Multivariate Imputation by Chained Equations in R,0.0,#ffbb78
-4.0845976,-0.94679475,neighbor,16120223,Multiple imputation by chained equations: what is it and how does it work?,0.03983491659164429,#ffbb78
3.8491704,-5.1245894,neighbor,16120223,The R Package geepack for Generalized Estimating Equations,0.0718989372253418,#ffbb78
-0.12814651,-7.5889335,neighbor,16120223,ipw: An R Package for Inverse Probability Weighting,0.07643455266952515,#ffbb78
5.128126,-4.297382,neighbor,16120223,glmulti: An R Package for Easy Automated Model Selection with (Generalized) Linear Models,0.08079922199249268,#ffbb78
3.325151,-0.37333786,neighbor,16120223,FactoMineR: An R Package for Multivariate Analysis,0.08116960525512695,#ffbb78
4.56315,-4.4858236,neighbor,16120223,Hierarchical Generalized Linear Models: The R Package HGLMMM,0.082527756690979,#ffbb78
4.901343,0.62683403,neighbor,16120223,reporttools: R Functions to Generate LaTeX Tables of Descriptive Statistics,0.08401381969451904,#ffbb78
5.5921507,-0.061193798,neighbor,16120223,A Handbook of Statistical Analyses Using R,0.08455407619476318,#ffbb78
-0.7331722,-11.5252495,neighbor,16120223,Semi-Instrumental Variables: A Test for Instrument Admissibility,0.0846395492553711,#ffbb78
6.945597,-3.0194468,neighbor,16120223,Effect Displays in R for Generalised Linear Models,0.0866122841835022,#ffbb78
8.214118,0.83234215,neighbor,16120223,Caching and Distributing Statistical Analyses in R,0.08754873275756836,#ffbb78
2.7453415,1.766608,neighbor,16120223,Bivariate Poisson and Diagonal Inflated Bivariate Poisson Regression Models in R,0.08780777454376221,#ffbb78
3.372265,-11.198568,neighbor,16120223,Independencies Induced from a Graphical Markov Model After Marginalization and Conditioning: The R Package ggm,0.08798694610595703,#ffbb78
6.212808,-0.49465021,neighbor,16120223,SAS and R,0.08814334869384766,#ffbb78
7.3788276,2.090273,neighbor,16120223,Reshaping Data with the reshape Package,0.08952075242996216,#ffbb78
-3.061912,9.909441,neighbor,16120223,Correction,0.0896914005279541,#ffbb78
1.7279187,5.4488893,neighbor,16120223,Sampling,0.08977347612380981,#ffbb78
-2.0307617,9.883195,neighbor,16120223,Correction,0.0904611349105835,#ffbb78
6.0315905,1.1383123,neighbor,16120223,CGIwithR: Facilities for processing web forms using R,0.09081864356994629,#ffbb78
-2.3127766,-5.4204206,neighbor,16120223,A marginalized conditional linear model for longitudinal binary data when informative dropout occurs in continuous time,0.0909767746925354,#ffbb78
4.2739487,-6.5997887,neighbor,16120223,poLCA: An R Package for Polytomous Variable Latent Class Analysis,0.09163272380828857,#ffbb78
-1.4472424,-12.059609,neighbor,16120223,Identifying confounders using additive noise models,0.09174007177352905,#ffbb78
2.5466359,6.9641156,neighbor,16120223,Information for Authors,0.09189063310623169,#ffbb78
2.4161541,-10.2822,neighbor,16120223,Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors,0.09227561950683594,#ffbb78
2.1450577,8.770318,neighbor,16120223,Reviewers,0.09234148263931274,#ffbb78
7.0608253,1.1451234,neighbor,16120223,Data Manipulation with R,0.09247803688049316,#ffbb78
-3.7762282,-4.201277,neighbor,16120223,Incomplete-data classification using logistic regression,0.09279578924179077,#ffbb78
-0.62729704,3.094597,neighbor,16120223,Multiple Imputations Applied to the DREAM3 Phosphoproteomics Challenge: A Winning Strategy,0.09292179346084595,#ffbb78
-4.2687306,-4.0943465,neighbor,16120223,Experimental analysis of methods for imputation of missing values in databases,0.09326159954071045,#ffbb78
5.137013,-6.3643246,neighbor,16120223,FlexMix: A general framework for finite mixture models and latent class regression in R,0.09351712465286255,#ffbb78
6.1173134,-1.6363786,neighbor,16120223,Computational Statistics: An Introduction to R,0.09382545948028564,#ffbb78
2.334433,8.906593,neighbor,16120223,Reviewers,0.09387928247451782,#ffbb78
8.729155,-2.9241111,neighbor,16120223,systemfit: A Package for Estimating Systems of Simultaneous Equations in R,0.0939909815788269,#ffbb78
0.20505239,-1.6599578,neighbor,16120223,"An Introduction to the Special Volume on ""Psychometrics in R""",0.09475237131118774,#ffbb78
-0.7166062,8.77731,neighbor,16120223,Correction,0.09478622674942017,#ffbb78
6.705296,-4.023729,neighbor,16120223,Formulating State Space Models in R with Focus on Longitudinal Regression Models,0.09497249126434326,#ffbb78
6.997887,0.00090976397,neighbor,16120223,R in a Nutshell,0.09504574537277222,#ffbb78
-0.6928443,-11.0450735,neighbor,16120223,Generalized Instrumental Variables,0.09536385536193848,#ffbb78
-1.3701974,7.155402,neighbor,16120223,Erratum,0.09567159414291382,#ffbb78
-1.2154003,7.3729806,neighbor,16120223,Erratum,0.09567159414291382,#ffbb78
-1.3657291,-1.0477637,neighbor,16120223,"Predicting gender differences as latent variables: summed scores, and individual item responses: a methods case study",0.09593582153320312,#ffbb78
9.698075,-2.3856797,neighbor,16120223,Extended Model Formulas in R : Multiple Parts and Multiple Responses,0.0960128903388977,#ffbb78
7.167335,-1.0703409,neighbor,16120223,Robust Statistical Methods With R,0.09607434272766113,#ffbb78
-3.0100935,7.2316427,neighbor,16120223,Erratum,0.09634464979171753,#ffbb78
6.478615,-5.920796,neighbor,16120223,"spikeSlabGAM: Bayesian Variable Selection, Model Choice and Regularization for Generalized Additive Mixed Models in R",0.09635066986083984,#ffbb78
5.7175446,-2.753604,neighbor,16120223,Generalized Additive Models: An Introduction with R,0.09642809629440308,#ffbb78
1.017675,-4.8113136,neighbor,16120223,CVTresh: R Package for Level-Dependent Cross-Validation Thresholding,0.09647566080093384,#ffbb78
2.0081728,8.177876,neighbor,16120223,Acknowledgement of Reviewers,0.09661781787872314,#ffbb78
5.5183797,-0.65096575,neighbor,16120223,"R Graphics, 2nd Edition",0.09664440155029297,#ffbb78
-3.14112,7.8340993,neighbor,16120223,Erratum,0.09674447774887085,#ffbb78
5.0089335,9.957433,neighbor,16120223,Committees,0.09710854291915894,#ffbb78
-0.40954506,-11.849706,neighbor,16120223,Causal Bounds and Instruments,0.09713643789291382,#ffbb78
8.379745,-4.4276915,neighbor,16120223,BB: An R Package for Solving a Large System of Nonlinear Equations and for Optimizing a High-Dimensional Nonlinear Objective Function,0.09725731611251831,#ffbb78
4.3402433,-0.025873827,neighbor,16120223,Beanplot: A Boxplot Alternative for Visual Comparison of Distributions,0.09725898504257202,#ffbb78
5.222792,3.2564225,neighbor,16120223,Quasi-variances in Xlisp-Stat and on the web,0.09750831127166748,#ffbb78
4.330635,1.8400503,neighbor,16120223,R Programs for Truncated Distributions,0.09753751754760742,#ffbb78
-4.6075697,0.19302025,neighbor,16120223,Validity and Power of Missing Data Imputation for Extreme Sampling and Terminal Measures Designs in Mediation Analysis,0.09759628772735596,#ffbb78
6.8767824,-6.364461,neighbor,16120223,DPpackage: Bayesian Semi- and Nonparametric Modeling in R,0.09765827655792236,#ffbb78
-0.63614357,-1.7299805,neighbor,16120223,plink: An R Package for Linking Mixed-Format Tests Using IRT-Based Methods,0.09809631109237671,#ffbb78
1.3317295,8.683454,neighbor,16120223,Thanks to Reviewers,0.0981244444847107,#ffbb78
1.1094481,10.066924,neighbor,16120223,Acknowledgement of Reviewers,0.09816980361938477,#ffbb78
-0.768932,-12.625862,neighbor,16120223,On Measurement Bias in Causal Inference,0.09817403554916382,#ffbb78
1.9460495,-2.4361956,neighbor,16120223,"Econometrics in R: Past, Present, and Future",0.09818792343139648,#ffbb78
8.490241,-0.81850016,neighbor,16120223,A Modern Approach to Regression with R,0.0984225869178772,#ffbb78
1.9817755,-2.5784552,neighbor,16120223,Panel data econometrics in R: The plm package,0.09855908155441284,#ffbb78
3.4826436,-11.236108,neighbor,16120223,Finding Non-Overlapping Clusters for Generalized Inference Over Graphical Models,0.09859347343444824,#ffbb78
-0.17782664,-12.831172,neighbor,16120223,On the Validity of Covariate Adjustment for Estimating Causal Effects,0.0987439751625061,#ffbb78
5.614666,-7.3407454,neighbor,16120223,Biometrical Modeling of Twin and Family Data Using Standard Mixed Model Software,0.09880471229553223,#ffbb78
5.0194736,-9.547202,neighbor,16120223,The Infinite Hierarchical Factor Regression Model,0.0988418459892273,#ffbb78
5.9121428,-10.372455,neighbor,16120223,Partial Correlation Estimation by Joint Sparse Regression Models,0.09890317916870117,#ffbb78
3.7705858,-1.3030821,neighbor,16120223,Principal Component Analysis,0.09899020195007324,#ffbb78
-2.472772,8.383504,neighbor,16120223,Errata,0.09920847415924072,#ffbb78
2.469007,5.3816924,neighbor,16120223,Summary and recommendations,0.09923446178436279,#ffbb78
1.6162443,0.6955964,neighbor,16120223,rpartOrdinal: An R Package for Deriving a Classification Tree for Predicting an Ordinal Response.,0.09929734468460083,#ffbb78
3.8992667,-8.413409,neighbor,16120223,Linear Latent Structure Analysis: from Foundations to Algorithms and Applications,0.0993189811706543,#ffbb78
7.124319,-0.06604818,neighbor,16120223,R in a Nutshell,0.09943825006484985,#ffbb78
7.9066534,4.165166,neighbor,16120223,Shrinkage observed-to-expected ratios for robust and transparent large-scale pattern discovery,0.09961599111557007,#ffbb78
2.1130915,9.400011,neighbor,16120223,Reviewers,0.09970986843109131,#ffbb78
-2.3212693,-2.9021122,neighbor,16120223,Missing data imputation and corrected statistics for large-scale behavioral databases,0.09989643096923828,#ffbb78
-2.3795943,7.5622454,neighbor,16120223,Erratum,0.09995788335800171,#ffbb78
-2.1499524,9.954896,neighbor,16120223,Correction,0.10011857748031616,#ffbb78
-1.5361209,6.7316175,neighbor,16120223,Erratum,0.1001548171043396,#ffbb78
4.2888126,8.031009,neighbor,16120223,Poster,0.10025721788406372,#ffbb78
4.107894,9.81822,neighbor,16120223,Committees,0.1004105806350708,#ffbb78
1.7733686,-11.108016,neighbor,16120223,Bayesian Discovery of Linear Acyclic Causal Models,0.10045737028121948,#ffbb78
4.702581,-11.0789585,neighbor,16120223,The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs,0.1004791259765625,#ffbb78
4.9180083,9.384677,neighbor,16120223,Committees,0.10052210092544556,#ffbb78
4.3739276,10.190254,neighbor,16120223,Committees,0.10052210092544556,#ffbb78
1.5134015,-11.520911,neighbor,16120223,A Theoretical Study of Y Structures for Causal Discovery,0.10078078508377075,#ffbb78
3.5511293,8.305304,neighbor,16120223,Sponsors,0.10081368684768677,#ffbb78
-1.2099836,10.638776,neighbor,16120223,Introduction,0.10082501173019409,#ffbb78
-2.078295,8.00944,neighbor,16120223,Erratum,0.10084176063537598,#ffbb78
4.4046307,-2.4113736,neighbor,16120223,Truecluster: robust scalable clustering with model selection,0.10094141960144043,#ffbb78
1.0230107,7.67023,neighbor,16120223,Reviewers,0.10098594427108765,#ffbb78
-4.3021197,-4.1625624,neighbor,16120223,"Autoencoder, Principal Component Analysis and Support Vector Regression for Data Imputation",0.10116243362426758,#ffbb78
2.8644385,-5.561306,neighbor,16120223,Penalized Clustering of Large-Scale Functional Data With Multiple Covariates,0.10122942924499512,#ffbb78
4.3459544,9.280138,neighbor,16120223,Committees,0.10131686925888062,#ffbb78
-2.5813472,-0.91726875,neighbor,16120223,Developing a predictive tool for psychological well-being among Chinese adolescents in the presence of missing data,0.10132122039794922,#ffbb78
7.870461,-6.4476347,neighbor,16120223,BART: Bayesian Additive Regression Trees,0.1013258695602417,#ffbb78
2.3142211,12.286313,neighbor,16120223,Calculating the Exact Pooled Variance,0.10133868455886841,#ffbb78
4.940755,-2.9004886,query,16326763,Continuous control with deep reinforcement learning,0.0,#ffbb78
6.549323,-1.941568,neighbor,16326763,Playing Atari with Deep Reinforcement Learning,0.03993189334869385,#ffbb78
4.765877,-5.659811,neighbor,16326763,End-to-End Training of Deep Visuomotor Policies,0.045698702335357666,#ffbb78
2.130525,0.53459907,neighbor,16326763,CORL: A Continuous-state Offset-dynamics Reinforcement Learner,0.046116769313812256,#ffbb78
-2.3553052,-4.0202117,neighbor,16326763,Reinforcement Learning in Robotics: Applications and Real-World Challenges,0.049161553382873535,#ffbb78
-6.7629113,-2.7455022,neighbor,16326763,Model-Based Reinforcement Learning in Continuous Environments Using Real-Time Constrained Optimization,0.049279093742370605,#ffbb78
-2.2071638,-2.9617615,neighbor,16326763,Reinforcement learning in robotics: A survey,0.05027109384536743,#ffbb78
5.623198,-5.549356,neighbor,16326763,From Pixels to Torques: Policy Learning with Deep Dynamical Models,0.05195838212966919,#ffbb78
1.2889582,-8.96657,neighbor,16326763,High-Dimensional Continuous Control Using Generalized Advantage Estimation,0.05275911092758179,#ffbb78
3.8682067,-6.8411975,neighbor,16326763,Exploring Deep and Recurrent Architectures for Optimal Control,0.05353671312332153,#ffbb78
5.594908,1.6605417,neighbor,16326763,Continuous Inverse Optimal Control with Locally Optimal Examples,0.05399078130722046,#ffbb78
-2.8848155,-4.121098,neighbor,16326763,Challenges for the policy representation when applying reinforcement learning in robotics,0.05467945337295532,#ffbb78
-2.2526731,-5.4105506,neighbor,16326763,Robot Skill Learning: From Reinforcement Learning to Evolution Strategies,0.05473148822784424,#ffbb78
-2.4080915,1.7163396,neighbor,16326763,V-MIN: Efficient Reinforcement Learning through Demonstrations and Relaxed Reward Demands,0.054947614669799805,#ffbb78
-10.687095,6.6683726,neighbor,16326763,Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret,0.055049359798431396,#ffbb78
5.1068096,-6.594502,neighbor,16326763,Policy Learning with Continuous Memory States for Partially Observed Robotic Control,0.05525410175323486,#ffbb78
0.15308604,5.558157,neighbor,16326763,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,0.05677604675292969,#ffbb78
2.3437645,4.483849,neighbor,16326763,Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping,0.05697476863861084,#ffbb78
3.226414,-4.7132087,neighbor,16326763,Towards Vision-Based Deep Reinforcement Learning for Robotic Motion Control,0.057264864444732666,#ffbb78
10.45498,-3.9454052,neighbor,16326763,Reinforcement learning for the soccer dribbling task,0.0582050085067749,#ffbb78
-1.2805635,0.19412771,neighbor,16326763,RTMBA: A Real-Time Model-Based Reinforcement Learning Architecture for robot control,0.05861008167266846,#ffbb78
-7.9061804,-1.8981565,neighbor,16326763,Optimism-driven exploration for nonlinear systems,0.05866271257400513,#ffbb78
-2.6949503,-8.78972,neighbor,16326763,Exploring parameter space in reinforcement learning,0.05997103452682495,#ffbb78
1.4569364,-3.970404,neighbor,16326763,Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning,0.060624897480010986,#ffbb78
-2.98197,-0.96374726,neighbor,16326763,Socially guided intrinsic motivation for robot learning of motor skills,0.06085991859436035,#ffbb78
-2.6882436,-11.894615,neighbor,16326763,Reinforcement Learning by Value Gradients,0.061433613300323486,#ffbb78
-5.304628,-2.716151,neighbor,16326763,Reinforcement learning with misspecified model classes,0.06152874231338501,#ffbb78
-0.46701398,2.7249198,neighbor,16326763,Safe Exploration of State and Action Spaces in Reinforcement Learning,0.06171417236328125,#ffbb78
1.0909954,-10.987069,neighbor,16326763,Trust Region Policy Optimization,0.06221693754196167,#ffbb78
-8.393643,-3.0087008,neighbor,16326763,Gaussian Processes for Data-Efficient Learning in Robotics and Control,0.06283164024353027,#ffbb78
-10.433342,2.2945082,neighbor,16326763,Multi-Task Policy Search,0.0629391074180603,#ffbb78
-10.894341,-4.5132985,neighbor,16326763,Model-based Path Integral Stochastic Control: A Bayesian Nonparametric Approach,0.06296265125274658,#ffbb78
7.215038,-1.8321154,neighbor,16326763,Distributed Deep Q-Learning,0.06302183866500854,#ffbb78
3.2581382,3.7931974,neighbor,16326763,Open-Loop Planning in Large-Scale Stochastic Domains,0.0631873607635498,#ffbb78
-1.4104857,12.128279,neighbor,16326763,Reinforcement Learning or Active Inference?,0.06349939107894897,#ffbb78
0.9054677,2.7115283,neighbor,16326763,Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration,0.06397390365600586,#ffbb78
1.0325903,-5.241484,neighbor,16326763,Fast Reinforcement Learning for Vision-guided Mobile Robots,0.06434166431427002,#ffbb78
-2.4680986,-11.495331,neighbor,16326763,"The Local Optimality of Reinforcement Learning by Value Gradients, and its Relationship to Policy Gradient Learning",0.06475299596786499,#ffbb78
-2.8183413,2.2818604,neighbor,16326763,Imitation Learning with Demonstrations and Shaping Rewards,0.06532931327819824,#ffbb78
-2.500541,5.6344852,neighbor,16326763,Expressing Arbitrary Reward Functions as Potential-Based Advice,0.065426766872406,#ffbb78
-12.0309725,-4.0299115,neighbor,16326763,GPU Based Path Integral Control with Learned Dynamics,0.06552296876907349,#ffbb78
-6.203198,9.552178,neighbor,16326763,Online Multi-Task Gradient Temporal-Difference Learning,0.06592792272567749,#ffbb78
-11.333357,1.4941113,neighbor,16326763,Learning Parameterized Skills,0.0659438967704773,#ffbb78
-4.34592,-6.0146737,neighbor,16326763,Combining Local and Global Direct Derivative-Free Optimization for Reinforcement Learning,0.06609469652175903,#ffbb78
8.861112,3.5638108,neighbor,16326763,Biologically plausible reinforcement learning of continuous actions,0.06698638200759888,#ffbb78
11.091576,-4.922148,neighbor,16326763,Off-Policy General Value Functions to Represent Dynamic Role Assignments in RoboCup 3D Soccer Simulation,0.06722688674926758,#ffbb78
1.7063323,-0.8006054,neighbor,16326763,Relational Reinforcement Learning with Continuous Actions by Combining Behavioural Cloning and Locally Weighted Regression,0.06739413738250732,#ffbb78
-5.6808853,-6.6664343,neighbor,16326763,Compulsory Flow Q-Learning: an RL algorithm for robot navigation based on partial-policy and macro-states,0.06761515140533447,#ffbb78
-0.2133735,7.6127315,neighbor,16326763,A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning,0.0676758885383606,#ffbb78
-1.1128229,3.8412678,neighbor,16326763,Scaling Up Reinforcement Learning through Targeted Exploration,0.06771057844161987,#ffbb78
-7.539753,0.59386796,neighbor,16326763,Bayesian exploration and interactive demonstration in continuous state MAXQ-learning,0.06793272495269775,#ffbb78
8.439618,3.996788,neighbor,16326763,Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer,0.06794476509094238,#ffbb78
-1.6389996,-9.048995,neighbor,16326763,Efficient Sample Reuse in Policy Gradients with Parameter-Based Exploration,0.06856900453567505,#ffbb78
-10.237293,7.158495,neighbor,16326763,Scaling life-long off-policy learning,0.06865906715393066,#ffbb78
0.35467723,9.752165,neighbor,16326763,Cover tree Bayesian reinforcement learning,0.0688067078590393,#ffbb78
-0.86890954,-12.5336895,neighbor,16326763,Policy gradient methods,0.06882685422897339,#ffbb78
-4.661374,2.7751,neighbor,16326763,Interactive Policy Learning through Confidence-Based Autonomy,0.06928485631942749,#ffbb78
-0.51660377,-2.2106037,neighbor,16326763,Metric State Space Reinforcement Learning for a Vision-Capable Mobile Robot,0.0693514347076416,#ffbb78
1.3101434,8.907729,neighbor,16326763,Monte Carlo Bayesian Reinforcement Learning,0.07009381055831909,#ffbb78
0.58623147,-0.13400486,neighbor,16326763,Smoothed Sarsa: Reinforcement learning for robot delivery tasks,0.07026219367980957,#ffbb78
-11.431761,-5.436639,neighbor,16326763,Adaptive Importance Sampling for Control and Inference,0.07036095857620239,#ffbb78
9.554173,2.2103486,neighbor,16326763,Scaled free-energy based reinforcement learning for robust and efficient learning in high-dimensional state spaces,0.07061922550201416,#ffbb78
-3.986184,-1.1631957,neighbor,16326763,Curiosity driven reinforcement learning for motion planning on humanoids,0.07127737998962402,#ffbb78
-0.92210037,-10.205743,neighbor,16326763,Efficient Gradient Estimation for Motor Control Learning,0.07141512632369995,#ffbb78
11.438963,-6.2223563,neighbor,16326763,Learning RoboCup-Keepaway with Kernels,0.07145494222640991,#ffbb78
-7.0628014,-8.796777,neighbor,16326763,Online State Elimination in Accelerated reinforcement Learning,0.07203865051269531,#ffbb78
6.9654403,1.6144555,neighbor,16326763,Maximum Entropy Deep Inverse Reinforcement Learning,0.07232815027236938,#ffbb78
-7.9154778,-9.349659,neighbor,16326763,Time manipulation technique for speeding up reinforcement learning in simulations,0.07251590490341187,#ffbb78
-5.0326066,7.0368547,neighbor,16326763,What good are actions? Accelerating learning using learned action priors,0.07280677556991577,#ffbb78
-9.94413,-1.9090718,neighbor,16326763,Modeling transition dynamics in MDPs with RKHS embeddings of conditional distributions,0.07315284013748169,#ffbb78
-1.1497096,5.192828,neighbor,16326763,Reward Shaping for Model-Based Bayesian Reinforcement Learning,0.07316017150878906,#ffbb78
-1.689572,13.097717,neighbor,16326763,Prospective Optimization,0.07322663068771362,#ffbb78
-10.004015,3.1482475,neighbor,16326763,Data-Efficient Generalization of Robot Skills with Contextual Policy Search,0.0734136700630188,#ffbb78
0.5428885,4.349559,neighbor,16326763,Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization,0.0734972357749939,#ffbb78
-5.1181445,2.4971502,neighbor,16326763,Policy transformation for learning from demonstration,0.07375580072402954,#ffbb78
-6.415375,5.303031,neighbor,16326763,Efficient Planning under Uncertainty with Macro-actions,0.07375609874725342,#ffbb78
-2.6162858,6.959848,neighbor,16326763,Off-Policy Shaping Ensembles in Reinforcement Learning,0.07378876209259033,#ffbb78
2.1166437,7.872364,neighbor,16326763,Reinforcement Learning with Parameterized Actions,0.07379859685897827,#ffbb78
-2.4511204,8.142654,neighbor,16326763,Linear Off-Policy Actor-Critic,0.07383650541305542,#ffbb78
11.584656,-2.882871,neighbor,16326763,The Self Organization of Context for Learning in MultiAgent Games,0.07384586334228516,#ffbb78
11.635668,-3.90495,neighbor,16326763,On Experiences in a Complex and Competitive Gaming Domain: Reinforcement Learning Meets RoboCup,0.073860764503479,#ffbb78
2.0648,9.840028,neighbor,16326763,Bayesian Learning of Noisy Markov Decision Processes,0.07412004470825195,#ffbb78
-3.6535327,-13.099365,neighbor,16326763,Actor-Critic Control with Reference Model Learning,0.0741349458694458,#ffbb78
2.2940578,6.7692018,neighbor,16326763,Exploring compact reinforcement-learning representations with linear regression,0.07430952787399292,#ffbb78
-0.9645937,-4.0345497,neighbor,16326763,Learning a DFT-based sequence with reinforcement learning: a NAO implementation,0.0751953125,#ffbb78
-8.315344,-4.1021743,neighbor,16326763,Efficient reinforcement learning for robots using informative simulated priors,0.07539701461791992,#ffbb78
-13.084631,-1.7503873,neighbor,16326763,Learning for control from multiple demonstrations,0.07578915357589722,#ffbb78
5.80796,7.910704,neighbor,16326763,Learning Symbolic Models of Stochastic Domains,0.07580775022506714,#ffbb78
8.236242,0.42445785,neighbor,16326763,Deep auto-encoder neural networks in reinforcement learning,0.07588231563568115,#ffbb78
-5.177419,-10.998797,neighbor,16326763,Constrained reinforcement learning from intrinsic and extrinsic rewards,0.07588553428649902,#ffbb78
-1.2287078,-7.749806,neighbor,16326763,Efficient Reuse of Previous Experiences to Improve Policies in Real Environment,0.07589226961135864,#ffbb78
-7.6579013,9.700372,neighbor,16326763,Multi-timescale nexting in a reinforcement learning robot,0.07607042789459229,#ffbb78
-3.8742895,0.5761245,neighbor,16326763,Sequence Learning by Backward Chaining in Synthetic Characters,0.076113760471344,#ffbb78
3.624347,5.4592505,neighbor,16326763,Integrating Sample-Based Planning and Model-Based Reinforcement Learning,0.07641476392745972,#ffbb78
3.4857755,-0.004336968,neighbor,16326763,Qualitative Planning with Quantitative Constraints for Online Learning of Robotic Behaviours,0.07647007703781128,#ffbb78
-0.87974447,-13.520403,neighbor,16326763,Gradient-based Reinforcement Planning in Policy-Search Methods,0.07652711868286133,#ffbb78
-7.1583595,3.8748355,neighbor,16326763,Policy search for multi-robot coordination under uncertainty,0.07679247856140137,#ffbb78
6.8765244,7.3364925,neighbor,16326763,Combining Learned Discrete and Continuous Action Models,0.0768936276435852,#ffbb78
-5.400569,9.501569,neighbor,16326763,Reducing Commitment to Tasks with Off-Policy Hierarchical Reinforcement Learning,0.07695269584655762,#ffbb78
4.5801454,8.176883,neighbor,16326763,Model-Based Bayesian Reinforcement Learning in Large Structured Domains,0.07696843147277832,#ffbb78
5.7181964,-9.804195,neighbor,16326763,A developmental agent for learning features environment models and general robotics tasks.,0.07708019018173218,#ffbb78
0.49359465,7.409627,query,1799558,Caffe: Convolutional Architecture for Fast Feature Embedding,0.0,#2ca02c
0.119490534,8.638008,neighbor,1799558,Fast Training of Convolutional Networks through FFTs,0.05532437562942505,#2ca02c
2.3102975,-1.3653988,neighbor,1799558,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.06113433837890625,#2ca02c
-1.043366,8.206266,neighbor,1799558,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.06295204162597656,#2ca02c
-3.345617,-8.574949,neighbor,1799558,Unsupervised Feature Learning by Deep Sparse Coding,0.06588739156723022,#2ca02c
-4.4450574,-1.0672628,neighbor,1799558,Convolutional Kernel Networks,0.06913506984710693,#2ca02c
10.493532,7.2668767,neighbor,1799558,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.06927651166915894,#2ca02c
3.529177,-8.699736,neighbor,1799558,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.06953483819961548,#2ca02c
4.846941,5.7983027,neighbor,1799558,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.07064604759216309,#2ca02c
12.091657,2.7381473,neighbor,1799558,Network In Network,0.07067102193832397,#2ca02c
14.735256,2.9400442,neighbor,1799558,Visualizing and Understanding Convolutional Networks,0.07194972038269043,#2ca02c
2.9432914,-2.1873224,neighbor,1799558,Learnable Pooling Regions for Image Classification,0.0725945234298706,#2ca02c
-2.0599096,9.945408,neighbor,1799558,One weird trick for parallelizing convolutional neural networks,0.07275116443634033,#2ca02c
2.978087,1.0402368,neighbor,1799558,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.07281064987182617,#2ca02c
10.89015,6.504921,neighbor,1799558,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.07287371158599854,#2ca02c
-2.5342245,10.971713,neighbor,1799558,Multi-GPU Training of ConvNets,0.07312589883804321,#2ca02c
11.346927,1.5884501,neighbor,1799558,Deep Epitomic Convolutional Neural Networks,0.07322961091995239,#2ca02c
-6.3015823,-10.804945,neighbor,1799558,Building high-level features using large scale unsupervised learning,0.07437920570373535,#2ca02c
-2.8501961,5.877479,neighbor,1799558,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.07588237524032593,#2ca02c
5.606734,-12.010318,neighbor,1799558,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.07623326778411865,#2ca02c
4.946929,3.9963017,neighbor,1799558,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.076457679271698,#2ca02c
-7.850633,-0.84905446,neighbor,1799558,Random Binary Mappings for Kernel Learning and Efficient SVM,0.07695072889328003,#2ca02c
-9.515126,-5.7270517,neighbor,1799558,Image Classification with the Fisher Vector: Theory and Practice,0.07709634304046631,#2ca02c
14.514559,4.290049,neighbor,1799558,Understanding Deep Architectures using a Recursive Convolutional Network,0.07837569713592529,#2ca02c
-4.468338,-9.372769,neighbor,1799558,Large-Scale Feature Learning With Spike-and-Slab Sparse Coding,0.07840138673782349,#2ca02c
6.330457,7.255812,neighbor,1799558,Maxout Networks,0.0784074068069458,#2ca02c
5.768439,8.996171,neighbor,1799558,Dropout Rademacher complexity of deep neural networks,0.07936155796051025,#2ca02c
8.472101,7.6160035,neighbor,1799558,Piecewise Linear Multilayer Perceptrons and Dropout,0.07964479923248291,#2ca02c
4.512691,-1.2786233,neighbor,1799558,Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models,0.07975775003433228,#2ca02c
-2.4147677,-7.5937,neighbor,1799558,Sparsity-Regularized HMAX for Visual Recognition,0.08013898134231567,#2ca02c
-7.937146,-11.087206,neighbor,1799558,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.08045893907546997,#2ca02c
0.7636104,-0.4838347,neighbor,1799558,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.08046340942382812,#2ca02c
7.9447474,3.36227,neighbor,1799558,Scheduled denoising autoencoders,0.08062916994094849,#2ca02c
2.979405,-5.2254567,neighbor,1799558,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.08119255304336548,#2ca02c
-2.8499758,-6.467356,neighbor,1799558,Efficient Learning of Sparse Invariant Representations,0.08143699169158936,#2ca02c
6.5340223,-11.5106,neighbor,1799558,Scalable Object Detection Using Deep Neural Networks,0.08163851499557495,#2ca02c
5.7401204,-0.9393673,neighbor,1799558,Differentiable Pooling for Hierarchical Feature Learning,0.0818108320236206,#2ca02c
-10.607884,-6.5653234,neighbor,1799558,Fisher and VLAD with FLAIR,0.08245491981506348,#2ca02c
-2.780298,13.515196,neighbor,1799558,Correction: A Hybrid CPU-GPU Accelerated Framework for Fast Mapping of High-Resolution Human Brain Connectome,0.08253157138824463,#2ca02c
-0.3791538,11.479121,neighbor,1799558,Scalable stacking and learning for building deep architectures,0.08275705575942993,#2ca02c
-8.835488,-6.474537,neighbor,1799558,Generalized Max Pooling,0.08277803659439087,#2ca02c
-3.4804556,10.23854,neighbor,1799558,GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training,0.08301389217376709,#2ca02c
4.7350073,-8.131158,neighbor,1799558,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.08323681354522705,#2ca02c
-8.075795,6.9002013,neighbor,1799558,Ускорение обучения нейронной сети для распознавания изображений с помощью технологии NVIDIA CUDA@@@Neural network training acceleration using NVIDIA CUDA technology for image recognition,0.08330309391021729,#2ca02c
-6.9076324,1.2905235,neighbor,1799558,Deep Multiple Kernel Learning,0.08457380533218384,#2ca02c
12.46005,7.718833,neighbor,1799558,Do Deep Nets Really Need to be Deep?,0.08531200885772705,#2ca02c
5.601292,6.722785,neighbor,1799558,Improving Deep Neural Networks with Probabilistic Maxout Units,0.08534491062164307,#2ca02c
10.952073,-6.1675606,neighbor,1799558,Feature Graph Architectures,0.08551013469696045,#2ca02c
-0.14300743,1.999863,neighbor,1799558,Fast image scanning with deep max-pooling convolutional neural networks,0.08613651990890503,#2ca02c
7.119965,-12.394055,neighbor,1799558,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.08621382713317871,#2ca02c
-0.037723288,-6.7238855,neighbor,1799558,Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition,0.08688968420028687,#2ca02c
-5.373434,-11.834283,neighbor,1799558,Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification,0.08689749240875244,#2ca02c
-6.1018443,-6.298839,neighbor,1799558,Fast Approximations to Structured Sparse Coding and Applications to Object Classification,0.08693987131118774,#2ca02c
-9.9088125,6.742668,neighbor,1799558,Computing OpenSURF on OpenCL and General Purpose GPU,0.08697360754013062,#2ca02c
-1.9367596,4.722018,neighbor,1799558,Multi-column deep neural networks for image classification,0.08699542284011841,#2ca02c
10.001004,-6.8158894,neighbor,1799558,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.08719760179519653,#2ca02c
-10.389824,5.965472,neighbor,1799558,Is the Game worth the Candle? - Evaluation of OpenCL for Object Detection Algorithm Optimization,0.08729064464569092,#2ca02c
5.5615144,-4.70785,neighbor,1799558,Unsupervised feature learning by augmenting single images,0.08754098415374756,#2ca02c
7.8782315,1.7742836,neighbor,1799558,Two SVDs produce more focal deep learning representations,0.08780872821807861,#2ca02c
-12.301483,6.1094284,neighbor,1799558,Mahotas: Open source software for scriptable computer vision,0.08795249462127686,#2ca02c
-5.5245748,1.0085628,neighbor,1799558,Large-Margin kNN Classification Using a Deep Encoder Network,0.08833998441696167,#2ca02c
-3.663901,-5.411597,neighbor,1799558,FastWavelet-Based Visual Classification,0.08837103843688965,#2ca02c
8.551505,4.857298,neighbor,1799558,An introduction to deep learning,0.08850818872451782,#2ca02c
7.393465,-1.7021722,neighbor,1799558,Deconvolutional networks,0.08873283863067627,#2ca02c
7.150836,-9.889919,neighbor,1799558,Deep learning for class-generic object detection,0.0892716646194458,#2ca02c
2.3002853,9.535742,neighbor,1799558,An Algorithm for Training Polynomial Networks,0.08955484628677368,#2ca02c
-7.219575,2.7657342,neighbor,1799558,Parallel multiclass stochastic gradient descent algorithms for classifying million images with very-high-dimensional signatures into thousands classes,0.08968281745910645,#2ca02c
-3.0465806,4.915667,neighbor,1799558,"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",0.09025543928146362,#2ca02c
2.4852655,-9.466093,neighbor,1799558,Neural Codes for Image Retrieval,0.09063708782196045,#2ca02c
-9.318106,-2.3582597,neighbor,1799558,Mercer kernels for object recognition with local features,0.09068655967712402,#2ca02c
-12.121761,-6.7158995,neighbor,1799558,Inverting and Visualizing Features for Object Detection,0.09098756313323975,#2ca02c
0.27350268,0.58714813,neighbor,1799558,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.09184050559997559,#2ca02c
-10.849427,-2.8648584,neighbor,1799558,Feature combination with Multi-Kernel Learning for fine-grained visual classification,0.09188926219940186,#2ca02c
8.326843,-4.807749,neighbor,1799558,Complexity of Representation and Inference in Compositional Models with Part Sharing,0.09195643663406372,#2ca02c
-3.8826349,-1.7199906,neighbor,1799558,Learning Invariant Representations with Local Transformations,0.09235173463821411,#2ca02c
8.140425,8.905531,neighbor,1799558,Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary Independent Stochastic Neurons,0.09251940250396729,#2ca02c
-5.8315864,-7.9791036,neighbor,1799558,Recklessly Approximate Sparse Coding,0.09270185232162476,#2ca02c
0.11379052,-3.7985344,neighbor,1799558,A neural computational model for bottom-up attention with invariant and overcomplete representation,0.09284943342208862,#2ca02c
-11.720521,5.6643124,neighbor,1799558,Experiences Using SciPy for Computer Vision Research,0.09302669763565063,#2ca02c
12.493067,4.164883,neighbor,1799558,An Analysis of the Connections Between Layers of Deep Neural Networks,0.09351241588592529,#2ca02c
2.366911,-3.5999854,neighbor,1799558,Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences,0.09352630376815796,#2ca02c
-7.463759,-4.9159107,neighbor,1799558,CSIFT based locality-constrained linear coding for image classification,0.0935295820236206,#2ca02c
8.172863,11.016332,neighbor,1799558,Improving neural networks by preventing co-adaptation of feature detectors,0.09353893995285034,#2ca02c
-1.4465688,-5.767722,neighbor,1799558,Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning?,0.0935600996017456,#2ca02c
6.8835497,10.686342,neighbor,1799558,Dropout Training as Adaptive Regularization,0.09365659952163696,#2ca02c
7.5930343,-13.710789,neighbor,1799558,BING: Binarized normed gradients for objectness estimation at 300fps,0.0936577320098877,#2ca02c
-7.339613,-11.93983,neighbor,1799558,Learning Deep Face Representation,0.09385740756988525,#2ca02c
9.820968,5.064893,neighbor,1799558,Joint Training of Deep Boltzmann Machines,0.09395730495452881,#2ca02c
10.731896,9.486245,neighbor,1799558,Discriminative Recurrent Sparse Auto-Encoders,0.09412342309951782,#2ca02c
-2.4459944,-3.2831876,neighbor,1799558,Deep Predictive Coding Networks,0.09418338537216187,#2ca02c
8.547412,-1.6041951,neighbor,1799558,Replicating Kernels with a Short Stride Allows Sparse Reconstructions with Fewer Independent Kernels,0.09458708763122559,#2ca02c
-2.5665596,-10.978233,neighbor,1799558,No more meta-parameter tuning in unsupervised sparse feature learning,0.0946512222290039,#2ca02c
-1.1870353,-8.128651,neighbor,1799558,Efficient Visual Coding: From Retina To V2,0.09471333026885986,#2ca02c
8.152477,-10.76378,neighbor,1799558,Generic Object Detection with Dense Neural Patterns and Regionlets,0.09484875202178955,#2ca02c
-4.5499463,10.890886,neighbor,1799558,Exploring the power of GPU's for training Polyglot language models,0.09509491920471191,#2ca02c
7.2561493,5.536986,neighbor,1799558,Avoiding pathologies in very deep networks,0.09519302845001221,#2ca02c
13.348055,5.378441,neighbor,1799558,Predicting Parameters in Deep Learning,0.09551781415939331,#2ca02c
-11.581198,-3.3815508,neighbor,1799558,Heterogeneous feature machines for visual recognition,0.09580564498901367,#2ca02c
-10.513633,0.9773168,neighbor,1799558,Random forest-LNS architecture and vision,0.09585946798324585,#2ca02c
-10.315705,-10.531782,neighbor,1799558,Effective discretization of Gabor features for real-time face detection,0.0959894061088562,#2ca02c
6.7081823,-8.459188,neighbor,1799558,Fine-grained object recognition with Gnostic Fields,0.09603410959243774,#2ca02c
-9.791339,-1.8760431,query,19011676,pROC: an open-source package for R and S+ to analyze and compare ROC curves,0.0,#2ca02c
-10.025804,-2.5335999,neighbor,19011676,ROCR: visualizing classifier performance in R,0.05091893672943115,#2ca02c
-5.456083,5.6138043,neighbor,19011676,A Handbook of Statistical Analyses Using R,0.05539596080780029,#2ca02c
-6.1772995,6.0543575,neighbor,19011676,SAS and R,0.06398123502731323,#2ca02c
-5.0349874,4.7852335,neighbor,19011676,reporttools: R Functions to Generate LaTeX Tables of Descriptive Statistics,0.0647655725479126,#2ca02c
-6.412114,5.7094707,neighbor,19011676,R in a Nutshell,0.06587499380111694,#2ca02c
-7.4115825,4.249974,neighbor,19011676,Caching and Distributing Statistical Analyses in R,0.06860840320587158,#2ca02c
-11.321983,2.2258503,neighbor,19011676,kernlab - An S4 Package for Kernel Methods in R,0.07185345888137817,#2ca02c
-6.7938933,7.28505,neighbor,19011676,Computational Statistics: An Introduction to R,0.07220286130905151,#2ca02c
-2.7864642,8.528919,neighbor,19011676,"An Introduction to the Special Volume on ""Psychometrics in R""",0.07351386547088623,#2ca02c
-4.215217,3.6369915,neighbor,19011676,Beanplot: A Boxplot Alternative for Visual Comparison of Distributions,0.07364898920059204,#2ca02c
-4.5264606,1.8830398,neighbor,19011676,FactoMineR: An R Package for Multivariate Analysis,0.07365584373474121,#2ca02c
-5.6963496,7.8385515,neighbor,19011676,Data Manipulation with R,0.07368594408035278,#2ca02c
-7.4192348,5.808228,neighbor,19011676,The R Commander: A Basic-Statistics Graphical User Interface to R,0.07407689094543457,#2ca02c
10.67581,-1.1486677,neighbor,19011676,The Medical Algorithms Project,0.07451236248016357,#2ca02c
-8.373855,6.2832355,neighbor,19011676,CGIwithR: Facilities for processing web forms using R,0.07489758729934692,#2ca02c
1.0901221,5.3767505,neighbor,19011676,Conducting Meta-Analyses in R with the metafor Package,0.07505381107330322,#2ca02c
-12.497298,2.6254554,neighbor,19011676,sparr: Analyzing Spatial Relative Risk Using Fixed and Adaptive Kernel Density Estimation in R,0.07533878087997437,#2ca02c
13.355099,-1.1620576,neighbor,19011676,Preface,0.07560062408447266,#2ca02c
4.864586,-2.497178,neighbor,19011676,Information for Authors,0.07581210136413574,#2ca02c
-5.6568365,3.4541326,neighbor,19011676,An Object-Oriented Framework for Statistical Simulation: The R Package simFrame,0.07592540979385376,#2ca02c
-5.869603,7.1868243,neighbor,19011676,Robust Statistical Methods With R,0.07622265815734863,#2ca02c
-2.5543423,2.4852898,neighbor,19011676,Effect Displays in R for Generalised Linear Models,0.07668215036392212,#2ca02c
10.749025,0.50649697,neighbor,19011676,Erratum,0.07812583446502686,#2ca02c
10.749025,0.50649697,neighbor,19011676,Erratum,0.07812583446502686,#2ca02c
6.992319,0.33932057,neighbor,19011676,Acknowledgement of Reviewers,0.07908082008361816,#2ca02c
6.1020737,-1.4871411,neighbor,19011676,Reviewers,0.07930028438568115,#2ca02c
-6.5159326,4.964035,neighbor,19011676,Toward a Common Framework for Statistical Analysis and Development,0.0796058177947998,#2ca02c
6.0728693,-1.2545004,neighbor,19011676,Reviewers,0.07989788055419922,#2ca02c
-10.088658,-3.169385,neighbor,19011676,"ROC curve, lift chart and calibration plot",0.08018243312835693,#2ca02c
-5.375742,0.88886595,neighbor,19011676,Principal Component Analysis,0.08053982257843018,#2ca02c
-9.54889,-4.8163743,neighbor,19011676,A boosting method for maximizing the partial area under the ROC curve,0.08097738027572632,#2ca02c
-9.976668,1.2842491,neighbor,19011676,Support Vector Machines in R,0.08147257566452026,#2ca02c
-9.597798,5.177105,neighbor,19011676,fgui: A Method for Automatically Creating Graphical User Interfaces for Command-Line R Packages.,0.08193117380142212,#2ca02c
14.86733,2.2025542,neighbor,19011676,Retraction,0.08229720592498779,#2ca02c
-3.5729222,3.8968868,neighbor,19011676,ggplot2: Elegant Graphics for Data Analysis,0.08314734697341919,#2ca02c
12.168445,0.9418291,neighbor,19011676,Erratum,0.08352696895599365,#2ca02c
-3.5762622,4.5651817,neighbor,19011676,Graphics for Statistics and Data Analysis with R,0.08354616165161133,#2ca02c
5.025126,-1.1911027,neighbor,19011676,Acknowledgement of Reviewers,0.08383268117904663,#2ca02c
13.072149,2.6489398,neighbor,19011676,Erratum,0.08395546674728394,#2ca02c
10.520557,3.1369317,neighbor,19011676,Correction,0.08403128385543823,#2ca02c
-2.6271622,0.35411024,neighbor,19011676,glmulti: An R Package for Easy Automated Model Selection with (Generalized) Linear Models,0.08422893285751343,#2ca02c
13.234516,1.8669134,neighbor,19011676,Erratum,0.0844690203666687,#2ca02c
6.4789147,-1.0878271,neighbor,19011676,Reviewers,0.0844954252243042,#2ca02c
12.486404,2.1647203,neighbor,19011676,Errata,0.08453881740570068,#2ca02c
-7.153744,2.3297734,neighbor,19011676,pinktoe: Semi-automatic Traversal of Trees,0.08463472127914429,#2ca02c
-0.43329087,3.3839128,neighbor,19011676,Quasi-variances in Xlisp-Stat and on the web,0.08469301462173462,#2ca02c
-9.4448185,5.1787124,neighbor,19011676,RGtk2: A Graphical User Interface Toolkit for R,0.08483028411865234,#2ca02c
-2.093078,0.412469,neighbor,19011676,Hierarchical Generalized Linear Models: The R Package HGLMMM,0.08523678779602051,#2ca02c
-11.691044,2.326377,neighbor,19011676,ks: Kernel Density Estimation and Kernel Discriminant Analysis for Multivariate Data in R,0.08547550439834595,#2ca02c
-3.9992435,5.3773994,neighbor,19011676,Quality Control for Statistical Graphics: The graphicsQC Package for R,0.08551007509231567,#2ca02c
-8.649478,8.968642,neighbor,19011676,R Programs for Truncated Distributions,0.08574026823043823,#2ca02c
-2.6291947,-1.1012028,neighbor,19011676,FlexMix: A general framework for finite mixture models and latent class regression in R,0.08599042892456055,#2ca02c
5.1884694,-4.811573,neighbor,19011676,Poster,0.08646541833877563,#2ca02c
-3.615479,8.459247,neighbor,19011676,"Econometrics in R: Past, Present, and Future",0.08659195899963379,#2ca02c
12.838359,0.6219321,neighbor,19011676,Erratum,0.08676260709762573,#2ca02c
9.387338,1.8427219,neighbor,19011676,Correction,0.08676391839981079,#2ca02c
-2.290608,2.4075968,neighbor,19011676,Effect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package,0.08685034513473511,#2ca02c
-10.81858,-3.2290325,neighbor,19011676,A critical analysis of variants of the AUC,0.08691906929016113,#2ca02c
-0.8160677,7.6431212,neighbor,19011676,rpartOrdinal: An R Package for Deriving a Classification Tree for Predicting an Ordinal Response.,0.08715617656707764,#2ca02c
5.338144,-0.19757378,neighbor,19011676,Thanks to Reviewers,0.08724170923233032,#2ca02c
5.8042345,-3.7783878,neighbor,19011676,Sponsors,0.08770400285720825,#2ca02c
11.8525305,1.7845963,neighbor,19011676,Erratum,0.08772468566894531,#2ca02c
4.8370337,-0.5210266,neighbor,19011676,Guest Reviewers,0.08827954530715942,#2ca02c
5.524746,-5.623767,neighbor,19011676,Preface,0.08836591243743896,#2ca02c
-8.119037,5.0714827,neighbor,19011676,RinRuby: Accessing the R Interpreter from Pure Ruby,0.08840930461883545,#2ca02c
13.925327,1.391012,neighbor,19011676,Erratum,0.08854174613952637,#2ca02c
7.948778,-6.213425,neighbor,19011676,Committees,0.08854234218597412,#2ca02c
7.286213,-6.949389,neighbor,19011676,Committees,0.08854234218597412,#2ca02c
5.991994,0.67341316,neighbor,19011676,List of Reviewers,0.0885920524597168,#2ca02c
5.8083572,-5.522322,neighbor,19011676,Preface,0.08863329887390137,#2ca02c
-4.5429277,6.761742,neighbor,19011676,A Modern Approach to Regression with R,0.08871889114379883,#2ca02c
6.961737,-6.3910675,neighbor,19011676,Committees,0.08875900506973267,#2ca02c
-8.056198,-2.2569351,neighbor,19011676,Applications of Machine Learning in Cancer Prediction and Prognosis,0.08899152278900146,#2ca02c
-6.156282,-2.7017229,neighbor,19011676,ofw: An R Package to Select Continuous Variables for Multiclass Classification with a Stochastic Wrapper Method,0.08899182081222534,#2ca02c
-6.9908385,10.328507,neighbor,19011676,Generalized and Customizable Sets in R,0.08902943134307861,#2ca02c
4.1108637,-6.4660764,neighbor,19011676,Table of Contents,0.08914577960968018,#2ca02c
4.1128044,-6.4672747,neighbor,19011676,Table of contents,0.08914577960968018,#2ca02c
-1.5465789,4.972793,neighbor,19011676,A Visual Basic Software for Computing Fisher\'s Exact Probability,0.08923280239105225,#2ca02c
13.934377,2.3338625,neighbor,19011676,Erratum,0.08936983346939087,#2ca02c
-3.5509331,-0.4558482,neighbor,19011676,Generalized Additive Models: An Introduction with R,0.08980375528335571,#2ca02c
-9.618623,6.779849,neighbor,19011676,R2WinBUGS: A Package for Running WinBUGS from R,0.09009629487991333,#2ca02c
9.654077,0.76116496,neighbor,19011676,CORRECTION,0.09024029970169067,#2ca02c
7.046561,-5.8526855,neighbor,19011676,Committees,0.09030693769454956,#2ca02c
-4.077335,10.865266,neighbor,19011676,clues: An R Package for Nonparametric Clustering Based on Local Shrinking,0.09039217233657837,#2ca02c
12.804942,1.3111048,neighbor,19011676,Erratum,0.09046781063079834,#2ca02c
13.7148075,0.89184606,neighbor,19011676,Erratum,0.09087264537811279,#2ca02c
3.055654,-3.8706913,neighbor,19011676,ACKNOWLEDGEMENTS,0.09088385105133057,#2ca02c
4.0829773,0.27076095,neighbor,19011676,Acknowledgment to reviewers,0.091114342212677,#2ca02c
-6.081881,-2.607012,neighbor,19011676,Feature Selection with the Boruta Package,0.09118223190307617,#2ca02c
-8.395139,-4.7296515,neighbor,19011676,BART: Bayesian Additive Regression Trees,0.09122002124786377,#2ca02c
7.032832,-3.8063185,neighbor,19011676,Acknowledgments,0.09142231941223145,#2ca02c
6.1473117,-4.3067083,neighbor,19011676,Sponsors,0.09158670902252197,#2ca02c
-1.3314185,0.65441495,neighbor,19011676,The R Package geepack for Generalized Estimating Equations,0.09160476922988892,#2ca02c
-5.466606,9.034407,neighbor,19011676,Reshaping Data with the reshape Package,0.09161484241485596,#2ca02c
7.8370647,-6.682829,neighbor,19011676,Committees,0.09161561727523804,#2ca02c
-8.8297415,-4.535186,neighbor,19011676,ada: An R Package for Stochastic Boosting,0.09174907207489014,#2ca02c
7.8104334,-5.338834,neighbor,19011676,Program Committee,0.09221071004867554,#2ca02c
-3.0208018,6.079617,neighbor,19011676,Importing Vector Graphics: The grImport Package for R,0.09231418371200562,#2ca02c
-7.245133,8.120066,neighbor,19011676,Statistical Methods for Communication Science,0.09239321947097778,#2ca02c
3.6581652,-3.485024,neighbor,19011676,Acknowledgements,0.09247517585754395,#2ca02c
1.6821189,-4.6110992,query,1957433,GloVe: Global Vectors for Word Representation,0.0,#2ca02c
0.94020617,-3.8359334,neighbor,1957433,Efficient Estimation of Word Representations in Vector Space,0.04058122634887695,#2ca02c
2.888215,-5.1733675,neighbor,1957433,Learning Word Representations with Hierarchical Sparse Coding,0.04780977964401245,#2ca02c
-1.5746648,-2.4468126,neighbor,1957433,Linguistic Regularities in Sparse and Explicit Word Representations,0.04969596862792969,#2ca02c
2.313795,-3.008717,neighbor,1957433,Dependency-Based Word Embeddings,0.0545961856842041,#2ca02c
-4.919626,-0.78459054,neighbor,1957433,Community Evaluation and Exchange of Word Vectors at wordvectors.org,0.05526447296142578,#2ca02c
0.7386004,-5.560674,neighbor,1957433,Word Embeddings through Hellinger PCA,0.055834054946899414,#2ca02c
4.431232,-3.8575106,neighbor,1957433,Factored Neural Language Models,0.05586230754852295,#2ca02c
-3.0134163,-5.3013844,neighbor,1957433,Inducing Language Networks from Continuous Space Word Representations,0.05811607837677002,#2ca02c
-0.07451261,-3.8787365,neighbor,1957433,WordRep: A Benchmark for Research on Learning Word Representations,0.06071972846984863,#2ca02c
-1.3297559,-7.768761,neighbor,1957433,Polyglot: Distributed Word Representations for Multilingual NLP,0.061060428619384766,#2ca02c
-6.0319896,0.18709576,neighbor,1957433,From Frequency to Meaning: Vector Space Models of Semantics,0.0644713044166565,#2ca02c
-1.8072283,-10.161677,neighbor,1957433,Multilingual Distributed Representations without Word Alignment,0.06576639413833618,#2ca02c
2.8847823,-3.438955,neighbor,1957433,word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method,0.0659286379814148,#2ca02c
6.7087803,-0.06533435,neighbor,1957433,A Classification Approach to Word Prediction,0.0661383867263794,#2ca02c
6.780297,-9.144122,neighbor,1957433,An alternative text representation to TF-IDF and Bag-of-Words,0.06641674041748047,#2ca02c
-1.0304813,0.41177794,neighbor,1957433,Improving Lexical Embeddings with Semantic Knowledge,0.06691926717758179,#2ca02c
-0.27739993,-2.539233,neighbor,1957433,Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds,0.06754350662231445,#2ca02c
-10.757825,4.1316805,neighbor,1957433,Multi-Relational Latent Semantic Analysis,0.06780767440795898,#2ca02c
-2.5169976,1.9335141,neighbor,1957433,Improving Distributional Semantic Vectors through Context Selection and Normalisation,0.06782710552215576,#2ca02c
-4.1791925,5.0200887,neighbor,1957433,Identifying semantic relations in a specialized corpus through distributional analysis of a cooccurrence tensor,0.06785887479782104,#2ca02c
-1.779609,1.380828,neighbor,1957433,"Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",0.06805747747421265,#2ca02c
7.2227497,-7.985894,neighbor,1957433,Distributed Representations of Sentences and Documents,0.07007026672363281,#2ca02c
10.831407,-0.6022897,neighbor,1957433,Nonlocal Language Modeling based on Context Co-occurrence Vectors,0.07030361890792847,#2ca02c
-3.7063315,4.2466617,neighbor,1957433,Distributional Memory: A General Framework for Corpus-Based Semantics,0.07128888368606567,#2ca02c
4.18236,-5.921091,neighbor,1957433,Using CCA to improve CCA: A new spectral method for estimating vector models of words,0.07211261987686157,#2ca02c
-6.8923497,-3.714857,neighbor,1957433,Language without words: A pointillist model for natural language processing,0.0727243423461914,#2ca02c
-3.2212467,-13.592429,neighbor,1957433,Exploiting Similarities among Languages for Machine Translation,0.07314854860305786,#2ca02c
-1.9260936,-12.692723,neighbor,1957433,Learning Multilingual Word Representations using a Bag-of-Words Autoencoder,0.07358711957931519,#2ca02c
-4.599082,8.90976,neighbor,1957433,Probabilistic Modeling of Joint-context in Distributional Similarity,0.0736115574836731,#2ca02c
0.5204948,1.7006565,neighbor,1957433,Improving sparse word similarity models with asymmetric measures,0.0737462043762207,#2ca02c
4.4773927,2.1152067,neighbor,1957433,Improving the Lexical Function Composition Model with Pathwise Optimized Elastic-Net Regression,0.07399362325668335,#2ca02c
-2.0671043,-0.42848054,neighbor,1957433,Learning Grounded Meaning Representations with Autoencoders,0.07422828674316406,#2ca02c
-7.565299,4.420058,neighbor,1957433,Expressing Implicit Semantic Relations without Supervision,0.07439190149307251,#2ca02c
-0.8687598,-6.7758746,neighbor,1957433,The Expressive Power of Word Embeddings,0.0745123028755188,#2ca02c
0.69192654,-10.365361,neighbor,1957433,A Multiplicative Model for Learning Distributed Text-Based Attribute Representations,0.07456094026565552,#2ca02c
-5.4849725,4.675649,neighbor,1957433,Using a high-dimensional graph of semantic space to model relationships among words,0.07484686374664307,#2ca02c
-8.03906,6.7319016,neighbor,1957433,Measuring Similarity from Word Pair Matrices with Syntagmatic and Paradigmatic Associations,0.07501959800720215,#2ca02c
3.2915814,0.49117535,neighbor,1957433,Prior Disambiguation of Word Tensors for Constructing Sentence Vectors,0.07573884725570679,#2ca02c
-2.3727252,-9.874418,neighbor,1957433,Multilingual Models for Compositional Distributed Semantics,0.07619792222976685,#2ca02c
3.0113046,-0.07122914,neighbor,1957433,Evaluating Neural Word Representations in Tensor-Based Compositional Settings,0.07621079683303833,#2ca02c
-4.278048,-2.6121457,neighbor,1957433,Deriving Adjectival Scales from Continuous Space Word Representations,0.07633614540100098,#2ca02c
-0.29156366,8.939253,neighbor,1957433,Random Walks for Text Semantic Similarity,0.07680165767669678,#2ca02c
-3.8424735,3.1018252,neighbor,1957433,SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation,0.0770576000213623,#2ca02c
-7.1033406,6.1656704,neighbor,1957433,Distributional Semantics Beyond Words: Supervised Learning of Analogy and Paraphrase,0.07711392641067505,#2ca02c
0.57180804,-7.887955,neighbor,1957433,An Exploration of Embeddings for Generalized Phrases,0.07714951038360596,#2ca02c
-8.763222,5.219782,neighbor,1957433,Measuring Semantic Similarity by Latent Relational Analysis,0.07719594240188599,#2ca02c
-2.771441,-11.690233,neighbor,1957433,Learning Bilingual Word Representations by Marginalizing Alignments,0.07742542028427124,#2ca02c
3.15633,-12.094195,neighbor,1957433,Compositional Morphology for Word Representations and Language Modelling,0.07749253511428833,#2ca02c
-7.9433484,9.718019,neighbor,1957433,Distributional Similarity for Chinese: Exploiting Characters and Radicals,0.07767784595489502,#2ca02c
-2.3409755,-2.6214576,neighbor,1957433,KNET: A General Framework for Learning Word Embedding using Morphological Knowledge,0.07775014638900757,#2ca02c
-3.7125103,10.042306,neighbor,1957433,AI-KU: Using Co-Occurrence Modeling for Semantic Similarity,0.07807844877243042,#2ca02c
5.3521233,-3.1619394,neighbor,1957433,Factorial Hidden Markov Models for Learning Representations of Natural Language,0.07812172174453735,#2ca02c
-4.76972,11.830848,neighbor,1957433,Improving Pointwise Mutual Information (PMI) by Incorporating Significant Co-occurrence,0.07826244831085205,#2ca02c
-0.6090609,4.9565444,neighbor,1957433,Using Curvature and Markov Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination,0.07831203937530518,#2ca02c
2.4609146,6.158054,neighbor,1957433,Semantic Word Cloud Representations: Hardness and Approximation Algorithms,0.07878148555755615,#2ca02c
-1.4116031,2.9916356,neighbor,1957433,Predicting sense convergence with distributional semantics: an application to the CogaLex 2014 shared task,0.07908540964126587,#2ca02c
9.018271,-9.4564705,neighbor,1957433,Sequential Document Representations and Simplicial Curves,0.07912158966064453,#2ca02c
-8.051499,0.7305878,neighbor,1957433,A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness,0.07955008745193481,#2ca02c
-0.46793085,-7.873902,neighbor,1957433,Substitute Based SCODE Word Embeddings in Supervised NLP Tasks,0.07993394136428833,#2ca02c
-4.7712283,6.0408597,neighbor,1957433,A Context-Theoretic Framework for Compositionality in Distributional Semantics,0.08005034923553467,#2ca02c
-5.821113,9.017195,neighbor,1957433,A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches,0.08032494783401489,#2ca02c
-9.186509,7.402656,neighbor,1957433,Word Vectors and Two Kinds of Similarity,0.08042603731155396,#2ca02c
-1.2568495,-11.526838,neighbor,1957433,Distributed Word Representation Learning for Cross-Lingual Dependency Parsing,0.08066999912261963,#2ca02c
-1.0861305,9.428795,neighbor,1957433,Discriminative Improvements to Distributional Sentence Similarity,0.08072131872177124,#2ca02c
-9.186661,10.99855,neighbor,1957433,Google distance between words,0.08075833320617676,#2ca02c
-2.4948082,-12.9885025,neighbor,1957433,An Autoencoder Approach to Learning Bilingual Word Representations,0.08149641752243042,#2ca02c
-0.7849505,-4.8195887,neighbor,1957433,Pre-Trained Multi-View Word Embedding Using Two-Side Neural Network,0.082683265209198,#2ca02c
-3.829198,-9.083768,neighbor,1957433,Distributed Representations of Geographically Situated Language,0.08285677433013916,#2ca02c
-1.0529541,5.617425,neighbor,1957433,A Computational Model to Disentangle Semantic Information Embedded in Word Association Norms,0.08289515972137451,#2ca02c
10.004622,-7.9217134,neighbor,1957433,Fast logistic regression for text categorization with variable-length n-grams,0.08324402570724487,#2ca02c
7.1078124,-5.466394,neighbor,1957433,Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification,0.08329331874847412,#2ca02c
10.223108,-0.32084897,neighbor,1957433,A Dynamic Language Model Based on Individual Word Domains,0.08342039585113525,#2ca02c
6.3886485,-7.164752,neighbor,1957433,Semantic Vector Machines,0.08349156379699707,#2ca02c
0.7955718,-0.780993,neighbor,1957433,Deep Learning from Web-Scale Corpora for Better Dictionary Interfaces,0.0835944414138794,#2ca02c
-9.71554,5.229591,neighbor,1957433,Human-Level Performance on Word Analogy Questions by Latent Relational Analysis,0.08369320631027222,#2ca02c
9.319515,-10.250366,neighbor,1957433,Fine: Information embedding for document classification,0.08420103788375854,#2ca02c
9.583675,-3.1499867,neighbor,1957433,Temporal Analysis of Language through Neural Language Models,0.08473944664001465,#2ca02c
8.499203,-5.944652,neighbor,1957433,Blinov: Distributed Representations of Words for Aspect-Based Sentiment Analysis at SemEval 2014,0.08499383926391602,#2ca02c
-9.3588295,4.740626,neighbor,1957433,Similarity of Semantic Relations,0.0850229263305664,#2ca02c
-5.017316,9.17665,neighbor,1957433,A General Framework for Distributional Similarity,0.08502441644668579,#2ca02c
-5.3220615,1.9513736,neighbor,1957433,A Systematic Study of Semantic Vector Space Model Parameters,0.08513540029525757,#2ca02c
-6.238226,11.889576,neighbor,1957433,Frequency Estimates for Statistical Word Similarity Measures,0.08514028787612915,#2ca02c
5.799379,1.3486708,neighbor,1957433,Semantic Composition and Decomposition: From Recognition to Generation,0.08520275354385376,#2ca02c
-4.9125233,10.331079,neighbor,1957433,Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity,0.08522748947143555,#2ca02c
-8.863945,10.634817,neighbor,1957433,The Google Similarity Distance,0.08527916669845581,#2ca02c
-11.687278,1.070609,neighbor,1957433,Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors,0.08536571264266968,#2ca02c
4.656301,-8.521456,neighbor,1957433,Fused Feature Representation Discovery for High-Dimensional and Sparse Data,0.08571070432662964,#2ca02c
-3.2257671,12.272202,neighbor,1957433,Feature Weighting for Co-occurrence-based Classification of Words,0.08575552701950073,#2ca02c
4.479356,-12.403331,neighbor,1957433,Language Modeling with Power Low Rank Ensembles,0.08578550815582275,#2ca02c
7.208188,-10.0428,neighbor,1957433,Sparse Topical Coding,0.08583635091781616,#2ca02c
10.413288,0.9424288,neighbor,1957433,A multi-class approach for modelling out-of-vocabulary words,0.08586388826370239,#2ca02c
-8.7960615,2.3259072,neighbor,1957433,Measuring semantic relatedness with vector space models and random walks,0.08629798889160156,#2ca02c
-3.1590533,7.371018,neighbor,1957433,Text: now in 2D! A framework for lexical expansion with contextual similarity,0.08665484189987183,#2ca02c
7.8517637,0.1411624,neighbor,1957433,Methods to Integrate a Language Model with Semantic Information for a Word Prediction Component,0.0867609977722168,#2ca02c
2.8540952,-7.143268,neighbor,1957433,Learning Preferences with Millions of Parameters by Enforcing Sparsity,0.08680504560470581,#2ca02c
-6.1311,10.194971,neighbor,1957433,Distributional measures of concept-distance: A task-oriented evaluation,0.08695924282073975,#2ca02c
5.921661,-11.817971,neighbor,1957433,zipfR: Word Frequency Modeling in R,0.08701664209365845,#2ca02c
7.1228848,-0.78072953,neighbor,1957433,Adaptive Language Modeling for Word Prediction,0.08702695369720459,#2ca02c
-7.642997,-0.30692935,neighbor,1957433,An Introduction to Random Indexing,0.08721321821212769,#2ca02c
0.9478227,4.5532713,neighbor,1957433,Infer the Semantic Orientation of Words by Optimizing Modularity,0.08745557069778442,#2ca02c
1.3085622,-3.631405,query,201646309,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,0.0,#98df8a
4.754395,-0.98843867,neighbor,201646309,Learning Sentence Embeddings Based on Weighted Contexts from Unlabelled Data,0.03865534067153931,#98df8a
1.4084375,-4.6142573,neighbor,201646309,Direct Network Transfer: Transfer Learning of Sentence Embeddings for Semantic Similarity,0.039521098136901855,#98df8a
7.074687,-3.8183832,neighbor,201646309,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,0.042827725410461426,#98df8a
-5.6580706,4.974126,neighbor,201646309,UMDeep at SemEval-2017 Task 1: End-to-End Shared Weight LSTM Model for Semantic Textual Similarity,0.043057024478912354,#98df8a
0.3104305,-0.5620657,neighbor,201646309,ACV-tree: A New Method for Sentence Similarity Modeling,0.043305039405822754,#98df8a
-3.0639944,5.5505404,neighbor,201646309,"LIPN-IIMAS at SemEval-2017 Task 1: Subword Embeddings, Attention Recurrent Neural Networks and Cross Word Alignment for Semantic Textual Similarity",0.04339057207107544,#98df8a
-0.5752386,6.4185,neighbor,201646309,UNBNLP at SemEval-2016 Task 1: Semantic Textual Similarity: A Unified Framework for Semantic Processing and Evaluation,0.043742835521698,#98df8a
-0.003774371,-0.17883728,neighbor,201646309,An Efficient Framework for Sentence Similarity Modeling,0.044389545917510986,#98df8a
3.6655827,4.6400576,neighbor,201646309,Correlation Coefficients and Semantic Textual Similarity,0.04499483108520508,#98df8a
2.4670148,-5.3101363,neighbor,201646309,Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks,0.04568988084793091,#98df8a
-3.0454392,9.982506,neighbor,201646309,ITNLP-AiKF at SemEval-2017 Task 1: Rich Features Based SVR for Semantic Textual Similarity Computing,0.0467570424079895,#98df8a
-4.0431824,-7.2650304,neighbor,201646309,Matching Natural Language Sentences with Hierarchical Sentence Factorization,0.047400474548339844,#98df8a
-1.7279029,9.988584,neighbor,201646309,ECNU at SemEval-2016 Task 1: Leveraging Word Embedding From Macro and Micro Views to Boost Performance for Semantic Textual Similarity,0.04750692844390869,#98df8a
-2.3556588,4.637681,neighbor,201646309,ASOBEK at SemEval-2016 Task 1: Sentence Representation with Character N-gram Embeddings for Semantic Textual Similarity,0.047778308391571045,#98df8a
6.2500544,-4.2500176,neighbor,201646309,Exploring Semantic Properties of Sentence Embeddings,0.04846298694610596,#98df8a
-4.345965,4.3854713,neighbor,201646309,Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model,0.04876101016998291,#98df8a
-3.89664,10.811815,neighbor,201646309,PurdueNLP at SemEval-2017 Task 1: Predicting Semantic Textual Similarity with Paraphrase and Event Embeddings,0.0495375394821167,#98df8a
-0.73288953,11.995649,neighbor,201646309,DCU-SEManiacs at SemEval-2016 Task 1: Synthetic Paragram Embeddings for Semantic Textual Similarity,0.04971104860305786,#98df8a
5.594975,-4.5330157,neighbor,201646309,Representing Sentences as Low-Rank Subspaces,0.049740493297576904,#98df8a
-1.0266958,-4.3447256,neighbor,201646309,Learning Thematic Similarity Metric from Article Sections Using Triplet Networks,0.04998201131820679,#98df8a
-5.5243716,7.028282,neighbor,201646309,"DT_Team at SemEval-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and Gaussian Mixture Model Output",0.05034291744232178,#98df8a
-5.847262,-1.4633324,neighbor,201646309,Predicting the Semantic Textual Similarity with Siamese CNN and LSTM,0.05093669891357422,#98df8a
-3.579091,-8.7133045,neighbor,201646309,Character-Based Neural Networks for Sentence Pair Modeling,0.051376283168792725,#98df8a
6.1200385,5.330149,neighbor,201646309,Charagram: Embedding Words and Sentences via Character n-grams,0.051615893840789795,#98df8a
-8.835519,6.3026757,neighbor,201646309,Learning Semantic Textual Similarity from Conversations,0.05162936449050903,#98df8a
2.8725135,-6.056727,neighbor,201646309,Testing the limits of unsupervised learning for semantic similarity,0.051705121994018555,#98df8a
-4.7411776,-8.786566,neighbor,201646309,Element-wise Bilinear Interaction for Sentence Matching,0.05171400308609009,#98df8a
-5.234636,5.684782,neighbor,201646309,WOLVESAAR at SemEval-2016 Task 1: Replicating the Success of Monolingual Word Alignment and Neural Embeddings for Semantic Textual Similarity,0.05209219455718994,#98df8a
-0.7137581,-9.736187,neighbor,201646309,Towards Universal Paraphrastic Sentence Embeddings,0.052245914936065674,#98df8a
5.6477337,0.94100416,neighbor,201646309,Word Mover’s Embedding: From Word2Vec to Document Embedding,0.05235016345977783,#98df8a
-0.44024816,11.000457,neighbor,201646309,"SERGIOJIMENEZ at SemEval-2016 Task 1: Effectively Combining Paraphrase Database, String Matching, WordNet, and Word Embedding for Semantic Textual Similarity",0.05268871784210205,#98df8a
7.9363213,-10.023684,neighbor,201646309,Continual Learning for Sentence Representations Using Conceptors,0.053308725357055664,#98df8a
0.17963296,5.4836926,neighbor,201646309,ECNU: Using Traditional Similarity Measurements and Word Embedding for Semantic Textual Similarity Estimation,0.05362427234649658,#98df8a
6.0394235,-2.391528,neighbor,201646309,Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features,0.05367821455001831,#98df8a
-0.44216216,-1.1747986,neighbor,201646309,CACV-tree: A New Computational Approach for Sentence Similarity Modeling,0.053760826587677,#98df8a
8.020952,-8.962574,neighbor,201646309,Unsupervised Learning of Sentence Representations Using Sequence Consistency,0.05397385358810425,#98df8a
1.0056269,-10.205329,neighbor,201646309,pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference,0.054597437381744385,#98df8a
2.7432592,-1.0146136,neighbor,201646309,A Word Embeddings Model for Sentence Similarity,0.05472320318222046,#98df8a
-1.1775414,8.3855915,neighbor,201646309,L2F/INESC-ID at SemEval-2017 Tasks 1 and 2: Lexical and semantic features in word and textual similarity,0.0548442006111145,#98df8a
7.46925,-6.341133,neighbor,201646309,Connecting Supervised and Unsupervised Sentence Embeddings,0.05579251050949097,#98df8a
6.0224485,-8.958614,neighbor,201646309,SentEval: An Evaluation Toolkit for Universal Sentence Representations,0.055964887142181396,#98df8a
-4.422411,9.016246,neighbor,201646309,ISCAS_NLP at SemEval-2016 Task 1: Sentence Similarity Based on Support Vector Regression using Multiple Features,0.05601245164871216,#98df8a
4.562624,5.422382,neighbor,201646309,Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes,0.0565456748008728,#98df8a
-3.476483,-6.7444534,neighbor,201646309,Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks,0.05662351846694946,#98df8a
8.2328825,-4.6475387,neighbor,201646309,Learning Compressed Sentence Representations for On-Device Text Processing,0.05666762590408325,#98df8a
-3.7630703,8.293462,neighbor,201646309,NaCTeM at SemEval-2016 Task 1: Inferring sentence-level semantic similarity from an ensemble of complementary lexical and sentence-level features,0.05673062801361084,#98df8a
-7.3034496,6.0024033,neighbor,201646309,SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation,0.05688267946243286,#98df8a
12.256476,-8.417372,neighbor,201646309,Learning Cross-Lingual Sentence Representations via a Multi-task Dual-Encoder Model,0.057359516620635986,#98df8a
-4.6480885,-0.68274903,neighbor,201646309,Sentence Semantic Similarity Model Using Convolutional Neural Networks,0.05758845806121826,#98df8a
-3.3327346,6.988302,neighbor,201646309,NUIG-UNLP at SemEval-2016 Task 1: Soft Alignment and Deep Learning for Semantic Textual Similarity,0.05780982971191406,#98df8a
12.089894,-6.9930882,neighbor,201646309,Scalable Cross-Lingual Transfer of Neural Sentence Embeddings,0.05804544687271118,#98df8a
-0.6218668,1.3522624,neighbor,201646309,Fusing Syntax and Word Embedding Knowledge for Measuring Semantic Similarity,0.058418869972229004,#98df8a
-6.2003484,-1.9486419,neighbor,201646309,Attentive Siamese LSTM Network for Semantic Textual Similarity Measure,0.05843275785446167,#98df8a
11.260177,-8.611115,neighbor,201646309,Multi-task Learning for Universal Sentence Embeddings: A Thorough Evaluation using Transfer and Auxiliary Tasks,0.05854189395904541,#98df8a
-5.2319956,-11.166654,neighbor,201646309,Dual-View Variational Autoencoders for Semi-Supervised Text Matching,0.058630287647247314,#98df8a
4.058423,2.2668607,neighbor,201646309,"Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors",0.058765947818756104,#98df8a
-2.8079846,8.075056,neighbor,201646309,NORMAS at SemEval-2016 Task 1: SEMSIM: A Multi-Feature Approach to Semantic Text Similarity,0.058786094188690186,#98df8a
7.7968645,2.4214523,neighbor,201646309,Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds,0.058792710304260254,#98df8a
0.47761118,9.043681,neighbor,201646309,LIPN-IIMAS at SemEval-2016 Task 1: Random Forest Regression Experiments on Align-and-Differentiate and Word Embeddings penalizing strategies,0.058834612369537354,#98df8a
11.731042,4.689376,neighbor,201646309,Composition of Sentence Embeddings: Lessons from Statistical Relational Learning,0.058935344219207764,#98df8a
2.355657,2.776938,neighbor,201646309,Learning Text Pair Similarity with Context-sensitive Autoencoders,0.05903637409210205,#98df8a
10.700602,5.3649783,neighbor,201646309,Learning Semantically and Additively Compositional Distributional Representations,0.059076547622680664,#98df8a
9.954709,-3.1478255,neighbor,201646309,Dependency Based Embeddings for Sentence Classification Tasks,0.05974000692367554,#98df8a
9.518949,0.07423716,neighbor,201646309,Meta-Embedding as Auxiliary Task Regularization,0.05976545810699463,#98df8a
-1.2098346,0.9769143,neighbor,201646309,A semantic textual similarity measurement model based on the syntactic-semantic representation,0.059867262840270996,#98df8a
13.503893,-8.221029,neighbor,201646309,Learning Distributed Representations for Multilingual Text Sequences,0.05992525815963745,#98df8a
-6.685792,-0.47825056,neighbor,201646309,Measuring Semantic Similarity Between Sentences Using A Siamese Neural Network,0.05993914604187012,#98df8a
-1.4521632,-10.669966,neighbor,201646309,Trimming and Improving Skip-thought Vectors,0.059986650943756104,#98df8a
-1.6277262,7.51873,neighbor,201646309,HHU at SemEval-2016 Task 1: Multiple Approaches to Measuring Semantic Textual Similarity,0.060133159160614014,#98df8a
1.494744,11.143958,neighbor,201646309,MathLingBudapest: Concept Networks for Semantic Similarity,0.06032562255859375,#98df8a
7.1310186,5.944345,neighbor,201646309,Evaluating Neural Word Representations in Tensor-Based Compositional Settings,0.060391247272491455,#98df8a
12.719499,-5.330522,neighbor,201646309,Improving Multilingual Sentence Embedding using Bi-directional Dual Encoder with Additive Margin Softmax,0.06039780378341675,#98df8a
10.497934,-8.346654,neighbor,201646309,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,0.06051349639892578,#98df8a
-7.656581,4.8163548,neighbor,201646309,Improving Multilingual Semantic Textual Similarity with Shared Sentence Encoder for Low-resource Languages,0.06053835153579712,#98df8a
-2.8711734,12.657544,neighbor,201646309,Amrita_CEN at SemEval-2016 Task 1: Semantic Relation from Word Embeddings in Higher Dimension,0.060581862926483154,#98df8a
6.2346377,2.3497868,neighbor,201646309,SeVeN: Augmenting Word Embeddings with Unsupervised Relation Vectors,0.060726702213287354,#98df8a
8.97132,0.5322264,neighbor,201646309,Angular-Based Word Meta-Embedding Learning,0.06103593111038208,#98df8a
-1.768738,6.6639752,neighbor,201646309,BIT at SemEval-2017 Task 1: Using Semantic Information Space to Evaluate Semantic Textual Similarity,0.061087608337402344,#98df8a
9.601182,-5.9691157,neighbor,201646309,Evaluation of sentence embeddings in downstream and linguistic probing tasks,0.061134397983551025,#98df8a
1.2906363,6.961159,neighbor,201646309,WSL: Sentence Similarity Using Semantic Distance Between Words,0.06133377552032471,#98df8a
-5.9903774,-6.8095784,neighbor,201646309,Inter-Weighted Alignment Network for Sentence Pair Modeling,0.06143754720687866,#98df8a
-5.9453177,10.488645,neighbor,201646309,UWB at SemEval-2016 Task 2: Interpretable Semantic Textual Similarity with Distributional Semantics for Chunks,0.06148397922515869,#98df8a
-0.26100203,7.4701705,neighbor,201646309,"UoB-UK at SemEval-2016 Task 1: A Flexible and Extendable System for Semantic Text Similarity using Types, Surprise and Phrase Linking",0.06149005889892578,#98df8a
-4.3338056,-10.874618,neighbor,201646309,A Cross-Sentence Latent Variable Model for Semi-Supervised Text Sequence Matching,0.061746299266815186,#98df8a
6.3592863,-7.8154097,neighbor,201646309,No Training Required: Exploring Random Encoders for Sentence Classification,0.06184667348861694,#98df8a
10.326528,-7.0446424,neighbor,201646309,Universal Sentence Encoder,0.06186354160308838,#98df8a
-7.637091,10.549282,neighbor,201646309,SemEval-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment,0.06201374530792236,#98df8a
5.7433553,-5.9871583,neighbor,201646309,Dis-S2V: Discourse Informed Sen2Vec,0.0621604323387146,#98df8a
9.278761,1.0733929,neighbor,201646309,Learning Meta-Embeddings by Using Ensembles of Embedding Sets,0.06254774332046509,#98df8a
-3.0917811,-9.151923,neighbor,201646309,"Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering",0.0625845193862915,#98df8a
7.6044726,-7.320821,neighbor,201646309,An efficient framework for learning sentence representations,0.06259763240814209,#98df8a
12.355363,-4.556401,neighbor,201646309,Concatenated Power Mean Word Embeddings as Universal Cross-Lingual Sentence Representations,0.06277233362197876,#98df8a
3.251788,8.818077,neighbor,201646309,Sew-Embed at SemEval-2017 Task 2: Language-Independent Concept Representations from a Semantically Enriched Wikipedia,0.0627816915512085,#98df8a
10.485588,-9.374582,neighbor,201646309,"Learning Robust, Transferable Sentence Representations for Text Classification",0.06293255090713501,#98df8a
9.980402,5.7904673,neighbor,201646309,Low-Rank Tensors for Verbs in Compositional Distributional Semantics,0.06298172473907471,#98df8a
5.5350738,-10.600121,neighbor,201646309,Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring,0.063060462474823,#98df8a
-2.4740686,-7.010481,neighbor,201646309,CAN: Enhancing Sentence Similarity Modeling with Collaborative and Adversarial Network,0.06318992376327515,#98df8a
-8.204685,10.243025,neighbor,201646309,CECL: a New Baseline and a Non-Compositional Approach for the Sick Benchmark,0.06322741508483887,#98df8a
-4.817543,3.2040038,neighbor,201646309,MITRE at SemEval-2017 Task 1: Simple Semantic Similarity,0.0633091926574707,#98df8a
-1.1203618,-8.943048,neighbor,201646309,Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator,0.0633230209350586,#98df8a
-0.9692343,1.4919606,query,206592484,Going deeper with convolutions,0.0,#98df8a
-1.8579491,1.7020904,neighbor,206592484,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.035577356815338135,#98df8a
-9.383815,7.2904572,neighbor,206592484,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.04345816373825073,#98df8a
-2.6348183,2.4508784,neighbor,206592484,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.048729002475738525,#98df8a
-4.5068483,3.3673522,neighbor,206592484,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.05110400915145874,#98df8a
-2.686144,1.3039396,neighbor,206592484,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.05185765027999878,#98df8a
0.23205012,1.7119647,neighbor,206592484,Visualizing and Understanding Convolutional Networks,0.05300283432006836,#98df8a
2.6759171,0.7127442,neighbor,206592484,Network In Network,0.056445419788360596,#98df8a
-1.4748408,-0.1052085,neighbor,206592484,Caffe: Convolutional Architecture for Fast Feature Embedding,0.05785304307937622,#98df8a
-9.665578,8.116936,neighbor,206592484,Scalable Object Detection Using Deep Neural Networks,0.05798482894897461,#98df8a
-6.9784737,14.699875,neighbor,206592484,Fast image scanning with deep max-pooling convolutional neural networks,0.058248281478881836,#98df8a
5.471578,1.4081877,neighbor,206592484,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.058367013931274414,#98df8a
0.67970943,-5.2488394,neighbor,206592484,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.05841249227523804,#98df8a
-0.44719723,-5.197113,neighbor,206592484,Multi-column deep neural networks for image classification,0.060382723808288574,#98df8a
-0.777789,-2.190569,neighbor,206592484,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.06095468997955322,#98df8a
4.827283,-1.1488664,neighbor,206592484,Understanding Deep Architectures using a Recursive Convolutional Network,0.06206679344177246,#98df8a
-7.7059917,9.424366,neighbor,206592484,Deep learning for class-generic object detection,0.06215852499008179,#98df8a
-0.97076213,-1.4006068,neighbor,206592484,Fast Training of Convolutional Networks through FFTs,0.06366032361984253,#98df8a
1.4701035,0.6300962,neighbor,206592484,Deep Epitomic Convolutional Neural Networks,0.06551474332809448,#98df8a
0.6847576,-2.6109738,neighbor,206592484,One weird trick for parallelizing convolutional neural networks,0.06700319051742554,#98df8a
10.570293,1.0742201,neighbor,206592484,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.06705266237258911,#98df8a
-0.023217697,-5.8532853,neighbor,206592484,"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",0.06782519817352295,#98df8a
-10.642708,8.736529,neighbor,206592484,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.06829166412353516,#98df8a
-5.453642,9.258083,neighbor,206592484,Fine-grained object recognition with Gnostic Fields,0.06910043954849243,#98df8a
7.528763,0.83746475,neighbor,206592484,Do Deep Nets Really Need to be Deep?,0.06993609666824341,#98df8a
-1.1947402,8.37774,neighbor,206592484,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.06999313831329346,#98df8a
6.660967,4.4910607,neighbor,206592484,An introduction to deep learning,0.07004773616790771,#98df8a
3.8312871,0.33814913,neighbor,206592484,An Analysis of the Connections Between Layers of Deep Neural Networks,0.07006204128265381,#98df8a
-4.887165,3.7862322,neighbor,206592484,Learnable Pooling Regions for Image Classification,0.07096344232559204,#98df8a
9.417134,1.3340712,neighbor,206592484,Piecewise Linear Multilayer Perceptrons and Dropout,0.07119840383529663,#98df8a
-7.6771398,12.738917,neighbor,206592484,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.07150864601135254,#98df8a
-2.2577798,11.326198,neighbor,206592484,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.07308870553970337,#98df8a
2.2156231,-5.3449073,neighbor,206592484,Committees of Deep Feedforward Networks Trained with Few Data,0.07310587167739868,#98df8a
-6.7517753,8.664496,neighbor,206592484,ImageNet Large Scale Visual Recognition Challenge,0.07329756021499634,#98df8a
5.6787577,2.3505235,neighbor,206592484,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.07337319850921631,#98df8a
-8.132265,7.8601346,neighbor,206592484,LSDA: Large Scale Detection through Adaptation,0.07382345199584961,#98df8a
-9.142671,10.882354,neighbor,206592484,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.07396095991134644,#98df8a
2.5722024,-3.080418,neighbor,206592484,Scalable stacking and learning for building deep architectures,0.07406902313232422,#98df8a
5.7868724,5.2684546,neighbor,206592484,Scheduled denoising autoencoders,0.07407242059707642,#98df8a
9.694643,2.6266549,neighbor,206592484,Dropout Rademacher complexity of deep neural networks,0.0746009349822998,#98df8a
11.443935,3.080428,neighbor,206592484,Improving Deep Neural Networks with Probabilistic Maxout Units,0.07486116886138916,#98df8a
2.9907923,5.4596815,neighbor,206592484,Differentiable Pooling for Hierarchical Feature Learning,0.075039803981781,#98df8a
1.5054241,10.815272,neighbor,206592484,A Probabilistic WKL Rule for Incremental Feature Learning and Pattern Recognition,0.07522821426391602,#98df8a
11.619857,4.1610975,neighbor,206592484,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.07643461227416992,#98df8a
-10.084166,2.6790543,neighbor,206592484,Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps,0.07648330926895142,#98df8a
-1.921653,4.3986,neighbor,206592484,Convolutional Kernel Networks,0.07709622383117676,#98df8a
-1.5275618,11.745253,neighbor,206592484,Building high-level features using large scale unsupervised learning,0.07735210657119751,#98df8a
-5.9807973,2.7121592,neighbor,206592484,Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models,0.07763350009918213,#98df8a
11.1245165,3.127129,neighbor,206592484,Maxout Networks,0.07780390977859497,#98df8a
-8.924654,1.315648,neighbor,206592484,Object recognition with hierarchical discriminant saliency networks,0.07790142297744751,#98df8a
-4.6243024,6.8552394,neighbor,206592484,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.07792043685913086,#98df8a
6.5262785,-1.279834,neighbor,206592484,Predicting Parameters in Deep Learning,0.07831835746765137,#98df8a
-7.5896564,13.490838,neighbor,206592484,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.07862037420272827,#98df8a
-3.0468643,6.392803,neighbor,206592484,From generic to specific deep representations for visual recognition,0.0790858268737793,#98df8a
-0.059724852,5.9048905,neighbor,206592484,Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition,0.07954579591751099,#98df8a
-3.4360187,7.0375385,neighbor,206592484,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.07970255613327026,#98df8a
1.3479693,-2.7978528,neighbor,206592484,Multi-GPU Training of ConvNets,0.07979100942611694,#98df8a
-8.675282,10.2115755,neighbor,206592484,Generic Object Detection with Dense Neural Patterns and Regionlets,0.08033031225204468,#98df8a
5.8656425,2.903364,neighbor,206592484,Deep learning,0.08040058612823486,#98df8a
-10.915554,7.6012316,neighbor,206592484,Self-taught object localization with deep networks,0.08064061403274536,#98df8a
-1.7983024,5.0880065,neighbor,206592484,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.08069771528244019,#98df8a
5.3534684,-3.0959666,neighbor,206592484,Feature Graph Architectures,0.08105558156967163,#98df8a
-5.0749745,5.225275,neighbor,206592484,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.08223730325698853,#98df8a
-0.60738355,11.181409,neighbor,206592484,Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification,0.08225435018539429,#98df8a
9.735203,4.772058,neighbor,206592484,Improving neural networks by preventing co-adaptation of feature detectors,0.08236187696456909,#98df8a
2.8319368,6.711744,neighbor,206592484,Deconvolutional networks,0.0823899507522583,#98df8a
1.463902,8.089376,neighbor,206592484,Complexity of Representation and Inference in Compositional Models with Part Sharing,0.08275806903839111,#98df8a
10.505623,6.0083327,neighbor,206592484,Learning from Noisy Labels with Deep Neural Networks,0.08294492959976196,#98df8a
-12.064641,11.795928,neighbor,206592484,Training a multi-exit cascade with linear asymmetric classification for efficient object detection,0.08317756652832031,#98df8a
-8.335601,-4.634918,neighbor,206592484,Neocortical frame-free vision sensing and processing through scalable Spiking ConvNet hardware,0.08380699157714844,#98df8a
-3.3660326,10.236783,neighbor,206592484,Local Naive Bayes Nearest Neighbor for image classification,0.08408021926879883,#98df8a
7.6756253,3.5078099,neighbor,206592484,Joint Training of Deep Boltzmann Machines,0.08411240577697754,#98df8a
13.054348,0.76763016,neighbor,206592484,Quadratic Features and Deep Architectures for Chunking,0.0842558741569519,#98df8a
6.941599,-1.5805253,neighbor,206592484,Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning,0.08461213111877441,#98df8a
-4.5941744,11.427299,neighbor,206592484,Fisher and VLAD with FLAIR,0.0846787691116333,#98df8a
5.6002197,10.592855,neighbor,206592484,Ускорение обучения нейронной сети для распознавания изображений с помощью технологии NVIDIA CUDA@@@Neural network training acceleration using NVIDIA CUDA technology for image recognition,0.08470535278320312,#98df8a
1.980238,6.0048985,neighbor,206592484,Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification,0.0847553014755249,#98df8a
6.8033285,2.9475427,neighbor,206592484,Deep belief networks,0.0859711766242981,#98df8a
-6.7527018,5.075409,neighbor,206592484,Visual Objects Classification with Sliding Spatial Pyramid Matching,0.08612513542175293,#98df8a
-5.166616,0.9105566,neighbor,206592484,Recurrent Models of Visual Attention,0.0862663984298706,#98df8a
-8.328511,-4.683735,neighbor,206592484,On scalable spiking convnet hardware for cortex-like visual sensory processing systems,0.08634525537490845,#98df8a
9.02368,-1.5229689,neighbor,206592484,Big Neural Networks Waste Capacity,0.08639401197433472,#98df8a
6.363417,6.522062,neighbor,206592484,Two SVDs produce more focal deep learning representations,0.08642178773880005,#98df8a
7.086647,10.873937,neighbor,206592484,Mapping LSSVM on digital hardware,0.08668643236160278,#98df8a
7.5658107,-6.2273884,neighbor,206592484,Deep Multiple Kernel Learning,0.08669763803482056,#98df8a
-12.720821,7.812074,neighbor,206592484,"Codemaps - Segment, Classify and Search Objects Locally",0.08680576086044312,#98df8a
0.97351855,5.6789417,neighbor,206592484,Sparsity-Regularized HMAX for Visual Recognition,0.08681857585906982,#98df8a
-3.6381576,-1.4488466,neighbor,206592484,Invariant Scattering Convolution Networks,0.08689022064208984,#98df8a
13.481108,3.4192197,neighbor,206592484,From Maxout to Channel-Out: Encoding Information on Sparse Pathways,0.08716475963592529,#98df8a
-12.396887,5.2369785,neighbor,206592484,Is the Game worth the Candle? - Evaluation of OpenCL for Object Detection Algorithm Optimization,0.08720618486404419,#98df8a
-11.471723,12.343879,neighbor,206592484,Local Decorrelation For Improved Detection,0.08729755878448486,#98df8a
-10.094196,9.591013,neighbor,206592484,"How good are detection proposals, really?",0.08735263347625732,#98df8a
-7.3175087,-1.3953252,neighbor,206592484,Efficient Visual Coding: From Retina To V2,0.08736169338226318,#98df8a
10.3566065,-0.009680576,neighbor,206592484,On the Number of Linear Regions of Deep Neural Networks,0.08765798807144165,#98df8a
11.192771,-1.1843786,neighbor,206592484,Intriguing properties of neural networks,0.0878986120223999,#98df8a
7.0324345,-6.371563,neighbor,206592484,Parallel multiclass stochastic gradient descent algorithms for classifying million images with very-high-dimensional signatures into thousands classes,0.08796906471252441,#98df8a
-8.102075,-2.4080386,neighbor,206592484,"Distributed, layered and reliable computing nets to represent neuronal receptive fields.",0.0879930853843689,#98df8a
9.388519,-4.20354,neighbor,206592484,The Margitron: A Generalised Perceptron with Margin,0.08802163600921631,#98df8a
-2.6487422,-6.21478,neighbor,206592484,Can Neural Networks Recognize Parts,0.08849114179611206,#98df8a
-8.667274,0.83366567,neighbor,206592484,A neural computational model for bottom-up attention with invariant and overcomplete representation,0.08856028318405151,#98df8a
-11.663526,9.524515,neighbor,206592484,Layered object detection for multi-class segmentation,0.08885180950164795,#98df8a
3.7916393,1.3771074,query,206592766,FaceNet: A unified embedding for face recognition and clustering,0.0,#d62728
4.2705684,2.423306,neighbor,206592766,Learning Face Representation from Scratch,0.03950995206832886,#d62728
4.4867363,2.8246124,neighbor,206592766,Learning Deep Face Representation,0.040788888931274414,#d62728
3.0835643,3.0065892,neighbor,206592766,Face recognition with learning-based descriptor,0.04085862636566162,#d62728
5.190423,0.6734372,neighbor,206592766,Deep Learning Face Representation by Joint Identification-Verification,0.04293113946914673,#d62728
3.5179534,-0.6713474,neighbor,206592766,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,0.045448362827301025,#d62728
0.62512875,-2.11965,neighbor,206592766,Leveraging Billions of Faces to Overcome Performance Barriers in Unconstrained Face Recognition,0.050360798835754395,#d62728
6.0457315,0.22443354,neighbor,206592766,"Deeply learned face representations are sparse, selective, and robust",0.05186206102371216,#d62728
8.657027,0.8647861,neighbor,206592766,Web-scale training for face identification,0.05428367853164673,#d62728
7.6319885,-1.691594,neighbor,206592766,Deep Learning Face Attributes in the Wild,0.05688142776489258,#d62728
1.6690447,3.92405,neighbor,206592766,Face Identification Using Large Feature Sets,0.058263182640075684,#d62728
7.529889,1.422229,neighbor,206592766,Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not?,0.06052374839782715,#d62728
5.543594,-1.5655944,neighbor,206592766,Building high-level features using large scale unsupervised learning,0.06286442279815674,#d62728
1.1722388,7.8123198,neighbor,206592766,"On robust face recognition via sparse coding: the good, the bad and the ugly",0.06353980302810669,#d62728
2.486171,-5.9052114,neighbor,206592766,Multi-view Face Detection Using Deep Convolutional Neural Networks,0.06379044055938721,#d62728
-2.14059,0.30348676,neighbor,206592766,"A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and Low-resolution",0.06414401531219482,#d62728
6.3337865,1.3073233,neighbor,206592766,DeepID3: Face Recognition with Very Deep Neural Networks,0.06454813480377197,#d62728
1.7932854,-1.677921,neighbor,206592766,A benchmark study of large-scale unconstrained face recognition,0.06468409299850464,#d62728
2.8791552,6.570731,neighbor,206592766,Generic Image Classification Approaches Excel on Face Recognition,0.06502395868301392,#d62728
7.0313754,3.7400012,neighbor,206592766,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.06517291069030762,#d62728
-0.79334784,5.44499,neighbor,206592766,Fusing Robust Face Region Descriptors via Multiple Metric Learning for Face Recognition in the Wild,0.06570899486541748,#d62728
-3.7820585,4.420889,neighbor,206592766,Combined Learning of Salient Local Descriptors and Distance Metrics for Image Set Face Verification,0.06573355197906494,#d62728
5.6743264,-3.838316,neighbor,206592766,FAME: Face Association through Model Evolution,0.06603240966796875,#d62728
4.3123116,-0.12180767,neighbor,206592766,Surpassing Human-Level Face Verification Performance on LFW with GaussianFace,0.06803929805755615,#d62728
-1.3844159,-3.7711906,neighbor,206592766,Face recognition in unconstrained videos with matched background similarity,0.0682782530784607,#d62728
-0.23374878,-1.8827715,neighbor,206592766,The challenge of face recognition from digital point-and-shoot cameras,0.06884521245956421,#d62728
10.54626,-6.419501,neighbor,206592766,Retrieval-based face annotation by weak label regularized local coordinate coding.,0.07000815868377686,#d62728
9.807127,-7.7115235,neighbor,206592766,FANS: face annotation by searching large-scale web facial images,0.0703742504119873,#d62728
3.2699466,3.9841316,neighbor,206592766,Enhancing Human Face Detection by Resampling Examples Through Manifolds,0.07049143314361572,#d62728
7.254499,6.823171,neighbor,206592766,Learning Boltzmann Distance Metric for Face Recognition,0.07049202919006348,#d62728
-9.016711,-3.1781864,neighbor,206592766,3D Face Recognition Based on Multiple Keypoint Descriptors and Sparse Representation,0.0706755518913269,#d62728
-5.5515194,-6.637649,neighbor,206592766,300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge,0.07069367170333862,#d62728
10.839405,-6.7564416,neighbor,206592766,Retrieval-Based Face Annotation by Weak Label Regularized Local Coordinate Coding,0.0707477331161499,#d62728
-3.5646918,3.1949732,neighbor,206592766,"Pose, illumination and expression invariant pairwise face-similarity measure via Doppelgänger list comparison",0.07113832235336304,#d62728
-6.116974,3.8018515,neighbor,206592766,A framework for improving the performance of verification algorithms with a low false positive rate requirement and limited training data,0.072060227394104,#d62728
3.1536005,-5.736589,neighbor,206592766,Aggregate channel features for multi-view face detection,0.07266026735305786,#d62728
7.5647345,-0.8446814,neighbor,206592766,Deep Attribute Networks,0.07303333282470703,#d62728
10.583879,-7.7981772,neighbor,206592766,Learning to name faces: a multimodal learning scheme for search-based face annotation,0.0730520486831665,#d62728
5.6085634,3.77161,neighbor,206592766,Face recognition using Deep PCA,0.07354241609573364,#d62728
0.2975928,-2.7803516,neighbor,206592766,Unconstrained face recognition: Establishing baseline human performance via crowdsourcing,0.07432126998901367,#d62728
-7.15089,-5.614866,neighbor,206592766,Benchmarking 3D Pose Estimation for Face Recognition,0.07435590028762817,#d62728
-2.699128,-7.9589005,neighbor,206592766,Robust Face Recognition by Constrained Part-based Alignment,0.07443702220916748,#d62728
1.4915532,9.216235,neighbor,206592766,Robust Face Recognition via Adaptive Sparse Representation,0.07452166080474854,#d62728
0.22281602,-0.8989602,neighbor,206592766,Beyond frontal faces: Improving Person Recognition using multiple cues,0.0746380090713501,#d62728
1.0194566,10.051822,neighbor,206592766,"Collaborative Representation for Classification, Sparse or Non-sparse?",0.07488101720809937,#d62728
0.43381876,8.799488,neighbor,206592766,Discriminative Local Sparse Representations for Robust Face Recognition,0.07490777969360352,#d62728
4.319809,4.762534,neighbor,206592766,Large-scale Supervised Hierarchical Feature Learning for Face Recognition,0.07491248846054077,#d62728
4.6951118,-8.118919,neighbor,206592766,Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos,0.07502424716949463,#d62728
9.644601,2.4989424,neighbor,206592766,How far can you get with a modern face recognition test set using only simple features?,0.0751078724861145,#d62728
8.125427,-9.159282,neighbor,206592766,Autotagging Facebook: Social network context improves photo annotation,0.07630753517150879,#d62728
1.5475471,-3.594136,neighbor,206592766,On Hair Recognition in the Wild by Machine,0.07645708322525024,#d62728
-7.8567147,-4.032694,neighbor,206592766,Dense 3D Face Correspondence,0.07657593488693237,#d62728
-3.116309,-9.746112,neighbor,206592766,Learning Deep Representation for Face Alignment with Auxiliary Attributes,0.07679051160812378,#d62728
-5.6986485,-6.9972305,neighbor,206592766,A comparative study of face landmarking techniques,0.07682681083679199,#d62728
0.71739167,7.445656,neighbor,206592766,Visible and Infrared Face Identification via Sparse Representation,0.07685256004333496,#d62728
-2.9537416,2.1251457,neighbor,206592766,LBP-based periocular recognition on challenging face datasets,0.07691127061843872,#d62728
2.4719841,5.5970263,neighbor,206592766,Face Identification with Second-Order Pooling,0.07700479030609131,#d62728
5.0045466,5.65698,neighbor,206592766,Incremental Nonnegative Matrix Factorization for Face Recognition,0.07752102613449097,#d62728
-3.6495578,-10.042244,neighbor,206592766,Transferring Landmark Annotations for Cross-Dataset Face Alignment,0.07803773880004883,#d62728
2.2020857,9.54473,neighbor,206592766,Locality Constrained Joint Dynamic Sparse Representation for Local Matching Based Face Recognition,0.07810753583908081,#d62728
9.070595,-7.206519,neighbor,206592766,A unified learning framework for auto face annotation by mining web facial images,0.07819056510925293,#d62728
-8.0226965,-3.1768515,neighbor,206592766,3-D Shape Matching for Face Analysis and Recognition,0.07830995321273804,#d62728
9.786319,-6.8067875,neighbor,206592766,Mining Weakly Labeled Web Facial Images for Search-Based Face Annotation,0.07837045192718506,#d62728
-1.3666524,6.317335,neighbor,206592766,"Large Scale Strongly Supervised Ensemble Metric Learning, with Applications to Face Verification and Retrieval",0.07856899499893188,#d62728
-8.065468,-4.9501705,neighbor,206592766,Canonical Face Depth Map: A Robust 3D Representation for Face Verification,0.07867085933685303,#d62728
-1.7200837,9.712868,neighbor,206592766,Coupling Alignments with Recognition for Still-to-Video Face Recognition,0.07917451858520508,#d62728
-7.230274,-8.025641,neighbor,206592766,Face frontalization for Alignment and Recognition,0.07942163944244385,#d62728
-4.461342,5.859874,neighbor,206592766,A compact discriminative representation for efficient image-set classification with application to biometric recognition,0.07948368787765503,#d62728
2.8654063,-12.907144,neighbor,206592766,Towards computational models of kinship verification,0.07958078384399414,#d62728
-3.2826347,-0.27126738,neighbor,206592766,3D face recognition: A robust multi-matcher approach to data degradations,0.07959198951721191,#d62728
-5.3967447,-3.7012947,neighbor,206592766,Which parts of the face give out your identity?,0.08005249500274658,#d62728
3.4976094,-2.8184674,neighbor,206592766,Maximizing all margins: Pushing face recognition with Kernel Plurality,0.08020061254501343,#d62728
10.229378,5.8818407,neighbor,206592766,Fisher and VLAD with FLAIR,0.08032745122909546,#d62728
4.62535,9.609817,neighbor,206592766,On optimizing subspaces for face recognition,0.08062893152236938,#d62728
-3.6900694,-2.1593003,neighbor,206592766,3D-aided face recognition from videos,0.08065837621688843,#d62728
-4.347516,-3.3261745,neighbor,206592766,Novel Architecture for 3D model in virtual communities from detected face,0.08090806007385254,#d62728
-1.0814999,3.4984264,neighbor,206592766,A Maximum Correlation Feature Descriptor for Heterogeneous Face Recognition,0.0810614824295044,#d62728
1.5792924,-6.119955,neighbor,206592766,A Fast and Accurate Unconstrained Face Detector,0.08114069700241089,#d62728
0.16328906,-7.3841896,neighbor,206592766,Occlusion Coherence: Localizing Occluded Faces with a Hierarchical Deformable Part Model,0.08152353763580322,#d62728
1.1293379,1.2107505,neighbor,206592766,End-to-End Photo-Sketch Generation via Fully Convolutional Representation Learning,0.08181935548782349,#d62728
-1.9010601,2.1353378,neighbor,206592766,Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face Recognition,0.0819317102432251,#d62728
0.077191725,-8.236504,neighbor,206592766,Active Testing for Face Detection and Localization,0.08201348781585693,#d62728
-4.3872094,7.753756,neighbor,206592766,Large margin image set representation and classification,0.0821344256401062,#d62728
3.2104404,-13.3195095,neighbor,206592766,Learning a Genetic Measure for Kinship Verification Using Facial Images,0.08222448825836182,#d62728
-5.7810745,-5.7735796,neighbor,206592766,Pose and Expression Independent Facial Landmark Localization Using Dense-SURF and the Hausdorff Distance,0.08235836029052734,#d62728
-9.884034,-2.611435,neighbor,206592766,3D face recognition for partial data using Semi-Coupled Dictionary Learning,0.08245855569839478,#d62728
-4.0282288,-7.743504,neighbor,206592766,Automatic facial landmark labeling with minimal supervision,0.08246582746505737,#d62728
6.0529585,7.910393,neighbor,206592766,Shared representation learning for heterogenous face recognition,0.08274674415588379,#d62728
9.879644,0.17075634,neighbor,206592766,Scaling up biologically-inspired computer vision: A case study in unconstrained face recognition on facebook,0.08282822370529175,#d62728
2.6073914,-12.724617,neighbor,206592766,Kinship verification in the wild: The first kinship verification competition,0.0828738808631897,#d62728
-5.4733267,0.6772388,neighbor,206592766,Face Verification Across Age Progression Using Discriminative Methods,0.08292245864868164,#d62728
-4.2357793,7.5983796,neighbor,206592766,Image Set-Based Collaborative Representation for Face Recognition,0.0831366777420044,#d62728
-7.992724,-1.6344609,neighbor,206592766,Intraclass Retrieval of Nonrigid 3D Objects: Application to Face Recognition,0.08328497409820557,#d62728
-1.9842999,-4.020699,neighbor,206592766,Face Video Database,0.08331680297851562,#d62728
10.072686,5.90958,neighbor,206592766,Image Classification with the Fisher Vector: Theory and Practice,0.08334845304489136,#d62728
6.969916,-7.492831,neighbor,206592766,Semi-supervised Learning with Constraints for Person Identification in Multimedia Data,0.0837249755859375,#d62728
3.093013,10.76427,neighbor,206592766,Face Recognition via Globality-Locality Preserving Projections,0.08379387855529785,#d62728
3.9657414,8.59094,neighbor,206592766,Face Recognition using Optimal Representation Ensemble,0.08389991521835327,#d62728
0.75303155,1.517363,neighbor,206592766,Multi-View Representation Based Face Sketch Synthesis,0.08398479223251343,#d62728
-1.6796414,-5.0124626,neighbor,206592766,Video face matching using subset selection and clustering of probabilistic Multi-Region Histograms,0.0841018557548523,#d62728
-2.0803041,-2.0839858,neighbor,206592766,Effective face frontalization in unconstrained images,0.0841485857963562,#d62728
-5.2102995,-0.32231534,query,206593880,Rethinking the Inception Architecture for Computer Vision,0.0,#d62728
-5.723611,0.66295695,neighbor,206593880,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.037330448627471924,#d62728
-4.129077,0.34132743,neighbor,206593880,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.043595194816589355,#d62728
-4.5799565,0.9582374,neighbor,206593880,Enhanced image classification with a fast-learning shallow convolutional neural network,0.04765486717224121,#d62728
-6.996559,-0.11983077,neighbor,206593880,Going deeper with convolutions,0.05010157823562622,#d62728
-2.3876638,0.8916832,neighbor,206593880,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.05094939470291138,#d62728
-7.797565,0.61487967,neighbor,206593880,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.05299800634384155,#d62728
3.0593555,-8.337251,neighbor,206593880,Fast R-CNN,0.0546988844871521,#d62728
-9.081051,1.8134849,neighbor,206593880,Convolutional neural networks at constrained time cost,0.05639636516571045,#d62728
-9.8294735,3.4182224,neighbor,206593880,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.05705225467681885,#d62728
3.334836,-7.27525,neighbor,206593880,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.057260751724243164,#d62728
8.389497,-0.9325735,neighbor,206593880,Multi-scale Recognition with DAG-CNNs,0.058805227279663086,#d62728
-7.005396,4.900882,neighbor,206593880,Large-Scale Deep Learning on the YFCC100M Dataset,0.05993545055389404,#d62728
11.44904,-5.897625,neighbor,206593880,Fully convolutional networks for semantic segmentation,0.06166499853134155,#d62728
4.0397077,-10.434826,neighbor,206593880,What Is Holding Back Convnets for Detection?,0.061847686767578125,#d62728
-2.4719098,3.9264457,neighbor,206593880,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.062095582485198975,#d62728
-9.546417,3.8586273,neighbor,206593880,Caffe: Convolutional Architecture for Fast Feature Embedding,0.062368810176849365,#d62728
-12.171337,1.4682779,neighbor,206593880,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.06335407495498657,#d62728
-12.092151,2.9537158,neighbor,206593880,Fast Training of Convolutional Networks through FFTs,0.06343817710876465,#d62728
1.1859311,-0.28573358,neighbor,206593880,Deep convolutional neural networks as generic feature extractors,0.06414461135864258,#d62728
-11.488122,1.0892584,neighbor,206593880,Training CNNs with Low-Rank Filters for Efficient Image Classification,0.06494903564453125,#d62728
-7.363567,-1.0614223,neighbor,206593880,Striving for Simplicity: The All Convolutional Net,0.0651598572731018,#d62728
-2.9151602,1.6528784,neighbor,206593880,Feature Representation in Convolutional Neural Networks,0.06554603576660156,#d62728
13.492575,-4.4937105,neighbor,206593880,Places205-VGGNet Models for Scene Recognition,0.06748205423355103,#d62728
-9.582406,5.041687,neighbor,206593880,Theano-based Large-Scale Visual Recognition with Multiple GPUs,0.06800633668899536,#d62728
1.4496962,-11.75945,neighbor,206593880,Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection,0.06814152002334595,#d62728
-9.941484,-0.22828029,neighbor,206593880,Deep Fried Convnets,0.06818914413452148,#d62728
-5.2346754,2.9981358,neighbor,206593880,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.06838148832321167,#d62728
1.0282425,0.27728033,neighbor,206593880,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.06847864389419556,#d62728
3.7901063,-6.481865,neighbor,206593880,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.06871259212493896,#d62728
2.2134607,1.5350034,neighbor,206593880,From generic to specific deep representations for visual recognition,0.0688098669052124,#d62728
6.5942936,-7.4628882,neighbor,206593880,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.06915700435638428,#d62728
0.35062522,-11.967296,neighbor,206593880,Convolutional Channel Features,0.06974571943283081,#d62728
5.696281,-6.145147,neighbor,206593880,Scalable Object Detection Using Deep Neural Networks,0.06976616382598877,#d62728
7.5473423,-10.36506,neighbor,206593880,Deformable part models are convolutional neural networks,0.07015460729598999,#d62728
6.6530323,-2.9254723,neighbor,206593880,Half-CNN: A General Framework for Whole-Image Regression,0.07030481100082397,#d62728
2.883005,1.6280295,neighbor,206593880,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.07038813829421997,#d62728
-1.5553354,-1.0370837,neighbor,206593880,Webly Supervised Learning of Convolutional Networks,0.07047086954116821,#d62728
-13.324762,-2.5922093,neighbor,206593880,Multi-column deep neural networks for image classification,0.07059407234191895,#d62728
-11.200714,7.433067,neighbor,206593880,Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems,0.07099604606628418,#d62728
4.9337945,-7.003033,neighbor,206593880,ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks,0.07124227285385132,#d62728
4.13668,7.6100802,neighbor,206593880,Confusing Deep Convolution Networks by Relabelling,0.07143235206604004,#d62728
-4.4483976,5.0489373,neighbor,206593880,A HMAX with LLC for visual recognition,0.0714840292930603,#d62728
10.726854,-2.7806656,neighbor,206593880,Recurrent Convolutional Neural Networks for Scene Parsing,0.07153773307800293,#d62728
5.526106,-1.4997532,neighbor,206593880,Attention for Fine-Grained Categorization,0.07164132595062256,#d62728
-4.0566864,2.0699272,neighbor,206593880,An Introduction to Convolutional Neural Networks,0.07201391458511353,#d62728
-6.288173,-3.763716,neighbor,206593880,Deep Clustered Convolutional Kernels,0.07204604148864746,#d62728
0.31354862,2.7768962,neighbor,206593880,Bilinear CNN Models for Fine-Grained Visual Recognition,0.0722435712814331,#d62728
5.9567904,-9.410071,neighbor,206593880,Mid-level Elements for Object Detection,0.07263374328613281,#d62728
4.282077,-3.400982,neighbor,206593880,The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition,0.07284396886825562,#d62728
10.532434,-5.672039,neighbor,206593880,ParseNet: Looking Wider to See Better,0.07286840677261353,#d62728
3.3409994,-3.9071133,neighbor,206593880,ImageNet Large Scale Visual Recognition Challenge,0.07288545370101929,#d62728
11.8584585,-6.7356124,neighbor,206593880,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.07332539558410645,#d62728
-12.821395,-2.740116,neighbor,206593880,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.07359248399734497,#d62728
-9.518674,-6.1261945,neighbor,206593880,Conditional Deep Learning for energy-efficient and enhanced pattern recognition,0.07369774580001831,#d62728
9.298339,-1.0050901,neighbor,206593880,DAG-Recurrent Neural Networks for Scene Labeling,0.07386809587478638,#d62728
-4.2563086,-4.9617043,neighbor,206593880,Modelling local deep convolutional neural network features to improve fine-grained image classification,0.07403093576431274,#d62728
0.31558263,-4.994608,neighbor,206593880,Building high-level features using large scale unsupervised learning,0.07404345273971558,#d62728
-9.117717,-1.4848084,neighbor,206593880,How far can we go without convolution: Improving fully-connected networks,0.07411378622055054,#d62728
3.1041646,2.9189901,neighbor,206593880,Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition,0.07424664497375488,#d62728
-4.941283,-5.3451023,neighbor,206593880,Fine-grained classification via mixture of deep convolutional neural networks,0.07428407669067383,#d62728
0.87000734,5.6120505,neighbor,206593880,Inverting Visual Representations with Convolutional Networks,0.07440400123596191,#d62728
-7.419822,2.9698281,neighbor,206593880,MatConvNet: Convolutional Neural Networks for MATLAB,0.07440721988677979,#d62728
12.298361,-5.4772196,neighbor,206593880,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,0.07450658082962036,#d62728
-1.6999825,4.157796,neighbor,206593880,Deep Spatial Pyramid: The Devil is Once Again in the Details,0.07533824443817139,#d62728
4.6833777,-8.369186,neighbor,206593880,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.07534092664718628,#d62728
11.122251,-3.3943326,neighbor,206593880,Deep Deconvolutional Networks for Scene Parsing,0.07545113563537598,#d62728
2.0027027,-11.762528,neighbor,206593880,Taking a deeper look at pedestrians,0.07562953233718872,#d62728
7.605364,-8.298862,neighbor,206593880,Learning to decompose for object detection and instance segmentation,0.07586735486984253,#d62728
-9.118662,-4.0652604,neighbor,206593880,Training Deeper Convolutional Networks with Deep Supervision,0.07616281509399414,#d62728
4.8835826,-1.0021511,neighbor,206593880,Fine-grained object recognition with Gnostic Fields,0.07634377479553223,#d62728
3.536607,-5.1033573,neighbor,206593880,1-HKUST: Object Detection in ILSVRC 2014,0.07731008529663086,#d62728
2.599584,-2.862527,neighbor,206593880,Deep learning for class-generic object detection,0.07764369249343872,#d62728
10.67233,5.0077257,neighbor,206593880,Handcrafted Local Features are Convolutional Neural Networks,0.07786333560943604,#d62728
4.2003164,7.6257906,neighbor,206593880,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,0.07794183492660522,#d62728
13.333969,-0.604712,neighbor,206593880,FlowNet: Learning Optical Flow with Convolutional Networks,0.07796066999435425,#d62728
6.405266,-5.4138083,neighbor,206593880,Object-centric Sampling for Fine-grained Image Classification,0.07801932096481323,#d62728
11.738399,5.325145,neighbor,206593880,Efficient Large Scale Video Classification,0.07821577787399292,#d62728
-1.1236397,2.4270654,neighbor,206593880,The treasure beneath convolutional layers: Cross-convolutional-layer pooling for image classification,0.07838225364685059,#d62728
-12.609143,3.5436697,neighbor,206593880,Fast Algorithms for Convolutional Neural Networks,0.07852572202682495,#d62728
-9.600986,0.833017,neighbor,206593880,Learning both Weights and Connections for Efficient Neural Network,0.07858031988143921,#d62728
-6.252198,8.347187,neighbor,206593880,Visualizing and Comparing Convolutional Neural Networks,0.07860010862350464,#d62728
4.247087,3.373131,neighbor,206593880,Exploring Invariances in Deep Convolutional Neural Networks Using Synthetic Images,0.07869541645050049,#d62728
10.959297,-6.85653,neighbor,206593880,Segmentation-aware convolutional nets,0.07887357473373413,#d62728
-11.376462,7.569348,neighbor,206593880,Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification,0.07900428771972656,#d62728
-6.547515,8.167,neighbor,206593880,Visualizing and Understanding Convolutional Networks,0.07919955253601074,#d62728
9.05436,1.1370683,neighbor,206593880,Recurrent Models of Visual Attention,0.079276442527771,#d62728
-8.977527,-4.419928,neighbor,206593880,Deeply-Supervised Nets,0.0794493556022644,#d62728
10.06643,5.327838,neighbor,206593880,Towards Good Practices for Very Deep Two-Stream ConvNets,0.07961821556091309,#d62728
-6.2137327,2.082018,neighbor,206593880,Deep Epitomic Convolutional Neural Networks,0.07973676919937134,#d62728
11.981023,5.1848783,neighbor,206593880,Exploiting Image-trained CNN Architectures for Unconstrained Video Classification,0.07988148927688599,#d62728
6.229759,-7.88856,neighbor,206593880,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.07991158962249756,#d62728
8.593083,-7.0254726,neighbor,206593880,Hypercolumns for object segmentation and fine-grained localization,0.08020210266113281,#d62728
-1.5868133,8.181018,neighbor,206593880,Scale-Invariant Convolutional Neural Networks,0.08024817705154419,#d62728
-0.9973547,-1.6054281,neighbor,206593880,Unsupervised network pretraining via encoding human design,0.08035498857498169,#d62728
0.9750337,5.7200384,neighbor,206593880,Understanding deep image representations by inverting them,0.08075839281082153,#d62728
0.52018404,-7.4798827,neighbor,206593880,Do Convnets Learn Correspondence?,0.08092892169952393,#d62728
-4.283727,-1.8242213,neighbor,206593880,Deeply-Recursive Convolutional Network for Image Super-Resolution,0.08096325397491455,#d62728
-11.8857155,-3.678991,neighbor,206593880,Committees of Deep Feedforward Networks Trained with Few Data,0.08099561929702759,#d62728
-7.177216,-5.6564503,neighbor,206593880,Mediated experts for deep convolutional networks,0.08104139566421509,#d62728
-1.683579,8.475685,neighbor,206593880,Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection,0.08117556571960449,#d62728
-0.55106497,2.0588753,query,206594692,Deep Residual Learning for Image Recognition,0.0,#d62728
-0.9585737,3.2168887,neighbor,206594692,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.034738898277282715,#d62728
0.5046983,2.8925245,neighbor,206594692,Going deeper with convolutions,0.03972303867340088,#d62728
1.4178401,1.8678135,neighbor,206594692,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.04065054655075073,#d62728
11.87162,-7.8849263,neighbor,206594692,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.041128695011138916,#d62728
1.3764569,3.3955934,neighbor,206594692,Striving for Simplicity: The All Convolutional Net,0.043396174907684326,#d62728
-10.806597,6.384304,neighbor,206594692,Fast R-CNN,0.04623842239379883,#d62728
-0.79920334,4.7698913,neighbor,206594692,Enhanced image classification with a fast-learning shallow convolutional neural network,0.04666954278945923,#d62728
-11.890934,6.7500553,neighbor,206594692,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.04732990264892578,#d62728
-1.5808681,-7.869192,neighbor,206594692,Training Deeper Convolutional Networks with Deep Supervision,0.04733872413635254,#d62728
-1.4509978,-7.078934,neighbor,206594692,Deeply-Supervised Nets,0.04824405908584595,#d62728
-2.2257516,3.4822094,neighbor,206594692,Rethinking the Inception Architecture for Computer Vision,0.04843592643737793,#d62728
6.2064753,5.2244816,neighbor,206594692,Training CNNs with Low-Rank Filters for Efficient Image Classification,0.04985159635543823,#d62728
-0.14622039,6.744142,neighbor,206594692,Feature Representation in Convolutional Neural Networks,0.05004382133483887,#d62728
5.741447,-5.3279037,neighbor,206594692,Learning Compact Convolutional Neural Networks with Nested Dropout,0.05020684003829956,#d62728
-1.8068055,4.807995,neighbor,206594692,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.051110029220581055,#d62728
8.192217,-4.4934826,neighbor,206594692,Maxout Networks,0.05113184452056885,#d62728
-1.3190026,6.6860523,neighbor,206594692,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.051249921321868896,#d62728
10.344194,-7.644829,neighbor,206594692,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),0.05145680904388428,#d62728
2.180677,-2.539706,neighbor,206594692,How far can we go without convolution: Improving fully-connected networks,0.051658034324645996,#d62728
2.376879,7.479374,neighbor,206594692,Network In Network,0.052022576332092285,#d62728
9.089528,2.0050607,neighbor,206594692,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.05205583572387695,#d62728
-9.584195,3.7756765,neighbor,206594692,Do Convnets Learn Correspondence?,0.05295497179031372,#d62728
-0.18523939,8.818148,neighbor,206594692,Inverting Visual Representations with Convolutional Networks,0.05354279279708862,#d62728
-0.9638049,-4.827397,neighbor,206594692,Natural Neural Networks,0.05368208885192871,#d62728
0.118451454,-0.92232853,neighbor,206594692,SimNets: A Generalization of Convolutional Networks,0.053897202014923096,#d62728
4.076935,-9.783203,neighbor,206594692,Training Very Deep Networks,0.05410951375961304,#d62728
8.779185,-3.8970914,neighbor,206594692,Improving Deep Neural Networks with Probabilistic Maxout Units,0.05448836088180542,#d62728
4.235093,-9.781649,neighbor,206594692,Highway Networks,0.05476266145706177,#d62728
10.619604,-0.2942989,neighbor,206594692,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",0.05546015501022339,#d62728
0.81632566,6.079211,neighbor,206594692,An Introduction to Convolutional Neural Networks,0.05558425188064575,#d62728
9.758301,-0.58609796,neighbor,206594692,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.05564451217651367,#d62728
12.325015,-1.1893685,neighbor,206594692,Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification,0.05576509237289429,#d62728
-3.3491514,9.919372,neighbor,206594692,From generic to specific deep representations for visual recognition,0.05585885047912598,#d62728
-2.9721942,5.870421,neighbor,206594692,Multi-column deep neural networks for image classification,0.05618739128112793,#d62728
-5.8281636,-0.72511727,neighbor,206594692,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.056305885314941406,#d62728
1.4867573,-6.968455,neighbor,206594692,Deep learning,0.056656837463378906,#d62728
3.7249477,7.3043203,neighbor,206594692,Deep Epitomic Convolutional Neural Networks,0.05679142475128174,#d62728
9.948556,1.8954105,neighbor,206594692,Learnable Pooling Regions for Image Classification,0.057224929332733154,#d62728
10.764421,5.226848,neighbor,206594692,A HMAX with LLC for visual recognition,0.05728429555892944,#d62728
1.7027825,-8.769395,neighbor,206594692,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.057396531105041504,#d62728
11.405465,-7.02584,neighbor,206594692,Empirical Evaluation of Rectified Activations in Convolutional Network,0.05753904581069946,#d62728
3.5634754,11.675405,neighbor,206594692,Visualizing and Understanding Convolutional Networks,0.05758148431777954,#d62728
2.1112072,5.5312276,neighbor,206594692,MatConvNet: Convolutional Neural Networks for MATLAB,0.05829823017120361,#d62728
-8.697352,-4.4805484,neighbor,206594692,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,0.058552324771881104,#d62728
5.713839,-2.1067052,neighbor,206594692,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,0.05873781442642212,#d62728
-15.447296,10.011683,neighbor,206594692,Fully convolutional networks for semantic segmentation,0.058750808238983154,#d62728
0.7250143,-3.9434302,neighbor,206594692,Scheduled denoising autoencoders,0.0588223934173584,#d62728
-8.781545,-3.7016923,neighbor,206594692,Confusing Deep Convolution Networks by Relabelling,0.05894380807876587,#d62728
7.343466,-6.5880046,neighbor,206594692,Piecewise Linear Multilayer Perceptrons and Dropout,0.05919724702835083,#d62728
-6.378435,-5.3163204,neighbor,206594692,Towards Open Set Deep Networks,0.05961328744888306,#d62728
9.767472,-1.9357071,neighbor,206594692,Fractional Max-Pooling,0.05971527099609375,#d62728
6.841203,5.2643366,neighbor,206594692,Convolutional neural networks with low-rank regularization,0.06042712926864624,#d62728
4.274789,3.4254086,neighbor,206594692,Caffe: Convolutional Architecture for Fast Feature Embedding,0.0606687068939209,#d62728
-16.00119,9.449876,neighbor,206594692,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.06074321269989014,#d62728
4.3676553,2.8254297,neighbor,206594692,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.06081891059875488,#d62728
6.4952326,-10.154493,neighbor,206594692,Representation Benefits of Deep Feedforward Networks,0.06087541580200195,#d62728
-7.50666,8.963007,neighbor,206594692,DAG-Recurrent Neural Networks for Scene Labeling,0.061413347721099854,#d62728
-2.689581,8.380091,neighbor,206594692,Deep convolutional neural networks as generic feature extractors,0.061477839946746826,#d62728
-7.3635283,9.652667,neighbor,206594692,Multi-scale Recognition with DAG-CNNs,0.061480164527893066,#d62728
-3.5326493,11.733004,neighbor,206594692,Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition,0.0615273118019104,#d62728
-7.925983,10.565134,neighbor,206594692,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.06156754493713379,#d62728
-12.467467,7.680528,neighbor,206594692,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.06158214807510376,#d62728
4.708656,-0.5997419,neighbor,206594692,Learning both Weights and Connections for Efficient Neural Network,0.061602771282196045,#d62728
10.730143,-8.635181,neighbor,206594692,Expressiveness of Rectifier Networks,0.0616111159324646,#d62728
-11.715508,5.001969,neighbor,206594692,Object Detection Networks on Convolutional Feature Maps,0.061683058738708496,#d62728
10.153748,-3.0994165,neighbor,206594692,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.061704933643341064,#d62728
12.071807,1.6840658,neighbor,206594692,Differentiable Pooling for Hierarchical Feature Learning,0.061795175075531006,#d62728
3.4163485,-1.5422152,neighbor,206594692,Deep Fried Convnets,0.06196761131286621,#d62728
1.336578,-7.5786915,neighbor,206594692,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.062080562114715576,#d62728
1.0549246,-5.468067,neighbor,206594692,Discriminative Recurrent Sparse Auto-Encoders,0.06238299608230591,#d62728
-15.141409,7.436943,neighbor,206594692,Learning to decompose for object detection and instance segmentation,0.06253659725189209,#d62728
3.1105645,11.736073,neighbor,206594692,Visualizing and Comparing Convolutional Neural Networks,0.06262511014938354,#d62728
12.313762,3.5808673,neighbor,206594692,Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification,0.06265252828598022,#d62728
0.20031504,-0.9415457,neighbor,206594692,Deep SimNets,0.06318199634552002,#d62728
-5.9158506,2.036904,neighbor,206594692,Convolutional Kernel Networks,0.06319475173950195,#d62728
1.0378549,-10.937431,neighbor,206594692,Do Deep Nets Really Need to be Deep?,0.06322962045669556,#d62728
3.930812,-11.655503,neighbor,206594692,FitNets: Hints for Thin Deep Nets,0.06333291530609131,#d62728
-6.4691296,6.834988,neighbor,206594692,Recurrent Models of Visual Attention,0.06337302923202515,#d62728
5.2033315,-3.147978,neighbor,206594692,Memory Bounded Deep Convolutional Networks,0.06342971324920654,#d62728
-5.798504,1.1782798,neighbor,206594692,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.06344175338745117,#d62728
5.514382,-6.6439075,neighbor,206594692,Dropout Rademacher complexity of deep neural networks,0.06344485282897949,#d62728
6.7032094,6.389027,neighbor,206594692,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.06354331970214844,#d62728
-12.996796,4.722923,neighbor,206594692,R-CNN minus R,0.06354624032974243,#d62728
-2.7401178,-1.4834704,neighbor,206594692,Conditional Deep Learning for energy-efficient and enhanced pattern recognition,0.06375241279602051,#d62728
-16.25221,10.375235,neighbor,206594692,Fully Connected Deep Structured Networks,0.06379973888397217,#d62728
-2.0428884,-4.39038,neighbor,206594692,Deep Fishing: Gradient Features from Deep Nets,0.06408220529556274,#d62728
-5.0096874,12.452216,neighbor,206594692,Fine-grained object recognition with Gnostic Fields,0.06447088718414307,#d62728
-2.480128,1.6468263,neighbor,206594692,Large-Scale Deep Learning on the YFCC100M Dataset,0.06454581022262573,#d62728
-13.142457,6.558625,neighbor,206594692,Scalable Object Detection Using Deep Neural Networks,0.06459790468215942,#d62728
-4.713169,4.6986823,neighbor,206594692,Webly Supervised Learning of Convolutional Networks,0.06481283903121948,#d62728
11.865525,4.486567,neighbor,206594692,Sparsity-Regularized HMAX for Visual Recognition,0.06490767002105713,#d62728
-8.533363,-2.729315,neighbor,206594692,Learning from Noisy Labels with Deep Neural Networks,0.06493496894836426,#d62728
3.5473099,1.3837261,neighbor,206594692,Convolutional neural networks at constrained time cost,0.0650818943977356,#d62728
5.366133,-8.508956,neighbor,206594692,GradNets: Dynamic Interpolation Between Neural Architectures,0.0651780366897583,#d62728
3.5974786,-6.3340654,neighbor,206594692,Partitioning Large Scale Deep Belief Networks Using Dropout,0.06520950794219971,#d62728
-14.654451,5.522933,neighbor,206594692,Self-taught object localization with deep networks,0.06523984670639038,#d62728
12.648921,-9.325043,neighbor,206594692,Adjustable Bounded Rectifiers: Towards Deep Binary Representations,0.06528013944625854,#d62728
-3.5162125,-8.2971525,neighbor,206594692,Mediated experts for deep convolutional networks,0.06533974409103394,#d62728
-14.09899,6.957455,neighbor,206594692,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.06563568115234375,#d62728
-3.9147205,10.497466,neighbor,206594692,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.06565356254577637,#d62728
-1.1939055,0.8211937,query,206770307,Fast R-CNN,0.0,#d62728
-1.781235,1.0491028,neighbor,206770307,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.03837317228317261,#d62728
0.8573034,-8.233076,neighbor,206770307,Going deeper with convolutions,0.04339355230331421,#d62728
-1.544496,8.974979,neighbor,206770307,Scalable Object Detection Using Deep Neural Networks,0.0458911657333374,#d62728
-2.0565765,-0.2408468,neighbor,206770307,Object Detection Networks on Convolutional Feature Maps,0.04618799686431885,#d62728
-2.5644693,8.07107,neighbor,206770307,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.04656130075454712,#d62728
0.38772878,7.709052,neighbor,206770307,"Scalable, High-Quality Object Detection",0.04677867889404297,#d62728
-1.7810867,2.8543978,neighbor,206770307,Deep learning for class-generic object detection,0.05153149366378784,#d62728
-3.4740245,6.287206,neighbor,206770307,Mid-level Elements for Object Detection,0.05269819498062134,#d62728
-4.099263,2.5352159,neighbor,206770307,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.05339735746383667,#d62728
1.2172952,7.898861,neighbor,206770307,What Makes for Effective Detection Proposals?,0.053413212299346924,#d62728
1.8569994,-7.7608056,neighbor,206770307,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.05343735218048096,#d62728
2.6928005,-8.085178,neighbor,206770307,Enhanced image classification with a fast-learning shallow convolutional neural network,0.0535014271736145,#d62728
-3.906319,2.6273723,neighbor,206770307,DeepID-Net: Deformable deep convolutional neural networks for object detection,0.054285645484924316,#d62728
2.14397,-6.6431046,neighbor,206770307,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.055453717708587646,#d62728
0.12323983,-5.367754,neighbor,206770307,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.05623781681060791,#d62728
0.7949042,-8.953719,neighbor,206770307,Striving for Simplicity: The All Convolutional Net,0.0572018027305603,#d62728
4.4491534,10.198156,neighbor,206770307,LSDA: Large Scale Detection through Adaptation,0.05759710073471069,#d62728
-1.4926784,4.517146,neighbor,206770307,Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction,0.05851757526397705,#d62728
2.166367,5.2092414,neighbor,206770307,1-HKUST: Object Detection in ILSVRC 2014,0.05864149332046509,#d62728
5.3437343,-10.410776,neighbor,206770307,Caffe: Convolutional Architecture for Fast Feature Embedding,0.05959111452102661,#d62728
-2.980194,3.3192158,neighbor,206770307,Generic Object Detection with Dense Neural Patterns and Regionlets,0.06051725149154663,#d62728
-3.787595,0.7618721,neighbor,206770307,Do More Dropouts in Pool5 Feature Maps for Better Object Detection,0.06064462661743164,#d62728
1.541142,7.8207088,neighbor,206770307,"How good are detection proposals, really?",0.06250196695327759,#d62728
-5.303235,6.9362674,neighbor,206770307,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.06301921606063843,#d62728
5.2991805,-10.779284,neighbor,206770307,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.06435322761535645,#d62728
-4.750777,8.995605,neighbor,206770307,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.06452286243438721,#d62728
-4.635267,6.0898957,neighbor,206770307,Max-Margin Object Detection,0.06478166580200195,#d62728
1.5405627,-13.663444,neighbor,206770307,Deeply-Supervised Nets,0.06517297029495239,#d62728
-1.628819,7.780558,neighbor,206770307,Boosting Convolutional Features for Robust Object Proposals,0.06568515300750732,#d62728
-1.0483898,9.60002,neighbor,206770307,Object-centric Sampling for Fine-grained Image Classification,0.06587767601013184,#d62728
1.2277836,2.8745866,neighbor,206770307,Attention for Fine-Grained Categorization,0.06618750095367432,#d62728
-3.3102312,5.376589,neighbor,206770307,Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection,0.06623709201812744,#d62728
2.8531404,11.266086,neighbor,206770307,Detector discovery in the wild: Joint multiple instance and representation learning,0.06624859571456909,#d62728
2.4111443,4.310119,neighbor,206770307,ImageNet Large Scale Visual Recognition Challenge,0.06645816564559937,#d62728
0.4468792,4.206709,neighbor,206770307,Part-Based R-CNNs for Fine-Grained Category Detection,0.06726878881454468,#d62728
2.9674535,-7.850543,neighbor,206770307,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.06747901439666748,#d62728
-11.234174,3.934401,neighbor,206770307,Convolutional Channel Features,0.06779974699020386,#d62728
-6.318706,7.6069207,neighbor,206770307,Deformable part models are convolutional neural networks,0.06877964735031128,#d62728
5.356064,3.3802106,neighbor,206770307,Training a multi-exit cascade with linear asymmetric classification for efficient object detection,0.06883013248443604,#d62728
-1.4668933,-1.4185289,neighbor,206770307,Do Convnets Learn Correspondence?,0.0693545937538147,#d62728
3.2754245,-11.587742,neighbor,206770307,MatConvNet: Convolutional Neural Networks for MATLAB,0.06949305534362793,#d62728
1.0193288,10.543679,neighbor,206770307,Self-taught object localization with deep networks,0.0696021318435669,#d62728
-6.4281497,9.8798485,neighbor,206770307,Hypercolumns for object segmentation and fine-grained localization,0.06979399919509888,#d62728
6.6395793,-6.3068066,neighbor,206770307,C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning,0.07010060548782349,#d62728
3.3678422,1.3066868,neighbor,206770307,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.070159912109375,#d62728
4.614486,-3.6582854,neighbor,206770307,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.07037526369094849,#d62728
5.0025682,-12.603847,neighbor,206770307,Fast Training of Convolutional Networks through FFTs,0.07076841592788696,#d62728
-2.090925,-6.73876,neighbor,206770307,Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification,0.07080268859863281,#d62728
-6.498259,11.509058,neighbor,206770307,Fully convolutional networks for semantic segmentation,0.07110494375228882,#d62728
-11.101079,6.4134393,neighbor,206770307,Joint Deep Learning for Pedestrian Detection,0.07115983963012695,#d62728
-10.202247,6.4225883,neighbor,206770307,Taking a deeper look at pedestrians,0.07160627841949463,#d62728
-0.3986604,-11.466243,neighbor,206770307,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.07187652587890625,#d62728
-0.9093925,6.3269925,neighbor,206770307,Detect2Rank: Combining Object Detectors Using Learning to Rank,0.07190650701522827,#d62728
-2.3600507,6.568699,neighbor,206770307,Regionlets for Generic Object Detection,0.07198148965835571,#d62728
1.4259548,2.3340652,neighbor,206770307,Fine-grained object recognition with Gnostic Fields,0.07235342264175415,#d62728
-1.6088437,-9.983967,neighbor,206770307,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.07286393642425537,#d62728
2.7111182,-9.370007,neighbor,206770307,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.07295972108840942,#d62728
2.3542998,-12.127756,neighbor,206770307,SimNets: A Generalization of Convolutional Networks,0.0730828046798706,#d62728
-8.823911,-0.5549721,neighbor,206770307,Transferring Rich Feature Hierarchies for Robust Visual Tracking,0.07344013452529907,#d62728
-1.8274615,12.614531,neighbor,206770307,Half-CNN: A General Framework for Whole-Image Regression,0.07370162010192871,#d62728
3.610088,6.675119,neighbor,206770307,Context Forest for efficient object detection with large mixture models,0.0737161636352539,#d62728
3.1264846,-5.7329783,neighbor,206770307,Modelling local deep convolutional neural network features to improve fine-grained image classification,0.07384061813354492,#d62728
6.4838686,-10.287119,neighbor,206770307,Theano-based Large-Scale Visual Recognition with Multiple GPUs,0.07420653104782104,#d62728
7.831924,-4.235179,neighbor,206770307,Building high-level features using large scale unsupervised learning,0.0742759108543396,#d62728
0.8202612,-4.620852,neighbor,206770307,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.074440598487854,#d62728
4.3656807,-12.323472,neighbor,206770307,Deep Fried Convnets,0.07454472780227661,#d62728
2.9855614,8.264691,neighbor,206770307,An active search strategy for efficient object class detection,0.07534551620483398,#d62728
1.5234485,-11.437731,neighbor,206770307,Network In Network,0.07553201913833618,#d62728
-3.1947937,-3.7939935,neighbor,206770307,Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection,0.07585287094116211,#d62728
-10.706452,6.016153,neighbor,206770307,Pedestrian Detection with Unsupervised Multi-stage Feature Learning,0.07608926296234131,#d62728
-1.3823198,-9.586523,neighbor,206770307,Multi-column deep neural networks for image classification,0.07626527547836304,#d62728
-11.099731,6.9859557,neighbor,206770307,Pedestrian detection aided by deep learning semantic tasks,0.07650643587112427,#d62728
-4.902378,4.9651017,neighbor,206770307,The Fastest Deformable Part Model for Object Detection,0.0765523910522461,#d62728
-3.388862,8.510489,neighbor,206770307,Deep Joint Task Learning for Generic Object Extraction,0.0768887996673584,#d62728
4.522657,5.68961,neighbor,206770307,Shared Random Ferns for Efficient Detection of Multiple Categories,0.07706999778747559,#d62728
1.3776746,-0.80215967,neighbor,206770307,Object Detectors Emerge in Deep Scene CNNs,0.0770915150642395,#d62728
4.9523168,-8.792925,neighbor,206770307,Large-Scale Deep Learning on the YFCC100M Dataset,0.07709556818008423,#d62728
-4.863451,11.212773,neighbor,206770307,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.07711493968963623,#d62728
0.33893684,-2.9688427,neighbor,206770307,Scene understanding based on Multi-Scale Pooling of deep learning features,0.07716655731201172,#d62728
5.5215096,-5.6345377,neighbor,206770307,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.07727175951004028,#d62728
5.0370665,-5.794166,neighbor,206770307,Convolutional Kernel Networks,0.07748973369598389,#d62728
-6.5177617,11.501058,neighbor,206770307,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.07758975028991699,#d62728
-6.9665866,4.7725277,neighbor,206770307,Local Decorrelation For Improved Detection,0.0775955319404602,#d62728
0.8904312,6.5265846,neighbor,206770307,Object Proposal Generation Using Two-Stage Cascade SVMs,0.07763427495956421,#d62728
4.040592,-2.7775674,neighbor,206770307,From generic to specific deep representations for visual recognition,0.07764393091201782,#d62728
4.487551,8.451246,neighbor,206770307,ARTOS - Adaptive Real-Time Object Detection System,0.07779765129089355,#d62728
-1.9483166,10.5149975,neighbor,206770307,Salient Object Detection: A Benchmark,0.07791012525558472,#d62728
-0.1716963,-5.6718025,neighbor,206770307,Learnable Pooling Regions for Image Classification,0.07799315452575684,#d62728
4.0373583,-2.22171,neighbor,206770307,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.07861119508743286,#d62728
-2.7187133,-6.8522944,neighbor,206770307,Fast image scanning with deep max-pooling convolutional neural networks,0.07867896556854248,#d62728
2.0016458,11.044351,neighbor,206770307,Weakly Supervised Object Localization with Multi-Fold Multiple Instance Learning,0.07868212461471558,#d62728
5.988958,-13.614491,neighbor,206770307,Speeding Up Neural Networks for Large Scale Classification using WTA Hashing,0.07868432998657227,#d62728
-4.256141,9.939697,neighbor,206770307,Layered object detection for multi-class segmentation,0.07898622751235962,#d62728
-8.264617,-0.6345764,neighbor,206770307,Tracking with deep neural networks,0.07905006408691406,#d62728
1.750742,-10.563063,neighbor,206770307,Deep Epitomic Convolutional Neural Networks,0.07920873165130615,#d62728
-11.190338,3.960557,neighbor,206770307,"Convolutional Channel Features For Pedestrian, Face and Edge Detection",0.07929736375808716,#d62728
-8.841794,-0.64535373,neighbor,206770307,DeepTrack: Learning Discriminative Feature Representations Online for Robust Visual Tracking,0.07937109470367432,#d62728
7.738302,-3.8201838,neighbor,206770307,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.07959175109863281,#d62728
1.0017555,12.91808,neighbor,206770307,Multiple Object Recognition with Visual Attention,0.07959491014480591,#d62728
1.3680661,-13.200637,neighbor,206770307,Maxout Networks,0.08025240898132324,#d62728
5.448677,-2.4343817,query,207930212,Momentum Contrast for Unsupervised Visual Representation Learning,0.0,#ff9896
5.1150417,-8.4361105,neighbor,207930212,A Classification approach towards Unsupervised Learning of Visual Representations,0.04696232080459595,#ff9896
2.7647293,-7.357241,neighbor,207930212,Unsupervised Visual Representation Learning by Context Prediction,0.047379493713378906,#ff9896
6.9070597,-1.1764328,neighbor,207930212,Progressive Recurrent Learning for Visual Recognition,0.04813992977142334,#ff9896
-6.053058,2.5340314,neighbor,207930212,Unsupervised Representation Learning by Predicting Image Rotations,0.04899001121520996,#ff9896
-2.509128,-0.85826564,neighbor,207930212,Multi-task Self-Supervised Visual Learning,0.049777090549468994,#ff9896
-2.1824713,2.1724381,neighbor,207930212,Leveraging Large-Scale Uncurated Data for Unsupervised Pre-training of Visual Features,0.05149185657501221,#ff9896
15.270399,5.2387223,neighbor,207930212,Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors,0.052696049213409424,#ff9896
-5.7679477,7.0507994,neighbor,207930212,Unsupervised Learning by Predicting Noise,0.052876174449920654,#ff9896
8.88175,1.946209,neighbor,207930212,Deep Residual Learning for Image Recognition,0.05300796031951904,#ff9896
7.540229,-10.712046,neighbor,207930212,Meta-Learning Deep Visual Words for Fast Video Object Segmentation,0.05456686019897461,#ff9896
-1.8993471,2.3682272,neighbor,207930212,Unsupervised Pre-Training of Image Features on Non-Curated Data,0.05493193864822388,#ff9896
-0.20329654,-3.5391414,neighbor,207930212,Data-Efficient Image Recognition with Contrastive Predictive Coding,0.055108487606048584,#ff9896
-7.096411,5.6525483,neighbor,207930212,Autoconvolution for Unsupervised Feature Learning,0.05552256107330322,#ff9896
-4.154039,-0.5015887,neighbor,207930212,Revisiting Self-Supervised Visual Representation Learning,0.05554693937301636,#ff9896
-4.4909697,5.739003,neighbor,207930212,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.055749475955963135,#ff9896
5.984177,-10.160895,neighbor,207930212,Learning Features by Watching Objects Move,0.055867135524749756,#ff9896
8.844026,-3.2659116,neighbor,207930212,Gated Channel Transformation for Visual Recognition,0.05636942386627197,#ff9896
12.603663,4.5555053,neighbor,207930212,Rethinking ImageNet Pre-Training,0.0564153790473938,#ff9896
3.9788926,-11.019011,neighbor,207930212,Unsupervised Learning of Visual Representations Using Videos,0.056820452213287354,#ff9896
-7.3064146,7.4351854,neighbor,207930212,Scheduled denoising autoencoders,0.05682981014251709,#ff9896
-1.8905957,4.7793446,neighbor,207930212,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.0573272705078125,#ff9896
12.59994,-4.502452,neighbor,207930212,Attention Augmented Convolutional Networks,0.057414114475250244,#ff9896
0.3060713,2.1115563,neighbor,207930212,Billion-scale semi-supervised learning for image classification,0.05746501684188843,#ff9896
11.265342,0.069175325,neighbor,207930212,Distributed Iterative Gating Networks for Semantic Segmentation,0.05783343315124512,#ff9896
0.17073755,4.209925,neighbor,207930212,HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning,0.057883262634277344,#ff9896
-7.0407176,-4.554156,neighbor,207930212,Competitive Learning Enriches Learning Representation and Accelerates the Fine-tuning of CNNs,0.058182477951049805,#ff9896
-1.020087,7.9734187,neighbor,207930212,Unsupervised Deep Learning by Neighbourhood Discovery,0.05819886922836304,#ff9896
2.4868858,8.239373,neighbor,207930212,Local Aggregation for Unsupervised Learning of Visual Embeddings,0.058306753635406494,#ff9896
-0.008210653,9.231177,neighbor,207930212,Unsupervised Feature Learning via Non-parametric Instance Discrimination,0.05843287706375122,#ff9896
-7.814818,0.89453006,neighbor,207930212,Unsupervised Representation Learning with Prior-Free and Adversarial Mechanism Embedded Autoencoders,0.058447837829589844,#ff9896
9.769925,-4.9124866,neighbor,207930212,Global-and-local attention networks for visual recognition,0.058909714221954346,#ff9896
2.2800293,7.2076564,neighbor,207930212,Joint Unsupervised Learning of Deep Representations and Image Clusters,0.05898815393447876,#ff9896
10.79437,-5.406445,neighbor,207930212,Efficient Attention: Attention with Linear Complexities,0.05990535020828247,#ff9896
10.02674,4.260991,neighbor,207930212,Bag of Tricks for Image Classification with Convolutional Neural Networks,0.06005924940109253,#ff9896
0.49129507,7.796857,neighbor,207930212,Object category learning and retrieval with weak supervision,0.060074687004089355,#ff9896
4.4989724,0.40810195,neighbor,207930212,A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark,0.06036311388015747,#ff9896
-3.2460096,-1.7389257,neighbor,207930212,Scaling and Benchmarking Self-Supervised Visual Representation Learning,0.060414791107177734,#ff9896
0.52925766,-5.2763705,neighbor,207930212,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.060422301292419434,#ff9896
10.069604,1.7228128,neighbor,207930212,Dual Path Networks,0.06058460474014282,#ff9896
3.9484963,-4.9724274,neighbor,207930212,Representation Learning by Learning to Count,0.06104862689971924,#ff9896
3.7940378,-11.632752,neighbor,207930212,Object-Centric Representation Learning from Unlabeled Videos,0.06110560894012451,#ff9896
-1.1042421,4.834244,neighbor,207930212,Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification,0.0611608624458313,#ff9896
9.522486,-8.6017065,neighbor,207930212,TKD: Temporal Knowledge Distillation for Active Perception,0.061476290225982666,#ff9896
-4.8774347,5.2815504,neighbor,207930212,Unsupervised feature learning by augmenting single images,0.06161743402481079,#ff9896
11.601685,1.2352391,neighbor,207930212,Log-DenseNet: How to Sparsify a DenseNet,0.06169837713241577,#ff9896
5.7200594,-5.2688494,neighbor,207930212,SIMCO: SIMilarity-based object COunting,0.061705052852630615,#ff9896
12.67481,-0.8918591,neighbor,207930212,"ESPNetv2: A Light-Weight, Power Efficient, and General Purpose Convolutional Neural Network",0.06192028522491455,#ff9896
-8.787908,5.7931037,neighbor,207930212,Natural Neural Networks,0.06217849254608154,#ff9896
3.7525208,-6.564721,neighbor,207930212,Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles,0.062200963497161865,#ff9896
-1.7348756,10.063502,neighbor,207930212,Object landmark discovery through unsupervised adaptation,0.062205612659454346,#ff9896
13.227521,1.5597714,neighbor,207930212,MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution,0.06242704391479492,#ff9896
-3.3097253,6.1046543,neighbor,207930212,Selective unsupervised feature learning with Convolutional Neural Network (S-CNN),0.06247913837432861,#ff9896
9.96897,-8.1965065,neighbor,207930212,ConvNet Architecture Search for Spatiotemporal Feature Learning,0.06248891353607178,#ff9896
13.331112,5.180203,neighbor,207930212,An Analysis of Pre-Training on Object Detection,0.0626019835472107,#ff9896
9.8340025,6.278578,neighbor,207930212,CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features,0.06275272369384766,#ff9896
-6.0279284,13.598767,neighbor,207930212,Efficient Learning of Sparse Invariant Representations,0.06291478872299194,#ff9896
-4.04243,3.414489,neighbor,207930212,Unsupervised network pretraining via encoding human design,0.06308823823928833,#ff9896
-10.626692,2.1666272,neighbor,207930212,Self-Supervised GANs via Auxiliary Rotation Loss,0.06310629844665527,#ff9896
2.1703477,3.049515,neighbor,207930212,Selfie: Self-supervised Pretraining for Image Embedding,0.06315881013870239,#ff9896
-7.3244467,11.111953,neighbor,207930212,Sparse Coding on Stereo Video for Object Detection,0.06316035985946655,#ff9896
-8.938949,1.5677341,neighbor,207930212,AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations Rather Than Data,0.06339216232299805,#ff9896
-6.389348,11.817131,neighbor,207930212,Unsupervised Feature Learning by Deep Sparse Coding,0.06352293491363525,#ff9896
-11.662979,2.567687,neighbor,207930212,Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks,0.0635918378829956,#ff9896
12.187097,2.8066099,neighbor,207930212,ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks,0.06359797716140747,#ff9896
-0.3929968,0.03617935,neighbor,207930212,Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning,0.06362247467041016,#ff9896
-2.7581391,0.12962684,neighbor,207930212,Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey,0.06363612413406372,#ff9896
11.252742,4.823688,neighbor,207930212,Revisiting Pre-training: An Efficient Training Method for Image Classification.,0.06370103359222412,#ff9896
1.8470646,-10.052305,neighbor,207930212,Learning Representations by Maximizing Mutual Information Across Views,0.06385523080825806,#ff9896
-5.9168134,-0.40810058,neighbor,207930212,Self-Supervised Representation Learning via Neighborhood-Relational Encoding,0.06394708156585693,#ff9896
9.252364,0.8328848,neighbor,207930212,Gated Convolutional Networks with Hybrid Connectivity for Image Classification,0.06397104263305664,#ff9896
1.9939247,6.4329844,neighbor,207930212,Deep Clustering for Unsupervised Learning of Visual Features,0.0640561580657959,#ff9896
-0.128681,-1.6536443,neighbor,207930212,Master's Thesis : Deep Learning for Visual Recognition,0.0640820860862732,#ff9896
2.4408858,-2.7620373,neighbor,207930212,Low-shot visual object recognition,0.06419724225997925,#ff9896
5.8645883,-8.691608,neighbor,207930212,Unsupervised Learning from Video to Detect Foreground Objects in Single Images,0.06435984373092651,#ff9896
12.431983,-7.7695746,neighbor,207930212,Non-local Neural Networks,0.06440460681915283,#ff9896
6.8996105,3.0704868,neighbor,207930212,"Piggyback: Adding Multiple Tasks to a Single, Fixed Network by Learning to Mask",0.06442660093307495,#ff9896
14.133228,7.287982,neighbor,207930212,DSOD: Learning Deeply Supervised Object Detectors from Scratch,0.06459599733352661,#ff9896
10.277173,-0.598133,neighbor,207930212,Batch-shaping for learning conditional channel gated networks,0.06459695100784302,#ff9896
-10.102773,1.1893038,neighbor,207930212,Semi-Supervised Learning with GANs: Revisiting Manifold Regularization,0.06461489200592041,#ff9896
2.467261,-7.9432616,neighbor,207930212,Weakly-Supervised Spatial Context Networks,0.06470108032226562,#ff9896
-0.36988088,-6.6957674,neighbor,207930212,Deep Attentional Structured Representation Learning for Visual Recognition,0.06473225355148315,#ff9896
14.339579,8.133324,neighbor,207930212,Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids,0.06480085849761963,#ff9896
-5.903992,12.886632,neighbor,207930212,Sparsity-Regularized HMAX for Visual Recognition,0.06486588716506958,#ff9896
8.93769,-11.224486,neighbor,207930212,Actor-Critic Instance Segmentation,0.06487607955932617,#ff9896
1.8578051,11.210801,neighbor,207930212,Detector discovery in the wild: Joint multiple instance and representation learning,0.06488943099975586,#ff9896
12.808416,-3.0071058,neighbor,207930212,Linear Context Transform Block,0.06499087810516357,#ff9896
-8.258822,9.241836,neighbor,207930212,Sparse Factorization Layers for Neural Networks with Limited Supervision,0.06504261493682861,#ff9896
7.963383,2.2789927,neighbor,207930212,Aggregated Residual Transformations for Deep Neural Networks,0.06511193513870239,#ff9896
11.244989,-3.7702878,neighbor,207930212,Learn To Pay Attention,0.06526374816894531,#ff9896
-10.854049,-0.681165,neighbor,207930212,Gradient Regularization Improves Accuracy of Discriminative Models,0.06528615951538086,#ff9896
6.7764215,-12.133859,neighbor,207930212,Multigrid Predictive Filter Flow for Unsupervised Learning on Videos,0.06538456678390503,#ff9896
-1.0717531,-7.373858,neighbor,207930212,DAG-Recurrent Neural Networks for Scene Labeling,0.0654338002204895,#ff9896
6.078921,2.0725405,neighbor,207930212,Target Aware Network Adaptation for Efficient Representation Learning,0.06572949886322021,#ff9896
7.6150546,5.385111,neighbor,207930212,"Fast, Better Training Trick - Random Gradient",0.06576073169708252,#ff9896
1.6897517,0.6709799,neighbor,207930212,Large-Scale Deep Learning on the YFCC100M Dataset,0.06579083204269409,#ff9896
2.3400738,-2.4240441,neighbor,207930212,Boosting Few-Shot Visual Learning With Self-Supervision,0.06583482027053833,#ff9896
14.797741,7.171009,neighbor,207930212,ScratchDet: Training Single-Shot Object Detectors From Scratch,0.06583833694458008,#ff9896
-10.290833,5.494105,neighbor,207930212,PixelVAE: A Latent Variable Model for Natural Images,0.06596416234970093,#ff9896
1.364,-5.9016585,neighbor,207930212,Visual Concept Recognition and Localization via Iterative Introspection,0.06598395109176636,#ff9896
-3.2829328,-4.3440447,neighbor,207930212,Hide-and-Seek: A Data Augmentation Technique for Weakly-Supervised Localization and Beyond,0.06600826978683472,#ff9896
-2.7948883,-1.1720842,query,211227,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,0.0,#ff9896
4.182846,4.044442,neighbor,211227,End-to-End Image Super-Resolution via Deep and Shallow Convolutional Networks,0.05042201280593872,#ff9896
3.387809,3.7313738,neighbor,211227,Image Super-Resolution Using Deep Convolutional Networks,0.051548540592193604,#ff9896
2.9763129,2.851906,neighbor,211227,Deeply Improved Sparse Coding for Image Super-Resolution,0.05165308713912964,#ff9896
4.9763637,3.3336356,neighbor,211227,Local- and holistic-structure preserving image super resolution via deep joint component learning,0.05361676216125488,#ff9896
-9.527322,-2.877875,neighbor,211227,Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks,0.054241716861724854,#ff9896
11.546785,-8.202434,neighbor,211227,Global-Local Face Upsampling Network,0.055725038051605225,#ff9896
1.515755,-3.8219657,neighbor,211227,Efficient Upsampling of Natural Images,0.055926740169525146,#ff9896
-12.197268,-1.1544889,neighbor,211227,Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks,0.05670827627182007,#ff9896
5.9384584,-2.4659903,neighbor,211227,Super-resolution from internet-scale scene matching,0.05671107769012451,#ff9896
2.6368375,4.4029937,neighbor,211227,Accurate Image Super-Resolution Using Very Deep Convolutional Networks,0.05765330791473389,#ff9896
4.2018585,2.4737437,neighbor,211227,Self-tuned deep super resolution,0.058379292488098145,#ff9896
7.7703238,-1.2287492,neighbor,211227,RAISR: Rapid and Accurate Image Super Resolution,0.058464765548706055,#ff9896
-11.815226,-0.34144887,neighbor,211227,Energy-based Generative Adversarial Network,0.05998730659484863,#ff9896
6.781971,2.1413393,neighbor,211227,Learning Super-Resolution Jointly From External and Internal Examples,0.06170773506164551,#ff9896
-3.577617,-1.3726593,neighbor,211227,Learning to generate images with perceptual similarity metrics,0.06228291988372803,#ff9896
-4.8607707,0.48296916,neighbor,211227,Super-Resolution with Deep Convolutional Sufficient Statistics,0.06319451332092285,#ff9896
3.9478161,-6.920737,neighbor,211227,Learning a Deep Convolutional Network for Light-Field Image Super-Resolution,0.06346207857131958,#ff9896
5.2769303,4.2344065,neighbor,211227,Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution,0.064738929271698,#ff9896
3.476554,5.111583,neighbor,211227,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,0.06596696376800537,#ff9896
8.681522,0.14542952,neighbor,211227,Single Image Superresolution Using Maximizing Self-Similarity Prior,0.06652718782424927,#ff9896
-8.939903,-2.6974418,neighbor,211227,Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis,0.06657886505126953,#ff9896
4.345827,-0.8770723,neighbor,211227,Image Zoom Completion,0.06665390729904175,#ff9896
3.8936472,5.833704,neighbor,211227,Super-resolution of compressed videos using convolutional neural networks,0.06820690631866455,#ff9896
10.94869,-5.721665,neighbor,211227,A Convex Approach for Image Hallucination,0.06889522075653076,#ff9896
-11.502574,-2.513114,neighbor,211227,Generative Image Modeling Using Style and Structure Adversarial Networks,0.07031196355819702,#ff9896
12.140646,-1.7744471,neighbor,211227,A Bayesian Nonparametric Approach to Image Super-Resolution,0.0710991621017456,#ff9896
11.479897,-5.7766824,neighbor,211227,Image hallucination with primal sketch priors,0.07110846042633057,#ff9896
1.0659559,2.206311,neighbor,211227,Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections,0.07164520025253296,#ff9896
-9.624831,2.074379,neighbor,211227,Semantic Image Inpainting with Deep Generative Models,0.07190227508544922,#ff9896
1.0389559,3.461375,neighbor,211227,Deep Convolution Networks for Compression Artifacts Reduction,0.07222408056259155,#ff9896
9.878858,3.9621265,neighbor,211227,Image super-resolution via dual-dictionary learning and sparse representation,0.07277083396911621,#ff9896
10.797137,6.0354285,neighbor,211227,Joint Prior Learning for Visual Sensor Network Noisy Image Super-Resolution,0.07278454303741455,#ff9896
5.6431046,-5.8514566,neighbor,211227,Similarity-Aware Patchwork Assembly for Depth Image Super-resolution,0.07292115688323975,#ff9896
1.5064852,-0.6994661,neighbor,211227,Joint Demosaicing and Denoising via Learned Nonparametric Random Fields,0.07332754135131836,#ff9896
8.211642,2.0608873,neighbor,211227,Locally regularized Anchored Neighborhood Regression for fast Super-Resolution,0.07376456260681152,#ff9896
2.5967438,5.1448627,neighbor,211227,Accelerating the Super-Resolution Convolutional Neural Network,0.0741240382194519,#ff9896
4.765585,-6.9993057,neighbor,211227,A Deep Primal-Dual Network for Guided Depth Super-Resolution,0.0746622085571289,#ff9896
-5.712809,-2.638338,neighbor,211227,Learning a Discriminative Model for the Perception of Realism in Composite Images,0.07545149326324463,#ff9896
-9.1718235,-4.5994163,neighbor,211227,Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep Extensions,0.07584553956985474,#ff9896
-11.368584,-1.5202452,neighbor,211227,Generating images with recurrent adversarial networks,0.07608109712600708,#ff9896
-7.6919746,-4.842202,neighbor,211227,Texture Synthesis Using Shallow Convolutional Networks with Random Filters,0.07630378007888794,#ff9896
9.125398,3.9597714,neighbor,211227,Image super-resolution using graph regularized block sparse representation,0.07632827758789062,#ff9896
-10.194681,-4.1781125,neighbor,211227,Generative Image Modeling Using Spatial LSTMs,0.07651078701019287,#ff9896
9.246255,-1.2962462,neighbor,211227,Image-adaptive Color Super-resolution,0.07654809951782227,#ff9896
5.168782,-6.6964087,neighbor,211227,ATGV-Net: Accurate Depth Super-Resolution,0.07655656337738037,#ff9896
-8.661349,-3.827925,neighbor,211227,Texture Networks: Feed-forward Synthesis of Textures and Stylized Images,0.07668846845626831,#ff9896
5.5471096,1.3290015,neighbor,211227,Seven Ways to Improve Example-Based Single Image Super Resolution,0.07700967788696289,#ff9896
10.629309,-1.479753,neighbor,211227,Overcoming Registration Uncertainty in Image Super-Resolution: Maximize or Marginalize?,0.07715106010437012,#ff9896
12.41956,-1.0803921,neighbor,211227,Image Super-Resolution via Sparse Bayesian Modeling of Natural Images,0.07720917463302612,#ff9896
-1.0778404,-6.0673704,neighbor,211227,A content-aware image prior,0.07722198963165283,#ff9896
11.036931,1.8818309,neighbor,211227,Modeling deformable gradient compositions for single-image super-resolution,0.07749676704406738,#ff9896
-14.189299,-1.3758711,neighbor,211227,Pixel-Level Domain Transfer,0.0776105523109436,#ff9896
1.727542,1.353818,neighbor,211227,Small Neural Networks can Denoise Image Textures Well: a Useful Complement to BM3D,0.07818561792373657,#ff9896
-7.508344,-5.6510687,neighbor,211227,Texture synthesis through convolutional neural networks and spectrum constraints,0.07873481512069702,#ff9896
-2.1430514,-6.1464777,neighbor,211227,Cascades of Regression Tree Fields for Image Restoration,0.07876324653625488,#ff9896
11.275394,-7.639249,neighbor,211227,Learning Face Hallucination in the Wild,0.07877004146575928,#ff9896
-8.868798,-5.3543363,neighbor,211227,Incorporating long-range consistency in CNN-based texture generation,0.0787956714630127,#ff9896
-7.1955085,-4.541624,neighbor,211227,Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks,0.07888823747634888,#ff9896
3.2372067,-0.79232043,neighbor,211227,Dictionary Learning for Deblurring and Digital Zoom,0.0791824460029602,#ff9896
8.196648,1.1924409,neighbor,211227,Single-image superresolution based on local regression and nonlocal self-similarity,0.07935750484466553,#ff9896
4.640704,7.1104126,neighbor,211227,Scalable Image Coding Based on Epitomes,0.08036023378372192,#ff9896
2.4820094,7.406059,neighbor,211227,A Comparison between Multi-Layer Perceptrons and Convolutional Neural Networks for Text Image Super-Resolution,0.08150714635848999,#ff9896
8.2563305,-2.5647154,neighbor,211227,Fidelity-Naturalness Evaluation of Single Image Super Resolution,0.08162504434585571,#ff9896
0.07329281,-3.8579693,neighbor,211227,Example-based image color and tone style enhancement,0.08168494701385498,#ff9896
-6.730128,0.054020688,neighbor,211227,Mixtures of Conditional Gaussian Scale Mixtures Applied to Multiscale Image Representations,0.08172827959060669,#ff9896
11.5335455,-6.983115,neighbor,211227,A two-step approach to hallucinating faces: global parametric model and local nonparametric model,0.0819094181060791,#ff9896
9.91433,3.0142462,neighbor,211227,Group-based single image super-resolution with online dictionary learning,0.08196443319320679,#ff9896
9.734042,1.7372664,neighbor,211227,Adaptive local nonparametric regression for fast single image super-resolution,0.08230918645858765,#ff9896
-11.954875,0.74838734,neighbor,211227,Autoencoding beyond pixels using a learned similarity metric,0.082430899143219,#ff9896
6.6102715,4.917702,neighbor,211227,Image super-resolution representation via image patches based on extreme learning machine,0.08258068561553955,#ff9896
7.654392,3.2383776,neighbor,211227,How Does the Low-Rank Matrix Decomposition Help Internal and External Learnings for Super-Resolution,0.08293920755386353,#ff9896
1.9112616,5.0258594,neighbor,211227,Deeply-Recursive Convolutional Network for Image Super-Resolution,0.08311772346496582,#ff9896
13.303738,2.7339106,neighbor,211227,Reliability-based mesh-to-grid image reconstruction,0.08355772495269775,#ff9896
6.455705,0.14451581,neighbor,211227,An Example-Based Super-Resolution Algorithm for Selfie Images,0.0837354063987732,#ff9896
12.578091,4.187867,neighbor,211227,Patch-based Statistical Performance Analysis of Upsampling for Precise Super-Resolution,0.08374261856079102,#ff9896
-0.6826414,3.3607001,neighbor,211227,Clearing the Skies: A Deep Network Architecture for Single-Image Rain Removal,0.08385944366455078,#ff9896
12.336149,1.4336104,neighbor,211227,A multi-frame super-resolution method based on the variable-exponent nonlinear diffusion regularizer,0.08392924070358276,#ff9896
-2.8066642,-6.380095,neighbor,211227,Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration,0.08442765474319458,#ff9896
-1.5586681,-2.5084338,neighbor,211227,Deep Neural Networks for HDR imaging,0.08452552556991577,#ff9896
-12.5623045,-3.3321307,neighbor,211227,Generative Visual Manipulation on the Natural Image Manifold,0.08468937873840332,#ff9896
6.5892386,1.3836977,neighbor,211227,Epitomic Image Super-Resolution,0.08478790521621704,#ff9896
2.8681097,-3.1170948,neighbor,211227,Sampling based scene-space video processing,0.08501386642456055,#ff9896
0.20664577,-5.9604354,neighbor,211227,Image restoration using online photo collections,0.08528029918670654,#ff9896
-2.351588,0.45068777,neighbor,211227,Filling in the details: Perceiving from low fidelity images,0.08544552326202393,#ff9896
0.9612132,-4.8180413,neighbor,211227,Multi-scale image harmonization,0.08552080392837524,#ff9896
1.8709348,-7.0516214,neighbor,211227,LIME: A Method for Low-light IMage Enhancement,0.08554846048355103,#ff9896
2.117103,-4.55668,neighbor,211227,Joint bilateral upsampling,0.08562034368515015,#ff9896
7.629344,6.771699,neighbor,211227,Selective Image Super-Resolution,0.08589917421340942,#ff9896
10.691189,3.6858768,neighbor,211227,Sparse Coding Approach for Multi-Frame Image Super Resolution,0.08595699071884155,#ff9896
-12.827534,-0.74648786,neighbor,211227,Improved Techniques for Training GANs,0.08599656820297241,#ff9896
-13.094459,-1.854778,neighbor,211227,Generating Images Part by Part with Composite Generative Adversarial Networks,0.08634591102600098,#ff9896
-12.192706,-1.9008439,neighbor,211227,Generative adversarial networks,0.08650785684585571,#ff9896
12.426697,0.44691545,neighbor,211227,Confidence-aware Levenberg-Marquardt optimization for joint motion estimation and super-resolution,0.08660024404525757,#ff9896
0.70936465,6.4655952,neighbor,211227,Studying Very Low Resolution Recognition Using Deep Networks,0.08669459819793701,#ff9896
9.882405,1.0756205,neighbor,211227,Single Image Super-Resolution via Image Smoothing,0.08682644367218018,#ff9896
10.546955,4.7807393,neighbor,211227,Image Super-Resolution Based on Sparsity Prior via Smoothed l0 Norm,0.0870293378829956,#ff9896
-7.3170877,-0.20766503,neighbor,211227,A note on the evaluation of generative models,0.08719944953918457,#ff9896
-13.468269,0.06812029,neighbor,211227,Adversarial Manipulation of Deep Representations,0.08737063407897949,#ff9896
-9.641263,2.0979302,neighbor,211227,Context Encoders: Feature Learning by Inpainting,0.08783125877380371,#ff9896
11.092695,-8.277385,neighbor,211227,Deep Cascaded Bi-Network for Face Hallucination,0.08784627914428711,#ff9896
-2.8852756,-3.240335,query,215827080,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.0,#9467bd
-3.8575358,-4.612374,neighbor,215827080,Semantic segmentation using regions and parts,0.040609776973724365,#9467bd
-2.0734448,-4.1015596,neighbor,215827080,Layered object detection for multi-class segmentation,0.04138904809951782,#9467bd
-4.4311585,-2.1607618,neighbor,215827080,Measuring the Objectness of Image Windows,0.050763845443725586,#9467bd
2.5638719,3.3814678,neighbor,215827080,Ensemble of exemplar-SVMs for object detection and beyond,0.050994277000427246,#9467bd
2.5670683,0.55994374,neighbor,215827080,Composite Models of Objects and Scenes for Category Recognition,0.05444687604904175,#9467bd
-6.218264,-4.6807117,neighbor,215827080,Semantic contours from inverse detectors,0.054606497287750244,#9467bd
1.4764906,-6.269404,neighbor,215827080,PartBook for image parsing,0.054720282554626465,#9467bd
8.193962,-6.688343,neighbor,215827080,An empirical study of context in object detection,0.05912888050079346,#9467bd
7.838563,11.002881,neighbor,215827080,Context-Aware Semi-Local Feature Detector,0.059262216091156006,#9467bd
-5.5041757,6.723395,neighbor,215827080,A novel method for object localization in digital images,0.05968606472015381,#9467bd
-6.5509214,-13.632932,neighbor,215827080,Image Segmentation by Cascaded Region Agglomeration,0.06056642532348633,#9467bd
1.8471745,-9.140594,neighbor,215827080,Recurrent Convolutional Neural Networks for Scene Parsing,0.06100571155548096,#9467bd
1.7444443,-3.3481936,neighbor,215827080,"Recursive compositional models: Representation, learning, and inference",0.06101924180984497,#9467bd
-0.52355695,-12.4660225,neighbor,215827080,Scene Segmentation with Low-Dimensional Semantic Representations and Conditional Random Fields,0.06150466203689575,#9467bd
-0.41796318,-8.413813,neighbor,215827080,Context by region ancestry,0.06175023317337036,#9467bd
0.009328382,-2.2441597,neighbor,215827080,Part and appearance sharing: Recursive Compositional Models for multi-view,0.06334024667739868,#9467bd
-4.692297,7.5087256,neighbor,215827080,Efficient Subwindow Search: A Branch and Bound Framework for Object Localization,0.06420755386352539,#9467bd
-4.1794653,6.0909204,neighbor,215827080,Fast concurrent object localization and recognition,0.06640344858169556,#9467bd
1.6650622,1.361115,neighbor,215827080,Beyond Local Appearance: Category Recognition from Pairwise Interactions of Simple Features,0.06732374429702759,#9467bd
12.589482,2.4564896,neighbor,215827080,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.06733465194702148,#9467bd
0.80012774,5.737338,neighbor,215827080,Learning an Alphabet of Shape and Appearance for Multi-Class Object Detection,0.06741183996200562,#9467bd
12.837481,1.2584568,neighbor,215827080,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.0676107406616211,#9467bd
-8.919692,3.7876718,neighbor,215827080,Efficient object detection and segmentation with a cascaded Hough Forest ISM,0.06821811199188232,#9467bd
-11.40038,0.76044333,neighbor,215827080,Shape-based pedestrian parsing,0.06829488277435303,#9467bd
-3.1071014,-8.312448,neighbor,215827080,Learning to Find Object Boundaries Using Motion Cues,0.06865334510803223,#9467bd
6.099327,8.092768,neighbor,215827080,Context-Based Object-Class Recognition and Retrieval by Generalized Correlograms,0.0691729187965393,#9467bd
0.6276661,7.3093524,neighbor,215827080,Shared Random Ferns for Efficient Detection of Multiple Categories,0.06917357444763184,#9467bd
-7.862639,-0.13552248,neighbor,215827080,Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection,0.0691760778427124,#9467bd
0.16530801,-15.667375,neighbor,215827080,Multiclass Image Segmentation Based on Pixel and Segment Level,0.06956428289413452,#9467bd
0.9501419,4.8612065,neighbor,215827080,Incremental learning of object detectors using a visual shape alphabet,0.06968331336975098,#9467bd
8.762285,-9.445909,neighbor,215827080,Improved semantic region labeling based on scene context,0.07060784101486206,#9467bd
-5.2219834,3.1564846,neighbor,215827080,Implicit spatial inference with sparse local features,0.0706254243850708,#9467bd
-4.095507,-15.912349,neighbor,215827080,A benchmark for semantic image segmentation,0.07070887088775635,#9467bd
0.62442625,-4.1488366,neighbor,215827080,Learning a Hierarchical Deformable Template for Rapid Deformable Object Parsing,0.07087051868438721,#9467bd
-1.8884153,13.383057,neighbor,215827080,Effective constructing training sets for object detection,0.07127666473388672,#9467bd
9.115883,-12.574224,neighbor,215827080,A hierarchical field framework for unified context-based classification,0.07131928205490112,#9467bd
9.476154,13.425572,neighbor,215827080,Salient Object Detection: A Discriminative Regional Feature Integration Approach,0.0715373158454895,#9467bd
-6.4096813,12.310278,neighbor,215827080,Training a multi-exit cascade with linear asymmetric classification for efficient object detection,0.07242411375045776,#9467bd
2.7801611,6.391118,neighbor,215827080,Sharing Visual Features for Multiclass and Multiview Object Detection,0.07329803705215454,#9467bd
2.9287286,-5.496033,neighbor,215827080,Recognition by association via learning per-exemplar distances,0.07345843315124512,#9467bd
10.836223,-11.916504,neighbor,215827080,Contextual Hierarchical Part-Driven Conditional Random Field Model for Object Category Detection,0.07353746891021729,#9467bd
13.326335,-0.30603588,neighbor,215827080,Fast image scanning with deep max-pooling convolutional neural networks,0.07370465993881226,#9467bd
-6.914069,-10.664182,neighbor,215827080,Unsupervised detection and segmentation of identical objects,0.07398319244384766,#9467bd
-0.73424387,-18.042696,neighbor,215827080,Using Multiple Segmentations to Discover Objects and their Extent in Image Collections,0.07424455881118774,#9467bd
6.4593196,-2.4351873,neighbor,215827080,Object Recognition by Scene Alignment,0.07489711046218872,#9467bd
-9.81283,1.1310666,neighbor,215827080,Pedestrian Detection with Unsupervised Multi-stage Feature Learning,0.07556426525115967,#9467bd
5.2061872,-8.255976,neighbor,215827080,Learning Object Location Predictors with Boosting and Grammar-Guided Feature Extraction,0.07615542411804199,#9467bd
4.3454585,5.712005,neighbor,215827080,Unsupervised and Transfer Learning under Uncertainty - From Object Detections to Scene Categorization,0.07628816366195679,#9467bd
13.883375,6.1394753,neighbor,215827080,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.07632356882095337,#9467bd
1.1066381,-16.346474,neighbor,215827080,Theme-Based Multi-class Object Recognition and Segmentation,0.07649081945419312,#9467bd
10.486344,13.416925,neighbor,215827080,Is Bottom-Up Attention Useful for Scene Recognition?,0.07649600505828857,#9467bd
2.2108212,-14.491929,neighbor,215827080,Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials,0.07674932479858398,#9467bd
-11.063814,3.522062,neighbor,215827080,Optimized Pedestrian Detection for Multiple and Occluded People,0.07688087224960327,#9467bd
8.727844,6.0420957,neighbor,215827080,Local Naive Bayes Nearest Neighbor for image classification,0.07697999477386475,#9467bd
1.9342458,-13.146789,neighbor,215827080,Harmony potentials for joint classification and segmentation,0.07712244987487793,#9467bd
5.4046082,12.584364,neighbor,215827080,Towards optimal bag-of-features for object categorization and semantic video retrieval,0.07712805271148682,#9467bd
-2.7019057,-16.106215,neighbor,215827080,Joint semantic segmentation by searching for compatible-competitive references,0.07741999626159668,#9467bd
11.8645315,5.3126936,neighbor,215827080,Learnable Pooling Regions for Image Classification,0.07748615741729736,#9467bd
3.909294,-9.031144,neighbor,215827080,A Hierarchical and Contextual Model for Aerial Image Parsing,0.0775572657585144,#9467bd
-9.299813,-4.2203755,neighbor,215827080,BUBL: An effective region labeling tool using a hexagonal lattice,0.07779580354690552,#9467bd
8.16556,-8.173029,neighbor,215827080,Using the forest to see the trees: exploiting context for visual object detection and localization,0.07781583070755005,#9467bd
8.304182,9.413659,neighbor,215827080,Color Descriptors for Object Category Recognition,0.07787436246871948,#9467bd
-10.8291025,1.994182,neighbor,215827080,Real-time pedestrian detection with deformable part models,0.07793331146240234,#9467bd
-9.220469,5.6443996,neighbor,215827080,Improved Foreground Detection via Block-Based Classifier Cascade With Probabilistic Decision Integration,0.07809573411941528,#9467bd
-10.059621,-8.395656,neighbor,215827080,Region-based Segmentation and Object Detection,0.07812517881393433,#9467bd
0.61738527,-0.58174413,neighbor,215827080,Learning shared body plans,0.07812684774398804,#9467bd
8.404325,-13.747336,neighbor,215827080,Semantic context modeling with maximal margin Conditional Random Fields for automatic image annotation,0.0782707929611206,#9467bd
8.907415,-16.7203,neighbor,215827080,Label to region by bi-layer sparsity priors,0.07852017879486084,#9467bd
7.4318533,-2.797147,neighbor,215827080,Fusing object detection and region appearance for image-text alignment,0.07862651348114014,#9467bd
6.993724,1.4093997,neighbor,215827080,Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models,0.07894325256347656,#9467bd
-7.2386017,-16.7252,neighbor,215827080,A hierarchical graph model for object cosegmentation,0.079082190990448,#9467bd
4.5258646,0.31637624,neighbor,215827080,Semantic Robot Vision Challenge: Current State and Future Directions,0.07916390895843506,#9467bd
-6.9546905,-5.5377107,neighbor,215827080,Generalized Boundaries from Multiple Image Interpretations,0.07920318841934204,#9467bd
4.5336256,13.519299,neighbor,215827080,Detection bank: an object detection based video representation for multimedia event recognition,0.07921969890594482,#9467bd
-3.16834,11.197861,neighbor,215827080,"An unsupervised, online learning framework for moving object detection",0.07959049940109253,#9467bd
-3.9684396,3.4125628,neighbor,215827080,Object Detection in Real Images,0.07993948459625244,#9467bd
12.086339,13.594927,neighbor,215827080,Biased Competition in Visual Processing Hierarchies: A Learning Approach Using Multiple Cues,0.08024269342422485,#9467bd
-7.6329594,-15.0432625,neighbor,215827080,SEEDS: Superpixels Extracted Via Energy-Driven Sampling,0.08028662204742432,#9467bd
4.1927905,3.5853834,neighbor,215827080,Blocks That Shout: Distinctive Parts for Scene Classification,0.08029025793075562,#9467bd
10.5767565,6.2398076,neighbor,215827080,Visual Objects Classification with Sliding Spatial Pyramid Matching,0.08033263683319092,#9467bd
-9.726365,8.618574,neighbor,215827080,Is the Game worth the Candle? - Evaluation of OpenCL for Object Detection Algorithm Optimization,0.0805426836013794,#9467bd
0.41427746,-14.116263,neighbor,215827080,Semantic Segmentation with Same Topic Constraints,0.0806930661201477,#9467bd
-10.316994,-9.160212,neighbor,215827080,Object detection and segmentation on a hierarchical region-based image representation,0.08072906732559204,#9467bd
12.595599,12.266151,neighbor,215827080,What are the Visual Features Underlying Rapid Object Recognition?,0.08129262924194336,#9467bd
5.7420363,8.700183,neighbor,215827080,Fast spatial pattern discovery integrating boosting with constellations of contextual descriptors,0.0813741683959961,#9467bd
-7.7111497,8.296196,neighbor,215827080,Computation of Rotation Local Invariant Features using the Integral Image for Real Time Object Detection,0.08144503831863403,#9467bd
2.4733334,9.873082,neighbor,215827080,MIS-Boost: Multiple Instance Selection Boosting,0.08184146881103516,#9467bd
-2.1891236,-11.6466255,neighbor,215827080,Indoor Semantic Segmentation using depth information,0.08207792043685913,#9467bd
7.457302,-15.691141,neighbor,215827080,Formulating semantic image annotation as a supervised learning problem,0.08242589235305786,#9467bd
15.4137335,5.305188,neighbor,215827080,Building high-level features using large scale unsupervised learning,0.08312326669692993,#9467bd
-1.0437256,10.111997,neighbor,215827080,Learning From a Small Number of Training Examples by Exploiting Object Categories,0.08323425054550171,#9467bd
-1.8570609,5.2587934,neighbor,215827080,Viewpoint Invariant Object Detector,0.08330833911895752,#9467bd
-2.1344573,-9.556272,neighbor,215827080,Geometric Context from Videos,0.08339077234268188,#9467bd
3.1223242,-2.6791728,neighbor,215827080,Complexity of Representation and Inference in Compositional Models with Part Sharing,0.0834810733795166,#9467bd
-4.4487925,-7.8779545,neighbor,215827080,Occlusion Reasoning for Object Detectionunder Arbitrary Viewpoint,0.08348846435546875,#9467bd
9.590036,-17.059298,neighbor,215827080,Label-to-region with continuity-biased bi-layer sparsity priors,0.08415955305099487,#9467bd
-5.970058,11.4460945,neighbor,215827080,Fast and Robust Object Detection Using Asymmetric Totally Corrective Boosting,0.08496403694152832,#9467bd
-2.6883771,2.072871,neighbor,215827080,Dynamical And-Or Graph Learning for Object Shape Modeling and Detection,0.08503645658493042,#9467bd
-1.8170972,11.930126,neighbor,215827080,Semi-Supervised Self-Training of Object Detection Models,0.08549368381500244,#9467bd
-5.9443746,-14.622018,neighbor,215827080,Image Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration,0.08556020259857178,#9467bd
4.3729467,3.165351,query,216078090,Auto-Encoding Variational Bayes,0.0,#9467bd
10.910189,4.155383,neighbor,216078090,Stochastic variational inference,0.052828311920166016,#9467bd
5.9062114,2.3965282,neighbor,216078090,Automated Variational Inference in Probabilistic Programming,0.05450063943862915,#9467bd
8.256041,5.7884336,neighbor,216078090,Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression,0.05750960111618042,#9467bd
-8.175225,2.8073921,neighbor,216078090,Layer-wise learning of deep generative models,0.061906397342681885,#9467bd
4.423784,5.289529,neighbor,216078090,Lifted Relational Variational Inference,0.06302356719970703,#9467bd
-7.4720297,1.4795073,neighbor,216078090,Deep AutoRegressive Networks,0.06380069255828857,#9467bd
-12.328801,3.26416,neighbor,216078090,Joint Training Deep Boltzmann Machines for Classification,0.06521672010421753,#9467bd
13.20387,7.365823,neighbor,216078090,Streaming Variational Bayes,0.0653611421585083,#9467bd
-1.0569546,2.26296,neighbor,216078090,Inference with Discriminative Posterior,0.06628566980361938,#9467bd
-7.2727895,0.3154143,neighbor,216078090,A Deep and Tractable Density Estimator,0.06638163328170776,#9467bd
7.522143,-3.1205459,neighbor,216078090,Automorphism Groups of Graphical Models and Lifted Variational Inference,0.06682980060577393,#9467bd
7.9106474,4.0618668,neighbor,216078090,Incorporating Expressive Graphical Models in VariationalApproximations: Chain-graphs and Hidden Variables,0.0671318769454956,#9467bd
-2.4193232,6.5414543,neighbor,216078090,Asymptotic Analysis of Generative Semi-Supervised Learning,0.06788969039916992,#9467bd
7.355824,-5.136639,neighbor,216078090,Efficient Lifting for Online Probabilistic Inference,0.06832355260848999,#9467bd
12.123475,3.3588734,neighbor,216078090,Sparse stochastic inference for latent Dirichlet allocation,0.06842803955078125,#9467bd
-1.627191,5.616722,neighbor,216078090,Generative Prior Knowledge for Discriminative Classification,0.06856614351272583,#9467bd
-1.5469939,-8.711606,neighbor,216078090,Bayesian inference with posterior regularization and applications to infinite latent SVMs,0.06947660446166992,#9467bd
3.2811894,-0.84916687,neighbor,216078090,Viterbi training in PRISM,0.07000041007995605,#9467bd
-3.69691,6.3203425,neighbor,216078090,Stochastic feature mapping for PAC-Bayes classification,0.07005304098129272,#9467bd
1.6783907,0.7093615,neighbor,216078090,Model-based machine learning,0.0701172947883606,#9467bd
-8.229433,4.3208666,neighbor,216078090,Disentangling Factors of Variation via Generative Entangling,0.07029902935028076,#9467bd
14.5014305,3.1189096,neighbor,216078090,Algorithms of the LDA model [REPORT],0.0703728199005127,#9467bd
-9.616348,0.7232156,neighbor,216078090,Deep Mixtures of Factor Analysers,0.0704803466796875,#9467bd
1.4933563,-11.085308,neighbor,216078090,Approximated Structured Prediction for Learning Large Scale Graphical Models,0.0707552433013916,#9467bd
14.066192,0.9615811,neighbor,216078090,Rethinking Collapsed Variational Bayes Inference for LDA,0.07080352306365967,#9467bd
-6.687055,-1.3294497,neighbor,216078090,High-Dimensional Probability Estimation with Deep Density Models,0.07090914249420166,#9467bd
12.42334,2.0759974,neighbor,216078090,Hybrid Variational/Gibbs Collapsed Inference in Topic Models,0.07097238302230835,#9467bd
-12.365412,2.6699119,neighbor,216078090,On Training Deep Boltzmann Machines,0.0715799331665039,#9467bd
13.759237,2.0713863,neighbor,216078090,Practical Collapsed Stochastic Variational Inference for the HDP,0.07195621728897095,#9467bd
-1.3588033,-11.702441,neighbor,216078090,Alternating Projections for Learning with Expectation Constraints,0.07209235429763794,#9467bd
5.321381,1.2284667,neighbor,216078090,Variational Program Inference,0.07261872291564941,#9467bd
-0.43318078,-5.601502,neighbor,216078090,Exact Maximum Margin Structure Learning of Bayesian Networks,0.0733230710029602,#9467bd
-4.1553946,-1.8010843,neighbor,216078090,Inference-less Density Estimation using Copula Bayesian Networks,0.07339179515838623,#9467bd
0.9521843,1.7531414,neighbor,216078090,Efficient Methods for Unsupervised Learning of Probabilistic Models,0.07381397485733032,#9467bd
-11.514361,0.50789255,neighbor,216078090,Learning the Structure of Deep Sparse Graphical Models,0.07396101951599121,#9467bd
9.441706,5.903097,neighbor,216078090,Variational Bayesian Inference with Stochastic Search,0.07397371530532837,#9467bd
13.2951,2.93101,neighbor,216078090,Stochastic collapsed variational Bayesian inference for latent Dirichlet allocation,0.07398843765258789,#9467bd
-13.177453,0.73979944,neighbor,216078090,On the quantitative analysis of deep belief networks,0.07408010959625244,#9467bd
7.0191894,7.612627,neighbor,216078090,Nonparametric variational inference,0.0744733214378357,#9467bd
-11.9810295,4.5826817,neighbor,216078090,Learning Feature Hierarchies with Centered Deep Boltzmann Machines,0.07487499713897705,#9467bd
0.44354784,-0.37119916,neighbor,216078090,A PAC-Bayesian Tutorial with A Dropout Bound,0.07501423358917236,#9467bd
2.9132204,-4.922665,neighbor,216078090,Closed-Form Learning of Markov Networks from Dependency Networks,0.07506895065307617,#9467bd
-12.643235,5.307097,neighbor,216078090,Learning with Hierarchical-Deep Models,0.07516711950302124,#9467bd
4.160452,-10.829048,neighbor,216078090,Piecewise Training for Undirected Models,0.07531845569610596,#9467bd
5.4496675,5.6543064,neighbor,216078090,Mean Field Variational Approximation for Continuous-Time Bayesian Networks,0.07556641101837158,#9467bd
-1.915017,-4.831436,neighbor,216078090,Large-Sample Learning of Bayesian Networks is NP-Hard,0.07628566026687622,#9467bd
4.31423,-0.007572289,neighbor,216078090,Monolingual Probabilistic Programming Using Generalized Coroutines,0.07667940855026245,#9467bd
-10.989886,-4.445131,neighbor,216078090,Generalized Denoising Auto-Encoders as Generative Models,0.07674282789230347,#9467bd
7.2761297,1.6812893,neighbor,216078090,Statistical mechanics of learning: a variational approach for real data.,0.07681828737258911,#9467bd
-14.419811,1.8806769,neighbor,216078090,Discrete restricted Boltzmann machines,0.07686787843704224,#9467bd
5.323048,-0.18351237,neighbor,216078090,Gibbs Sampling in Open-Universe Stochastic Languages,0.07699072360992432,#9467bd
3.5402346,-10.465096,neighbor,216078090,Piecewise training for structured prediction,0.07712113857269287,#9467bd
-2.83904,-4.4910035,neighbor,216078090,Fast Exact Inference for Recursive Cardinality Models,0.07726240158081055,#9467bd
-0.17910965,-6.014742,neighbor,216078090,Maximum Margin Bayesian Networks,0.0775938630104065,#9467bd
-2.051745,-9.828116,neighbor,216078090,Online Bayesian Passive-Aggressive Learning,0.07766538858413696,#9467bd
-13.775808,4.720421,neighbor,216078090,Modeling Documents with Deep Boltzmann Machines,0.07777392864227295,#9467bd
-2.2872748,1.9819598,neighbor,216078090,Dynamic Blocking and Collapsing for Gibbs Sampling,0.07796663045883179,#9467bd
-10.570257,5.321538,neighbor,216078090,Deep Learning of Representations: Looking Forward,0.07802003622055054,#9467bd
14.96296,5.38796,neighbor,216078090,Characterizing A Database of Sequential Behaviors with Latent Dirichlet Hidden Markov Models,0.07814586162567139,#9467bd
-10.639474,-4.4205713,neighbor,216078090,What regularized auto-encoders learn from the data-generating distribution,0.07823032140731812,#9467bd
2.2885766,-10.180297,neighbor,216078090,Hinge-loss Markov Random Fields: Convex Inference for Structured Prediction,0.07824969291687012,#9467bd
-5.905702,1.9923558,neighbor,216078090,Products of Hidden Markov Models: It Takes N>1 to Tango,0.07849311828613281,#9467bd
7.6409035,-7.243044,neighbor,216078090,Learning Markov graphs up to edit distance,0.07855021953582764,#9467bd
0.47106618,-8.478629,neighbor,216078090,Bayesian Random Fields: The Bethe-Laplace Approximation,0.07861119508743286,#9467bd
5.7912927,3.9930015,neighbor,216078090,Approximate Inference and Stochastic Optimal Control,0.07871085405349731,#9467bd
13.270124,5.148965,neighbor,216078090,Continuous Time Dynamic Topic Models,0.07877475023269653,#9467bd
3.1974788,6.403178,neighbor,216078090,Variational Dual-Tree Framework for Large-Scale Transition Matrix Approximation,0.07887321710586548,#9467bd
0.53136265,-12.600963,neighbor,216078090,Learning Graphical Model Parameters with Approximate Marginal Inference,0.07902252674102783,#9467bd
3.2364514,-6.1031694,neighbor,216078090,Learning Markov Networks with Context-Specific Independences,0.07909119129180908,#9467bd
12.049389,5.460419,neighbor,216078090,Hierarchical relational models for document networks,0.07930499315261841,#9467bd
4.0877867,-4.1120644,neighbor,216078090,Mixture-of-Parents Maximum Entropy Markov Models,0.07933920621871948,#9467bd
-1.9757245,-12.153505,neighbor,216078090,On primal and dual sparsity of Markov networks,0.07969707250595093,#9467bd
4.7104583,-6.34048,neighbor,216078090,Submodular decomposition framework for inference in associative Markov networks with global constraints,0.07982230186462402,#9467bd
-4.6999555,5.74877,neighbor,216078090,Bias-variance tradeoff in hybrid generative-discriminative models,0.07993054389953613,#9467bd
-3.9290547,-8.190656,neighbor,216078090,Bayesian Optimization in a Billion Dimensions via Random Embeddings,0.0799551010131836,#9467bd
16.275017,3.5579464,neighbor,216078090,Variational Learning for Finite Dirichlet Mixture Models and Applications,0.08008760213851929,#9467bd
0.09990937,10.253028,neighbor,216078090,A Bayesian Matrix Factorization Model for Relational Data,0.08012253046035767,#9467bd
-10.024231,-0.08080725,neighbor,216078090,Convolutional Factor Graphs as Probabilistic Models,0.08041888475418091,#9467bd
-4.5871396,3.7013564,neighbor,216078090,Bounding the Test Log-Likelihood of Generative Models,0.08042442798614502,#9467bd
-0.6133006,10.134998,neighbor,216078090,Exploiting compositionality to explore a large space of model structures,0.08053022623062134,#9467bd
-7.7749114,6.9844017,neighbor,216078090,On Nonparametric Guidance for Learning Autoencoder Representations,0.08096522092819214,#9467bd
-2.2797768,-2.9960337,neighbor,216078090,Exact Inference in Networks with Discrete Children of Continuous Parents,0.08116859197616577,#9467bd
2.722746,-6.8161287,neighbor,216078090,Constraint-free Graphical Model with Fast Learning Algorithm,0.08122706413269043,#9467bd
7.657283,6.645726,neighbor,216078090,Approximate inference via variational sampling,0.08124208450317383,#9467bd
5.3613644,-8.838045,neighbor,216078090,Conditional Probability Tree Estimation Analysis and Algorithms,0.0813630223274231,#9467bd
9.206454,-3.8185365,neighbor,216078090,Scaling the Indian Buffet Process via Submodular Maximization,0.08142292499542236,#9467bd
8.544401,10.149525,neighbor,216078090,Dual Decomposition for Marginal Inference,0.08144819736480713,#9467bd
-0.0535007,3.72082,neighbor,216078090,Domain Knowledge Uncertainty and Probabilistic Parameter Constraints,0.0815896987915039,#9467bd
-5.1857615,0.5951231,neighbor,216078090,Discrete MDL Predicts in Total Variation,0.08168035745620728,#9467bd
1.9160229,-5.118752,neighbor,216078090,Mean Field Inference in Dependency Networks: An Empirical Study,0.0817461609840393,#9467bd
-7.4879427,-3.2695875,neighbor,216078090,Minimum Probability Flow Learning,0.08193659782409668,#9467bd
9.576673,7.5461345,neighbor,216078090,Fast Variational Inference in the Conjugate Exponential Family,0.08196413516998291,#9467bd
-10.665145,3.4941795,neighbor,216078090,An introduction to deep learning,0.0820196270942688,#9467bd
10.787035,-0.8277753,neighbor,216078090,Herded Gibbs Sampling,0.08209311962127686,#9467bd
2.2982788,-3.154114,neighbor,216078090,Learning Markov Logic Networks via Functional Gradient Boosting,0.08213889598846436,#9467bd
-2.8067179,7.4204226,neighbor,216078090,Exponential Family Hybrid Semi-Supervised Learning,0.08214545249938965,#9467bd
16.163658,0.9728588,neighbor,216078090,Variational Bayesian GMM for speech recognition,0.08214753866195679,#9467bd
1.888462,2.9323707,neighbor,216078090,Lipschitz Parametrization of Probabilistic Graphical Models,0.08232933282852173,#9467bd
7.4865155,-9.145175,neighbor,216078090,Transductive Rademacher Complexity and Its Applications,0.08233356475830078,#9467bd
-1.3122865,-2.8154376,neighbor,216078090,Initialization and Restart in Stochastic Local Search: Computing a Most Probable Explanation in Bayesian Networks,0.08243387937545776,#9467bd
11.753645,-8.565615,query,225039882,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,0.0,#c5b0d5
11.077701,-8.637662,neighbor,225039882,Visual Transformers: Token-based Image Representation and Processing for Computer Vision,0.04756438732147217,#c5b0d5
15.416895,-7.9304075,neighbor,225039882,Injecting Hierarchy with U-Net Transformers,0.05535787343978882,#c5b0d5
-2.1215036,4.5982113,neighbor,225039882,Deep Residual Learning for Image Recognition,0.059117674827575684,#c5b0d5
16.578968,-10.71854,neighbor,225039882,Reformer: The Efficient Transformer,0.05980914831161499,#c5b0d5
-2.7374322,-6.8314743,neighbor,225039882,Attention Inspiring Receptive-Fields Network for Learning Invariant Representations,0.06038165092468262,#c5b0d5
-5.1273003,4.4020314,neighbor,225039882,Aggregated Residual Transformations for Deep Neural Networks,0.06076157093048096,#c5b0d5
-1.2682948,-5.5193195,neighbor,225039882,Residual Attention Network for Image Classification,0.06106674671173096,#c5b0d5
5.368338,3.6799927,neighbor,225039882,Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,0.06303375959396362,#c5b0d5
3.0476992,-7.7260437,neighbor,225039882,Attention-Guided Spatial Transformer Networks for Fine-Grained Visual Recognition,0.06310224533081055,#c5b0d5
-12.571211,0.14519131,neighbor,225039882,Deep FisherNet for Image Classification,0.06375926733016968,#c5b0d5
1.1385428,7.211916,neighbor,225039882,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.06422358751296997,#c5b0d5
5.8606377,-6.749006,neighbor,225039882,Deep Attentional Structured Representation Learning for Visual Recognition,0.06474894285202026,#c5b0d5
-1.6738999,-7.703765,neighbor,225039882,Learn to Pay Attention Via Switchable Attention for Image Recognition,0.06508952379226685,#c5b0d5
2.244648,2.4870296,neighbor,225039882,Billion-scale semi-supervised learning for image classification,0.06590837240219116,#c5b0d5
16.407343,-7.1081376,neighbor,225039882,Multi-branch Attentive Transformer,0.06590938568115234,#c5b0d5
-6.0921555,-9.592961,neighbor,225039882,Big Transfer (BiT): General Visual Representation Learning,0.06594794988632202,#c5b0d5
4.352096,-6.6019955,neighbor,225039882,Enriched Deep Recurrent Visual Attention Model for Multiple Object Recognition,0.06626254320144653,#c5b0d5
-1.3145013,-8.751176,neighbor,225039882,"Reproduction Report on ""Learn to Pay Attention""",0.06629472970962524,#c5b0d5
0.30523592,9.390479,neighbor,225039882,ZipNet: ZFNet-level Accuracy with 48× Fewer Parameters,0.06648051738739014,#c5b0d5
16.430521,-9.449797,neighbor,225039882,Efficient Transformers: A Survey,0.06666690111160278,#c5b0d5
9.401851,-5.0115113,neighbor,225039882,Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks,0.06667524576187134,#c5b0d5
13.270295,-6.0074005,neighbor,225039882,Entangled Transformer for Image Captioning,0.06692779064178467,#c5b0d5
-0.46895695,-6.5385976,neighbor,225039882,DCANet: Learning Connected Attentions for Convolutional Neural Networks,0.06700414419174194,#c5b0d5
-3.760412,10.336291,neighbor,225039882,SparseNet: A Sparse DenseNet for Image Classification,0.06709891557693481,#c5b0d5
0.74125373,11.295846,neighbor,225039882,Binarized Neural Networks on the ImageNet Classification Task,0.0675763487815857,#c5b0d5
7.4683957,8.168501,neighbor,225039882,Document Image Classification using SqueezeNet Convolutional Neural Network,0.06782054901123047,#c5b0d5
2.7006311,-10.131381,neighbor,225039882,Global-and-local attention networks for visual recognition,0.06788867712020874,#c5b0d5
-2.7966962,3.2328477,neighbor,225039882,Improved Residual Networks for Image and Video Recognition,0.0679808259010315,#c5b0d5
2.1364937,7.9727707,neighbor,225039882,DecomposeMe: Simplifying ConvNets for End-to-End Learning,0.06805139780044556,#c5b0d5
-0.49942866,4.9610744,neighbor,225039882,Deep Isometric Learning for Visual Recognition,0.06839585304260254,#c5b0d5
-5.0028214,9.092838,neighbor,225039882,SliceOut: Training Transformers and CNNs faster while using less memory,0.06841146945953369,#c5b0d5
2.2401881,-7.6965322,neighbor,225039882,Spanet: Spatial Pyramid Attention Network for Enhanced Image Recognition,0.06902456283569336,#c5b0d5
3.6541705,1.3189467,neighbor,225039882,A Simple Framework for Contrastive Learning of Visual Representations,0.06911498308181763,#c5b0d5
-4.059376,-4.8053036,neighbor,225039882,Dual Channel Attention Networks,0.06912839412689209,#c5b0d5
5.7958727,6.693967,neighbor,225039882,GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing,0.06919229030609131,#c5b0d5
6.355998,4.744224,neighbor,225039882,Master's Thesis : Deep Learning for Visual Recognition,0.06936711072921753,#c5b0d5
0.4037393,-5.876581,neighbor,225039882,DIANet: Dense-and-Implicit Attention Network,0.06955486536026001,#c5b0d5
-1.3715088,6.8071375,neighbor,225039882,Striving for Simplicity: The All Convolutional Net,0.06976336240768433,#c5b0d5
1.2639863,-9.711202,neighbor,225039882,Exploring Self-Attention for Image Recognition,0.06977558135986328,#c5b0d5
3.6081495,7.7646513,neighbor,225039882,"ESPNetv2: A Light-Weight, Power Efficient, and General Purpose Convolutional Neural Network",0.06987494230270386,#c5b0d5
4.231599,-11.529789,neighbor,225039882,Saccader: Improving Accuracy of Hard Attention Models for Vision,0.07020527124404907,#c5b0d5
-3.963397,-7.9398293,neighbor,225039882,Rotate to Attend: Convolutional Triplet Attention Module,0.07023084163665771,#c5b0d5
-3.1285024,5.064447,neighbor,225039882,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.07030272483825684,#c5b0d5
-3.7653122,-3.824524,neighbor,225039882,DFA: Improving Convolutional Networks with Dual Fusion Attention Module,0.07039999961853027,#c5b0d5
0.99823135,3.0439458,neighbor,225039882,Bag of Tricks for Image Classification with Convolutional Neural Networks,0.07048124074935913,#c5b0d5
2.7710814,-4.924111,neighbor,225039882,Recurrent Soft Attention Model for Common Object Recognition,0.07061314582824707,#c5b0d5
17.551073,-10.053184,neighbor,225039882,Do Transformers Need Deep Long-Range Memory?,0.07065784931182861,#c5b0d5
-9.907754,-0.7083106,neighbor,225039882,A Novel BoVW Mimicking End-To-End Trainable CNN Classification Framework Using Optimal Transport Theory,0.07069545984268188,#c5b0d5
-8.871742,3.6154222,neighbor,225039882,DeSTNet: Densely Fused Spatial Transformer Networks,0.07073771953582764,#c5b0d5
4.0351667,-1.8210344,neighbor,225039882,Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition,0.0707893967628479,#c5b0d5
9.412185,-1.4286895,neighbor,225039882,Learning Finer-class Networks for Universal Representations,0.07079517841339111,#c5b0d5
9.049288,-0.60600764,neighbor,225039882,From generic to specific deep representations for visual recognition,0.07090133428573608,#c5b0d5
-6.73826,-2.0962827,neighbor,225039882,On the Exploration of Convolutional Fusion Networks for Visual Recognition,0.07104533910751343,#c5b0d5
-0.29514983,-10.416612,neighbor,225039882,An Empirical Study of Spatial Attention Mechanisms in Deep Networks,0.07117664813995361,#c5b0d5
-1.4907238,13.26739,neighbor,225039882,Training Binary Neural Networks with Real-to-Binary Convolutions,0.07144051790237427,#c5b0d5
-1.1622223,10.607505,neighbor,225039882,DenseNet Models for Tiny ImageNet Classification,0.07144588232040405,#c5b0d5
-1.3190167,-10.705495,neighbor,225039882,Attention that does not Explain Away,0.07168823480606079,#c5b0d5
-8.437954,1.8271929,neighbor,225039882,Volumetric Transformer Networks,0.07178717851638794,#c5b0d5
17.538136,-11.744421,neighbor,225039882,Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention,0.07180517911911011,#c5b0d5
-0.011433597,11.8454275,neighbor,225039882,XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,0.07184803485870361,#c5b0d5
5.340399,-9.810882,neighbor,225039882,Focus Longer to See Better: Recursively Refined Attention for Fine-Grained Image Classification,0.07189768552780151,#c5b0d5
15.982627,-12.075651,neighbor,225039882,Generating Long Sequences with Sparse Transformers,0.07192325592041016,#c5b0d5
6.1368065,-9.704514,neighbor,225039882,Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-Grained Image Recognition,0.07193869352340698,#c5b0d5
2.0065253,-12.085747,neighbor,225039882,Deep Reinforced Attention Learning for Quality-Aware Visual Recognition,0.07197552919387817,#c5b0d5
-0.4972934,13.072965,neighbor,225039882,Back to Simplicity: How to Train Accurate BNNs from Scratch?,0.07200175523757935,#c5b0d5
14.155243,-6.3061266,neighbor,225039882,Doubly Attentive Transformer Machine Translation,0.07202798128128052,#c5b0d5
-4.289069,-1.272637,neighbor,225039882,Parallel Convolutional Networks for Image Recognition via a Discriminator,0.0720338225364685,#c5b0d5
-7.0645065,-7.7248654,neighbor,225039882,Pay Attention to Convolution Filters: Towards Fast and Accurate Fine-Grained Transfer Learning,0.07213038206100464,#c5b0d5
-7.851574,-6.687076,neighbor,225039882,Learning Transferable Architectures for Scalable Image Recognition,0.0725024938583374,#c5b0d5
-2.6404939,8.438108,neighbor,225039882,Analysis and Optimization of Convolutional Neural Network Architectures,0.07262951135635376,#c5b0d5
-9.76681,-5.98987,neighbor,225039882,A Simple Cache Model for Image Recognition,0.07266396284103394,#c5b0d5
-2.3295321,-3.230555,neighbor,225039882,ADNet: Adaptively Dense Convolutional Neural Networks,0.07268655300140381,#c5b0d5
-3.3723059,2.4569464,neighbor,225039882,Resnet in Resnet: Generalizing Residual Architectures,0.07273095846176147,#c5b0d5
17.035652,-5.0084457,neighbor,225039882,GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples,0.07276803255081177,#c5b0d5
-5.5967946,12.773916,neighbor,225039882,Scheduled denoising autoencoders,0.07290858030319214,#c5b0d5
4.62977,-1.5630728,neighbor,225039882,Deep Predictive Coding Network for Object Recognition,0.07298719882965088,#c5b0d5
-6.3668246,5.807655,neighbor,225039882,Convolutional Networks with Adaptive Inference Graphs,0.07304227352142334,#c5b0d5
1.8342589,-3.3363461,neighbor,225039882,Dynamic Computational Time for Visual Attention,0.07312041521072388,#c5b0d5
-0.3513672,-0.75913554,neighbor,225039882,Progressive Neural Networks for Image Classification,0.07321417331695557,#c5b0d5
4.104334,-9.719635,neighbor,225039882,STNet: Selective Tuning of Convolutional Networks for Object Localization,0.07323002815246582,#c5b0d5
0.28525603,15.231829,neighbor,225039882,Learning Effective Binary Visual Representations with Deep Networks,0.07327687740325928,#c5b0d5
2.490663,-1.2412329,neighbor,225039882,Progressive Recurrent Learning for Visual Recognition,0.07342445850372314,#c5b0d5
2.8166752,8.959386,neighbor,225039882,Dynamic Convolution: Attention Over Convolution Kernels,0.07366001605987549,#c5b0d5
-5.860958,-1.5845783,neighbor,225039882,Combining CNN with Hand-Crafted Features for Image Classification,0.07371777296066284,#c5b0d5
-12.882496,0.18673009,neighbor,225039882,Deep FisherNet for Object Classification,0.07371878623962402,#c5b0d5
8.081581,0.7396718,neighbor,225039882,Training neural networks to have brain-like representations improves object recognition performance,0.07371973991394043,#c5b0d5
3.323177,5.0885963,neighbor,225039882,Taylor Convolutional Networks for Image Classification,0.07386595010757446,#c5b0d5
17.37884,-8.322013,neighbor,225039882,Analyzing Redundancy in Pretrained Transformer Models,0.07388919591903687,#c5b0d5
-8.931662,-1.2760053,neighbor,225039882,Multiple VLAD Encoding of CNNs for Image Classification,0.07402259111404419,#c5b0d5
4.8922915,0.8447212,neighbor,225039882,Data-Efficient Image Recognition with Contrastive Predictive Coding,0.07403051853179932,#c5b0d5
10.181028,-2.65984,neighbor,225039882,Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance,0.07407534122467041,#c5b0d5
-4.218786,6.402981,neighbor,225039882,Revisiting the Importance of Individual Units in CNNs via Ablation,0.07408076524734497,#c5b0d5
0.5404611,-1.6748267,neighbor,225039882,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,0.07410287857055664,#c5b0d5
8.577272,1.8961109,neighbor,225039882,CIFAR10 to Compare Visual Recognition Performance between Deep Neural Networks and Humans,0.07414573431015015,#c5b0d5
-1.222186,8.059113,neighbor,225039882,About pyramid structure in convolutional neural networks,0.07417500019073486,#c5b0d5
-6.712531,3.1535716,neighbor,225039882,Equivariant Transformer Networks,0.07429349422454834,#c5b0d5
3.503417,11.308704,neighbor,225039882,Bilinear Convolutional Neural Networks for Fine-Grained Visual Recognition,0.07439965009689331,#c5b0d5
-4.7140064,13.517344,neighbor,225039882,Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks,0.0744282603263855,#c5b0d5
-1.523115,3.2151217,neighbor,225039882,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",0.07452058792114258,#c5b0d5
1.5774587,6.098698,neighbor,225039882,A Guide to Convolutional Neural Networks for Computer Vision,0.07454109191894531,#c5b0d5
-4.0707836,-1.1823738,query,2375110,Xception: Deep Learning with Depthwise Separable Convolutions,0.0,#8c564b
-3.1010365,-2.051117,neighbor,2375110,Going deeper with convolutions,0.04043388366699219,#8c564b
-1.0235914,-1.8973619,neighbor,2375110,DecomposeMe: Simplifying ConvNets for End-to-End Learning,0.043521225452423096,#8c564b
2.44367,5.1912203,neighbor,2375110,Deep Residual Learning for Image Recognition,0.0445517897605896,#8c564b
-5.823269,0.18677625,neighbor,2375110,Understanding Convolutional Neural Networks,0.04693257808685303,#8c564b
-4.8229775,-2.6271114,neighbor,2375110,Striving for Simplicity: The All Convolutional Net,0.04757285118103027,#8c564b
-1.6879766,-0.8964415,neighbor,2375110,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.04770880937576294,#8c564b
-9.116659,-5.6967163,neighbor,2375110,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",0.048165082931518555,#8c564b
-10.333734,-1.2365785,neighbor,2375110,SimNets: A Generalization of Convolutional Networks,0.048407554626464844,#8c564b
0.73217314,9.519562,neighbor,2375110,Training Deeper Convolutional Networks with Deep Supervision,0.05044835805892944,#8c564b
3.1264389,-0.91857463,neighbor,2375110,Local Binary Convolutional Neural Networks,0.050863444805145264,#8c564b
1.3393542,-0.4899677,neighbor,2375110,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.050898849964141846,#8c564b
2.5623093,6.1771193,neighbor,2375110,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",0.05091249942779541,#8c564b
-0.6144194,6.6690297,neighbor,2375110,Learning Compact Convolutional Neural Networks with Nested Dropout,0.05143141746520996,#8c564b
-1.0596534,5.5982065,neighbor,2375110,Diving deeper into mentee networks,0.052409350872039795,#8c564b
4.142881,5.5810666,neighbor,2375110,Deep Residual Networks with Exponential Linear Unit,0.052474915981292725,#8c564b
-6.0026007,-2.0353599,neighbor,2375110,About pyramid structure in convolutional neural networks,0.05285978317260742,#8c564b
-3.972202,8.194403,neighbor,2375110,Training Very Deep Networks,0.05296528339385986,#8c564b
4.148519,-0.7318703,neighbor,2375110,XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,0.05313253402709961,#8c564b
-10.380477,-0.79285735,neighbor,2375110,Deep SimNets,0.05313694477081299,#8c564b
15.639056,-3.3665347,neighbor,2375110,Flattened Convolutional Neural Networks for Feedforward Acceleration,0.05545002222061157,#8c564b
-9.057314,2.6895187,neighbor,2375110,Network In Network,0.05554020404815674,#8c564b
-10.169176,-5.6510715,neighbor,2375110,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.05651450157165527,#8c564b
-0.030852312,9.213234,neighbor,2375110,Deeply-Supervised Nets,0.05656099319458008,#8c564b
1.9784571,8.848432,neighbor,2375110,Residual CNDS,0.0569530725479126,#8c564b
4.530277,0.35658574,neighbor,2375110,Binarized Neural Networks on the ImageNet Classification Task,0.05745375156402588,#8c564b
-6.4854674,-5.0657253,neighbor,2375110,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.0574873685836792,#8c564b
-4.4550486,1.9933752,neighbor,2375110,An Introduction to Convolutional Neural Networks,0.05750226974487305,#8c564b
12.26242,-1.9082986,neighbor,2375110,Fast ConvNets Using Group-Wise Brain Damage,0.058286428451538086,#8c564b
-8.983083,-2.938676,neighbor,2375110,Max-min convolutional neural networks for image classification,0.05831092596054077,#8c564b
5.1309032,7.8527913,neighbor,2375110,Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks,0.058319807052612305,#8c564b
-0.17844015,0.10169788,neighbor,2375110,Enhanced image classification with a fast-learning shallow convolutional neural network,0.058525681495666504,#8c564b
11.51801,-1.132634,neighbor,2375110,Faster CNNs with Direct Sparse Convolutions and Guided Pruning,0.05871468782424927,#8c564b
7.0488896,-2.5921237,neighbor,2375110,DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients,0.05887174606323242,#8c564b
-5.7523026,2.419012,neighbor,2375110,MatConvNet: Convolutional Neural Networks for MATLAB,0.0589333176612854,#8c564b
-2.9409685,6.0425406,neighbor,2375110,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.05908834934234619,#8c564b
-4.432539,8.19601,neighbor,2375110,Highway Networks,0.05916392803192139,#8c564b
11.49584,-3.3389533,neighbor,2375110,Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups,0.05920153856277466,#8c564b
-6.6131506,6.9127293,neighbor,2375110,Convolutional Neural Fabrics,0.05945044755935669,#8c564b
4.113425,7.2897887,neighbor,2375110,Convolutional Residual Memory Networks,0.059658050537109375,#8c564b
8.01852,1.2493378,neighbor,2375110,Scalable stacking and learning for building deep architectures,0.06026482582092285,#8c564b
-8.030844,4.420982,neighbor,2375110,Deep Epitomic Convolutional Neural Networks,0.06032824516296387,#8c564b
12.744489,-4.426672,neighbor,2375110,Training CNNs with Low-Rank Filters for Efficient Image Classification,0.060421109199523926,#8c564b
12.030016,1.725018,neighbor,2375110,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.06066817045211792,#8c564b
5.9769278,-0.83206344,neighbor,2375110,Binarized Neural Networks,0.06066948175430298,#8c564b
6.1528473,-6.1099706,neighbor,2375110,Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures,0.06071817874908447,#8c564b
4.6871166,11.673415,neighbor,2375110,X-CNN: Cross-modal convolutional neural networks for sparse datasets,0.06072753667831421,#8c564b
-3.8457043,1.0529574,neighbor,2375110,Feature Representation in Convolutional Neural Networks,0.06078827381134033,#8c564b
-9.339025,-4.6278353,neighbor,2375110,Fractional Max-Pooling,0.060873568058013916,#8c564b
14.398293,-3.6867108,neighbor,2375110,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.06089818477630615,#8c564b
-7.65618,-7.7111278,neighbor,2375110,Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification,0.06092667579650879,#8c564b
13.785168,-0.81700754,neighbor,2375110,Fast Training of Convolutional Networks through FFTs,0.0611729621887207,#8c564b
11.317644,-6.84708,neighbor,2375110,Convexified Convolutional Neural Networks,0.061302900314331055,#8c564b
8.531615,-3.5951204,neighbor,2375110,Deep Fried Convnets,0.06139451265335083,#8c564b
-4.896938,-5.5759797,neighbor,2375110,Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification,0.06164127588272095,#8c564b
-12.710105,3.3585505,neighbor,2375110,Convolution by Evolution: Differentiable Pattern Producing Networks,0.06186020374298096,#8c564b
0.9122063,4.139513,neighbor,2375110,Deeply-Fused Nets,0.061886608600616455,#8c564b
11.220848,1.6861755,neighbor,2375110,PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions,0.0620003342628479,#8c564b
13.213309,-5.023446,neighbor,2375110,Convolutional neural networks with low-rank regularization,0.06203508377075195,#8c564b
0.18121623,-5.516744,neighbor,2375110,Large-Scale Deep Learning on the YFCC100M Dataset,0.06251734495162964,#8c564b
-4.875229,4.659121,neighbor,2375110,Understanding Deep Architectures using a Recursive Convolutional Network,0.06253278255462646,#8c564b
-9.200899,2.2842147,neighbor,2375110,Convolution in Convolution for Network in Network,0.0626600980758667,#8c564b
-2.7241175,0.4542198,neighbor,2375110,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.06267422437667847,#8c564b
11.010978,3.6945722,neighbor,2375110,Convolutional neural networks at constrained time cost,0.06289434432983398,#8c564b
10.524597,-1.0858667,neighbor,2375110,Memory Bounded Deep Convolutional Networks,0.0629313588142395,#8c564b
-0.60405797,2.5378332,neighbor,2375110,Refining Architectures of Deep Convolutional Neural Networks,0.06313526630401611,#8c564b
-0.3559763,1.1530823,neighbor,2375110,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.06313687562942505,#8c564b
5.275966,4.6080794,neighbor,2375110,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),0.0631723403930664,#8c564b
2.9458864,3.0814624,neighbor,2375110,How far can we go without convolution: Improving fully-connected networks,0.06352096796035767,#8c564b
-9.9726925,9.674822,neighbor,2375110,Visualizing and Understanding Convolutional Networks,0.06357896327972412,#8c564b
-9.575501,10.767523,neighbor,2375110,A Tour of TensorFlow,0.06366896629333496,#8c564b
-10.20365,9.055318,neighbor,2375110,Visualizing and Comparing Convolutional Neural Networks,0.06371265649795532,#8c564b
-4.553195,11.717083,neighbor,2375110,FractalNet: Ultra-Deep Neural Networks without Residuals,0.06413716077804565,#8c564b
0.7679759,-11.061248,neighbor,2375110,Representation Benefits of Deep Feedforward Networks,0.06423664093017578,#8c564b
-2.756985,1.7633574,neighbor,2375110,A Taxonomy of Deep Convolutional Neural Nets for Computer Vision,0.06424498558044434,#8c564b
-10.360513,-8.037865,neighbor,2375110,Differentiable Pooling for Hierarchical Feature Learning,0.06444680690765381,#8c564b
1.8764352,11.841055,neighbor,2375110,Mediated experts for deep convolutional networks,0.06450223922729492,#8c564b
0.7758764,-2.0809314,neighbor,2375110,Conditional Deep Learning for energy-efficient and enhanced pattern recognition,0.06459993124008179,#8c564b
5.5479794,3.8038046,neighbor,2375110,Parametric Exponential Linear Unit for Deep Convolutional Neural Networks,0.064730703830719,#8c564b
-12.547094,2.0992901,neighbor,2375110,Network Morphism,0.06481844186782837,#8c564b
6.6868315,-5.3283157,neighbor,2375110,Learning both Weights and Connections for Efficient Neural Network,0.06483709812164307,#8c564b
-7.4606967,-5.4204264,neighbor,2375110,Learnable Pooling Regions for Image Classification,0.06525719165802002,#8c564b
5.702505,-2.260314,neighbor,2375110,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,0.06537139415740967,#8c564b
-0.48317686,-3.3148315,neighbor,2375110,Shoot to Know What: An Application of Deep Networks on Mobile Devices,0.0654367208480835,#8c564b
14.995405,2.757222,neighbor,2375110,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.0656745433807373,#8c564b
7.177098,-3.3417025,neighbor,2375110,Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations,0.06572753190994263,#8c564b
0.43463922,-11.048617,neighbor,2375110,Deep vs. shallow networks : An approximation theory perspective,0.06593292951583862,#8c564b
9.597104,-6.049575,neighbor,2375110,Learning separable fixed-point kernels for deep convolutional neural networks,0.06612414121627808,#8c564b
4.9390087,11.086473,neighbor,2375110,Swapout: Learning an ensemble of deep architectures,0.06614553928375244,#8c564b
-1.2197286,8.345605,neighbor,2375110,An introduction to deep learning,0.06638354063034058,#8c564b
-2.7365637,-3.8526936,neighbor,2375110,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.06643801927566528,#8c564b
2.8529527,-3.2173605,neighbor,2375110,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.06647449731826782,#8c564b
5.929081,6.876543,neighbor,2375110,Weighted residuals for very deep networks,0.0665925145149231,#8c564b
-12.151171,-2.4730315,neighbor,2375110,Group Equivariant Convolutional Networks,0.06674623489379883,#8c564b
13.543409,1.233979,neighbor,2375110,One weird trick for parallelizing convolutional neural networks,0.06692332029342651,#8c564b
9.186857,-9.0718565,neighbor,2375110,A Kronecker-factored approximate Fisher matrix for convolution layers,0.06736040115356445,#8c564b
-1.0926598,11.674078,neighbor,2375110,Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks,0.06741571426391602,#8c564b
-10.719054,-9.01263,neighbor,2375110,Towards deep compositional networks,0.06763648986816406,#8c564b
-2.7849708,9.744809,neighbor,2375110,Do Deep Nets Really Need to be Deep?,0.06765902042388916,#8c564b
5.7201343,-4.610153,neighbor,2375110,Compressing Neural Networks with the Hashing Trick,0.06769376993179321,#8c564b
-3.3872476,-7.6316404,neighbor,2375110,Do Semantic Parts Emerge in Convolutional Neural Networks?,0.06794142723083496,#8c564b
-2.5955899,2.1396801,query,245335280,High-Resolution Image Synthesis with Latent Diffusion Models,0.0,#8c564b
-1.6426935,3.156537,neighbor,245335280,Cascaded Diffusion Models for High Fidelity Image Generation,0.03409159183502197,#8c564b
-1.2262164,-0.9945482,neighbor,245335280,InfinityGAN: Towards Infinite-Pixel Image Synthesis,0.03636181354522705,#8c564b
-3.354121,1.6343005,neighbor,245335280,More Control for Free! Image Synthesis with Semantic Diffusion Guidance,0.037006497383117676,#8c564b
-4.1638517,1.9036345,neighbor,245335280,Vector Quantized Diffusion Model for Text-to-Image Synthesis,0.03772127628326416,#8c564b
-4.549217,3.6546302,neighbor,245335280,ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models,0.038240790367126465,#8c564b
-0.9289249,1.4976861,neighbor,245335280,"EdiBERT, a generative model for image editing",0.039537906646728516,#8c564b
-1.891987,7.018301,neighbor,245335280,Stylegan-Induced Data-Driven Regularization for Inverse Problems,0.03958016633987427,#8c564b
-3.8790755,-2.7284465,neighbor,245335280,Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation,0.03962665796279907,#8c564b
-2.729999,3.0275679,neighbor,245335280,Palette: Image-to-Image Diffusion Models,0.04034453630447388,#8c564b
-6.671119,-7.61678,neighbor,245335280,Aggregated Contextual Transformations for High-Resolution Image Inpainting,0.04046761989593506,#8c564b
-1.3569857,6.2509246,neighbor,245335280,Generative Imaging and Image Processing via Generative Encoder,0.04068833589553833,#8c564b
0.6527552,6.4505734,neighbor,245335280,Image Processing Using Multi-Code GAN Prior,0.0416874885559082,#8c564b
-8.070228,-7.46482,neighbor,245335280,Generator pyramid for high-resolution image inpainting,0.04199486970901489,#8c564b
-8.5336075,-8.160635,neighbor,245335280,Structure First Detail Next: Image Inpainting with Pyramid Generator,0.04300534725189209,#8c564b
1.745207,3.2367055,neighbor,245335280,Diffusion Models Beat GANs on Image Synthesis,0.04308593273162842,#8c564b
-2.979656,0.8655258,neighbor,245335280,SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations,0.043650686740875244,#8c564b
-10.009607,-9.9372835,neighbor,245335280,Sketch-Guided Scenery Image Outpainting,0.043894946575164795,#8c564b
-1.1789407,10.472832,neighbor,245335280,Deep Image Prior,0.04414010047912598,#8c564b
-6.8128786,-9.128846,neighbor,245335280,Generative image inpainting with residual attention learning,0.04431706666946411,#8c564b
-5.6655183,-9.726464,neighbor,245335280,Image Inpainting via Enhanced Generative Adversarial Network,0.044330716133117676,#8c564b
-1.6503927,7.241208,neighbor,245335280,Bayesian Image Reconstruction using Deep Generative Models,0.044450998306274414,#8c564b
-2.6556916,7.3573275,neighbor,245335280,Regularization via Deep Generative Models: an Analysis Point of View,0.0445483922958374,#8c564b
-2.4678192,-1.4936621,neighbor,245335280,Nested Scale-Editing for Conditional Image Synthesis,0.045033156871795654,#8c564b
3.663936,3.3158915,neighbor,245335280,Pioneer Networks: Progressively Growing Generative Autoencoder,0.0453149676322937,#8c564b
-5.145368,0.5670578,neighbor,245335280,ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis,0.04538911581039429,#8c564b
2.5933325,15.439934,neighbor,245335280,GLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution,0.045473456382751465,#8c564b
3.7064497,-3.0209131,neighbor,245335280,Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks,0.045636117458343506,#8c564b
3.8505328,0.75808245,neighbor,245335280,Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications,0.04575282335281372,#8c564b
-10.053453,-6.9197016,neighbor,245335280,CISI-net: Explicit Latent Content Inference and Imitated Style Rendering for Image Inpainting,0.04612773656845093,#8c564b
4.5297914,14.009722,neighbor,245335280,Image Super-Resolution via Iterative Refinement,0.04620945453643799,#8c564b
-3.4706538,-10.400219,neighbor,245335280,Boundless: Generative Adversarial Networks for Image Extension,0.04653310775756836,#8c564b
-4.6245947,-7.100067,neighbor,245335280,Diverse Image Inpainting with Bidirectional and Autoregressive Transformers,0.046803414821624756,#8c564b
7.731647,1.1398295,neighbor,245335280,Image2StyleGAN++: How to Edit the Embedded Images?,0.046949565410614014,#8c564b
4.5666914,12.81894,neighbor,245335280,Tarsier: Evolving Noise Injection in Super-Resolution GANs,0.04756873846054077,#8c564b
0.20137611,7.4649377,neighbor,245335280,Latent Convolutional Models,0.047942399978637695,#8c564b
-6.8919744,-10.090423,neighbor,245335280,Image Inpainting via Generative Multi-column Convolutional Neural Networks,0.048450469970703125,#8c564b
2.8391936,-0.8644627,neighbor,245335280,Anycost GANs for Interactive Image Synthesis and Editing,0.04846125841140747,#8c564b
5.784285,-1.7173434,neighbor,245335280,A Good Image Generator Is What You Need for High-Resolution Video Synthesis,0.048579275608062744,#8c564b
-7.4760866,-7.6587853,neighbor,245335280,Texture-aware Multi-resolution Image Inpainting,0.04871249198913574,#8c564b
-4.690955,3.9731445,neighbor,245335280,Denoising Diffusion Implicit Models,0.049164652824401855,#8c564b
1.1687154,1.3958013,neighbor,245335280,Using latent space regression to analyze and leverage compositionality in GANs,0.04921305179595947,#8c564b
1.9810452,13.699812,neighbor,245335280,ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks,0.04956459999084473,#8c564b
-7.5150814,-9.312867,neighbor,245335280,Generative Image Inpainting with Contextual Attention,0.04967236518859863,#8c564b
3.3180656,14.110818,neighbor,245335280,A Fully Progressive Approach to Single-Image Super-Resolution,0.049806833267211914,#8c564b
-1.0394852,10.8431635,neighbor,245335280,Multi-Level Encoder-Decoder Architectures for Image Restoration,0.049837350845336914,#8c564b
2.2913072,0.30781758,neighbor,245335280,SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing,0.05008119344711304,#8c564b
0.30036643,-3.2107816,neighbor,245335280,RG-Flow: a hierarchical and explainable flow model based on renormalization group and sparse prior,0.050172507762908936,#8c564b
-5.864695,-6.446576,neighbor,245335280,InOut: Diverse Image Outpainting via GAN Inversion,0.05046999454498291,#8c564b
0.90568036,-2.202942,neighbor,245335280,Patchwise Generative ConvNet: Training Energy-Based Models from a Single Natural Image for Internal Learning,0.050549209117889404,#8c564b
4.9544587,2.8372946,neighbor,245335280,One Model to Reconstruct Them All: A Novel Way to Use the Stochastic Noise in StyleGAN,0.05062830448150635,#8c564b
3.0240352,-4.1240845,neighbor,245335280,Conditional Generative ConvNets for Exemplar-Based Texture Synthesis,0.050714194774627686,#8c564b
2.4693449,12.982167,neighbor,245335280,SRVAE: super resolution using variational autoencoders,0.05087524652481079,#8c564b
2.657362,13.745031,neighbor,245335280,DPSRGAN: Dilation Patch Super-Resolution Generative Adversarial Networks,0.05093759298324585,#8c564b
-8.4059305,-9.543071,neighbor,245335280,DeepGIN: Deep Generative Inpainting Network for Extreme Image Inpainting,0.05097252130508423,#8c564b
-4.1454706,-8.731233,neighbor,245335280,JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting,0.05102717876434326,#8c564b
-0.73095226,12.814843,neighbor,245335280,DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better,0.051055073738098145,#8c564b
2.6221783,14.575774,neighbor,245335280,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,0.051087915897369385,#8c564b
5.0142508,0.08524435,neighbor,245335280,A survey and taxonomy of adversarial neural networks for text‐to‐image synthesis,0.05109363794326782,#8c564b
-5.654017,-8.551729,neighbor,245335280,Deep Fusion Local-Content and Global-Semantic for Image Inpainting,0.05110234022140503,#8c564b
2.4318056,-1.2806011,neighbor,245335280,Training End-to-end Single Image Generators without GANs,0.05117243528366089,#8c564b
-4.8272166,-11.092743,neighbor,245335280,Free-Form Image Inpainting With Gated Convolution,0.051173269748687744,#8c564b
3.203856,3.2508895,neighbor,245335280,Large Scale GAN Training for High Fidelity Natural Image Synthesis,0.05122286081314087,#8c564b
3.3985538,14.975141,neighbor,245335280,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,0.05132943391799927,#8c564b
3.3471906,13.174891,neighbor,245335280,Joint Denoising and Super-Resolution via Generative Adversarial Training,0.05138683319091797,#8c564b
-3.0322363,6.4457316,neighbor,245335280,Intermediate Layer Optimization for Inverse Problems using Deep Generative Models,0.051481544971466064,#8c564b
-7.646012,-9.117324,neighbor,245335280,Deep Generative Model for Image Inpainting With Local Binary Pattern Learning and Spatial Attention,0.05151087045669556,#8c564b
-10.403582,-9.091043,neighbor,245335280,Diversity-Generated Image Inpainting with Style Extraction,0.05152320861816406,#8c564b
1.4455582,3.6760747,neighbor,245335280,BIGRoC: Boosting Image Generation via a Robust Classifier,0.051559388637542725,#8c564b
-8.9402895,-10.568336,neighbor,245335280,Guided Image Inpainting: Replacing an Image Region by Pulling Content From Another Image,0.05161052942276001,#8c564b
-4.4607625,-12.12311,neighbor,245335280,Prior Guided GAN Based Semantic Inpainting,0.05161285400390625,#8c564b
4.6031246,0.4427276,neighbor,245335280,A Survey of State-of-the-Art GAN-based Approaches to Image Synthesis,0.051699697971343994,#8c564b
2.507949,0.6319732,neighbor,245335280,Navigating the GAN Parameter Space for Semantic Image Editing,0.051717936992645264,#8c564b
-5.249856,-9.146196,neighbor,245335280,Eigan: Enhanced Inpainting Generative Adversarial Network,0.05172508955001831,#8c564b
2.5985994,6.026668,neighbor,245335280,Detail Fusion GAN: High-Quality Translation for Unpaired Images with GAN-based Data Augmentation,0.05179333686828613,#8c564b
4.3575287,0.40409803,neighbor,245335280,Image Generation with Gans-based Techniques: A Survey,0.05181097984313965,#8c564b
1.9048891,14.609282,neighbor,245335280,Efficient Super Resolution for Large-Scale Images Using Attentional GAN,0.051811039447784424,#8c564b
-10.480038,-10.061138,neighbor,245335280,ReGO: Reference-Guided Outpainting for Scenery Image,0.05187809467315674,#8c564b
-0.9278167,11.357604,neighbor,245335280,Restormer: Efficient Transformer for High-Resolution Image Restoration,0.05191695690155029,#8c564b
1.3574255,13.05287,neighbor,245335280,Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution,0.05207395553588867,#8c564b
-7.259634,-5.7800765,neighbor,245335280,PD-GAN: Probabilistic Diverse GAN for Image Inpainting,0.052118897438049316,#8c564b
-6.501022,-12.851454,neighbor,245335280,Painting Outside the Box: Image Outpainting with GANs,0.05212056636810303,#8c564b
1.1584634,-1.8257345,neighbor,245335280,Recurrent SinGAN: Towards Scale-Agnostic Single Image GANs,0.052130162715911865,#8c564b
-6.115043,-11.1499605,neighbor,245335280,Semantic Image Inpainting with Progressive Generative Networks,0.05227017402648926,#8c564b
4.152466,7.2223454,neighbor,245335280,An Acceleration Framework for High Resolution Image Synthesis,0.052291691303253174,#8c564b
1.8106505,8.884441,neighbor,245335280,Toward Interactive Modulation for Photo-Realistic Image Restoration,0.05232113599777222,#8c564b
3.7415223,4.3606243,neighbor,245335280,MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks,0.05233258008956909,#8c564b
-7.749918,-10.935825,neighbor,245335280,Distillation-guided Image Inpainting,0.052396297454833984,#8c564b
5.203573,3.7690256,neighbor,245335280,Deep Automodulators,0.052520573139190674,#8c564b
3.6762662,-2.7723684,neighbor,245335280,TextureGAN: Controlling Deep Image Synthesis with Texture Patches,0.05253744125366211,#8c564b
3.156512,2.1811628,neighbor,245335280,Improved Image Generation via Sparse Modeling,0.052540123462677,#8c564b
-5.581736,4.303121,neighbor,245335280,Label-Efficient Semantic Segmentation with Diffusion Models,0.052558839321136475,#8c564b
-5.755657,-10.362814,neighbor,245335280,Image Fine-grained Inpainting,0.05269211530685425,#8c564b
-7.1164823,-12.90497,neighbor,245335280,Enhanced Residual Networks for Context-based Image Outpainting,0.05272120237350464,#8c564b
-7.459394,-4.903613,neighbor,245335280,Semantic Pyramid for Image Generation,0.05279475450515747,#8c564b
-1.4590051,12.930265,neighbor,245335280,Self-augmented deep generative network for blind image deblurring,0.05296391248703003,#8c564b
1.1443763,6.184445,neighbor,245335280,Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation,0.05298197269439697,#8c564b
5.458259,2.29845,neighbor,245335280,Content-Aware GAN Compression,0.05308324098587036,#8c564b
6.406714,4.5270057,neighbor,245335280,IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis,0.053159475326538086,#8c564b
3.790943,12.075974,neighbor,245335280,Self-Supervised Fine-tuning for Image Enhancement of Super-Resolution Deep Neural Networks,0.05320382118225098,#8c564b
3.5055766,4.478876,neighbor,245335280,BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled Representation Learning and Image Synthesis,0.053221940994262695,#8c564b
-1.5511187,-1.9150033,query,28637672,Statistical Learning Theory,0.0,#c49c94
-2.1499188,-0.0853792,neighbor,28637672,Machine Learning,0.04233509302139282,#c49c94
-4.3192825,4.8080277,neighbor,28637672,Statistical Machine Learning,0.044232189655303955,#c49c94
0.19901036,9.293112,neighbor,28637672,Machine Learning with Neural Networks,0.04631757736206055,#c49c94
-8.727732,-6.0751247,neighbor,28637672,Learning Theory and Support Vector Machines - a primer,0.04817932844161987,#c49c94
-5.9996195,6.8285117,neighbor,28637672,Statistical Pattern Recognition,0.04987901449203491,#c49c94
0.7721682,2.7676754,neighbor,28637672,Introduction to Machine Learning,0.0504876971244812,#c49c94
-3.2155385,11.017622,neighbor,28637672,Machine Learning and Deep Learning: Introduction and Applications,0.05096620321273804,#c49c94
-1.2877845,8.791456,neighbor,28637672,Machine Learning: Models And Algorithms,0.051342010498046875,#c49c94
-6.141535,-0.48412916,neighbor,28637672,The Elements of Statistical Learning,0.05159187316894531,#c49c94
2.687837,2.4656022,neighbor,28637672,Machine Learning: Review,0.05269891023635864,#c49c94
-3.1315815,0.37592703,neighbor,28637672,"Machine Learning with R, Third Edition (Book Review)",0.05356705188751221,#c49c94
-4.3194027,4.014136,neighbor,28637672,A Brief Introduction to Machine Learning for Engineers,0.053772807121276855,#c49c94
-6.249224,9.069185,neighbor,28637672,Machine Learning and Pattern Recognition,0.05378520488739014,#c49c94
-3.9642642,10.69086,neighbor,28637672,Machine Learning: A Way of Dealing with Artificial Intelligence,0.05388474464416504,#c49c94
3.897599,4.0403323,neighbor,28637672,Role of Mathematics in Machine Learning,0.05412226915359497,#c49c94
-2.3099306,2.7251382,neighbor,28637672,"Patterns, predictions, and actions: A story about machine learning",0.05490821599960327,#c49c94
1.806758,2.294981,neighbor,28637672,Learning machine learning,0.05505412817001343,#c49c94
-0.9176976,13.420472,neighbor,28637672,Artificial Neural Networks,0.05548739433288574,#c49c94
0.41120392,7.781015,neighbor,28637672,"When and How to Apply Statistics, Machine Learning and Deep Learning Techniques",0.056665658950805664,#c49c94
2.6570978,0.9810785,neighbor,28637672,Applied Machine Learning Strategies,0.05748546123504639,#c49c94
-9.529815,-7.722167,neighbor,28637672,Support Vector Machines,0.058143019676208496,#c49c94
-9.898281,-7.8300757,neighbor,28637672,Support Vector Machines,0.058143019676208496,#c49c94
-0.26230434,11.973417,neighbor,28637672,Machine learning for neuroscience,0.058297574520111084,#c49c94
1.0022807,10.244711,neighbor,28637672,The Principles of Deep Learning Theory,0.05846905708312988,#c49c94
9.571418,-1.4733953,neighbor,28637672,Review on Machine learning and its application in Atmospheric science and Human Behavior Recognition,0.05851125717163086,#c49c94
-0.82659143,-10.640889,neighbor,28637672,Nearest Centroid: A Bridge between Statistics and Machine Learning,0.05863314867019653,#c49c94
0.30989882,-1.7245659,neighbor,28637672,Simple Learning Classifier Machine,0.05910974740982056,#c49c94
7.6784616,-1.1142505,neighbor,28637672,A Research on Machine Learning Methods and Its Applications,0.059217095375061035,#c49c94
6.153247,-1.9785609,neighbor,28637672,Review on Studies of Machine Learning Algorithms,0.05933523178100586,#c49c94
3.2534575,6.202339,neighbor,28637672,Machine Learning: Mathematical Theory and Scientific Applications,0.059754133224487305,#c49c94
2.0803947,8.837349,neighbor,28637672,Machine Learning for Signal Processing,0.059759318828582764,#c49c94
-9.556366,2.1737185,neighbor,28637672,Classification,0.05983936786651611,#c49c94
3.9900813,-9.741419,neighbor,28637672,"Naive Bayes vs Logistic Regression: Theory, Implementation and Experimental Validation",0.0599132776260376,#c49c94
-1.4724034,15.286886,neighbor,28637672,An Overview of Neural Network,0.06022864580154419,#c49c94
-0.8496877,14.925957,neighbor,28637672,Fundamentals of Neural Networks,0.06047964096069336,#c49c94
3.1778102,-5.442148,neighbor,28637672,An empirical comparison of supervised learning algorithms,0.06080508232116699,#c49c94
-6.444957,8.860139,neighbor,28637672,Pattern recognition and machine learning,0.0608525276184082,#c49c94
1.5143487,-5.3870087,neighbor,28637672,Supervised Machine Learning Techniques: An Overview with Applications to Banking,0.061097145080566406,#c49c94
-4.7639055,11.899463,neighbor,28637672,Artificial Intelligence and Machine Learning,0.06134366989135742,#c49c94
7.369249,-0.28340346,neighbor,28637672,Machine Learning Aspects and its Applications Towards Different Research Areas,0.06137460470199585,#c49c94
7.177592,-5.585402,neighbor,28637672,Classification Techniques in Machine Learning: Applications and Issues,0.06141561269760132,#c49c94
5.4397197,-11.124898,neighbor,28637672,Constructing decision rules from naive bayes model for robust and low complexity classification,0.061426520347595215,#c49c94
-9.811734,-6.585975,neighbor,28637672,Support Vectors Machine: a Tutorial with R,0.06172788143157959,#c49c94
-3.712143,1.4531593,neighbor,28637672,"Explanatory Model Analysis: Explore, Explain, and Examine Predictive Models",0.0623016357421875,#c49c94
-9.588783,3.2512662,neighbor,28637672,Book Review,0.06230175495147705,#c49c94
-7.0030837,2.2722952,neighbor,28637672,Probability and Bayesian Modeling,0.06233590841293335,#c49c94
-0.78185123,4.7902555,neighbor,28637672,Logic of Machine Learning,0.062349021434783936,#c49c94
0.5659852,-4.9847875,neighbor,28637672,"The Theory Behind Overfitting, Cross Validation, Regularization, Bagging, and Boosting: Tutorial",0.06239134073257446,#c49c94
-4.548733,0.52925026,neighbor,28637672,"Statistical Data Analytics. Foundations for Data Mining, Informatics, and Knowledge Discovery",0.06250935792922974,#c49c94
-8.6231365,-7.8079295,neighbor,28637672,Learning with Support Vector Machines,0.0625794529914856,#c49c94
-4.672505,2.1706367,neighbor,28637672,"Statistical Model Building, Machine Learning, and the Ah-Ha Moment",0.06266218423843384,#c49c94
-4.2413096,14.4064,neighbor,28637672,"A clarification of misconceptions, myths and desired status of artificial intelligence",0.06267476081848145,#c49c94
-1.0177208,-3.5450976,neighbor,28637672,Weighted Machine Learning,0.06313002109527588,#c49c94
7.203057,-2.0673678,neighbor,28637672,A Survey on Machine Learning Approaches and Its Techniques:,0.06315779685974121,#c49c94
-0.20121047,14.29872,neighbor,28637672,Neural Computing,0.06323915719985962,#c49c94
-2.4163353,6.7711425,neighbor,28637672,Ensamble Learning: An Approach in Artificial Intelligence,0.06331062316894531,#c49c94
5.134252,2.2926693,neighbor,28637672,Understanding the underlying algorithms and theories of machine learning,0.0634756088256836,#c49c94
-4.775432,-2.5190318,neighbor,28637672,Learning customized and optimized lists of rules with mathematical programming,0.06349509954452515,#c49c94
3.59806,-4.488227,neighbor,28637672,Regression and Classification in Supervised Learning,0.06349802017211914,#c49c94
-4.3719006,14.178573,neighbor,28637672,"Artificial Intelligence: A Clarification of Misconceptions, Myths and Desired Status",0.06354933977127075,#c49c94
4.220091,-10.764937,neighbor,28637672,An Implementation of Naive Bayes Classifier,0.06408756971359253,#c49c94
-3.2790277,-0.98788595,neighbor,28637672,Correction to: KS(conf): A Light-Weight Test if a Multiclass Classifier Operates Outside of Its Specifications,0.06428933143615723,#c49c94
4.5638266,-11.786375,neighbor,28637672,Bayes and Naive Bayes Classifier,0.06430685520172119,#c49c94
5.1078787,-5.916084,neighbor,28637672,A Systematic Comparison of Supervised Classifiers,0.06435757875442505,#c49c94
6.080666,-1.3592935,neighbor,28637672,Study On Machine Learning Algorithms,0.06464892625808716,#c49c94
2.393644,-1.2722079,neighbor,28637672,A brief history of learning classifier systems: from CS-1 to XCS and its variants,0.06466209888458252,#c49c94
1.1769137,11.010604,neighbor,28637672,Deep Learning in Science,0.06468236446380615,#c49c94
6.4492016,-4.2320223,neighbor,28637672,A State of Art Techniques on Machine Learning Algorithms: A Perspective of Supervised Learning Approaches in Data Classification,0.0647578239440918,#c49c94
-6.0485263,4.2850146,neighbor,28637672,How to make a machine learn continuously: a tutorial of the Bayesian approach,0.06481468677520752,#c49c94
-5.3673263,12.718897,neighbor,28637672,Artificial Intelligence,0.06484687328338623,#c49c94
-2.5037067,-6.470713,neighbor,28637672,Category Trees - Classifiers that Branch on Category,0.06512528657913208,#c49c94
-4.9616275,-4.544868,neighbor,28637672,Rule Induction Partitioning Estimator,0.06525146961212158,#c49c94
2.823484,6.5834084,neighbor,28637672,Machine learning and statistical physics: preface,0.06527554988861084,#c49c94
-4.607146,-5.3361425,neighbor,28637672,RCAR Framework: Building a Regularized Class Association Rules Model in a Categorical Data Space,0.06545138359069824,#c49c94
-1.8963975,12.851875,neighbor,28637672,Towards Machine Intelligence,0.06550955772399902,#c49c94
5.6176143,9.138859,neighbor,28637672,Machine Learning Techniques for Accountability,0.06556969881057739,#c49c94
6.042503,-12.553187,neighbor,28637672,Bayesian decision rules to classification problems,0.06566506624221802,#c49c94
-0.8595068,2.7043948,neighbor,28637672,Informed Machine Learning – A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems,0.06580764055252075,#c49c94
-0.6403243,0.63487685,neighbor,28637672,Machine learning and medicine: book review and commentary,0.06580960750579834,#c49c94
2.2233584,-1.2648737,neighbor,28637672,Learning classifier systems,0.06595748662948608,#c49c94
3.0195644,-7.207622,neighbor,28637672,Comparison of 14 different families of classification algorithms on 115 binary datasets,0.06596291065216064,#c49c94
-9.158986,3.779482,neighbor,28637672,The Book Review Column,0.06600308418273926,#c49c94
-9.900839,-9.297252,neighbor,28637672,"Generalizing, decoding, and optimizing support vector machine classification",0.06603038311004639,#c49c94
-0.12687878,-9.061251,neighbor,28637672,Nearest Neighbor Classifiers Versus Random Forests and Support Vector Machines,0.06607198715209961,#c49c94
-1.8187637,-11.712047,neighbor,28637672,Bayesian Model Selection Methods for Mutual and Symmetric $k$-Nearest Neighbor Classification,0.06607621908187866,#c49c94
-2.8611035,-2.9132583,neighbor,28637672,Design of Data-Driven Mathematical Laws for Optimal Statistical Classification Systems,0.06609529256820679,#c49c94
3.493707,-11.335821,neighbor,28637672,On the Use of m-Probability-Estimation and Imprecise Probabilities in the Naïve Bayes Classifier,0.06626617908477783,#c49c94
-1.927862,3.5305352,neighbor,28637672,The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to be More Effective at Data Analysis,0.06629341840744019,#c49c94
1.6176362,4.176244,neighbor,28637672,Category Theory in Machine Learning,0.0663231611251831,#c49c94
-6.406945,13.281797,neighbor,28637672,Introduction to Artificial Intelligence,0.06633496284484863,#c49c94
-9.974624,-1.1207298,neighbor,28637672,A non-linear learning & classification algorithm that achieves full training accuracy with stellar classification accuracy,0.06634950637817383,#c49c94
-0.10198783,-13.284978,neighbor,28637672,Multiclass Probabilistic Classification Vector Machine,0.0665094256401062,#c49c94
5.742749,5.0118136,neighbor,28637672,A Mathematical Theory of Learning,0.0668897032737732,#c49c94
6.721975,1.1213738,neighbor,28637672,Machine Learning from Theory to Algorithms: An Overview,0.06692004203796387,#c49c94
9.085413,-5.110169,neighbor,28637672,Learning regression problems by using classifiers,0.0669335126876831,#c49c94
7.152795,-4.6315303,neighbor,28637672,A Literature Survey on Classification Algorithms of Machine Learning,0.0669519305229187,#c49c94
-0.9282426,-11.493409,neighbor,28637672,The k conditional nearest neighbor algorithm for classification and class probability estimation,0.06699395179748535,#c49c94
3.451963,-12.735708,neighbor,28637672,The mysterious optimality of Naive Bayes: Estimation of the probability in the system of “classifiers”,0.067066490650177,#c49c94
-10.550618,-6.4553504,neighbor,28637672,Support Vector Machines in R,0.0672723650932312,#c49c94
8.097851,-6.7664976,neighbor,28637672,Prediction Models in Machine Learning by Classification and Regression,0.06733900308609009,#c49c94
-3.4862514,-3.2813241,query,2930547,ImageNet Large Scale Visual Recognition Challenge,0.0,#c49c94
-2.1216087,-2.5426302,neighbor,2930547,Deep learning for class-generic object detection,0.04797297716140747,#c49c94
-3.962132,0.15033162,neighbor,2930547,Part-Based R-CNNs for Fine-Grained Category Detection,0.053087830543518066,#c49c94
-3.0500178,1.1191661,neighbor,2930547,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.0538637638092041,#c49c94
-2.2423255,0.35643995,neighbor,2930547,Scalable Object Detection Using Deep Neural Networks,0.05588972568511963,#c49c94
2.5945895,-9.295692,neighbor,2930547,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.058008015155792236,#c49c94
1.6232125,-10.682378,neighbor,2930547,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.05867493152618408,#c49c94
-5.1127954,6.0714545,neighbor,2930547,Learning an Alphabet of Shape and Appearance for Multi-Class Object Detection,0.05878424644470215,#c49c94
-10.815796,6.09378,neighbor,2930547,LSDA: Large Scale Detection through Adaptation,0.05913221836090088,#c49c94
-6.7762537,1.2921972,neighbor,2930547,Beyond Local Appearance: Category Recognition from Pairwise Interactions of Simple Features,0.05966752767562866,#c49c94
-4.583772,-2.6176918,neighbor,2930547,Semantic Robot Vision Challenge: Current State and Future Directions,0.06055092811584473,#c49c94
1.511537,-3.8426409,neighbor,2930547,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.06076538562774658,#c49c94
-1.3322703,-4.377201,neighbor,2930547,Fine-grained object recognition with Gnostic Fields,0.061465561389923096,#c49c94
-5.5444336,6.1353607,neighbor,2930547,Incremental learning of object detectors using a visual shape alphabet,0.062495291233062744,#c49c94
4.227994,-9.043088,neighbor,2930547,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.06373757123947144,#c49c94
-2.64845,5.1271076,neighbor,2930547,"How good are detection proposals, really?",0.0644713044166565,#c49c94
5.915506,2.9158833,neighbor,2930547,SUN database: Large-scale scene recognition from abbey to zoo,0.06508618593215942,#c49c94
-7.827958,-0.44778854,neighbor,2930547,Weakly-supervised Discovery of Visual Pattern Configurations,0.06529444456100464,#c49c94
-5.7223763,7.8616643,neighbor,2930547,Shared Random Ferns for Efficient Detection of Multiple Categories,0.06544440984725952,#c49c94
4.827306,-10.004453,neighbor,2930547,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.06595849990844727,#c49c94
-4.1800976,-5.451635,neighbor,2930547,Zero-shot Learning with Deep Neural Networks for Object Recognition,0.06601792573928833,#c49c94
5.1425567,10.972538,neighbor,2930547,Object Recognition by Scene Alignment,0.06606781482696533,#c49c94
-0.42578027,-14.591882,neighbor,2930547,Comparing state-of-the-art visual features on invariant object recognition tasks,0.066242516040802,#c49c94
-1.4978962,-0.2798356,neighbor,2930547,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.06648856401443481,#c49c94
-6.2704797,5.127836,neighbor,2930547,Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation,0.06651163101196289,#c49c94
-6.009118,1.8300546,neighbor,2930547,Composite Models of Objects and Scenes for Category Recognition,0.06666380167007446,#c49c94
3.9356594,-14.175518,neighbor,2930547,Sparse Output Coding for Large-Scale Visual Recognition,0.06741064786911011,#c49c94
1.588652,-11.343565,neighbor,2930547,From generic to specific deep representations for visual recognition,0.06752610206604004,#c49c94
-0.79691875,-8.854512,neighbor,2930547,Building high-level features using large scale unsupervised learning,0.06903070211410522,#c49c94
0.12242643,5.6645966,neighbor,2930547,Color Descriptors for Object Category Recognition,0.06904244422912598,#c49c94
-4.206123,4.084715,neighbor,2930547,Ensemble of exemplar-SVMs for object detection and beyond,0.06912899017333984,#c49c94
-8.2975,7.8310056,neighbor,2930547,Dynamic visual category learning,0.06924927234649658,#c49c94
-3.3544118,8.512169,neighbor,2930547,Local Metric Learning for Exemplar-Based Object Detection,0.06944090127944946,#c49c94
1.1898034,1.3521113,neighbor,2930547,PartBook for image parsing,0.06986844539642334,#c49c94
8.040154,-6.264644,neighbor,2930547,Visual Objects Classification with Sliding Spatial Pyramid Matching,0.07020509243011475,#c49c94
4.2548046,5.750217,neighbor,2930547,Unsupervised and Transfer Learning under Uncertainty - From Object Detections to Scene Categorization,0.07028317451477051,#c49c94
0.9407671,4.0624638,neighbor,2930547,Context-Based Object-Class Recognition and Retrieval by Generalized Correlograms,0.07037711143493652,#c49c94
-0.18742134,-6.5616455,neighbor,2930547,Fisher and VLAD with FLAIR,0.07056242227554321,#c49c94
2.434411,7.3499928,neighbor,2930547,Instance classification with prototype selection,0.0705716609954834,#c49c94
7.4493327,-9.2842245,neighbor,2930547,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.07064241170883179,#c49c94
2.9908888,-7.5531673,neighbor,2930547,Efficient On-the-fly Category Retrieval Using ConvNets and GPUs,0.07108438014984131,#c49c94
3.2310255,-2.5357392,neighbor,2930547,Learning Discriminative Hierarchical Features for Object Recognition,0.07142388820648193,#c49c94
-2.8491352,6.739904,neighbor,2930547,Viewpoint Invariant Object Detector,0.07170480489730835,#c49c94
6.1901145,-3.048695,neighbor,2930547,Approximate nearest neighbor search to support manual image annotation of large domain-specific datasets,0.07206207513809204,#c49c94
5.857054,-7.497334,neighbor,2930547,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.0721096396446228,#c49c94
3.671817,3.902436,neighbor,2930547,Blocks That Shout: Distinctive Parts for Scene Classification,0.07246589660644531,#c49c94
-6.505035,-3.48362,neighbor,2930547,Report from the first international workshop on computer vision meets databases (CVDB 2004),0.0725293755531311,#c49c94
4.880788,-3.6643982,neighbor,2930547,Local Naive Bayes Nearest Neighbor for image classification,0.07274550199508667,#c49c94
9.676251,-2.5980027,neighbor,2930547,Heterogeneous feature machines for visual recognition,0.07286792993545532,#c49c94
7.232663,6.9386635,neighbor,2930547,Associative Embeddings for Large-Scale Knowledge Transfer with Self-Assessment,0.07297402620315552,#c49c94
3.4070275,-0.5430611,neighbor,2930547,On Taxonomies for Multi-class Image Categorization,0.07321763038635254,#c49c94
0.3828279,-8.2209015,neighbor,2930547,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.07368201017379761,#c49c94
-1.7025291,-1.8291123,neighbor,2930547,Generic Object Detection with Dense Neural Patterns and Regionlets,0.07388657331466675,#c49c94
-9.408265,7.645073,neighbor,2930547,ARTOS - Adaptive Real-Time Object Detection System,0.07402360439300537,#c49c94
-2.465974,4.272574,neighbor,2930547,Object Proposal Generation Using Two-Stage Cascade SVMs,0.07404637336730957,#c49c94
10.4183445,2.1271544,neighbor,2930547,Semantics-preserving bag-of-words models for efficient image annotation,0.0742722749710083,#c49c94
-3.4611938,-11.803527,neighbor,2930547,The Rijksmuseum Challenge: Museum-Centered Visual Recognition,0.07431435585021973,#c49c94
6.509476,-8.2095995,neighbor,2930547,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.07481592893600464,#c49c94
-3.0345447,11.087911,neighbor,2930547,Are all training examples equally valuable?,0.07482719421386719,#c49c94
1.2530546,1.9364014,neighbor,2930547,Context Driven Scene Parsing with Attention to Rare Classes,0.07486152648925781,#c49c94
-5.606045,-0.038539734,neighbor,2930547,Contextual Hierarchical Part-Driven Conditional Random Field Model for Object Category Detection,0.07526707649230957,#c49c94
-3.2546957,2.854671,neighbor,2930547,Regionlets for Generic Object Detection,0.07528746128082275,#c49c94
7.770171,7.200251,neighbor,2930547,Visual-Semantic Scene Understanding by Sharing Labels in a Context Network,0.07556581497192383,#c49c94
3.737412,-2.2867987,neighbor,2930547,Learning mid-level features from object hierarchy for image classification,0.07580053806304932,#c49c94
-10.01717,-6.952629,neighbor,2930547,Deep Convolutional Ranking for Multilabel Image Annotation,0.07586944103240967,#c49c94
-2.3487303,-8.525796,neighbor,2930547,Occlusion Coherence: Localizing Occluded Faces with a Hierarchical Deformable Part Model,0.07619112730026245,#c49c94
-7.1875906,2.4896197,neighbor,2930547,"Recursive compositional models: Representation, learning, and inference",0.07622414827346802,#c49c94
11.141399,0.91289824,neighbor,2930547,Large-scale visual concept detection with explicit kernel maps and power mean SVM,0.07661378383636475,#c49c94
-7.775968,4.6756554,neighbor,2930547,Dynamical And-Or Graph Learning for Object Shape Modeling and Detection,0.0766611099243164,#c49c94
-8.453856,-9.971716,neighbor,2930547,Label-to-region with continuity-biased bi-layer sparsity priors,0.0768858790397644,#c49c94
10.326422,0.5782822,neighbor,2930547,Towards optimal bag-of-features for object categorization and semantic video retrieval,0.0772753357887268,#c49c94
9.1233,0.8904125,neighbor,2930547,Evaluating bag-of-visual-words representations in scene classification,0.07752376794815063,#c49c94
3.7033179,-12.945971,neighbor,2930547,Unsupervised Feature Learning by Deep Sparse Coding,0.07752418518066406,#c49c94
4.8974524,12.085367,neighbor,2930547,Regression-Based Image Alignment for General Object Categories,0.07760417461395264,#c49c94
1.9903567,-4.90171,neighbor,2930547,Deep learning,0.07778769731521606,#c49c94
-8.390066,-9.833863,neighbor,2930547,Label to region by bi-layer sparsity priors,0.077789306640625,#c49c94
-0.37256655,3.4896774,neighbor,2930547,Context-Aware Semi-Local Feature Detector,0.07783710956573486,#c49c94
12.0034685,-0.38826367,neighbor,2930547,All About VLAD,0.07794153690338135,#c49c94
3.3063235,-11.909484,neighbor,2930547,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.07805043458938599,#c49c94
4.069388,11.625912,neighbor,2930547,Unsupervised detection and segmentation of identical objects,0.0781397819519043,#c49c94
5.671581,-10.7543335,neighbor,2930547,Multi-column deep neural networks for image classification,0.07840967178344727,#c49c94
-8.492538,3.1108274,neighbor,2930547,Building Part-Based Object Detectors via 3D Geometry,0.07865071296691895,#c49c94
5.472846,6.53362,neighbor,2930547,Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with Implicit Low-rank Transformations,0.07880133390426636,#c49c94
-1.5421262,2.336689,neighbor,2930547,An empirical study of context in object detection,0.07886278629302979,#c49c94
-3.6292899,9.304984,neighbor,2930547,Learning From a Small Number of Training Examples by Exploiting Object Categories,0.0789649486541748,#c49c94
5.9346433,10.538908,neighbor,2930547,Fusing object detection and region appearance for image-text alignment,0.07899415493011475,#c49c94
-2.7019846,-12.485052,neighbor,2930547,Fine-Grained Visual Classification of Aircraft,0.07902544736862183,#c49c94
-0.7277779,-15.196529,neighbor,2930547,Why is Real-World Visual Object Recognition Hard?,0.07933896780014038,#c49c94
0.015982175,8.922525,neighbor,2930547,On the impact of learning hierarchical representations for visual recognition in robotics,0.07940554618835449,#c49c94
9.253789,-3.6315858,neighbor,2930547,Feature combination with Multi-Kernel Learning for fine-grained visual classification,0.07944464683532715,#c49c94
9.948908,-1.7366722,neighbor,2930547,Feature and Region Selection for Visual Learning,0.07949811220169067,#c49c94
-9.027202,-6.4205055,neighbor,2930547,HCP: A Flexible CNN Framework for Multi-Label Image Classification,0.07965666055679321,#c49c94
-5.384577,3.714276,neighbor,2930547,Fast concurrent object localization and recognition,0.0797494649887085,#c49c94
-4.8892493,-6.131467,neighbor,2930547,Predicting Failures of Vision Systems,0.07978218793869019,#c49c94
-9.20024,-1.8106833,neighbor,2930547,Discriminative Maximum Margin Image Object Categorization with Exact Inference,0.07979005575180054,#c49c94
8.351528,1.0478677,neighbor,2930547,Scene categorization with multiscale category specific visual words,0.07990115880966187,#c49c94
5.9698405,2.6682703,neighbor,2930547,SUN Database: Exploring a Large Collection of Scene Categories,0.07995682954788208,#c49c94
-9.477773,1.2273493,neighbor,2930547,Layered object detection for multi-class segmentation,0.08002424240112305,#c49c94
0.32552394,-12.349477,neighbor,2930547,Visual Transfer Learning: Informal Introduction and Literature Overview,0.0801243782043457,#c49c94
4.695509,0.5459517,neighbor,2930547,All vehicles are cars: subclass preferences in container concepts,0.08025127649307251,#c49c94
7.418881,-8.022581,neighbor,2930547,Learnable Pooling Regions for Image Classification,0.08026659488677979,#c49c94
-1.9073695,4.6742587,query,3252915,Latent semantic analysis,0.0,#e377c2
-2.9017863,4.0468125,neighbor,3252915,A probabilistic model for Latent Semantic Indexing,0.05654942989349365,#e377c2
-0.8933036,5.4048653,neighbor,3252915,Latent semantic analysis of textual data,0.0647040605545044,#e377c2
-4.267221,4.4327106,neighbor,3252915,Computing Term Translation Probabilities with Generalized Latent Semantic Analysis,0.06825292110443115,#e377c2
1.061959,4.0670958,neighbor,3252915,A Rough Concept Recognition Approach for Information Retrieval Based on Latent Semantic Analysis,0.07001179456710815,#e377c2
-3.4427273,-1.3117495,neighbor,3252915,A Comparative Analysis of Latent Variable Models for Web Page Classification,0.07262617349624634,#e377c2
-4.6305475,-3.568686,neighbor,3252915,Latent Dirichlet Allocation,0.07581114768981934,#e377c2
-3.107472,7.2280726,neighbor,3252915,An Introduction to Random Indexing,0.0782313346862793,#e377c2
-1.545573,2.582097,neighbor,3252915,Latent semantic analysis for multiple-type interrelated data objects,0.07945841550827026,#e377c2
-3.6070871,2.221055,neighbor,3252915,A Differential LSI Method for Document Classification,0.0817880630493164,#e377c2
-6.5276394,12.429159,neighbor,3252915,E-Assessment using Latent Semantic Analysis,0.08254408836364746,#e377c2
-2.211561,-0.15638876,neighbor,3252915,Comparing LDA with pLSI as a Dimensionality Reduction Method in Document Clustering,0.08511495590209961,#e377c2
12.514829,-1.1205014,neighbor,3252915,Principal Component Analysis,0.0859299898147583,#e377c2
-4.7078967,5.3998346,neighbor,3252915,Improving latent semantic indexing based classifier with information gain,0.0868118405342102,#e377c2
5.9340253,4.715525,neighbor,3252915,Exploring term-document matrices from matrix models in text mining,0.08751988410949707,#e377c2
13.149336,-0.74438345,neighbor,3252915,Principal Component Analysis,0.08779704570770264,#e377c2
6.6232247,4.90575,neighbor,3252915,Extracting latent structures in numerical classification: an investigation using two factor models,0.08783316612243652,#e377c2
-0.6842743,0.7694057,neighbor,3252915,Document clustering using locality preserving indexing,0.08801591396331787,#e377c2
10.986351,4.940842,neighbor,3252915,Linear Latent Structure Analysis: from Foundations to Algorithms and Applications,0.08827507495880127,#e377c2
-14.961557,-4.3434677,neighbor,3252915,Multimedia Data Mining and Knowledge Discovery,0.0900229811668396,#e377c2
-10.859649,0.09908358,neighbor,3252915,Inferring query models by computing information flow,0.09015059471130371,#e377c2
4.7766747,-3.1551175,neighbor,3252915,Design and implementation of a web mining system for organizing search engine results,0.0903593897819519,#e377c2
-6.6155005,3.6406493,neighbor,3252915,Improve latent semantic analysis based language model by integrating multiple level knowledge,0.09055405855178833,#e377c2
18.54055,0.9881372,neighbor,3252915,相对论平均场理论对Se，Kr，Sr和Zr同位素链形状共存的系统研究,0.09190917015075684,#e377c2
-17.50937,-2.7019124,neighbor,3252915,Automatic Linguistic Indexing of Pictures by a Statistical Modeling Approach,0.0923837423324585,#e377c2
-3.283423,7.94141,neighbor,3252915,Random Indexing using Statistical Weight Functions,0.09256768226623535,#e377c2
-6.7119207,-5.0163794,neighbor,3252915,Model-Based Hierarchical Clustering,0.09285569190979004,#e377c2
10.864648,-0.15407121,neighbor,3252915,Exploratory Data Mining and Data Cleaning,0.09331768751144409,#e377c2
4.7220874,-1.0561091,neighbor,3252915,A Method of Cluster-Based Indexing of Textual Data,0.09410178661346436,#e377c2
-6.436952,-1.3369516,neighbor,3252915,Evaluating topic models for information retrieval,0.09438318014144897,#e377c2
-2.298177,-4.453807,neighbor,3252915,Topic Analysis Using a Finite Mixture Model,0.0944666862487793,#e377c2
-0.30316645,10.943654,neighbor,3252915,Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL,0.09452962875366211,#e377c2
-7.028309,-0.74680054,neighbor,3252915,Two-stage language models for information retrieval,0.09529328346252441,#e377c2
-7.564705,-6.4829917,neighbor,3252915,Fine: Information embedding for document classification,0.09538382291793823,#e377c2
-1.3365669,13.120337,neighbor,3252915,Measuring Semantic Similarity by Latent Relational Analysis,0.09561800956726074,#e377c2
11.442722,5.2216306,neighbor,3252915,Learning Measurement Models for Unobserved Variables,0.09580689668655396,#e377c2
-0.41949043,12.8676,neighbor,3252915,Similarity of Semantic Relations,0.09592181444168091,#e377c2
11.476231,-1.9946005,neighbor,3252915,Principal Component Analysis and Effective K-Means Clustering,0.09630328416824341,#e377c2
1.623857,-6.1122456,neighbor,3252915,GENERATION OF A SET OF KEY TERMS CHARACTERISING TEXT DOCUMENTS,0.0963318943977356,#e377c2
1.3654883,3.1491985,neighbor,3252915,A Novel Approach to Semantic Indexing Based on Concept,0.09658986330032349,#e377c2
-4.704523,-4.9965014,neighbor,3252915,The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies,0.09689897298812866,#e377c2
-1.2192936,-4.6653113,neighbor,3252915,Searching for Topics in a Large Collection of Texts,0.09716236591339111,#e377c2
-14.398731,-1.4400749,neighbor,3252915,Assessing the Filtering and Browsing Utility of Automatic Semantic Concepts for Multimedia Retrieval,0.09717434644699097,#e377c2
-1.278685,12.272253,neighbor,3252915,Human-Level Performance on Word Analogy Questions by Latent Relational Analysis,0.09726399183273315,#e377c2
-3.7964964,12.146565,neighbor,3252915,Effect of tuned parameters on an LSA multiple choice questions answering model,0.09770715236663818,#e377c2
4.431405,-11.168302,neighbor,3252915,Learning Document-Level Semantic Properties from Free-Text Annotations,0.09821969270706177,#e377c2
18.820528,-0.28844908,neighbor,3252915,Keyword index,0.09834122657775879,#e377c2
-14.681773,-2.4487922,neighbor,3252915,Video Retrieval Based on Semantic Concepts,0.09844201803207397,#e377c2
0.79431945,12.240192,neighbor,3252915,The Google Similarity Distance,0.09860396385192871,#e377c2
3.5300424,-7.190846,neighbor,3252915,Utilizing the World Wide Web as an Encyclopedia: Extracting Term Descriptions from Semi-Structured Texts,0.09861111640930176,#e377c2
3.362003,-2.6041083,neighbor,3252915,UA-ZSA: Web Page Clustering on the basis of Name Disambiguation,0.09872567653656006,#e377c2
-1.2173797,-6.730815,neighbor,3252915,Simple Semantics in Topic Detection and Tracking,0.0991525650024414,#e377c2
-0.42341948,-3.566067,neighbor,3252915,Detecting Multiple Facets of an Event using Graph-Based Unsupervised Methods,0.09915703535079956,#e377c2
2.1064117,-8.567836,neighbor,3252915,Enhancing Linguistically Oriented Automatic Keyword Extraction,0.09916025400161743,#e377c2
-5.447696,-3.2229717,neighbor,3252915,The Author-Topic Model for Authors and Documents,0.09921586513519287,#e377c2
2.5675642,13.420455,neighbor,3252915,Toward a unification of text and link analysis,0.09922486543655396,#e377c2
-3.2328238,-3.277822,neighbor,3252915,Organizing the OCA: learning faceted subjects from a library of digital books,0.09931588172912598,#e377c2
15.499552,0.04279309,neighbor,3252915,Classification,0.09955668449401855,#e377c2
-5.411852,-10.981329,neighbor,3252915,Locally Weighted Naive Bayes,0.09955793619155884,#e377c2
4.555767,-9.605777,neighbor,3252915,A Language Model Approach to Keyphrase Extraction,0.09960567951202393,#e377c2
-1.7590492,-9.924348,neighbor,3252915,Fragments and Text Categorization,0.09976840019226074,#e377c2
-11.106761,6.29802,neighbor,3252915,The support vector decomposition machine,0.09981751441955566,#e377c2
2.1845317,5.088923,neighbor,3252915,Influence of a priori Knowledge on Medical Document Categorization,0.10001128911972046,#e377c2
-17.239388,-1.4856063,neighbor,3252915,Toward Bridging the Annotation-Retrieval Gap in Image Search,0.10015106201171875,#e377c2
13.169613,0.9569036,neighbor,3252915,Factor Analysis and Alternating Minimization,0.10018998384475708,#e377c2
8.305653,-1.8794864,neighbor,3252915,Information-based clustering.,0.1003497838973999,#e377c2
-6.6142488,10.983628,neighbor,3252915,Automatic Essay Grading with Probabilistic Latent Semantic Analysis,0.10089486837387085,#e377c2
-5.989964,-11.1953125,neighbor,3252915,"Comparison of lazy Bayesian rule, and tree-augmented Bayesian learning",0.10103332996368408,#e377c2
16.304037,-0.9181735,neighbor,3252915,Introduction,0.10103952884674072,#e377c2
-8.200658,-3.8238044,neighbor,3252915,Language Model-Based Document Clustering Using Random Walks,0.10113948583602905,#e377c2
4.2904954,-8.965716,neighbor,3252915,Domain-specific keyphrase extraction,0.10115987062454224,#e377c2
-15.849124,-3.418422,neighbor,3252915,"Semantic Indexing of Multimedia Content Using Visual, Audio, and Text Cues",0.10122472047805786,#e377c2
-2.9148722,-10.232206,neighbor,3252915,A structure-sensitive framework for text categorization,0.10133397579193115,#e377c2
-4.5716286,-2.0836031,neighbor,3252915,HTM: A Topic Model for Hypertexts,0.10136562585830688,#e377c2
6.058756,0.15293173,neighbor,3252915,Introduction to information retrieval,0.10141277313232422,#e377c2
14.303137,-1.8350097,neighbor,3252915,Bayesian Data Analysis,0.10161608457565308,#e377c2
19.638876,-1.3958354,neighbor,3252915,One thousand words,0.10177910327911377,#e377c2
-11.204302,-0.6133669,neighbor,3252915,Usage based Indexing of Web Resources with Natural Language Processing,0.10190927982330322,#e377c2
-0.7144481,-5.500561,neighbor,3252915,Contribution to topic identification by using word similarity,0.10192865133285522,#e377c2
18.915276,-1.9061753,neighbor,3252915,One thousand words,0.10206598043441772,#e377c2
-6.1426706,11.74313,neighbor,3252915,E-Assessment using Latent Semantic Analysis in the Computer Science Domain: A Pilot Study,0.10207390785217285,#e377c2
0.41154438,-9.668559,neighbor,3252915,Automatic vs manual categorisation of documents in Spanish,0.10221874713897705,#e377c2
-5.2833605,-6.314983,neighbor,3252915,An Improved Hierarchical Bayesian Model of Language for Document Classification,0.10239458084106445,#e377c2
-11.830256,-6.4110923,neighbor,3252915,Learning Word Meanings and Descriptive Parameter Spaces from Music,0.10240155458450317,#e377c2
0.47856572,9.8235035,neighbor,3252915,Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus,0.10240620374679565,#e377c2
0.28770965,0.2846515,neighbor,3252915,Hybrid Document Indexing with Spectral Embedding,0.10268920660018921,#e377c2
-4.6250844,-8.160007,neighbor,3252915,Investigating Unsupervised Learning for Text Categorization Bootstrapping,0.1028280258178711,#e377c2
-9.050126,-3.0641174,neighbor,3252915,UVA: Language Modeling Techniques for Web People Search,0.10301119089126587,#e377c2
3.786535,-0.5943513,neighbor,3252915,A new approach for fuzzy clustering of Web documents,0.10319191217422485,#e377c2
4.4411783,-4.7625666,neighbor,3252915,Improving Knowledge Discovery in Document Collections through Combining Text Retrieval and Link Analysis Techniques,0.10337883234024048,#e377c2
17.318354,-1.5605676,neighbor,3252915,Correction,0.1035301685333252,#e377c2
-15.324767,-1.891914,neighbor,3252915,Can High-Level Concepts Fill the Semantic Gap in Video Retrieval? A Case Study With Broadcast News,0.10360729694366455,#e377c2
2.473218,9.130172,neighbor,3252915,HAL-Based Cascaded Model for Variable-Length Semantic Pattern Induction from Psychiatry Web Resources,0.10361605882644653,#e377c2
1.0211902,-3.7672567,neighbor,3252915,Automatic concept identification in goal-oriented conversations,0.10368585586547852,#e377c2
3.4571142,8.9122925,neighbor,3252915,Exploring the use of natural language systems for fact identification: Towards the automatic construction of healthcare portals,0.10372793674468994,#e377c2
4.133377,12.860598,neighbor,3252915,Document Representation and Multilevel Measures of Document Similarity,0.10389012098312378,#e377c2
-10.865437,4.919247,neighbor,3252915,Text Document Pre-Processing Using the Bayes Formula for Classification Based on the Vector Space Model,0.10395950078964233,#e377c2
17.527771,-0.42206338,neighbor,3252915,List of reviewers,0.10416388511657715,#e377c2
-7.318038,3.1064646,neighbor,3252915,Latent Semantic Information in Maximum Entropy Language Models for Conversational Speech Recognition,0.10424923896789551,#e377c2
-2.7208648,-11.636063,neighbor,3252915,Temporal Feature Modification for Retrospective Categorization,0.10424935817718506,#e377c2
17.063711,0.59074855,neighbor,3252915,Acknowledgements,0.1042742133140564,#e377c2
0.08532559,-3.003808,query,3429309,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",0.0,#e377c2
0.23528007,-3.603104,neighbor,3429309,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.019581258296966553,#e377c2
-0.9890977,-2.8864214,neighbor,3429309,Fully convolutional networks for semantic segmentation,0.020280659198760986,#e377c2
2.672695,-1.3769585,neighbor,3429309,Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation,0.025933802127838135,#e377c2
0.35698745,-0.9502249,neighbor,3429309,Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation,0.028569400310516357,#e377c2
-2.7459278,-6.774322,neighbor,3429309,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,0.028909265995025635,#e377c2
1.8233726,-3.3651733,neighbor,3429309,Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform,0.030073881149291992,#e377c2
-2.8759599,2.7401993,neighbor,3429309,Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation,0.030385494232177734,#e377c2
2.9369998,-1.3230054,neighbor,3429309,Exploring Context with Deep Structured Models for Semantic Segmentation,0.031350135803222656,#e377c2
-1.2475723,0.9682279,neighbor,3429309,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.031646013259887695,#e377c2
-0.67180693,2.1246426,neighbor,3429309,Learning Deconvolution Network for Semantic Segmentation,0.03215658664703369,#e377c2
-2.3079247,-2.2969465,neighbor,3429309,Segmentation-aware convolutional nets,0.03283286094665527,#e377c2
0.7966346,-1.2781823,neighbor,3429309,ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation,0.034069955348968506,#e377c2
-3.4155307,-2.237685,neighbor,3429309,Learning Dense Convolutional Embeddings for Semantic Segmentation,0.03407663106918335,#e377c2
3.1589744,-3.9412708,neighbor,3429309,Convolutional Random Walk Networks for Semantic Image Segmentation,0.03417348861694336,#e377c2
2.0091097,0.7219597,neighbor,3429309,Attention to Scale: Scale-Aware Semantic Image Segmentation,0.034521639347076416,#e377c2
4.6248364,0.19266103,neighbor,3429309,Semantic Part Segmentation with Deep Learning,0.03507709503173828,#e377c2
-1.2059793,-0.8576992,neighbor,3429309,ParseNet: Looking Wider to See Better,0.03558778762817383,#e377c2
3.3509018,-3.1777656,neighbor,3429309,Semantic Segmentation with Boundary Neural Fields,0.036599576473236084,#e377c2
-0.86322486,-2.2329516,neighbor,3429309,Fully Connected Deep Structured Networks,0.03694295883178711,#e377c2
-2.9673972,-4.184911,neighbor,3429309,Feedforward semantic segmentation with zoom-out features,0.03751075267791748,#e377c2
0.056608114,0.2685339,neighbor,3429309,Combining the Best of Graphical Models and ConvNets for Semantic Segmentation,0.03765702247619629,#e377c2
4.4551387,-5.916273,neighbor,3429309,Fast Semantic Image Segmentation with High Order Context and Guided Filtering,0.03771477937698364,#e377c2
1.1971102,5.858271,neighbor,3429309,Instance-Aware Semantic Segmentation via Multi-task Network Cascades,0.038789331912994385,#e377c2
6.368703,-6.565656,neighbor,3429309,"Fast, Exact and Multi-scale Inference for Semantic Image Segmentation with Deep Gaussian CRFs",0.038967788219451904,#e377c2
-2.1991634,3.6720946,neighbor,3429309,"Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation",0.039082348346710205,#e377c2
-4.095679,-5.0206428,neighbor,3429309,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling,0.03912186622619629,#e377c2
-1.7599299,-3.9913578,neighbor,3429309,Multi-Scale Context Aggregation by Dilated Convolutions,0.03977406024932861,#e377c2
-2.6835449,3.4198809,neighbor,3429309,Weakly Supervised Semantic Segmentation with Convolutional Networks,0.04000169038772583,#e377c2
-0.2950911,2.8635027,neighbor,3429309,Object Boundary Guided Semantic Segmentation,0.04000884294509888,#e377c2
0.601897,2.2343202,neighbor,3429309,Convolutional feature masking for joint object and stuff segmentation,0.040539443492889404,#e377c2
-3.7807004,4.5481176,neighbor,3429309,Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network,0.04079771041870117,#e377c2
5.5513635,-4.5656543,neighbor,3429309,Conditional Random Fields as Recurrent Neural Networks,0.041147053241729736,#e377c2
6.940038,9.54745,neighbor,3429309,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.04190903902053833,#e377c2
-6.662365,-0.14962061,neighbor,3429309,Learning High-level Prior with Convolutional Neural Networks for Semantic Segmentation,0.04208266735076904,#e377c2
-2.188346,-0.29163003,neighbor,3429309,Context Tricks for Cheap Semantic Segmentation,0.04337829351425171,#e377c2
-2.6452446,-8.012701,neighbor,3429309,Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding,0.043725669384002686,#e377c2
-4.8741326,-2.473623,neighbor,3429309,Dense Image Labeling Using Deep Convolutional Neural Networks,0.04413837194442749,#e377c2
-4.513072,5.621216,neighbor,3429309,STC: A Simple to Complex Framework for Weakly-Supervised Semantic Segmentation,0.04426002502441406,#e377c2
-1.02393,-4.8387017,neighbor,3429309,Dense CNN Learning with Equivalent Mappings,0.044505298137664795,#e377c2
1.1872655,-7.0767527,neighbor,3429309,ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation,0.0446125864982605,#e377c2
3.5728061,1.8262832,neighbor,3429309,Joint Object and Part Segmentation Using Deep Learned Potentials,0.04480248689651489,#e377c2
-3.5618987,2.2692313,neighbor,3429309,Constrained Convolutional Neural Networks for Weakly Supervised Segmentation,0.04490083456039429,#e377c2
1.5749245,5.1618533,neighbor,3429309,Bridging Category-level and Instance-level Semantic Image Segmentation,0.04500025510787964,#e377c2
-3.778367,3.7513318,neighbor,3429309,Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation,0.04511153697967529,#e377c2
10.498629,-4.576037,neighbor,3429309,Semantic video segmentation: Exploring inference efficiency,0.045557498931884766,#e377c2
-2.8379536,5.713449,neighbor,3429309,Weakly-Supervised Semantic Segmentation Using Motion Cues,0.04576456546783447,#e377c2
-1.2529787,4.6635633,neighbor,3429309,What's the Point: Semantic Segmentation with Point Supervision,0.04623967409133911,#e377c2
5.1769643,7.423242,neighbor,3429309,Learning to Refine Object Segments,0.0463261604309082,#e377c2
5.3336263,-4.900133,neighbor,3429309,Higher Order Conditional Random Fields in Deep Neural Networks,0.04749023914337158,#e377c2
4.6321287,3.7052224,neighbor,3429309,Semantic Segmentation with Object Clique Potential,0.04773479700088501,#e377c2
-0.021153776,-10.593856,neighbor,3429309,Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks,0.04823958873748779,#e377c2
-4.266149,-0.25060195,neighbor,3429309,Improving spatial codification in semantic segmentation,0.04908627271652222,#e377c2
6.9072742,-5.310644,neighbor,3429309,Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation,0.04946047067642212,#e377c2
-0.9225646,10.292247,neighbor,3429309,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.04962635040283203,#e377c2
7.1566644,9.548227,neighbor,3429309,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.049716949462890625,#e377c2
-5.745501,1.9031898,neighbor,3429309,Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation,0.04988753795623779,#e377c2
8.626843,-0.81415284,neighbor,3429309,Ontology-Based Semantic Image Segmentation Using Mixture Models and Multiple CRFs,0.050439655780792236,#e377c2
7.9327226,-0.38547996,neighbor,3429309,Scene Segmentation with Low-Dimensional Semantic Representations and Conditional Random Fields,0.05062985420227051,#e377c2
-6.5476046,3.7100356,neighbor,3429309,Joint Calibration for Semantic Segmentation,0.05079621076583862,#e377c2
8.99538,11.715751,neighbor,3429309,Hypercolumns for object segmentation and fine-grained localization,0.050879478454589844,#e377c2
-1.9460208,9.872179,neighbor,3429309,Pushing the Boundaries of Boundary Detection using Deep Learning,0.05104804039001465,#e377c2
7.4983263,8.549542,neighbor,3429309,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.05200892686843872,#e377c2
10.412731,-4.572119,neighbor,3429309,Beyond Semantic Image Segmentation : Exploring Efficient Inference in Video,0.05216318368911743,#e377c2
-8.972328,-8.567769,neighbor,3429309,Deep Deconvolutional Networks for Scene Parsing,0.05238819122314453,#e377c2
-7.964062,-5.3590064,neighbor,3429309,Semantic Object Parsing with Local-Global Long Short-Term Memory,0.053826749324798584,#e377c2
-8.6439085,-7.4575195,neighbor,3429309,Deep hierarchical parsing for semantic segmentation,0.05413377285003662,#e377c2
0.68646455,-5.102233,neighbor,3429309,Mapping Auto-context Decision Forests to Deep ConvNets for Semantic Segmentation,0.05452382564544678,#e377c2
7.4187536,12.3805065,neighbor,3429309,Learning Deep Features for Discriminative Localization,0.05452662706375122,#e377c2
-1.5120758,-12.051192,neighbor,3429309,Constrained Parametric Proposals and Pooling Methods for Semantic Segmentation in RGB-D Images,0.05570363998413086,#e377c2
-4.169761,-7.2814136,neighbor,3429309,SEMANTIC SEGMENTATION OF AERIAL IMAGES WITH AN ENSEMBLE OF CNNS,0.05571192502975464,#e377c2
5.832787,-3.083738,neighbor,3429309,Joint Training of Generic CNN-CRF Models with Stochastic Optimization,0.05669510364532471,#e377c2
-1.8468984,-6.8665466,neighbor,3429309,Training Constrained Deconvolutional Networks for Road Scene Semantic Segmentation,0.05676984786987305,#e377c2
7.9342766,-1.9832507,neighbor,3429309,Semantic Segmentation with Same Topic Constraints,0.05684143304824829,#e377c2
-5.5991106,-4.4516063,neighbor,3429309,Pixel-Level Encoding and Depth Layering for Instance-Level Semantic Labeling,0.057208240032196045,#e377c2
-0.37415186,11.886891,neighbor,3429309,Semantic Amodal Segmentation,0.05730557441711426,#e377c2
8.1459055,6.6305823,neighbor,3429309,Semantic segmentation using regions and parts,0.05741584300994873,#e377c2
3.8381834,5.898713,neighbor,3429309,Proposal-Free Network for Instance-Level Object Segmentation,0.05801934003829956,#e377c2
5.4656577,-7.5341334,neighbor,3429309,Deeply Learning the Messages in Message Passing Inference,0.058635592460632324,#e377c2
3.579161,9.061161,neighbor,3429309,Deep Joint Task Learning for Generic Object Extraction,0.05865657329559326,#e377c2
7.3453994,-7.552912,neighbor,3429309,Variational reaction-diffusion systems for semantic segmentation,0.0588192343711853,#e377c2
8.7223835,9.814568,neighbor,3429309,DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization,0.058824121952056885,#e377c2
8.916248,0.22397876,neighbor,3429309,Dense Semantic Image Segmentation with Objects and Attributes,0.05990320444107056,#e377c2
5.665799,9.105274,neighbor,3429309,Learning to decompose for object detection and instance segmentation,0.060326576232910156,#e377c2
-2.1783159,10.524515,neighbor,3429309,PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset,0.06041055917739868,#e377c2
7.586051,11.275455,neighbor,3429309,ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks,0.06041520833969116,#e377c2
2.1847992,4.2032576,neighbor,3429309,End-to-End Instance Segmentation and Counting with Recurrent Attention,0.06043034791946411,#e377c2
7.1631274,7.3288927,neighbor,3429309,Layered object detection for multi-class segmentation,0.06043177843093872,#e377c2
6.7319922,11.426626,neighbor,3429309,Scalable Object Detection Using Deep Neural Networks,0.060472846031188965,#e377c2
7.4605603,-4.587544,neighbor,3429309,Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials,0.06088399887084961,#e377c2
-8.1859,-9.591187,neighbor,3429309,Scene Parsing With Integration of Parametric and Non-Parametric Models,0.061056554317474365,#e377c2
-1.792632,-11.831316,neighbor,3429309,Learning Rich Features from RGB-D Images for Object Detection and Segmentation,0.061447322368621826,#e377c2
4.0619197,7.953352,neighbor,3429309,Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation,0.062408506870269775,#e377c2
1.3243995,-10.11705,neighbor,3429309,Discriminative Training of Deep Fully Connected Continuous CRFs With Task-Specific Loss,0.06245696544647217,#e377c2
9.773028,8.711209,neighbor,3429309,Deformable part models are convolutional neural networks,0.0627373456954956,#e377c2
3.7057583,6.573786,neighbor,3429309,Reversible Recursive Instance-Level Object Segmentation,0.0627598762512207,#e377c2
5.916101,11.2088375,neighbor,3429309,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.0628122091293335,#e377c2
-0.9514588,-11.402158,neighbor,3429309,Indoor Semantic Segmentation using depth information,0.06290102005004883,#e377c2
-8.870624,-9.90891,neighbor,3429309,Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering,0.06299889087677002,#e377c2
-8.809011,-8.196316,neighbor,3429309,Recurrent Convolutional Neural Networks for Scene Parsing,0.06305462121963501,#e377c2
-1.7619203,11.0375,neighbor,3429309,Situational object boundary detection,0.06305921077728271,#e377c2
-0.022349749,-2.1476548,query,436933,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.0,#7f7f7f
-0.6416022,-2.2950714,neighbor,436933,Learnable Pooling Regions for Image Classification,0.039236485958099365,#7f7f7f
2.9701183,6.2895517,neighbor,436933,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.04606068134307861,#7f7f7f
0.8503268,-0.63830143,neighbor,436933,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.053760647773742676,#7f7f7f
-0.57952976,-10.184205,neighbor,436933,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.0548589825630188,#7f7f7f
2.261623,11.569504,neighbor,436933,Scalable Object Detection Using Deep Neural Networks,0.055106282234191895,#7f7f7f
5.5749016,-6.8054295,neighbor,436933,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.0555838942527771,#7f7f7f
-0.47953153,-0.30444112,neighbor,436933,Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences,0.05569642782211304,#7f7f7f
3.1390727,11.418966,neighbor,436933,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.05761682987213135,#7f7f7f
-1.1892482,4.2032,neighbor,436933,Visual Objects Classification with Sliding Spatial Pyramid Matching,0.05847179889678955,#7f7f7f
-1.5531621,-4.355906,neighbor,436933,Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models,0.059232234954833984,#7f7f7f
0.8319313,-8.751305,neighbor,436933,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.06192547082901001,#7f7f7f
4.1992407,-9.948219,neighbor,436933,Deep Epitomic Convolutional Neural Networks,0.06327962875366211,#7f7f7f
-2.1553493,-5.729808,neighbor,436933,Differentiable Pooling for Hierarchical Feature Learning,0.06342655420303345,#7f7f7f
-9.106912,2.0253065,neighbor,436933,A neural computational model for bottom-up attention with invariant and overcomplete representation,0.06407004594802856,#7f7f7f
3.310292,-10.095755,neighbor,436933,Network In Network,0.06487911939620972,#7f7f7f
2.0445828,-4.294607,neighbor,436933,Convolutional Kernel Networks,0.0656084418296814,#7f7f7f
1.4817494,11.027414,neighbor,436933,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.06715595722198486,#7f7f7f
3.2511473,-0.42552558,neighbor,436933,Unsupervised Feature Learning by Deep Sparse Coding,0.06756472587585449,#7f7f7f
9.154182,-12.233404,neighbor,436933,Fast image scanning with deep max-pooling convolutional neural networks,0.06784480810165405,#7f7f7f
4.52756,12.4479475,neighbor,436933,Generic Object Detection with Dense Neural Patterns and Regionlets,0.06874775886535645,#7f7f7f
0.47678077,1.7527877,neighbor,436933,Generalized Max Pooling,0.06893110275268555,#7f7f7f
2.8146398,7.1085863,neighbor,436933,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.06966519355773926,#7f7f7f
6.87536,0.33407563,neighbor,436933,PCANet: A Simple Deep Learning Baseline for Image Classification?,0.07003206014633179,#7f7f7f
7.350549,-9.654463,neighbor,436933,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.07113182544708252,#7f7f7f
7.0600653,-8.422361,neighbor,436933,Multi-column deep neural networks for image classification,0.07276874780654907,#7f7f7f
-0.2094386,3.4928875,neighbor,436933,Feature normalization for part-based image classification,0.07373589277267456,#7f7f7f
1.1354034,4.1995068,neighbor,436933,Local Naive Bayes Nearest Neighbor for image classification,0.07414126396179199,#7f7f7f
-3.872476,-6.7359986,neighbor,436933,Deconvolutional networks,0.07458800077438354,#7f7f7f
0.70889235,-9.641375,neighbor,436933,Improving Deep Neural Networks with Probabilistic Maxout Units,0.07478684186935425,#7f7f7f
4.8736677,10.861412,neighbor,436933,Deep learning for class-generic object detection,0.07482486963272095,#7f7f7f
6.214475,-11.266057,neighbor,436933,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.0748632550239563,#7f7f7f
-0.22015318,-12.288253,neighbor,436933,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.07593709230422974,#7f7f7f
-5.3109183,-2.5654473,neighbor,436933,Sparsity-Regularized HMAX for Visual Recognition,0.07615429162979126,#7f7f7f
5.5432525,3.2840002,neighbor,436933,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.07737421989440918,#7f7f7f
-4.4601007,-1.5777409,neighbor,436933,Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition,0.07794618606567383,#7f7f7f
-6.585937,-0.6003045,neighbor,436933,FastWavelet-Based Visual Classification,0.07842069864273071,#7f7f7f
7.4676876,-0.31049046,neighbor,436933,Learning Deep Face Representation,0.07865792512893677,#7f7f7f
6.6758947,-1.0218729,neighbor,436933,Building high-level features using large scale unsupervised learning,0.07875096797943115,#7f7f7f
-0.091656186,10.44327,neighbor,436933,Recurrent Convolutional Neural Networks for Scene Parsing,0.07901996374130249,#7f7f7f
0.8715614,12.534749,neighbor,436933,A novel method for object localization in digital images,0.07904589176177979,#7f7f7f
0.6488098,-11.566424,neighbor,436933,Piecewise Linear Multilayer Perceptrons and Dropout,0.07944363355636597,#7f7f7f
5.838853,-11.951251,neighbor,436933,Fast Training of Convolutional Networks through FFTs,0.07959121465682983,#7f7f7f
1.840016,-5.732831,neighbor,436933,Invariant Scattering Convolution Networks,0.08023172616958618,#7f7f7f
-1.6718639,-7.757257,neighbor,436933,Scheduled denoising autoencoders,0.08141767978668213,#7f7f7f
-2.236817,-9.9675865,neighbor,436933,Signal recovery from Pooling Representations,0.08183920383453369,#7f7f7f
-9.163456,5.7730556,neighbor,436933,Salient Object Detection: A Discriminative Regional Feature Integration Approach,0.08239972591400146,#7f7f7f
7.8197412,-8.91631,neighbor,436933,"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",0.08250278234481812,#7f7f7f
2.5440872,14.549677,neighbor,436933,The Fastest Deformable Part Model for Object Detection,0.0826265811920166,#7f7f7f
1.8559973,13.436897,neighbor,436933,Regionlets for Generic Object Detection,0.0826767086982727,#7f7f7f
-2.7283413,10.912253,neighbor,436933,Context-Aware Semi-Local Feature Detector,0.08274996280670166,#7f7f7f
-2.5772707,2.7675436,neighbor,436933,Image Classification Using Sparse Coding and Spatial Pyramid Matching,0.08307278156280518,#7f7f7f
-0.7961757,1.7743354,neighbor,436933,Spatial pooling of heterogeneous features for image applications,0.08326476812362671,#7f7f7f
4.968631,3.9211085,neighbor,436933,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.08337247371673584,#7f7f7f
-9.058488,4.7732377,neighbor,436933,Is Bottom-Up Attention Useful for Scene Recognition?,0.08348405361175537,#7f7f7f
-4.7984533,-3.4151464,neighbor,436933,Efficient Learning of Sparse Invariant Representations,0.08354657888412476,#7f7f7f
6.7569647,-12.489938,neighbor,436933,One weird trick for parallelizing convolutional neural networks,0.08354973793029785,#7f7f7f
-9.56571,-4.6546106,neighbor,436933,Comparison between Frame-Constrained Fix-Pixel-Value and Frame-Free Spiking-Dynamic-Pixel ConvNets for Visual Processing,0.08374863862991333,#7f7f7f
6.1004996,9.822846,neighbor,436933,Fine-grained object recognition with Gnostic Fields,0.08382701873779297,#7f7f7f
2.9640205,15.909157,neighbor,436933,Local Decorrelation For Improved Detection,0.08385294675827026,#7f7f7f
-9.404573,1.4070387,neighbor,436933,Modelling selective attention with Hodgkin-Huxley neurons,0.08462774753570557,#7f7f7f
4.150338,2.3986998,neighbor,436933,Comparing state-of-the-art visual features on invariant object recognition tasks,0.08498966693878174,#7f7f7f
-8.465401,6.0312567,neighbor,436933,Efficient Salient Region Detection with Soft Image Abstraction,0.08523696660995483,#7f7f7f
6.049994,7.832184,neighbor,436933,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.08524876832962036,#7f7f7f
4.326887,-5.0889397,neighbor,436933,Deformation Models for Image Recognition,0.08538669347763062,#7f7f7f
3.6001732,-8.633122,neighbor,436933,Visualizing and Understanding Convolutional Networks,0.085540771484375,#7f7f7f
5.5660105,-0.37983936,neighbor,436933,Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification,0.08556431531906128,#7f7f7f
4.533739,15.144717,neighbor,436933,Joint Deep Learning for Pedestrian Detection,0.08569324016571045,#7f7f7f
2.0443442,-12.253002,neighbor,436933,Dropout Rademacher complexity of deep neural networks,0.0866628885269165,#7f7f7f
0.056353133,7.5702505,neighbor,436933,"Codemaps - Segment, Classify and Search Objects Locally",0.08685535192489624,#7f7f7f
-2.6748712,4.021714,neighbor,436933,CSIFT based locality-constrained linear coding for image classification,0.08699405193328857,#7f7f7f
1.2725497,2.227188,neighbor,436933,Image Classification with the Fisher Vector: Theory and Practice,0.08701515197753906,#7f7f7f
-5.996658,-5.5471425,neighbor,436933,Complexity of Representation and Inference in Compositional Models with Part Sharing,0.08705508708953857,#7f7f7f
-4.5931497,7.283415,neighbor,436933,Visual object recognition using multi-scale local binary patterns and line segment feature,0.08721768856048584,#7f7f7f
7.141757,5.1726236,neighbor,436933,Feature combination with Multi-Kernel Learning for fine-grained visual classification,0.0872197151184082,#7f7f7f
1.2187921,-10.131873,neighbor,436933,Maxout Networks,0.08776330947875977,#7f7f7f
-6.551006,-2.7059584,neighbor,436933,Efficient Visual Coding: From Retina To V2,0.08810997009277344,#7f7f7f
2.525136,8.532782,neighbor,436933,Image Segmentation by Cascaded Region Agglomeration,0.0881466269493103,#7f7f7f
-5.7204933,8.5801935,neighbor,436933,Hardware Based Scale- and Rotation-Invariant Feature Extraction: A Retrospective Analysis and Future Directions,0.08857738971710205,#7f7f7f
-0.046577293,9.167365,neighbor,436933,PartBook for image parsing,0.08880990743637085,#7f7f7f
-4.5583606,10.922436,neighbor,436933,Rapid Online Analysis of Local Feature Detectors and Their Complementarity,0.08885592222213745,#7f7f7f
-9.774672,-0.4778866,neighbor,436933,Local Visual Energy Mechanisms Revealed by Detection of Global Patterns,0.08892911672592163,#7f7f7f
2.0786114,10.025726,neighbor,436933,Layered object detection for multi-class segmentation,0.08903676271438599,#7f7f7f
-3.8696585,-2.9697537,neighbor,436933,Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning?,0.08915156126022339,#7f7f7f
4.8941035,15.374206,neighbor,436933,Pedestrian Detection with Unsupervised Multi-stage Feature Learning,0.08940935134887695,#7f7f7f
1.4557962,15.799091,neighbor,436933,Training a multi-exit cascade with linear asymmetric classification for efficient object detection,0.08945441246032715,#7f7f7f
-5.4612403,12.259717,neighbor,436933,Experiences Using SciPy for Computer Vision Research,0.0895158052444458,#7f7f7f
-8.027378,2.6139975,neighbor,436933,Biologically Inspired Hierarchical Model for Feature Extraction and Localization,0.08984535932540894,#7f7f7f
5.8966265,6.8656526,neighbor,436933,Learning Discriminative Hierarchical Features for Object Recognition,0.09001851081848145,#7f7f7f
-4.4475613,-7.637612,neighbor,436933,Replicating Kernels with a Short Stride Allows Sparse Reconstructions with Fewer Independent Kernels,0.09006762504577637,#7f7f7f
-4.290229,-9.37356,neighbor,436933,"Image denoising with multi-layer perceptrons, part 1: comparison with existing algorithms and with bounds",0.0900951623916626,#7f7f7f
8.2546215,4.9792957,neighbor,436933,Mercer kernels for object recognition with local features,0.09010368585586548,#7f7f7f
-5.958938,-9.5288725,neighbor,436933,Improving Multiscale Recurrent Pattern Image Coding With Deblocking Filtering,0.0901450514793396,#7f7f7f
4.8267307,-13.403978,neighbor,436933,Scalable stacking and learning for building deep architectures,0.09021931886672974,#7f7f7f
2.8122947,0.24110973,neighbor,436933,Sparse Spatial Coding: A Novel Approach to Visual Recognition,0.09024357795715332,#7f7f7f
-0.45918795,13.302456,neighbor,436933,BING: Binarized normed gradients for objectness estimation at 300fps,0.0903477668762207,#7f7f7f
-2.3706985,6.3184075,neighbor,436933,"A Boosting, Sparsity- Constrained Bilinear Model for Object Recognition",0.09039610624313354,#7f7f7f
-7.7858295,1.097477,neighbor,436933,What are the Visual Features Underlying Rapid Object Recognition?,0.0904470682144165,#7f7f7f
6.687,-5.4552617,neighbor,436933,Interpolating destin features for image classification,0.09066683053970337,#7f7f7f
1.4306082,-3.9930277,neighbor,436933,Learning Invariant Representations with Local Transformations,0.09067010879516602,#7f7f7f
-5.450665,9.219701,neighbor,436933,FREAK: Fast Retina Keypoint,0.0907774567604065,#7f7f7f
2.7158155,5.6987424,query,4650265,XGBoost: A Scalable Tree Boosting System,0.0,#7f7f7f
8.276563,-5.790071,neighbor,4650265,A Boosting Framework on Grounds of Online Learning,0.054380953311920166,#7f7f7f
-4.3480005,-2.4822106,neighbor,4650265,A Direct Approach to Multi-class Boosting and Extensions,0.05440276861190796,#7f7f7f
3.6429842,9.663754,neighbor,4650265,Statistically adaptive learning for a general class of cost functions (SA L-BFGS),0.055625975131988525,#7f7f7f
-0.5735264,0.647892,neighbor,4650265,Theoretical and Empirical Analysis of a Parallel Boosting Algorithm,0.056166231632232666,#7f7f7f
-3.9361544,-0.19929667,neighbor,4650265,ABC-boost: adaptive base class boost for multi-class classification,0.05692911148071289,#7f7f7f
-0.34066817,-4.899528,neighbor,4650265,Boosting as a Product of Experts,0.05700176954269409,#7f7f7f
-3.5706375,-6.5000973,neighbor,4650265,Functional Frank-Wolfe Boosting for General Loss Functions,0.05732184648513794,#7f7f7f
-4.5663524,-2.979516,neighbor,4650265,Fast Training of Effective Multi-class Boosting Using Coordinate Descent Optimization,0.06066066026687622,#7f7f7f
7.8172007,-6.2905393,neighbor,4650265,Communication Efficient Distributed Agnostic Boosting,0.061117589473724365,#7f7f7f
-4.171986,-0.10691178,neighbor,4650265,Fast ABC-Boost for Multi-Class Classification,0.06211555004119873,#7f7f7f
0.13347404,-3.7571573,neighbor,4650265,SelfieBoost: A Boosting Algorithm for Deep Learning,0.0637388825416565,#7f7f7f
-3.2415783,-5.4632397,neighbor,4650265,Re-scale boosting for regression and classification,0.06432592868804932,#7f7f7f
9.137157,-5.659292,neighbor,4650265,Optimal and Adaptive Algorithms for Online Boosting,0.06512254476547241,#7f7f7f
2.0335453,-4.646148,neighbor,4650265,Obtaining Calibrated Probabilities from Boosting,0.06554019451141357,#7f7f7f
5.4043713,-9.285753,neighbor,4650265,StructBoost: Boosting Methods for Predicting Structured Output Variables,0.06626802682876587,#7f7f7f
-1.8359354,-2.005693,neighbor,4650265,Online coordinate boosting,0.0664597749710083,#7f7f7f
-0.5349018,-5.8664846,neighbor,4650265,"A Theory of Probabilistic Boosting, Decision Trees and Matryoshki",0.06650394201278687,#7f7f7f
9.587138,4.7229586,neighbor,4650265,DART: Dropouts meet Multiple Additive Regression Trees,0.06657344102859497,#7f7f7f
-3.8182926,-7.206494,neighbor,4650265,Generalized Boosting Algorithms for Convex Optimization,0.06759542226791382,#7f7f7f
8.966774,0.19425471,neighbor,4650265,Feature-Budgeted Random Forest,0.06837654113769531,#7f7f7f
-4.1308613,-8.576153,neighbor,4650265,Analysis of boosting algorithms using the smooth margin function,0.0684555172920227,#7f7f7f
-1.4018235,-7.9900784,neighbor,4650265,"Gradient boosting machines, a tutorial",0.06879371404647827,#7f7f7f
-1.8255273,-5.8097067,neighbor,4650265,A theory of multiclass boosting,0.06957584619522095,#7f7f7f
-0.47826108,-7.978315,neighbor,4650265,The Evolution of Boosting Algorithms,0.07012474536895752,#7f7f7f
-0.2532149,-0.7530476,neighbor,4650265,Empirical Comparisons of Online Boosting Algorithms,0.07015323638916016,#7f7f7f
3.974779,10.449896,neighbor,4650265,Accelerated Parallel Optimization Methods for Large Scale Machine Learning,0.07065242528915405,#7f7f7f
6.0499372,2.509892,neighbor,4650265,Scalable Semi-Supervised Classifier Aggregation,0.07085937261581421,#7f7f7f
0.16984287,2.7105222,neighbor,4650265,Classification with boosting of extreme learning machine over arbitrarily partitioned data,0.07099467515945435,#7f7f7f
-1.0686474,-9.51722,neighbor,4650265,"Comment: Boosting Algorithms: Regularization, Prediction and Model Fitting",0.07154607772827148,#7f7f7f
-6.3395534,0.5234416,neighbor,4650265,MIS-Boost: Multiple Instance Selection Boosting,0.07159006595611572,#7f7f7f
-0.21883394,-0.84470034,neighbor,4650265,Empirical Comparisons of Online Boosting Algorithms with Running Time,0.07190394401550293,#7f7f7f
2.456264,-11.005247,neighbor,4650265,"BROOF: Exploiting Out-of-Bag Errors, Boosting and Random Forests for Effective Automated Classification",0.07192867994308472,#7f7f7f
-6.0318713,-7.220791,neighbor,4650265,Learning Nonlinear Functions Using Regularized Greedy Forest,0.07207965850830078,#7f7f7f
5.571485,6.4957743,neighbor,4650265,"Single-pass online learning: performance, voting schemes and online feature selection",0.07287979125976562,#7f7f7f
8.282513,2.7023785,neighbor,4650265,Evasion and Hardening of Tree Ensemble Classifiers,0.07373827695846558,#7f7f7f
0.6246794,1.7570636,neighbor,4650265,Online Bagging and Boosting for Imbalanced Data Streams,0.07377731800079346,#7f7f7f
-2.836373,-9.343581,neighbor,4650265,Boosting with early stopping: Convergence and consistency,0.0745002031326294,#7f7f7f
-5.539896,-4.702804,neighbor,4650265,Margin Distribution Controlled Boosting,0.07486283779144287,#7f7f7f
9.009098,-0.92149013,neighbor,4650265,Random Composite Forests,0.07490187883377075,#7f7f7f
-1.3608781,-3.9265537,neighbor,4650265,Efficient Learning of Ensembles with QuadBoost,0.07534962892532349,#7f7f7f
-2.5468264,-8.220347,neighbor,4650265,"Margins, Shrinkage, and Boosting",0.07547026872634888,#7f7f7f
4.9015718,10.431981,neighbor,4650265,A reliable effective terascale linear learning system,0.0756373405456543,#7f7f7f
-3.1916993,-3.7080593,neighbor,4650265,RandomBoost: Simplified Multiclass Boosting Through Randomization,0.07582950592041016,#7f7f7f
-8.319063,-3.5015914,neighbor,4650265,Large Scale Classification with Support Vector Machine Algorithms,0.07589089870452881,#7f7f7f
0.14548841,-7.4921355,neighbor,4650265,ada: An R Package for Stochastic Boosting,0.07624328136444092,#7f7f7f
-5.286168,-4.3369617,neighbor,4650265,Boosting Through Optimization of Margin Distributions,0.07688283920288086,#7f7f7f
-3.8457217,0.54595006,neighbor,4650265,ABC-LogitBoost for Multi-class Classification,0.07712012529373169,#7f7f7f
4.1624136,4.21144,neighbor,4650265,COMET: A Recipe for Learning and Using Large Ensembles on Massive Data,0.0774083137512207,#7f7f7f
6.0727844,2.755313,neighbor,4650265,A Framework for Scalable Cost-sensitive Learning Based on Combing Probabilities and Benefits,0.07815718650817871,#7f7f7f
1.9706029,0.0028540113,neighbor,4650265,A Bayesian Approach for Online Classifier Ensemble,0.0786857008934021,#7f7f7f
-1.0181189,8.525467,neighbor,4650265,Learning from Imbalanced Data in Relational Domains: A Soft Margin Approach,0.07869738340377808,#7f7f7f
-0.7197709,7.9742827,neighbor,4650265,Fast learning of relational dependency networks,0.07907533645629883,#7f7f7f
-6.8130674,-2.3142073,neighbor,4650265,MKBoost: A Framework of Multiple Kernel Boosting,0.07926887273788452,#7f7f7f
0.6718628,-5.364741,neighbor,4650265,Advances in Boosting,0.07932025194168091,#7f7f7f
0.98056346,7.440206,neighbor,4650265,Deep Broad Learning - Big Models for Big Data,0.07958805561065674,#7f7f7f
9.584804,4.282489,neighbor,4650265,Tree-Based Ensemble Multi-Task Learning Method for Classification and Regression,0.07993412017822266,#7f7f7f
9.114291,9.831175,neighbor,4650265,SOLAR: Scalable Online Learning Algorithms for Ranking,0.08027398586273193,#7f7f7f
3.5325663,0.6445888,neighbor,4650265,Bayesian Model Averaging Naive Bayes (BMA-NB): Averaging over an Exponential Number of Feature Models in Linear Time,0.08093607425689697,#7f7f7f
10.490646,9.629623,neighbor,4650265,A general magnitude-preserving boosting algorithm for search ranking,0.0812796950340271,#7f7f7f
14.251716,2.1636233,neighbor,4650265,Random Forests Can Hash,0.08169376850128174,#7f7f7f
4.4014683,-4.5016828,neighbor,4650265,Efficient Algorithms for Decision Tree Cross-validation,0.08177244663238525,#7f7f7f
10.784466,-0.45463434,neighbor,4650265,Learning Ensembles of Cutset Networks,0.0818830132484436,#7f7f7f
-4.7091045,3.075351,neighbor,4650265,Detection with multi-exit asymmetric boosting,0.0819242000579834,#7f7f7f
3.182844,7.890435,neighbor,4650265,Active Sampler: Light-weight Accelerator for Complex Data Analytics at Scale,0.08196026086807251,#7f7f7f
3.6488981,7.137768,neighbor,4650265,Dynamic trees for streaming and massive data contexts,0.08211928606033325,#7f7f7f
6.576025,6.673714,neighbor,4650265,Online Classification Using a Voted RDA Method,0.08219563961029053,#7f7f7f
2.64083,10.35847,neighbor,4650265,"Finito: A faster, permutable incremental gradient method for big data problems",0.08223927021026611,#7f7f7f
1.8479381,-6.7576456,neighbor,4650265,Adabook and Multibook - Adaptive Boosting with Chance Correction,0.08249324560165405,#7f7f7f
3.3978658,-4.1659336,neighbor,4650265,An empirical comparison of supervised learning algorithms,0.08281850814819336,#7f7f7f
-3.6500983,4.576435,neighbor,4650265,Fast classification using sparse decision DAGs,0.0828392505645752,#7f7f7f
7.442251,6.827294,neighbor,4650265,Online Importance Weight Aware Updates,0.08318603038787842,#7f7f7f
2.5753386,-11.575971,neighbor,4650265,Boosting to correct inductive bias in text classification,0.08347547054290771,#7f7f7f
9.13009,0.78149843,neighbor,4650265,Optimally Pruning Decision Tree Ensembles With Feature Cost,0.08370661735534668,#7f7f7f
-0.57420033,9.867115,neighbor,4650265,Optimizing Non-decomposable Performance Measures: A Tale of Two Classes,0.08381855487823486,#7f7f7f
0.8146005,-7.4055204,neighbor,4650265,adabag: An R Package for Classification with Boosting and Bagging,0.08390188217163086,#7f7f7f
10.242847,-1.9191355,neighbor,4650265,Conditional Probability Tree Estimation Analysis and Algorithms,0.08394235372543335,#7f7f7f
-5.197136,-9.263752,neighbor,4650265,LPBoost with Strong Classifiers,0.08399784564971924,#7f7f7f
4.6961102,11.793246,neighbor,4650265,Using More Data to Speed-up Training Time,0.08434081077575684,#7f7f7f
-7.464765,-7.690333,neighbor,4650265,Feature selection via regularized trees,0.08434325456619263,#7f7f7f
0.38315263,-10.17234,neighbor,4650265,BART: Bayesian Additive Regression Trees,0.084719717502594,#7f7f7f
1.4720203,-8.321338,neighbor,4650265,Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers,0.0847436785697937,#7f7f7f
6.269492,10.26346,neighbor,4650265,Machine Learning at Scale,0.08492279052734375,#7f7f7f
-1.0520718,-12.342005,neighbor,4650265,gBoost: a mathematical programming approach to graph classification and regression,0.0849342942237854,#7f7f7f
14.311177,2.2200143,neighbor,4650265,Fast Supervised Hashing with Decision Trees for High-Dimensional Data,0.08498471975326538,#7f7f7f
-4.9976487,2.762044,neighbor,4650265,Online Learning Asymmetric Boosted Classifiers for Object Detection,0.08501428365707397,#7f7f7f
3.3759007,-9.323903,neighbor,4650265,Alternating Decision Forests,0.08501821756362915,#7f7f7f
-2.8814356,4.0123634,neighbor,4650265,A Greedy Approach for Building Classification Cascades,0.08502018451690674,#7f7f7f
2.5243165,4.0525765,neighbor,4650265,The Big Data Bootstrap,0.08510738611221313,#7f7f7f
-9.683411,1.0470402,neighbor,4650265,Online Boosting Algorithms for Anytime Transfer and Multitask Learning,0.08521580696105957,#7f7f7f
3.3237634,1.8503728,neighbor,4650265,Budgeted Learning of Naive-Bayes Classifiers,0.08546656370162964,#7f7f7f
6.35966,-2.4491022,neighbor,4650265,Canonical Correlation Forests,0.08559143543243408,#7f7f7f
4.6157928,-2.3196106,neighbor,4650265,Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions,0.08570778369903564,#7f7f7f
5.4482884,0.4979814,neighbor,4650265,A study of selective neighborhood-based naive Bayes for efficient lazy learning,0.08585387468338013,#7f7f7f
-3.770385,-11.236713,neighbor,4650265,$L_2$ boosting in kernel regression,0.08603733777999878,#7f7f7f
-9.564751,1.0367894,neighbor,4650265,Selective Transfer Between Learning Tasks Using Task-Based Boosting,0.08607101440429688,#7f7f7f
2.295167,12.086293,neighbor,4650265,Feature Selection with Annealing for Computer Vision and Big Data Learning,0.08611011505126953,#7f7f7f
4.0394607,-7.06184,neighbor,4650265,On Boosting Sparse Parities,0.08613049983978271,#7f7f7f
6.069139,-3.6804557,neighbor,4650265,Cross-conformal predictors,0.08616042137145996,#7f7f7f
5.71361,0.3411973,neighbor,4650265,Batched Lazy Decision Trees,0.08645159006118774,#7f7f7f
-6.9866695,-2.5802438,neighbor,4650265,From Kernel Machines to Ensemble Learning,0.08648300170898438,#7f7f7f
10.533756,5.651293,query,46701966,The Elements of Statistical Learning,0.0,#c7c7c7
-0.2424733,2.87872,neighbor,46701966,Classification,0.05942046642303467,#c7c7c7
10.795521,6.2646055,neighbor,46701966,Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author),0.06442153453826904,#c7c7c7
12.702586,5.3148346,neighbor,46701966,The mysterious optimality of Naive Bayes: Estimation of the probability in the system of “classifiers”,0.07227027416229248,#c7c7c7
-5.875658,-5.6621804,neighbor,46701966,Sampling,0.07239258289337158,#c7c7c7
10.721763,4.420285,neighbor,46701966,Selection Criterion for Log-Linear Models Using Statistical Learning Theory,0.07505100965499878,#c7c7c7
-8.473611,-5.339984,neighbor,46701966,An Introduction,0.0775112509727478,#c7c7c7
-8.133559,-4.329761,neighbor,46701966,Introduction,0.08084404468536377,#c7c7c7
-3.51282,-4.78857,neighbor,46701966,Preface,0.08108353614807129,#c7c7c7
-0.79473084,-4.553257,neighbor,46701966,Acknowledgements,0.08121323585510254,#c7c7c7
8.414523,4.6562905,neighbor,46701966,Statistical mechanics of learning: a variational approach for real data.,0.0812179446220398,#c7c7c7
-5.564479,0.14700525,neighbor,46701966,Preface,0.08183431625366211,#c7c7c7
-2.3216944,-7.9444795,neighbor,46701966,Reviewers,0.08251583576202393,#c7c7c7
-5.753106,3.2203674,neighbor,46701966,Preface,0.0825505256652832,#c7c7c7
14.773127,3.6835005,neighbor,46701966,Iterative Bayes,0.08415424823760986,#c7c7c7
-4.9684467,-4.331274,neighbor,46701966,In Memoriam,0.08426624536514282,#c7c7c7
-6.3289204,-2.9992309,neighbor,46701966,Preliminary Material,0.08430701494216919,#c7c7c7
-5.031884,0.85687166,neighbor,46701966,Preface,0.08467632532119751,#c7c7c7
8.659475,5.5444984,neighbor,46701966,Information theory and learning: a physical approach,0.0847177505493164,#c7c7c7
4.4509273,3.7361495,neighbor,46701966,"The Mind within the Net: Models of Learning, Thinking and Acting",0.08478569984436035,#c7c7c7
4.288795,-4.61335,neighbor,46701966,Dealing with uncertainty,0.08481526374816895,#c7c7c7
3.0366557,7.554507,neighbor,46701966,10.1162/153244303765208412,0.086528480052948,#c7c7c7
-3.8060057,-8.611236,neighbor,46701966,Reviewers,0.08668363094329834,#c7c7c7
4.849861,4.248088,neighbor,46701966,Statistical Physics of Feedforward Neural Networks,0.08669674396514893,#c7c7c7
10.217027,3.362128,neighbor,46701966,On Information Regularization,0.0870736837387085,#c7c7c7
13.447904,7.5787086,neighbor,46701966,The Bayes Linear Programming Language [B/D],0.08707571029663086,#c7c7c7
5.753038,-7.4673424,neighbor,46701966,Combining the Performance Strengths of the Logistic Regression and Neural Network Models: A Medical Outcomes Approach,0.08715975284576416,#c7c7c7
15.469073,3.3834152,neighbor,46701966,"Comparison of lazy Bayesian rule, and tree-augmented Bayesian learning",0.08734381198883057,#c7c7c7
-5.5373125,-9.95547,neighbor,46701966,Call for Book Reviewers,0.08747881650924683,#c7c7c7
12.998534,0.5183984,neighbor,46701966,Types of Cost in Inductive Concept Learning,0.08779925107955933,#c7c7c7
-5.6240063,-2.0931146,neighbor,46701966,Preface,0.08791065216064453,#c7c7c7
-6.3250093,1.4760854,neighbor,46701966,Preface,0.08818334341049194,#c7c7c7
16.254257,6.7174287,neighbor,46701966,Robust Feature Selection by Mutual Information Distributions,0.08826857805252075,#c7c7c7
9.2832985,6.884651,neighbor,46701966,Algorithmic statistics,0.08854252099990845,#c7c7c7
-2.6914907,-6.457377,neighbor,46701966,Committees,0.08882415294647217,#c7c7c7
-0.7101144,-7.816874,neighbor,46701966,Content,0.08913439512252808,#c7c7c7
4.886061,-4.5848393,neighbor,46701966,Analysing Sensitivity Data from Probabilistic Networks,0.08922284841537476,#c7c7c7
-1.7613994,-7.086279,neighbor,46701966,Sponsors,0.08923816680908203,#c7c7c7
-2.1881652,-8.504848,neighbor,46701966,Reviewers,0.08925706148147583,#c7c7c7
-2.372607,-10.448511,neighbor,46701966,Quasi-variances in Xlisp-Stat and on the web,0.08946919441223145,#c7c7c7
13.93138,7.3479223,neighbor,46701966,Bayesian Analysis,0.08959656953811646,#c7c7c7
-6.8467884,-0.9294072,neighbor,46701966,Preface,0.0897594690322876,#c7c7c7
11.910933,7.8150206,neighbor,46701966,Learning Measurement Models for Unobserved Variables,0.08981609344482422,#c7c7c7
1.9120625,-9.098595,neighbor,46701966,INTRODUCTION BY GUEST EDITORS,0.08985555171966553,#c7c7c7
-2.4895496,-5.71626,neighbor,46701966,Committees,0.08996748924255371,#c7c7c7
-3.2070549,-2.999716,neighbor,46701966,Table of Contents,0.0900680422782898,#c7c7c7
-2.2528863,-4.352901,neighbor,46701966,List of participants,0.09023374319076538,#c7c7c7
0.69241,2.640043,neighbor,46701966,Extension of fixed point clustering: A cluster criterion,0.09035420417785645,#c7c7c7
-7.520369,-8.49005,neighbor,46701966,Book Review,0.09048283100128174,#c7c7c7
-4.5547676,2.0014915,neighbor,46701966,Preface,0.0905269980430603,#c7c7c7
-1.3847789,3.536388,neighbor,46701966,Foundations of Wavelet Networks and Applications,0.09062033891677856,#c7c7c7
-6.466974,-0.17385802,neighbor,46701966,Preface,0.09069186449050903,#c7c7c7
12.054502,2.7621047,neighbor,46701966,Exploiting unlabeled data in ensemble methods,0.09072285890579224,#c7c7c7
-1.9552557,-6.0039597,neighbor,46701966,Program Committee,0.09076642990112305,#c7c7c7
11.020696,1.0504781,neighbor,46701966,Robust Classification for Imprecise Environments,0.09084510803222656,#c7c7c7
-2.5150042,-2.3213496,neighbor,46701966,Statistics and Measurements,0.09122425317764282,#c7c7c7
-8.60031,-1.5780417,neighbor,46701966,Preface,0.09123903512954712,#c7c7c7
13.191031,-0.6513227,neighbor,46701966,Learning When Training Data are Costly: The Effect of Class Distribution on Tree Induction,0.0913742184638977,#c7c7c7
3.5649567,-2.842383,neighbor,46701966,Risk assessment: ‘numbers’ and ‘values’,0.09163433313369751,#c7c7c7
10.210656,8.407913,neighbor,46701966,Extracting hidden information from knowledge networks.,0.09183824062347412,#c7c7c7
16.231058,4.3867135,neighbor,46701966,Evaluation of the Performance of the Markov Blanket Bayesian Classifier Algorithm,0.09191673994064331,#c7c7c7
6.7151885,6.6868014,neighbor,46701966,Statistical mechanics of learning with soft margin classifiers.,0.09193569421768188,#c7c7c7
-0.5540883,-9.388083,neighbor,46701966,Erratum,0.09209173917770386,#c7c7c7
-7.5595584,-8.761149,neighbor,46701966,Book Review,0.09233301877975464,#c7c7c7
12.635383,-0.41591266,neighbor,46701966,A Model of Inductive Bias Learning,0.09237295389175415,#c7c7c7
1.6618471,0.30619013,neighbor,46701966,Fuzzy Learning and Applications,0.0923890471458435,#c7c7c7
-2.0201762,1.4205747,neighbor,46701966,Why maximum entropy? A non-axiomatic approach,0.09245920181274414,#c7c7c7
-8.106302,-0.40742028,neighbor,46701966,Preface,0.09251558780670166,#c7c7c7
-6.2112765,-7.72819,neighbor,46701966,Editorial,0.09289735555648804,#c7c7c7
3.103395,-0.6714987,neighbor,46701966,What Is Fuzzy Probability Theory?,0.09293782711029053,#c7c7c7
10.305807,2.1643116,neighbor,46701966,An Uncertainty Framework for Classification,0.0929674506187439,#c7c7c7
13.5631075,1.4432079,neighbor,46701966,Efficient Algorithms for Decision Tree Cross-validation,0.09312742948532104,#c7c7c7
3.106735,-0.93971527,neighbor,46701966,"Interval-valued statistics, fuzzy logic, and their use in computational semantics",0.09327846765518188,#c7c7c7
-4.736102,-6.803157,neighbor,46701966,About the Authors,0.09355294704437256,#c7c7c7
15.664354,4.7843437,neighbor,46701966,A New Bayesian Tree Learning Method with Reduced Time and Space Complexity,0.09358841180801392,#c7c7c7
-4.157673,1.0216069,neighbor,46701966,Preface,0.09368014335632324,#c7c7c7
0.41376105,5.2726755,neighbor,46701966,Symbolic Methodology in Numeric Data Mining: Relational Techniques for Financial Applications,0.09385323524475098,#c7c7c7
-8.901799,-6.065169,neighbor,46701966,Introduction,0.0938568115234375,#c7c7c7
0.92293775,-6.430958,neighbor,46701966,Subject index,0.09393894672393799,#c7c7c7
0.92181724,-6.431785,neighbor,46701966,Subject Index,0.09393894672393799,#c7c7c7
7.5691304,-1.580528,neighbor,46701966,Inductive logic: from data analysis to experimental design,0.09415578842163086,#c7c7c7
6.516769,6.8319626,neighbor,46701966,Learning curves for Soft Margin Classifiers,0.09420168399810791,#c7c7c7
-0.76839024,-9.682942,neighbor,46701966,Erratum,0.0942983627319336,#c7c7c7
14.517482,4.863925,neighbor,46701966,Classifier Learning with Supervised Marginal Likelihood,0.09430873394012451,#c7c7c7
-1.0480292,0.6783484,neighbor,46701966,Mathematics in independent component analysis,0.09458327293395996,#c7c7c7
-4.4300814,-0.24589016,neighbor,46701966,Preface,0.0946187973022461,#c7c7c7
-7.640604,0.13681285,neighbor,46701966,Preface,0.0946805477142334,#c7c7c7
14.823752,2.8493075,neighbor,46701966,Locally Weighted Naive Bayes,0.0947306752204895,#c7c7c7
-8.954894,1.1128064,neighbor,46701966,Preface,0.09476137161254883,#c7c7c7
-0.33166888,-11.32297,neighbor,46701966,Da sobrevivência do analista,0.09495621919631958,#c7c7c7
-4.2943373,-11.20916,neighbor,46701966,Editor's note,0.0950160026550293,#c7c7c7
6.2382345,5.4575086,neighbor,46701966,On-line learning through simple perceptron with a margin,0.09514468908309937,#c7c7c7
3.2981591,3.1332757,neighbor,46701966,Confusion,0.09541565179824829,#c7c7c7
-7.419715,1.8426803,neighbor,46701966,Preface,0.095431387424469,#c7c7c7
-7.5443873,-1.5956146,neighbor,46701966,Preface,0.09562993049621582,#c7c7c7
6.172288,8.122514,neighbor,46701966,Hierarchical Learning in Polynomial Support Vector Machines,0.09591931104660034,#c7c7c7
-10.090526,-3.1347773,neighbor,46701966,New Products,0.0959286093711853,#c7c7c7
17.363365,4.6288934,neighbor,46701966,On Local Optima in Learning Bayesian Networks,0.09617030620574951,#c7c7c7
14.833581,0.8757329,neighbor,46701966,Model selection via meta-learning: a comparative study,0.09625053405761719,#c7c7c7
6.137082,3.870838,neighbor,46701966,Learning structured data from unspecific reinforcement,0.09626448154449463,#c7c7c7
16.839521,6.0029497,neighbor,46701966,A Distance-Based Branch and Bound Feature Selection Algorithm,0.0964011549949646,#c7c7c7
-3.9076443,7.8954782,query,502946,The Cityscapes Dataset for Semantic Urban Scene Understanding,0.0,#bcbd22
-4.1041985,9.011692,neighbor,502946,Layered Interpretation of Street View Images,0.04736453294754028,#bcbd22
-3.0441325,7.596339,neighbor,502946,Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer,0.049002647399902344,#bcbd22
-7.680565,1.5495673,neighbor,502946,Understand scene categories by objects: A semantic regularized scene classifier using Convolutional Neural Networks,0.054845988750457764,#bcbd22
-7.4715657,-0.28292418,neighbor,502946,Locally Supervised Deep Hybrid Model for Scene Recognition,0.05511707067489624,#bcbd22
-3.5325122,2.46281,neighbor,502946,Places205-VGGNet Models for Scene Recognition,0.055410563945770264,#bcbd22
7.175092,2.358031,neighbor,502946,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,0.05679386854171753,#bcbd22
-8.358745,6.06157,neighbor,502946,Understanding RealWorld Indoor Scenes with Synthetic Data,0.05940866470336914,#bcbd22
-5.847363,0.6895336,neighbor,502946,Object Detectors Emerge in Deep Scene CNNs,0.06033933162689209,#bcbd22
-6.03157,5.959386,neighbor,502946,Learning Rich Features from RGB-D Images for Object Detection and Segmentation,0.06160378456115723,#bcbd22
-7.315092,-1.77094,neighbor,502946,Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification,0.06211429834365845,#bcbd22
1.2628725,6.7493305,neighbor,502946,Scene Segmentation with Low-Dimensional Semantic Representations and Conditional Random Fields,0.06212979555130005,#bcbd22
-12.760236,0.9808383,neighbor,502946,SUN database: Large-scale scene recognition from abbey to zoo,0.06341809034347534,#bcbd22
-9.125399,5.921078,neighbor,502946,SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes,0.06395179033279419,#bcbd22
3.2613811,-5.087474,neighbor,502946,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.06486958265304565,#bcbd22
6.099504,2.5275452,neighbor,502946,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling,0.06596648693084717,#bcbd22
6.198074,1.0702683,neighbor,502946,Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding,0.06619155406951904,#bcbd22
0.19604224,-10.139343,neighbor,502946,Data-driven 3D Voxel Patterns for object category recognition,0.06660854816436768,#bcbd22
8.678143,2.3636067,neighbor,502946,Fully convolutional networks for semantic segmentation,0.06722879409790039,#bcbd22
0.5005317,4.3169727,neighbor,502946,Context Driven Scene Parsing with Attention to Rare Classes,0.06789422035217285,#bcbd22
-4.2040296,11.185928,neighbor,502946,Visual Object Recognition with 3D-Aware Features in KITTI Urban Scenes,0.06827646493911743,#bcbd22
3.8079333,-3.1058264,neighbor,502946,Pedestrian detection aided by deep learning semantic tasks,0.06868356466293335,#bcbd22
-5.7269163,5.4343705,neighbor,502946,Constrained Parametric Proposals and Pooling Methods for Semantic Segmentation in RGB-D Images,0.0695420503616333,#bcbd22
-0.66832525,11.381143,neighbor,502946,Sensor fusion for semantic segmentation of urban scenes,0.07006388902664185,#bcbd22
0.53377175,3.395012,neighbor,502946,Scene Parsing With Integration of Parametric and Non-Parametric Models,0.07006889581680298,#bcbd22
-6.2146893,-0.756845,neighbor,502946,Scene understanding based on Multi-Scale Pooling of deep learning features,0.07035577297210693,#bcbd22
-0.08855866,-11.393143,neighbor,502946,Multi-View Priors for Learning Detectors from Sparse Viewpoint Data,0.07072818279266357,#bcbd22
-6.7765403,4.974685,neighbor,502946,Indoor Semantic Segmentation using depth information,0.0708014965057373,#bcbd22
0.9070677,1.6990238,neighbor,502946,Deep Deconvolutional Networks for Scene Parsing,0.0708397626876831,#bcbd22
-4.422896,-3.1290257,neighbor,502946,Unsupervised Visual Representation Learning by Context Prediction,0.07100141048431396,#bcbd22
2.0358958,1.4040357,neighbor,502946,Deep hierarchical parsing for semantic segmentation,0.07107460498809814,#bcbd22
-0.2589448,7.146189,neighbor,502946,Geometric Context from Videos,0.07139140367507935,#bcbd22
7.8393598,-3.1335156,neighbor,502946,Semantic Amodal Segmentation,0.07156896591186523,#bcbd22
-8.524152,-1.7858695,neighbor,502946,A Discriminative Representation of Convolutional Features for Indoor Scene Recognition,0.0715794563293457,#bcbd22
-12.943698,-1.5126165,neighbor,502946,Visual descriptors for scene categorization: experimental evaluation,0.07165521383285522,#bcbd22
-13.3565235,1.3926041,neighbor,502946,SUN Database: Exploring a Large Collection of Scene Categories,0.0716865062713623,#bcbd22
3.3422663,6.8782253,neighbor,502946,Data-Driven Scene Understanding with Adaptively Retrieved Exemplars,0.07200217247009277,#bcbd22
-12.31615,1.9768164,neighbor,502946,"Basic level scene understanding: categories, attributes and structures",0.07221448421478271,#bcbd22
9.358516,1.0365151,neighbor,502946,Learning Dense Convolutional Embeddings for Semantic Segmentation,0.0723273754119873,#bcbd22
-12.4677305,-4.724524,neighbor,502946,NetVLAD: CNN Architecture for Weakly Supervised Place Recognition,0.07246828079223633,#bcbd22
-1.4424503,0.43173137,neighbor,502946,DAG-Recurrent Neural Networks for Scene Labeling,0.0724862813949585,#bcbd22
12.092302,3.4468555,neighbor,502946,What's the Point: Semantic Segmentation with Point Supervision,0.07261437177658081,#bcbd22
-8.880781,0.5131843,neighbor,502946,Multi-Object Classification and Unsupervised Scene Understanding Using Deep Learning Features and Latent Tree Probabilistic Models,0.07273358106613159,#bcbd22
4.3726044,7.3683066,neighbor,502946,A Novel Knowledge-Compatibility Benchmarker for Semantic Segmentation,0.07283270359039307,#bcbd22
1.5539968,4.4301596,neighbor,502946,Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering,0.07295596599578857,#bcbd22
-12.772835,-6.5710297,neighbor,502946,Predicting and Understanding Urban Perception with Convolutional Neural Networks,0.07298964262008667,#bcbd22
0.4986305,1.3088541,neighbor,502946,Recurrent Convolutional Neural Networks for Scene Parsing,0.07320380210876465,#bcbd22
4.789434,-9.01414,neighbor,502946,DOC: Deep OCclusion Estimation from a Single Image,0.07342123985290527,#bcbd22
-4.231131,-2.7525468,neighbor,502946,Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles,0.07342326641082764,#bcbd22
-1.2606989,-6.324053,neighbor,502946,ImageNet Large Scale Visual Recognition Challenge,0.07343333959579468,#bcbd22
12.661595,-0.17645532,neighbor,502946,STC: A Simple to Complex Framework for Weakly-Supervised Semantic Segmentation,0.07380908727645874,#bcbd22
-0.92380273,11.187245,neighbor,502946,Fusion Based Holistic Road Scene Understanding,0.07422548532485962,#bcbd22
9.387587,5.5483127,neighbor,502946,Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform,0.07428097724914551,#bcbd22
9.260281,3.4421937,neighbor,502946,Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation,0.07435119152069092,#bcbd22
-12.453826,2.4917257,neighbor,502946,Basic level scene understanding: from labels to structure and beyond,0.07444322109222412,#bcbd22
-4.644793,-4.641955,neighbor,502946,Mine the fine: Fine-grained fragment discovery,0.07467490434646606,#bcbd22
0.37564588,-12.194939,neighbor,502946,Towards Scene Understanding with Detailed 3D Object Representations,0.07471376657485962,#bcbd22
7.915839,5.035398,neighbor,502946,Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation,0.07477962970733643,#bcbd22
-7.3407235,3.7677026,neighbor,502946,Unsupervised Joint Feature Learning and Encoding for RGB-D Scene Labeling,0.07594245672225952,#bcbd22
1.9460142,7.6647315,neighbor,502946,Dense Semantic Image Segmentation with Objects and Attributes,0.07613515853881836,#bcbd22
11.780753,1.8095907,neighbor,502946,Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation,0.07613760232925415,#bcbd22
10.657737,2.79004,neighbor,502946,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.07616245746612549,#bcbd22
8.30821,-4.50269,neighbor,502946,PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset,0.07670122385025024,#bcbd22
7.5198617,-4.2966447,neighbor,502946,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.07672244310379028,#bcbd22
1.0537107,-5.2862096,neighbor,502946,ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks,0.07714611291885376,#bcbd22
7.83659,1.2444307,neighbor,502946,Feedforward semantic segmentation with zoom-out features,0.07755035161972046,#bcbd22
-3.7285795,11.784755,neighbor,502946,Monocular vision based road marking recognition for driver assistance and safety,0.07764029502868652,#bcbd22
10.320088,4.4311066,neighbor,502946,ParseNet: Looking Wider to See Better,0.07782566547393799,#bcbd22
2.2110863,3.0225844,neighbor,502946,A Hierarchical and Contextual Model for Aerial Image Parsing,0.07797712087631226,#bcbd22
1.8080437,10.658407,neighbor,502946,Structured Output Prediction for Semantic Perception in Autonomous Vehicles,0.078244149684906,#bcbd22
2.0438538,-5.5167456,neighbor,502946,Scalable Object Detection Using Deep Neural Networks,0.07830643653869629,#bcbd22
7.415229,5.220127,neighbor,502946,Exploring Context with Deep Structured Models for Semantic Segmentation,0.07847386598587036,#bcbd22
-7.536573,-4.1080894,neighbor,502946,Fast Discovery of Discriminative Mid-level Patches,0.07869261503219604,#bcbd22
1.5505749,-6.816985,neighbor,502946,"You Only Look Once: Unified, Real-Time Object Detection",0.07874232530593872,#bcbd22
-14.128958,-4.827317,neighbor,502946,Geo-Distinctive Visual Element Matching for Location Estimation of Images,0.07879191637039185,#bcbd22
14.531064,1.1568358,neighbor,502946,Learning from Weak and Noisy Labels for Semantic Segmentation,0.07892113924026489,#bcbd22
13.287,2.18064,neighbor,502946,Weakly-Supervised Semantic Segmentation Using Motion Cues,0.07922643423080444,#bcbd22
12.589308,0.9922394,neighbor,502946,Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network,0.0794939398765564,#bcbd22
-0.97670674,-7.2847633,neighbor,502946,1-HKUST: Object Detection in ILSVRC 2014,0.07950174808502197,#bcbd22
10.773912,-0.19181627,neighbor,502946,Instance-Aware Semantic Segmentation via Multi-task Network Cascades,0.07968425750732422,#bcbd22
-1.1605433,-5.26345,neighbor,502946,The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition,0.07968634366989136,#bcbd22
-8.206689,-4.254404,neighbor,502946,Contour Detection-Based Discovery of Mid-Level Discriminative Patches for Scene Classification,0.07987087965011597,#bcbd22
3.0076118,-4.6541777,neighbor,502946,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.07995724678039551,#bcbd22
-10.4042015,5.193291,neighbor,502946,DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding,0.08016693592071533,#bcbd22
-13.119201,-3.8467548,neighbor,502946,Leveraging image based prior for visual place recognition,0.08037018775939941,#bcbd22
-12.954101,-2.483011,neighbor,502946,CENTRIST: A Visual Descriptor for Scene Categorization,0.08039486408233643,#bcbd22
-2.671345,-0.29929745,neighbor,502946,Better Exploiting OS-CNNs for Better Event Recognition in Images,0.08047163486480713,#bcbd22
8.428201,3.5988038,neighbor,502946,ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation,0.08049547672271729,#bcbd22
11.323108,5.4196196,neighbor,502946,Context Tricks for Cheap Semantic Segmentation,0.08049583435058594,#bcbd22
-3.7383773,-5.516946,neighbor,502946,Orientational Spatial Part Modeling for Fine-Grained Visual Categorization,0.08052259683609009,#bcbd22
-1.3393162,-10.907626,neighbor,502946,Learning Deep Object Detectors from 3D Models,0.0805862545967102,#bcbd22
9.910978,1.7961856,neighbor,502946,Segmentation-aware convolutional nets,0.08059054613113403,#bcbd22
1.0482194,-8.600786,neighbor,502946,Ensemble of exemplar-SVMs for object detection and beyond,0.0809674859046936,#bcbd22
-2.2884195,-8.104296,neighbor,502946,Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey,0.08105182647705078,#bcbd22
9.302714,8.857528,neighbor,502946,Pixel-wise Segmentation of Street with Neural Networks,0.08106416463851929,#bcbd22
2.690317,-6.565068,neighbor,502946,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.08107030391693115,#bcbd22
6.871444,6.7145576,neighbor,502946,Semantic video segmentation: Exploring inference efficiency,0.08117610216140747,#bcbd22
-5.7602158,8.728452,neighbor,502946,Camera Elevation Estimation from a Single Mountain Landscape Photograph,0.08120429515838623,#bcbd22
-9.275265,-2.3026335,neighbor,502946,A Spatial Layout and Scale Invariant Feature Representation for Indoor Scene Classification,0.08129900693893433,#bcbd22
-11.327094,-4.235012,neighbor,502946,Deep Convolutional Features for Image Based Retrieval and Scene Categorization,0.08132487535476685,#bcbd22
4.65824,-5.5069623,neighbor,502946,Hypercolumns for object segmentation and fine-grained localization,0.08144640922546387,#bcbd22
0.7415965,-4.103849,query,5299559,Pyramid Scene Parsing Network,0.0,#bcbd22
-0.36855322,-4.7881856,neighbor,5299559,Multi-Path Feedback Recurrent Neural Networks for Scene Parsing,0.03417879343032837,#bcbd22
0.8632586,-4.9396787,neighbor,5299559,Deep hierarchical parsing for semantic segmentation,0.038150012493133545,#bcbd22
2.5823212,-5.6648455,neighbor,5299559,Scene Parsing With Integration of Parametric and Non-Parametric Models,0.04009443521499634,#bcbd22
0.061722413,-5.651977,neighbor,5299559,Recurrent Convolutional Neural Networks for Scene Parsing,0.04414057731628418,#bcbd22
2.4124303,-3.5971663,neighbor,5299559,Video Scene Parsing with Predictive Feature Learning,0.04736793041229248,#bcbd22
-5.330705,-4.316365,neighbor,5299559,Multi-Level Contextual RNNs With Attention Model for Scene Labeling,0.048884689807891846,#bcbd22
0.39160383,-6.3590794,neighbor,5299559,Deep Deconvolutional Networks for Scene Parsing,0.05023527145385742,#bcbd22
1.3038093,-0.67289644,neighbor,5299559,Semantic Understanding of Scenes Through the ADE20K Dataset,0.050536394119262695,#bcbd22
5.146033,10.069017,neighbor,5299559,Improving Fully Convolution Network for Semantic Segmentation,0.05089247226715088,#bcbd22
4.016935,-6.621418,neighbor,5299559,Context Driven Scene Parsing with Attention to Rare Classes,0.05090862512588501,#bcbd22
-1.835336,-5.082183,neighbor,5299559,Semantic Object Parsing with Local-Global Long Short-Term Memory,0.0517885684967041,#bcbd22
3.8340814,7.414325,neighbor,5299559,ParseNet: Looking Wider to See Better,0.0524255633354187,#bcbd22
3.1665118,5.9322467,neighbor,5299559,Exploring Context with Deep Structured Models for Semantic Segmentation,0.05339515209197998,#bcbd22
-11.245586,-2.930502,neighbor,5299559,Scene understanding based on Multi-Scale Pooling of deep learning features,0.05413633584976196,#bcbd22
-0.06346993,-1.6430343,neighbor,5299559,Deep Structured Scene Parsing by Learning with Image Descriptions,0.05676549673080444,#bcbd22
3.0487657,6.193805,neighbor,5299559,Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation,0.057180047035217285,#bcbd22
3.4908361,8.5005,neighbor,5299559,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",0.05728495121002197,#bcbd22
1.618698,4.7204485,neighbor,5299559,Recalling Holistic Information for Semantic Segmentation,0.05734372138977051,#bcbd22
5.3606963,7.9086714,neighbor,5299559,Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation,0.057375550270080566,#bcbd22
-7.991025,-2.4948711,neighbor,5299559,Knowledge Guided Disambiguation for Large-Scale Scene Classification With Multi-Resolution CNNs,0.057429492473602295,#bcbd22
-1.6281606,-3.5841618,neighbor,5299559,Geometric Scene Parsing with Hierarchical LSTM,0.05799597501754761,#bcbd22
2.785182,10.014941,neighbor,5299559,Feedforward semantic segmentation with zoom-out features,0.058206260204315186,#bcbd22
-6.494024,-4.182669,neighbor,5299559,DAG-Recurrent Neural Networks for Scene Labeling,0.05825817584991455,#bcbd22
2.96611,-6.8811264,neighbor,5299559,Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering,0.058562278747558594,#bcbd22
2.5036414,-8.485171,neighbor,5299559,PartBook for image parsing,0.059141695499420166,#bcbd22
-8.370817,-7.5669603,neighbor,5299559,Two-Stream Contextualized CNN for Fine-Grained Image Classification,0.05982959270477295,#bcbd22
-9.222942,-1.6916751,neighbor,5299559,Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition,0.05999380350112915,#bcbd22
6.7347417,9.273481,neighbor,5299559,RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation,0.06022083759307861,#bcbd22
7.4888353,-9.768341,neighbor,5299559,A Local-Global Approach to Semantic Segmentation in Aerial Images,0.06042677164077759,#bcbd22
-5.0953245,-3.8921344,neighbor,5299559,Scene Labeling using Recurrent Neural Networks with Explicit Long Range Contextual Dependency,0.06098240613937378,#bcbd22
0.5529962,4.7011824,neighbor,5299559,Joint Object and Part Segmentation Using Deep Learned Potentials,0.06113111972808838,#bcbd22
2.4882722,12.37617,neighbor,5299559,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,0.06210017204284668,#bcbd22
2.165272,14.118088,neighbor,5299559,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,0.06228756904602051,#bcbd22
9.055807,8.4740305,neighbor,5299559,Attention to Scale: Scale-Aware Semantic Image Segmentation,0.062296390533447266,#bcbd22
5.837068,10.919186,neighbor,5299559,Multi-Scale Context Aggregation by Dilated Convolutions,0.06241399049758911,#bcbd22
-8.331317,-4.510091,neighbor,5299559,Multi-scale Recognition with DAG-CNNs,0.06298106908798218,#bcbd22
3.9054394,9.453876,neighbor,5299559,Fully convolutional networks for semantic segmentation,0.06362712383270264,#bcbd22
-6.6334715,3.115659,neighbor,5299559,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.06381762027740479,#bcbd22
6.1652355,7.743544,neighbor,5299559,ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation,0.06385737657546997,#bcbd22
-5.5972047,4.482362,neighbor,5299559,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.06428366899490356,#bcbd22
7.4589715,-9.883854,neighbor,5299559,Dual Local-Global Contextual Pathways for Recognition in Aerial Imagery,0.06441229581832886,#bcbd22
6.397287,-9.700325,neighbor,5299559,A Hierarchical and Contextual Model for Aerial Image Parsing,0.06462019681930542,#bcbd22
3.0737834,3.3224587,neighbor,5299559,Instance-Aware Semantic Segmentation via Multi-task Network Cascades,0.0651056170463562,#bcbd22
5.662392,-6.615399,neighbor,5299559,Spatially Constrained Location Prior for scene parsing,0.06559514999389648,#bcbd22
-5.499406,-6.753003,neighbor,5299559,The Cityscapes Dataset for Semantic Urban Scene Understanding,0.0658649206161499,#bcbd22
6.759689,-4.5475917,neighbor,5299559,Scene Segmentation with Low-Dimensional Semantic Representations and Conditional Random Fields,0.06594687700271606,#bcbd22
-1.1510612,8.515672,neighbor,5299559,Improving spatial codification in semantic segmentation,0.06637340784072876,#bcbd22
-7.2465262,3.109513,neighbor,5299559,Attentive Contexts for Object Detection,0.06718486547470093,#bcbd22
2.6507995,0.73364776,neighbor,5299559,Bottom-up Instance Segmentation using Deep Higher-Order CRFs,0.06752943992614746,#bcbd22
-8.980357,-2.206202,neighbor,5299559,Locally Supervised Deep Hybrid Model for Scene Recognition,0.06765472888946533,#bcbd22
1.5128089,10.728039,neighbor,5299559,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling,0.06803810596466064,#bcbd22
-13.403609,-3.9814005,neighbor,5299559,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.06807982921600342,#bcbd22
-8.132451,3.9650326,neighbor,5299559,ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization,0.06818211078643799,#bcbd22
-2.9135592,0.21417934,neighbor,5299559,Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning,0.06887519359588623,#bcbd22
3.3994973,1.0024793,neighbor,5299559,End-to-End Instance Segmentation and Counting with Recurrent Attention,0.0689888596534729,#bcbd22
-10.738145,-6.7848067,neighbor,5299559,Weakly-supervised Discriminative Patch Learning via CNN for Fine-grained Recognition,0.06919717788696289,#bcbd22
2.1716247,8.495621,neighbor,5299559,Context Tricks for Cheap Semantic Segmentation,0.06956368684768677,#bcbd22
2.105339,6.83286,neighbor,5299559,Fast Semantic Image Segmentation with High Order Context and Guided Filtering,0.06969523429870605,#bcbd22
7.5121546,0.18524072,neighbor,5299559,Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency,0.06990605592727661,#bcbd22
-9.368638,-1.0094042,neighbor,5299559,Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification,0.06995904445648193,#bcbd22
0.20888093,9.115161,neighbor,5299559,Convolutional feature masking for joint object and stuff segmentation,0.07000428438186646,#bcbd22
-11.136949,-1.4873905,neighbor,5299559,"Scene Recognition with CNNs: Objects, Scales and Dataset Bias",0.07005339860916138,#bcbd22
-9.322367,-6.77196,neighbor,5299559,Part-Stacked CNN for Fine-Grained Visual Categorization,0.07023721933364868,#bcbd22
6.923002,11.791822,neighbor,5299559,Mixed context networks for semantic segmentation,0.07043451070785522,#bcbd22
4.80027,9.361978,neighbor,5299559,Segmentation-aware convolutional nets,0.07046765089035034,#bcbd22
10.111669,8.72758,neighbor,5299559,Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net,0.0708765983581543,#bcbd22
7.172961,-7.2368035,neighbor,5299559,Context by region ancestry,0.07093119621276855,#bcbd22
-4.5210614,4.548242,neighbor,5299559,Hypercolumns for object segmentation and fine-grained localization,0.07111072540283203,#bcbd22
-2.5347967,-2.4349678,neighbor,5299559,DeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding,0.07130199670791626,#bcbd22
5.3066,6.6167536,neighbor,5299559,Combining the Best of Graphical Models and ConvNets for Semantic Segmentation,0.07132971286773682,#bcbd22
-3.493271,-7.171723,neighbor,5299559,Pixel-Level Encoding and Depth Layering for Instance-Level Semantic Labeling,0.07136446237564087,#bcbd22
5.614109,-1.217278,neighbor,5299559,Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes,0.07138478755950928,#bcbd22
2.2025733,-9.6613,neighbor,5299559,Semantic Graph Construction for Weakly-Supervised Image Parsing,0.07144612073898315,#bcbd22
3.9128203,10.92245,neighbor,5299559,Fully Connected Deep Structured Networks,0.0715714693069458,#bcbd22
8.165487,-3.7649353,neighbor,5299559,EFFICIENT SEMANTIC SEGMENTATION OF MAN-MADE SCENES USING FULLY-CONNECTED CONDITIONAL RANDOM FIELD,0.07176226377487183,#bcbd22
-2.6393838,1.8734955,neighbor,5299559,Learning Semantic Part-Based Models from Google Images,0.07181531190872192,#bcbd22
-12.278448,-0.35996282,neighbor,5299559,Places205-VGGNet Models for Scene Recognition,0.07191646099090576,#bcbd22
3.134663,-2.7947018,neighbor,5299559,Learning Dynamic Hierarchical Models for Anytime Scene Labeling,0.07205849885940552,#bcbd22
3.2523758,12.936391,neighbor,5299559,The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,0.07222956418991089,#bcbd22
-2.9499984,2.0745196,neighbor,5299559,DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization,0.07231360673904419,#bcbd22
-11.502673,-3.3760338,neighbor,5299559,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.07277357578277588,#bcbd22
-4.9702244,6.043323,neighbor,5299559,Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation,0.07317608594894409,#bcbd22
6.4663577,4.028138,neighbor,5299559,STC: A Simple to Complex Framework for Weakly-Supervised Semantic Segmentation,0.07385390996932983,#bcbd22
0.4441017,6.716921,neighbor,5299559,Semantic Part Segmentation with Deep Learning,0.07390373945236206,#bcbd22
-8.2036,4.5684304,neighbor,5299559,Learning Deep Features for Discriminative Localization,0.07454878091812134,#bcbd22
6.210117,-1.2907201,neighbor,5299559,Syntactic image parsing using ontology and semantic descriptions,0.07457315921783447,#bcbd22
7.434615,6.475957,neighbor,5299559,Segmentation from Natural Language Expressions,0.07478517293930054,#bcbd22
-6.7069716,4.320849,neighbor,5299559,Scalable Object Detection Using Deep Neural Networks,0.0750957727432251,#bcbd22
4.2609444,13.552817,neighbor,5299559,Deep structured features for semantic segmentation,0.07541650533676147,#bcbd22
1.0312762,12.418274,neighbor,5299559,Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding,0.07550835609436035,#bcbd22
-10.253778,0.017426595,neighbor,5299559,Object Detectors Emerge in Deep Scene CNNs,0.07559597492218018,#bcbd22
4.029382,4.30143,neighbor,5299559,What's the Point: Semantic Segmentation with Point Supervision,0.07595628499984741,#bcbd22
-10.107706,-6.7179937,neighbor,5299559,Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition,0.07605242729187012,#bcbd22
5.975702,4.4265823,neighbor,5299559,Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network,0.07628887891769409,#bcbd22
3.276747,-8.491852,neighbor,5299559,Adaptive Nonparametric Image Parsing,0.07635420560836792,#bcbd22
2.8883195,2.147963,neighbor,5299559,Bridging Category-level and Instance-level Semantic Image Segmentation,0.0765160322189331,#bcbd22
-7.8822956,-0.4520177,neighbor,5299559,Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles,0.07653045654296875,#bcbd22
5.0328465,-4.7317348,neighbor,5299559,Data-Driven Scene Understanding with Adaptively Retrieved Exemplars,0.07658147811889648,#bcbd22
-1.6226494,-9.703562,neighbor,5299559,Part and appearance sharing: Recursive Compositional Models for multi-view,0.07658159732818604,#bcbd22
-13.697996,-4.427098,neighbor,5299559,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.07667183876037598,#bcbd22
1.2475896,-4.86851,query,54465873,Mask R-CNN,0.0,#bcbd22
-0.5078392,-7.2716994,neighbor,54465873,Shape-aware Instance Segmentation,0.04113286733627319,#bcbd22
-3.143613,-9.711824,neighbor,54465873,Instance-Aware Semantic Segmentation via Multi-task Network Cascades,0.042257606983184814,#bcbd22
1.6020792,-6.5187826,neighbor,54465873,Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation,0.04286283254623413,#bcbd22
0.7558033,2.238801,neighbor,54465873,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.042864084243774414,#bcbd22
0.5195061,2.3451953,neighbor,54465873,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.04387843608856201,#bcbd22
-4.7212224,3.818127,neighbor,54465873,DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization,0.04485964775085449,#bcbd22
-1.2195216,-4.134907,neighbor,54465873,Learning to Refine Object Segments,0.04521661996841431,#bcbd22
0.18267615,-7.3607063,neighbor,54465873,Reversible Recursive Instance-Level Object Segmentation,0.045421302318573,#bcbd22
-2.3140945,-3.8971193,neighbor,54465873,Pixel Objectness,0.04717987775802612,#bcbd22
0.8367634,0.6436641,neighbor,54465873,Learning to decompose for object detection and instance segmentation,0.04723787307739258,#bcbd22
1.2315446,10.796546,neighbor,54465873,"You Only Look Once: Unified, Real-Time Object Detection",0.049094974994659424,#bcbd22
3.3540168,9.903864,neighbor,54465873,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.04924827814102173,#bcbd22
0.4232179,9.667595,neighbor,54465873,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.04933673143386841,#bcbd22
-2.4890995,3.716749,neighbor,54465873,Subcategory-Aware Convolutional Neural Networks for Object Proposals and Detection,0.04967939853668213,#bcbd22
4.2901745,15.980326,neighbor,54465873,R-CNNs for Pose Estimation and Action Detection,0.049937665462493896,#bcbd22
-3.322207,-10.486227,neighbor,54465873,Fully Convolutional Instance-Aware Semantic Segmentation,0.050789833068847656,#bcbd22
-0.45997,-8.372456,neighbor,54465873,Proposal-Free Network for Instance-Level Object Segmentation,0.05098390579223633,#bcbd22
-1.2885909,-5.346733,neighbor,54465873,A Holistic Approach for Data-Driven Object Cutout,0.05123436450958252,#bcbd22
-3.6829467,-7.808577,neighbor,54465873,Semantic Part Segmentation with Deep Learning,0.051956355571746826,#bcbd22
-3.88556,3.454653,neighbor,54465873,Part-Based R-CNNs for Fine-Grained Category Detection,0.05203825235366821,#bcbd22
-2.736765,-11.386393,neighbor,54465873,Bridging Category-level and Instance-level Semantic Image Segmentation,0.05249267816543579,#bcbd22
5.727825,1.0805354,neighbor,54465873,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.05262035131454468,#bcbd22
2.3517244,11.674117,neighbor,54465873,Action-Driven Object Detection with Top-Down Visual Attentions,0.05281496047973633,#bcbd22
1.12188,-1.9402575,neighbor,54465873,Layered object detection for multi-class segmentation,0.05315619707107544,#bcbd22
-4.612822,9.920789,neighbor,54465873,Self-taught object localization with deep networks,0.05315685272216797,#bcbd22
-0.19569562,-1.2278372,neighbor,54465873,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.053257107734680176,#bcbd22
-0.7948143,-2.7738998,neighbor,54465873,Learning to Segment Object Candidates,0.053415000438690186,#bcbd22
3.6963928,6.6432905,neighbor,54465873,"Scalable, High-Quality Object Detection",0.05355048179626465,#bcbd22
5.0879645,18.123228,neighbor,54465873,Associative Embedding: End-to-End Learning for Joint Detection and Grouping,0.053685545921325684,#bcbd22
-1.2851306,1.6543169,neighbor,54465873,Hypercolumns for object segmentation and fine-grained localization,0.053790509700775146,#bcbd22
4.0679483,17.820522,neighbor,54465873,Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields,0.05409109592437744,#bcbd22
-0.2306564,4.2665,neighbor,54465873,CRAFT Objects from Images,0.054117023944854736,#bcbd22
4.116848,17.216887,neighbor,54465873,Towards Accurate Multi-person Pose Estimation in the Wild,0.05415230989456177,#bcbd22
1.8317332,5.761484,neighbor,54465873,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.05443263053894043,#bcbd22
-1.0770731,9.185418,neighbor,54465873,ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks,0.054549992084503174,#bcbd22
-5.5402346,3.8899963,neighbor,54465873,Learning Semantic Part-Based Models from Google Images,0.05475538969039917,#bcbd22
1.7121567,3.8529465,neighbor,54465873,Object Detection via End-to-End Integration of Aspect Ratio and Context Aware Part-based Models and Fully Convolutional Networks,0.054765522480010986,#bcbd22
4.6273575,5.234111,neighbor,54465873,Training Region-Based Object Detectors with Online Hard Example Mining,0.05507856607437134,#bcbd22
9.358367,5.4315825,neighbor,54465873,Deep learning for class-generic object detection,0.055378615856170654,#bcbd22
2.9096432,-5.740617,neighbor,54465873,Deep Joint Task Learning for Generic Object Extraction,0.055512964725494385,#bcbd22
-6.4005013,-3.5700803,neighbor,54465873,Learning Video Object Segmentation from Static Images,0.055555760860443115,#bcbd22
2.8566065,-7.561819,neighbor,54465873,FastMask: Segment Multi-scale Object Candidates in One Shot,0.05578768253326416,#bcbd22
-6.1965237,7.0118356,neighbor,54465873,The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition,0.05604618787765503,#bcbd22
-3.1436248,-2.3506079,neighbor,54465873,Visual chunking: A list prediction framework for region-based object detection,0.05614370107650757,#bcbd22
2.741889,6.244682,neighbor,54465873,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.056188225746154785,#bcbd22
-4.8220506,-8.458848,neighbor,54465873,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",0.05632251501083374,#bcbd22
1.1899471,-12.045083,neighbor,54465873,Monocular Object Instance Segmentation and Depth Ordering with CNNs,0.05664408206939697,#bcbd22
6.488925,13.025219,neighbor,54465873,Learning to Detect Human-Object Interactions,0.05664539337158203,#bcbd22
6.4876895,-0.89635915,neighbor,54465873,Deformable part models are convolutional neural networks,0.05677706003189087,#bcbd22
2.4979196,-1.9350224,neighbor,54465873,Semantic segmentation using regions and parts,0.057098984718322754,#bcbd22
3.5911093,-2.5208855,neighbor,54465873,Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts,0.05710113048553467,#bcbd22
-4.5454617,10.759375,neighbor,54465873,Improving Weakly-Supervised Object Localization By Micro-Annotation,0.05748021602630615,#bcbd22
-7.8778844,-6.399061,neighbor,54465873,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.05759251117706299,#bcbd22
-6.0751357,-10.672149,neighbor,54465873,What's the Point: Semantic Segmentation with Point Supervision,0.057694971561431885,#bcbd22
-6.0784664,9.807703,neighbor,54465873,Weakly Supervised Object Localization with Multi-Fold Multiple Instance Learning,0.057724177837371826,#bcbd22
8.337292,4.186337,neighbor,54465873,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.05796724557876587,#bcbd22
2.0747964,7.3342586,neighbor,54465873,Fast R-CNN,0.058043599128723145,#bcbd22
9.798309,-0.62583137,neighbor,54465873,"Fine-grained pose prediction, normalization, and recognition",0.05810576677322388,#bcbd22
8.610162,4.4504123,neighbor,54465873,DeepID-Net: Deformable deep convolutional neural networks for object detection,0.058217406272888184,#bcbd22
-2.0799062,-13.004539,neighbor,54465873,End-to-End Instance Segmentation and Counting with Recurrent Attention,0.05833548307418823,#bcbd22
2.6546974,12.743789,neighbor,54465873,DeepProposals: Hunting Objects and Actions by Cascading Deep Convolutional Layers,0.05851435661315918,#bcbd22
-5.1595597,-9.50424,neighbor,54465873,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.05855149030685425,#bcbd22
4.81924,2.23342,neighbor,54465873,Max-Margin Object Detection,0.05867117643356323,#bcbd22
-2.036122,10.4563465,neighbor,54465873,Learning Deep Features for Discriminative Localization,0.058734357357025146,#bcbd22
-4.3965416,-13.223356,neighbor,54465873,Semantic Understanding of Scenes Through the ADE20K Dataset,0.059287071228027344,#bcbd22
1.020228,8.627177,neighbor,54465873,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.059343814849853516,#bcbd22
-4.398743,-10.534478,neighbor,54465873,Weakly Supervised Semantic Segmentation with Convolutional Networks,0.05942130088806152,#bcbd22
3.0744135,1.5525609,neighbor,54465873,Regionlets for Generic Object Detection,0.059422314167022705,#bcbd22
1.8932552,9.508631,neighbor,54465873,Scalable Object Detection Using Deep Neural Networks,0.0594940185546875,#bcbd22
9.439498,1.2549639,neighbor,54465873,Mine the fine: Fine-grained fragment discovery,0.05952346324920654,#bcbd22
-1.4119155,-9.486023,neighbor,54465873,Semantic Segmentation with Object Clique Potential,0.05965590476989746,#bcbd22
5.79959,8.15578,neighbor,54465873,1-HKUST: Object Detection in ILSVRC 2014,0.05992865562438965,#bcbd22
-6.2740617,9.112212,neighbor,54465873,We Don’t Need No Bounding-Boxes: Training Object Class Detectors Using Only Human Verification,0.06015127897262573,#bcbd22
0.42333806,-9.15307,neighbor,54465873,Object Detection Free Instance Segmentation With Labeling Transformations,0.060242414474487305,#bcbd22
11.115985,-0.67018324,neighbor,54465873,Improved Deep Learning of Object Category Using Pose Information,0.06032067537307739,#bcbd22
3.467543,3.1072128,neighbor,54465873,Mid-level Elements for Object Detection,0.060339391231536865,#bcbd22
0.19389923,7.483942,neighbor,54465873,LocNet: Improving Localization Accuracy for Object Detection,0.06080228090286255,#bcbd22
-5.013927,1.8686702,neighbor,54465873,Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition,0.06116068363189697,#bcbd22
5.1033287,16.581358,neighbor,54465873,Deep Poselets for Human Detection,0.06127285957336426,#bcbd22
3.5922632,0.58493304,neighbor,54465873,Parallelized deformable part models with effective hypothesis pruning,0.06141233444213867,#bcbd22
5.4686866,5.153276,neighbor,54465873,Weakly- and Semi-Supervised Object Detection with Expectation-Maximization Algorithm,0.06142371892929077,#bcbd22
7.0065193,-6.6349034,neighbor,54465873,A CNN Cascade for Landmark Guided Semantic Part Segmentation,0.061485469341278076,#bcbd22
5.0170074,7.724726,neighbor,54465873,Zoom Out-and-In Network with Recursive Training for Object Proposal,0.061501145362854004,#bcbd22
3.2938838,8.930884,neighbor,54465873,Feature Pyramid Networks for Object Detection,0.06157106161117554,#bcbd22
-0.52705145,4.9980264,neighbor,54465873,Weakly Supervised Cascaded Convolutional Networks,0.06160271167755127,#bcbd22
9.657546,-3.096662,neighbor,54465873,TemplateNet for Depth-Based Object Instance Recognition,0.06182903051376343,#bcbd22
-6.874695,-8.992644,neighbor,54465873,Recalling Holistic Information for Semantic Segmentation,0.0618935227394104,#bcbd22
-2.847459,-12.792247,neighbor,54465873,Bottom-up Instance Segmentation using Deep Higher-Order CRFs,0.061934590339660645,#bcbd22
-6.2465887,-3.6809032,neighbor,54465873,FusionSeg: Learning to Combine Motion and Appearance for Fully Automatic Segmentation of Generic Objects in Videos,0.06207013130187988,#bcbd22
5.444228,-3.1384377,neighbor,54465873,Learning a Hierarchical Deformable Template for Rapid Deformable Object Parsing,0.062217533588409424,#bcbd22
-2.2170227,9.824885,neighbor,54465873,ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization,0.06241607666015625,#bcbd22
-5.1402283,-6.940958,neighbor,54465873,Convolutional feature masking for joint object and stuff segmentation,0.06242263317108154,#bcbd22
-1.7358279,0.29108807,neighbor,54465873,Segmentation of Objects by Hashing,0.06253069639205933,#bcbd22
7.4537272,13.020795,neighbor,54465873,Unsupervised Learning of Important Objects from First-Person Videos,0.06262457370758057,#bcbd22
-3.5925503,4.8164024,neighbor,54465873,Hierarchical part detection with deep neural networks,0.06262606382369995,#bcbd22
5.233045,12.517458,neighbor,54465873,Exploring Person Context and Local Scene Context for Object Detection,0.0627593994140625,#bcbd22
-5.455524,-8.470971,neighbor,54465873,Fully convolutional networks for semantic segmentation,0.0630108118057251,#bcbd22
3.7870288,-0.7362022,neighbor,54465873,Shape-based pedestrian parsing,0.0630490779876709,#bcbd22
-6.1781826,1.8943629,neighbor,54465873,PartBook for image parsing,0.06315553188323975,#bcbd22
-2.8417354,8.578774,neighbor,54465873,Unsupervised Visual Representation Learning by Context Prediction,0.06319177150726318,#bcbd22
-1.423186,-9.633387,query,61050894,ggplot2: Elegant Graphics for Data Analysis,0.0,#17becf
-0.5844543,-10.405037,neighbor,61050894,Scatterplot3d - an R package for visualizing multivariate data,0.04504019021987915,#17becf
-1.2065897,-8.696467,neighbor,61050894,Beanplot: A Boxplot Alternative for Visual Comparison of Distributions,0.04646611213684082,#17becf
-3.3931143,-6.078555,neighbor,61050894,A Handbook of Statistical Analyses Using R,0.051490187644958496,#17becf
-7.520546,-4.0952053,neighbor,61050894,Effect Displays in R for Generalised Linear Models,0.05178964138031006,#17becf
-3.8887475,-8.451664,neighbor,61050894,Quality Control for Statistical Graphics: The graphicsQC Package for R,0.05187875032424927,#17becf
-3.167359,-6.91001,neighbor,61050894,reporttools: R Functions to Generate LaTeX Tables of Descriptive Statistics,0.05217522382736206,#17becf
-0.5628884,-11.638745,neighbor,61050894,Interactive and Dynamic Graphics for Data Analysis: With R and GGobi,0.05546903610229492,#17becf
-0.13560097,-11.785751,neighbor,61050894,Lattice: Multivariate Data Visualization with R,0.05627644062042236,#17becf
-4.7361016,-7.9056134,neighbor,61050894,CGIwithR: Facilities for processing web forms using R,0.05673253536224365,#17becf
-3.189661,-3.775234,neighbor,61050894,Computational Statistics: An Introduction to R,0.06041836738586426,#17becf
-5.485808,-7.497596,neighbor,61050894,The R Commander: A Basic-Statistics Graphical User Interface to R,0.062349915504455566,#17becf
0.14454675,-8.526719,neighbor,61050894,FactoMineR: An R Package for Multivariate Analysis,0.06344908475875854,#17becf
3.7558112,4.31576,neighbor,61050894,Information for Authors,0.06387746334075928,#17becf
-3.9891443,-5.4999614,neighbor,61050894,Data Manipulation with R,0.06393498182296753,#17becf
-5.479909,-2.6315188,neighbor,61050894,Generalized Additive Models: An Introduction with R,0.06408572196960449,#17becf
-6.499979,-2.1880088,neighbor,61050894,glmulti: An R Package for Easy Automated Model Selection with (Generalized) Linear Models,0.06422489881515503,#17becf
2.6607223,1.7002888,neighbor,61050894,Thanks to Reviewers,0.0652235746383667,#17becf
-4.046181,-9.441929,neighbor,61050894,Importing Vector Graphics: The grImport Package for R,0.06564253568649292,#17becf
-8.32247,-7.659139,neighbor,61050894,An Introduction to StatCrunch 3.0,0.066017746925354,#17becf
2.504443,-5.449759,neighbor,61050894,Principal Component Analysis,0.06770485639572144,#17becf
2.4295607,2.8263702,neighbor,61050894,Acknowledgement of Reviewers,0.06886786222457886,#17becf
-2.0836203,-3.7490973,neighbor,61050894,Statistical Methods for Communication Science,0.06913632154464722,#17becf
-0.24434924,4.1026216,neighbor,61050894,ACKNOWLEDGEMENTS,0.06931900978088379,#17becf
2.0237403,2.0158603,neighbor,61050894,Guest Reviewers,0.06942933797836304,#17becf
-6.7445297,-9.31713,neighbor,61050894,R2WinBUGS: A Package for Running WinBUGS from R,0.0695641040802002,#17becf
-5.5158753,-10.607697,neighbor,61050894,Interactive Multivariate Data Analysis in R with the ade4 and ade4TkGUI Packages,0.07021850347518921,#17becf
3.4250603,2.4484222,neighbor,61050894,Reviewers,0.07022136449813843,#17becf
2.8445635,7.3723145,neighbor,61050894,Preface,0.07098805904388428,#17becf
-1.7523469,-7.4542117,neighbor,61050894,A Visual Basic Software for Computing Fisher\'s Exact Probability,0.0712239146232605,#17becf
7.6229105,2.9969213,neighbor,61050894,Correction,0.07131856679916382,#17becf
-4.4184318,-4.000408,neighbor,61050894,A Modern Approach to Regression with R,0.07132518291473389,#17becf
10.312784,1.3494269,neighbor,61050894,Erratum,0.07158952951431274,#17becf
-3.6352,-4.4285364,neighbor,61050894,Robust Statistical Methods With R,0.07168781757354736,#17becf
8.886585,1.462217,neighbor,61050894,Erratum,0.07170212268829346,#17becf
8.89301,1.4685943,neighbor,61050894,Erratum,0.07170212268829346,#17becf
7.6768804,1.7940686,neighbor,61050894,CORRECTION,0.07193785905838013,#17becf
-8.823222,-6.974878,neighbor,61050894,Quasi-variances in Xlisp-Stat and on the web,0.07265102863311768,#17becf
0.3431717,-10.211332,neighbor,61050894,The Strucplot Framework: Visualizing Multi-way Contingency Tables with vcd,0.07279783487319946,#17becf
5.6369886,9.06996,neighbor,61050894,In Memoriam,0.07293212413787842,#17becf
3.2995906,2.5731726,neighbor,61050894,Reviewers,0.0729876160621643,#17becf
-0.9374012,-4.574369,neighbor,61050894,Getting the most from your curves: Exploring and reporting data using informative graphical techniques,0.07318305969238281,#17becf
-5.428663,-8.599895,neighbor,61050894,RinRuby: Accessing the R Interpreter from Pure Ruby,0.07320225238800049,#17becf
0.8290522,-12.044116,neighbor,61050894,Visualizing Categorical Data,0.0732877254486084,#17becf
-2.646759,-10.683951,neighbor,61050894,spectrino Software: Spectra Visualization and Preparation for R,0.07339590787887573,#17becf
0.22890247,-12.3464985,neighbor,61050894,Graphics of Large Datasets: Visualizing a Million,0.07347911596298218,#17becf
-0.3933451,-6.8315325,neighbor,61050894,Metaplot: A Novel Stata Graph for Assessing Heterogeneity at a Glance,0.07398325204849243,#17becf
-7.7375364,-4.000335,neighbor,61050894,Effect Displays in R for Multinomial and Proportional-Odds Logit Models: Extensions to the effects Package,0.07405364513397217,#17becf
2.652123,6.99417,neighbor,61050894,Preface,0.07418322563171387,#17becf
9.759272,5.1427045,neighbor,61050894,Correction,0.07427769899368286,#17becf
0.5095781,3.9804282,neighbor,61050894,Acknowledgements,0.07434016466140747,#17becf
10.822295,5.645773,neighbor,61050894,Correction,0.07452267408370972,#17becf
9.519928,5.2181344,neighbor,61050894,Correction,0.07505422830581665,#17becf
10.936447,0.56294626,neighbor,61050894,Erratum,0.07506859302520752,#17becf
7.2769513,1.5357689,neighbor,61050894,Correction,0.07513302564620972,#17becf
0.46238577,6.93935,neighbor,61050894,Table of Contents,0.07532733678817749,#17becf
0.46244657,6.9395638,neighbor,61050894,Table of contents,0.07532733678817749,#17becf
-8.056534,-1.5813019,neighbor,61050894,High-dimensional Graphical Model Search with gRapHD R Package,0.07562875747680664,#17becf
-5.5801477,-5.2947946,neighbor,61050894,Using R for data analysis and graphing in an introductory physics laboratory,0.07567101716995239,#17becf
10.74095,2.1140234,neighbor,61050894,Erratum,0.07593822479248047,#17becf
4.2326217,9.192945,neighbor,61050894,Preliminary Material,0.07594257593154907,#17becf
2.0077002,6.1704164,neighbor,61050894,Poster,0.07599985599517822,#17becf
3.2136903,7.672957,neighbor,61050894,Preface,0.07613515853881836,#17becf
3.0801985,9.3152075,neighbor,61050894,Preface,0.07640957832336426,#17becf
2.5039423,-10.522469,neighbor,61050894,ROCR: visualizing classifier performance in R,0.0764622688293457,#17becf
2.967774,0.6762535,neighbor,61050894,List of Reviewers,0.07667893171310425,#17becf
-1.3116133,-12.681974,neighbor,61050894,Automating the Creation of Interactive Glyph-supplemented Scatterplots for Visualizing Algorithm Results,0.07669568061828613,#17becf
5.5866566,5.659387,neighbor,61050894,Sampling,0.07673448324203491,#17becf
10.84027,1.532507,neighbor,61050894,Erratum,0.07673764228820801,#17becf
-6.1704926,-1.1049267,neighbor,61050894,The VGAM Package for Categorical Data Analysis,0.07674235105514526,#17becf
-2.5741842,-5.607146,neighbor,61050894,Toward a Common Framework for Statistical Analysis and Development,0.0767938494682312,#17becf
-7.843839,-2.7142792,neighbor,61050894,A common platform for graphical models in R,0.07683765888214111,#17becf
5.900672,-3.8706515,neighbor,61050894,How to Avoid Potential Pitfalls in Recurrence Plot Based Data Analysis,0.07700878381729126,#17becf
2.110536,4.5636544,neighbor,61050894,Sponsors,0.07739722728729248,#17becf
11.570828,1.6856147,neighbor,61050894,Erratum,0.07752221822738647,#17becf
2.0877202,5.204789,neighbor,61050894,Sponsors,0.0775446891784668,#17becf
-9.154054,-4.209114,neighbor,61050894,The R Package geepack for Generalized Estimating Equations,0.07766294479370117,#17becf
12.979229,-0.18800673,neighbor,61050894,Preface,0.07785844802856445,#17becf
12.572936,2.8008435,neighbor,61050894,Retraction,0.07818090915679932,#17becf
7.4257116,7.17883,neighbor,61050894,Editorial,0.07845300436019897,#17becf
11.637944,2.573529,neighbor,61050894,Erratum,0.07852917909622192,#17becf
3.6193953,2.0446842,neighbor,61050894,Reviewers,0.07864725589752197,#17becf
-6.1493797,-8.020946,neighbor,61050894,fgui: A Method for Automatically Creating Graphical User Interfaces for Command-Line R Packages.,0.0787123441696167,#17becf
4.0124707,0.3817745,neighbor,61050894,Acknowledgement of Reviewers,0.0788833498954773,#17becf
4.9546638,2.5543647,neighbor,61050894,Reviewers,0.07899731397628784,#17becf
2.565374,-5.28448,neighbor,61050894,CAR: A MATLAB Package to Compute Correspondence Analysis with Rotations,0.07905888557434082,#17becf
1.6224037,4.3823805,neighbor,61050894,Sponsors,0.07907015085220337,#17becf
1.3528277,8.622575,neighbor,61050894,MP users guide,0.07912933826446533,#17becf
3.7206204,8.774884,neighbor,61050894,Preface,0.07944673299789429,#17becf
3.8379781,7.142463,neighbor,61050894,Acknowledgements,0.07958340644836426,#17becf
-5.9656153,-6.466246,neighbor,61050894,Using R via PHP for Teaching Purposes: R-php,0.07961130142211914,#17becf
12.128155,1.7137061,neighbor,61050894,Erratum,0.07976299524307251,#17becf
-4.1545534,-7.0476527,neighbor,61050894,Caching and Distributing Statistical Analyses in R,0.07989346981048584,#17becf
10.260259,5.119018,neighbor,61050894,Correction,0.0799981951713562,#17becf
4.8562655,8.10712,neighbor,61050894,Acknowledgments,0.08007574081420898,#17becf
11.90712,1.1734213,neighbor,61050894,Erratum,0.08019614219665527,#17becf
2.0013545,-7.273182,neighbor,61050894,clues: An R Package for Nonparametric Clustering Based on Local Shrinking,0.08027714490890503,#17becf
4.5126753,4.6701365,neighbor,61050894,About the Authors,0.08030813932418823,#17becf
0.6272054,-13.799133,neighbor,61050894,APPLICATIONS OF STATISTICAL DATA MINING METHODS,0.08031368255615234,#17becf
-2.9110737,-2.340784,neighbor,61050894,Modern Statistics by Kriging,0.08031630516052246,#17becf
0.23520346,-2.7633975,neighbor,61050894,CircStat: AMATLABToolbox for Circular Statistics,0.08031833171844482,#17becf
0.40823242,-6.1788197,query,740063,A Survey on Transfer Learning,0.0,#17becf
-0.2952107,-5.123605,neighbor,740063,Large margin transductive transfer learning,0.04078829288482666,#17becf
8.37527,-11.8586,neighbor,740063,Learning the Shared Subspace for Multi-task Clustering and Transductive Transfer Classification,0.055504560470581055,#17becf
-0.6748238,-8.173434,neighbor,740063,A risk minimization framework for domain adaptation,0.05565768480300903,#17becf
1.6246783,-5.788529,neighbor,740063,Knowledge transfer via multiple model local structure mapping,0.06047070026397705,#17becf
3.0443337,-11.244809,neighbor,740063,Cross-Lingual Adaptation Using Structural Correspondence Learning,0.06692880392074585,#17becf
8.994933,-12.5807295,neighbor,740063,Heterogeneous Transfer Learning for Image Clustering via the SocialWeb,0.07493144273757935,#17becf
4.153466,-13.193576,neighbor,740063,Improving SCL Model for Sentiment-Transfer Learning,0.07608753442764282,#17becf
7.273022,-8.818228,neighbor,740063,Large-scale collaborative prediction using a nonparametric random effects model,0.07629132270812988,#17becf
2.2323651,-8.152857,neighbor,740063,Domain Adaptation for Statistical Classifiers,0.07632303237915039,#17becf
-0.07279126,-9.280961,neighbor,740063,Domain Adaptation: Learning Bounds and Algorithms,0.07654684782028198,#17becf
3.8505893,-5.2670345,neighbor,740063,Composition of Conditional Random Fields for Transfer Learning,0.0767139196395874,#17becf
8.025072,-7.1665163,neighbor,740063,Bayesian Multitask Learning with Latent Hierarchies,0.07680189609527588,#17becf
-5.081793,2.1175997,neighbor,740063,Unsupervised Supervised Learning II: Training Margin Based Classifiers without Labels,0.07713538408279419,#17becf
-1.4998298,-2.5347433,neighbor,740063,Convex Point Estimation using Undirected Bayesian Transfer Hierarchies,0.07813626527786255,#17becf
9.396618,-7.958689,neighbor,740063,A Convex Formulation for Learning Task Relationships in Multi-Task Learning,0.07821500301361084,#17becf
2.126581,-9.492849,neighbor,740063,Frustratingly Easy Domain Adaptation,0.08127808570861816,#17becf
2.8879113,0.8979492,neighbor,740063,The Induction and Transfer of Declarative Bias,0.08143609762191772,#17becf
10.415509,-8.129627,neighbor,740063,Clustered Multi-Task Learning: A Convex Formulation,0.08408725261688232,#17becf
-1.4429117,-9.708703,neighbor,740063,A theory of learning from different domains,0.08773493766784668,#17becf
1.9379151,2.8143675,neighbor,740063,A Model of Inductive Bias Learning,0.0883110761642456,#17becf
3.9873114,-8.979671,neighbor,740063,Domain adaptive bootstrapping for named entity recognition,0.08885812759399414,#17becf
7.295733,6.4515657,neighbor,740063,Learning with Unlabeled Data for Text Categorization Using a Bootstrapping and a Feature Projection Technique,0.09150689840316772,#17becf
-1.8888053,11.10807,neighbor,740063,ABC-boost: adaptive base class boost for multi-class classification,0.09209048748016357,#17becf
6.900332,-14.847514,neighbor,740063,Co-clustering based classification for out-of-domain documents,0.09253937005996704,#17becf
7.810565,5.348719,neighbor,740063,Investigating Unsupervised Learning for Text Categorization Bootstrapping,0.09387141466140747,#17becf
13.960921,2.3307717,neighbor,740063,Local and Global Regressive Mapping for Manifold Learning with Out-of-Sample Extrapolation,0.09397995471954346,#17becf
-4.020244,4.852665,neighbor,740063,Introduction to Machine Learning,0.09446209669113159,#17becf
-12.014694,-0.33970782,neighbor,740063,Metric and Kernel Learning Using a Linear Transformation,0.09534716606140137,#17becf
0.6503646,-10.797921,neighbor,740063,Multiple Source Adaptation and the Rényi Divergence,0.09649252891540527,#17becf
9.857273,-6.218733,neighbor,740063,Feature hashing for large scale multitask learning,0.09665578603744507,#17becf
6.4795628,4.9994664,neighbor,740063,Domain Kernels for Text Categorization,0.0967642068862915,#17becf
5.184261,-14.103461,neighbor,740063,Cross-domain sentiment classification via spectral feature alignment,0.0967874526977539,#17becf
4.063167,-7.241267,neighbor,740063,Hierarchical Bayesian Domain Adaptation,0.09689152240753174,#17becf
-0.08234525,-13.917318,neighbor,740063,Automatic audio tagging using covariate shift adaptation,0.09836089611053467,#17becf
5.4101353,-3.589873,neighbor,740063,Cross-Task Knowledge-Constrained Self Training,0.09863477945327759,#17becf
-4.51874,-16.34142,neighbor,740063,On the role of tracking in stationary environments,0.09887760877609253,#17becf
-12.589243,0.8130818,neighbor,740063,"Statistical Translation, Heat Kernels and Expected Distances",0.0989721417427063,#17becf
13.611947,3.1708806,neighbor,740063,Interactive Learning Using Manifold Geometry,0.09945231676101685,#17becf
-2.6657155,5.9912214,neighbor,740063,An empirical comparison of supervised learning algorithms,0.09959495067596436,#17becf
-5.0247073,1.0194765,neighbor,740063,On Information Regularization,0.09972995519638062,#17becf
-9.216191,4.3029366,neighbor,740063,Learning the unified kernel machines for classification,0.10025560855865479,#17becf
8.521625,4.540673,neighbor,740063,Text Categorization from Category Name via Lexical Reference,0.1005479097366333,#17becf
-8.14954,8.861199,neighbor,740063,A family of large margin linear classifiers and its application in dynamic environments,0.10116547346115112,#17becf
3.6789694,11.763005,neighbor,740063,Uncertainty Reduction in Collaborative Bootstrapping: Measure and Algorithm,0.10131752490997314,#17becf
6.7969823,-15.888009,neighbor,740063,Efficient Cross-Domain Classification of Weblogs,0.10175102949142456,#17becf
-8.510986,-0.10303102,neighbor,740063,A dependence maximization view of clustering,0.10177582502365112,#17becf
-10.038095,3.512925,neighbor,740063,A Semi-Supervised Framework for Feature Mapping and Multiclass Classification,0.10181301832199097,#17becf
12.264499,-4.2102323,neighbor,740063,Multi-Label Learning with Weak Label,0.10207533836364746,#17becf
-7.8809023,5.541335,neighbor,740063,Kernels and Ensembles,0.10224652290344238,#17becf
-1.6952173,6.0080776,neighbor,740063,Data mining in metric space: an empirical analysis of supervised learning performance criteria,0.102558434009552,#17becf
9.328194,-0.95806783,neighbor,740063,Learning from Multiple Outlooks,0.10256928205490112,#17becf
-8.596944,-1.8352866,neighbor,740063,A General Model for Multiple View Unsupervised Learning,0.10279250144958496,#17becf
-0.2080878,0.6633551,neighbor,740063,A data-dependent distance measure for transductive instance-based learning,0.10281693935394287,#17becf
14.067392,-0.62776214,neighbor,740063,Multi-parametric solution-path algorithm for instance-weighted support vector machines,0.10289782285690308,#17becf
-0.7248609,12.933293,neighbor,740063,Robust multi-class boosting,0.10298293828964233,#17becf
11.895267,-8.920403,neighbor,740063,Client–Server Multitask Learning From Distributed Datasets,0.10302704572677612,#17becf
-4.3747964,3.3782732,neighbor,740063,Adaptive Sparseness for Supervised Learning,0.10309797525405884,#17becf
-3.0696998,-6.156442,neighbor,740063,Query Expansion with the Minimum User Feedback by Transductive Learning,0.10323667526245117,#17becf
-3.5993123,7.1822696,neighbor,740063,A bias correction for the minimum error rate in cross-validation,0.10328280925750732,#17becf
-6.5379515,3.2374666,neighbor,740063,Supervised Machine Learning with a Novel Pointwise Density Estimator,0.10356026887893677,#17becf
0.5062807,13.948199,neighbor,740063,Adaptive clustering ensembles,0.1041039228439331,#17becf
2.0257657,6.3928924,neighbor,740063,An interactive algorithm for asking and incorporating feature feedback into support vector machines,0.1043163537979126,#17becf
3.8817778,9.794629,neighbor,740063,Exploiting unlabeled data to enhance ensemble diversity,0.10433304309844971,#17becf
-11.778481,-1.539053,neighbor,740063,Semi-supervised distance metric learning for collaborative image retrieval and clustering,0.104381263256073,#17becf
-6.257018,9.437618,neighbor,740063,Adaptive Bayesian network classifiers,0.10495728254318237,#17becf
9.163902,-2.26019,neighbor,740063,Multi-View Learning over Structured and Non-Identical Outputs,0.10496211051940918,#17becf
0.24363607,2.7458124,neighbor,740063,Types of Cost in Inductive Concept Learning,0.10496854782104492,#17becf
3.4860754,8.522728,neighbor,740063,Improving Semi-Supervised Support Vector Machines Through Unlabeled Instances Selection,0.10497701168060303,#17becf
-13.046381,-2.3303337,neighbor,740063,Regularization in Matrix Relevance Learning,0.10500651597976685,#17becf
-3.2253225,2.3110094,neighbor,740063,Surrogate Learning - An Approach for Semi-Supervised Classification,0.10512387752532959,#17becf
-5.512281,-16.030466,neighbor,740063,Correction: Temporal-Difference Reinforcement Learning with Distributed Representations,0.10517603158950806,#17becf
-0.5009104,5.0760093,neighbor,740063,Generalized skewing for functions with continuous and nominal attributes,0.10519671440124512,#17becf
11.18889,-3.2108104,neighbor,740063,Semi-Supervised Dimension Reduction for Multi-Label Classification,0.10529500246047974,#17becf
0.016349038,9.677296,neighbor,740063,Boosting to correct inductive bias in text classification,0.10547351837158203,#17becf
-10.678803,-5.5218663,neighbor,740063,On Large Margin Hierarchical Classification With Multiple Paths,0.10568588972091675,#17becf
-8.548753,-14.507604,neighbor,740063,Statistical Mechanics of Time-Domain Ensemble Learning(General),0.10577571392059326,#17becf
-10.553504,-6.9904,neighbor,740063,Learning hierarchical multi-category text classification models,0.10578608512878418,#17becf
-6.4569197,-7.417799,neighbor,740063,Optimizing Linear and Quadratic Data Transformations for Classification Tasks,0.10591721534729004,#17becf
-2.7195435,-1.0887064,neighbor,740063,On the Relationship between the Posterior and Optimal Similarity,0.10603678226470947,#17becf
-6.1238656,6.086849,neighbor,740063,A Nonconformity Approach to Model Selection for SVMs,0.10611969232559204,#17becf
1.2014625,4.287054,neighbor,740063,Generalized and Heuristic-Free Feature Construction for Improved Accuracy,0.10622268915176392,#17becf
-4.9454494,-0.6026295,neighbor,740063,Query Learning with Exponential Query Costs,0.10641622543334961,#17becf
0.6272409,-2.417346,neighbor,740063,Generative Prior Knowledge for Discriminative Classification,0.10643595457077026,#17becf
-9.396202,0.75611454,neighbor,740063,Learning Isometric Separation Maps,0.10652559995651245,#17becf
-2.0191553,11.702588,neighbor,740063,Fast ABC-Boost for Multi-Class Classification,0.10653018951416016,#17becf
5.943291,1.1598091,neighbor,740063,Coaxing Confidences from an Old Friend: Probabilistic Classifications from Transformation Rule Lists,0.10670125484466553,#17becf
3.9088557,-2.9829323,neighbor,740063,Reading to Learn: Constructing Features from Semantic Abstracts,0.10677188634872437,#17becf
-10.845109,-7.619138,neighbor,740063,Hierarchical Multi-Label Text Categorization with Global Margin Maximization,0.10691499710083008,#17becf
-11.222693,6.356501,neighbor,740063,Large-Margin kNN Classification Using a Deep Encoder Network,0.10719150304794312,#17becf
5.7753305,0.72619075,neighbor,740063,Coaxing Confidences from an Old Freind: Probabilistic Classifications from Transformation Rule Lists,0.10747665166854858,#17becf
-2.191779,13.418326,neighbor,740063,Boosting Through Optimization of Margin Distributions,0.10763770341873169,#17becf
-2.4033816,-7.3687487,neighbor,740063,Learning to rank only using training data from related domain,0.10768723487854004,#17becf
-3.934394,11.725388,neighbor,740063,"Comment: Boosting Algorithms: Regularization, Prediction and Model Fitting",0.1077800989151001,#17becf
-2.3212478,9.515901,neighbor,740063,Obtaining Calibrated Probabilities from Boosting,0.10779500007629395,#17becf
-5.385826,-3.1700048,neighbor,740063,MedLDA: maximum margin supervised topic models for regression and classification,0.10782438516616821,#17becf
12.7805195,-2.2464247,neighbor,740063,Non-I.I.D. Multi-Instance Dimensionality Reduction by Learning a Maximum Bag Margin Subspace,0.10794508457183838,#17becf
7.013834,-2.6990225,neighbor,740063,Bootstrapping Coreference Classifiers with Multiple Machine Learning Algorithms,0.10819309949874878,#17becf
1.980011,11.501508,neighbor,740063,A mixture-of-experts framework for text classification,0.10824847221374512,#17becf
-4.930672,-10.73493,neighbor,740063,Robustness and generalization,0.10827583074569702,#17becf
-9.030181,-14.521947,neighbor,740063,Statistical Mechanics of Linear and Nonlinear Time-Domain Ensemble Learning,0.10833948850631714,#17becf
2.7913694,-6.761999,query,7624311,"Software survey: VOSviewer, a computer program for bibliometric mapping",0.0,#9edae5
1.4653742,-7.783681,neighbor,7624311,El análisis de co-citas como método de investigación en Bibliotecología y Ciencia de la Información,0.07032406330108643,#9edae5
4.0903087,-2.4788852,neighbor,7624311,Visual overviews for discovering key papers and influences across research fronts,0.07791537046432495,#9edae5
4.43849,-5.440812,neighbor,7624311,Clickstream Data Yields High-Resolution Maps of Science,0.07937783002853394,#9edae5
3.2702925,-9.099821,neighbor,7624311,Automatic term identification for bibliometric mapping,0.08010363578796387,#9edae5
5.409232,-3.2474306,neighbor,7624311,Mapping topics and topic bursts in PNAS,0.08115029335021973,#9edae5
5.6368184,-12.787607,neighbor,7624311,Clustering of scientific fields by integrating text mining and bibliometrics,0.08595746755599976,#9edae5
-0.35146984,-6.7151365,neighbor,7624311,Análisis de la visibilidad internacional de la producción científica argentina en las bases de datos SSCI y A&HCI en la década de 1990-2000: estudio bibliométrico,0.08875405788421631,#9edae5
1.485137,15.383234,neighbor,7624311,Gephi: An Open Source Software for Exploring and Manipulating Networks,0.09118419885635376,#9edae5
-0.2747167,18.151674,neighbor,7624311,VANLO - Interactive visual exploration of aligned biological networks,0.09171223640441895,#9edae5
4.5731974,-13.3932085,neighbor,7624311,Semantic relatedness hits bibliographic data,0.09199899435043335,#9edae5
-0.14141819,-8.0732565,neighbor,7624311,"Maps on the basis of the Arts & Humanities Citation Index: The journals Leonardo and Art Journal versus ""Digital Humanities"" as a topic",0.09247535467147827,#9edae5
1.7265973,-10.822671,neighbor,7624311,Co-occurrence matrices and their applications in information science: Extending ACA to the Web environment,0.09377312660217285,#9edae5
-8.212255,11.677845,neighbor,7624311,"Auxilary Cartographic Functions in R: North Arrow, Scale Bar, and Label with Leader Arrow",0.09389209747314453,#9edae5
8.245025,-3.2856104,neighbor,7624311,Dynamic animations of journal maps: Indicators of structural changes and interdisciplinary developments,0.09391391277313232,#9edae5
3.9524415,13.672864,neighbor,7624311,Integrating statistics and visualization: case studies of gaining clarity during exploratory data analysis,0.09437239170074463,#9edae5
1.9340205,16.493135,neighbor,7624311,ZAME: Interactive Large-Scale Graph Visualization,0.09532642364501953,#9edae5
5.3317347,-15.516682,neighbor,7624311,Mapping subsets of scholarly information,0.09593367576599121,#9edae5
0.76428014,0.2808699,neighbor,7624311,An Introduction to the Dataverse Network as an Infrastructure for Data Sharing,0.0959852933883667,#9edae5
4.6720924,10.512893,neighbor,7624311,Taxonomías de la visualización de información,0.09699010848999023,#9edae5
2.4410608,-2.4025266,neighbor,7624311,"myADS-arXiv - a Tailor-Made, Open Access, Virtual Journal",0.09783977270126343,#9edae5
-4.6527457,-3.6153135,neighbor,7624311,Concierge: Personal Database Software for Managing Digital Research Resources,0.09826326370239258,#9edae5
12.784659,-8.71627,neighbor,7624311,A new approach to analyzing patterns of collaboration in co-authorship networks: mesoscopic analysis and interpretation,0.0984351634979248,#9edae5
8.097285,-15.654749,neighbor,7624311,Measuring conference quality by mining program committee characteristics,0.09928387403488159,#9edae5
7.70713,-7.545272,neighbor,7624311,Journal status,0.09943389892578125,#9edae5
-1.5615635,13.794983,neighbor,7624311,Visualizing Geographic Information: VisualPoints vs CartoDraw,0.09944260120391846,#9edae5
3.5961015,16.700006,neighbor,7624311,"GMine: a system for scalable, interactive graph visualization and mining",0.10009676218032837,#9edae5
3.3021781,14.750223,neighbor,7624311,MatrixExplorer: a Dual-Representation System to Explore Social Networks,0.10028642416000366,#9edae5
6.2260466,-11.205861,neighbor,7624311,Phenomenological Approach to Profile Impact of Scientific Research: citation Mining,0.10082948207855225,#9edae5
-2.8869622,11.147165,neighbor,7624311,Graphics of Large Datasets: Visualizing a Million,0.1009836196899414,#9edae5
6.1966357,10.618314,neighbor,7624311,Sistemas de visualización para bibliotecas digitales,0.101412832736969,#9edae5
-0.4628559,13.562366,neighbor,7624311,Voronoi treemaps,0.10169392824172974,#9edae5
1.662682,1.7968696,neighbor,7624311,Web 2.0 and official statistics: The case for a multi-disciplinary approach,0.10225868225097656,#9edae5
-3.5511189,-2.4442482,neighbor,7624311,MyLibrary: A Digital Library Framework and Toolkit,0.10248255729675293,#9edae5
5.1852107,-1.5561919,neighbor,7624311,Towards Visual Exploration of Topic Shifts,0.1026146411895752,#9edae5
-4.735464,12.887666,neighbor,7624311,Automating the Creation of Interactive Glyph-supplemented Scatterplots for Visualizing Algorithm Results,0.10284304618835449,#9edae5
12.048435,-11.736221,neighbor,7624311,Mapping the Evolution of Scientific Fields,0.10321247577667236,#9edae5
8.951712,-14.257074,neighbor,7624311,TopSeer: A Novel Scholar Search Engine based on Community Detection in Citation Network,0.10367673635482788,#9edae5
-6.420147,-4.963847,neighbor,7624311,Automated extraction of chemical structure information from digital raster images,0.10422152280807495,#9edae5
-0.772129,15.022732,neighbor,7624311,Fast Point-Feature Label Placement for Dynamic Visualizations,0.10485285520553589,#9edae5
-1.4928335,-5.392369,neighbor,7624311,Un programa nacional de acceso a la información científica,0.10490047931671143,#9edae5
3.7879217,7.0239472,neighbor,7624311,Metadata visualisation with VisMeB,0.10514283180236816,#9edae5
-3.3336067,5.365061,neighbor,7624311,Optimizing earth science decision-making through technology integration,0.10554587841033936,#9edae5
-4.838009,0.038159396,neighbor,7624311,CARACTERIZAÇÃO DO CIBERGÊNERO HOME PAGE CORPORATIVA OU INSTITUCIONAL,0.10555797815322876,#9edae5
1.1415504,-11.664874,neighbor,7624311,Co-word Analysis using the Chinese Character Set,0.1055956482887268,#9edae5
-4.696382,-6.194842,neighbor,7624311,scriptLattes: an open-source knowledge extraction system from the Lattes platform,0.10576081275939941,#9edae5
13.305882,-10.422766,neighbor,7624311,The structure of scientific collaboration networks.,0.10579705238342285,#9edae5
0.46317798,11.407402,neighbor,7624311,Interactive Data Visualization using Mondrian,0.10585194826126099,#9edae5
-7.4046073,11.766551,neighbor,7624311,Proportional Symbol Mapping in R,0.10588473081588745,#9edae5
8.323622,3.327343,neighbor,7624311,Literature Fingerprinting: A New Method for Visual Literary Analysis,0.10600566864013672,#9edae5
0.24328367,18.951904,neighbor,7624311,2.5D Visualisation of Overlapping Biological Networks,0.10607808828353882,#9edae5
9.38284,-7.6366863,neighbor,7624311,Performance-related differences of bibliometric statistical properties of research groups: Cumulative advantages and hierarchically layered networks,0.10611826181411743,#9edae5
-1.6338509,2.819309,neighbor,7624311,"Counting Cows and Cabbages - Web-bases Extraction, Delivery and Discovery of Geo-Referenced Data",0.10646039247512817,#9edae5
-1.761201,17.69823,neighbor,7624311,A novel method for large tree visualization,0.10647547245025635,#9edae5
2.0636744,-12.624552,neighbor,7624311,"Similarity Measures, Author Cocitation Analysis, and Information Theory",0.10648071765899658,#9edae5
-5.315442,9.519784,neighbor,7624311,A Video Tour through ViSta 6.4,0.10652327537536621,#9edae5
-11.888332,0.67575175,neighbor,7624311,MiSearch adaptive pubMed search tool,0.10666340589523315,#9edae5
-8.340673,16.133783,neighbor,7624311,The NeuARt II system: a viewing tool for neuroanatomical data based on published neuroanatomical atlases,0.10731238126754761,#9edae5
5.956386,8.622359,neighbor,7624311,MedioVis: visual information seeking in digital libraries,0.10738950967788696,#9edae5
-8.052103,7.168312,neighbor,7624311,Importing Vector Graphics: The grImport Package for R,0.1077844500541687,#9edae5
2.73635,19.339556,neighbor,7624311,Visualizing Global Patterns in Huge Digraphs,0.10798794031143188,#9edae5
-0.13028125,8.914128,neighbor,7624311,Visualization for Periodic Population Movement between Distinct Localities,0.10820531845092773,#9edae5
-5.248659,-0.94359547,neighbor,7624311,Online Resources,0.10822939872741699,#9edae5
-9.388169,-4.2381473,neighbor,7624311,Turkish Journal of Chemistry’nin Bibliyometrik Analizi,0.10824263095855713,#9edae5
-6.5283957,-13.061357,neighbor,7624311,Productivity and impact of astronomical facilities: A statistical study of publications and citations,0.10853058099746704,#9edae5
-7.2184916,-0.133871,neighbor,7624311,Content,0.10857492685317993,#9edae5
-8.812015,3.1172426,neighbor,7624311,A Handbook of Statistical Analyses Using R,0.10869479179382324,#9edae5
-2.3219728,8.06463,neighbor,7624311,Design and Usability of an Enhanced Geographic Information System for Exploration of Multivariate Health Statistics,0.10871142148971558,#9edae5
-8.834805,0.37645683,neighbor,7624311,Reviewers,0.10880005359649658,#9edae5
13.450132,-11.485956,neighbor,7624311,ANALYSIS AND MODELING OF SCIENCE COLLABORATION NETWORKS,0.10890424251556396,#9edae5
9.080397,-9.171919,neighbor,7624311,The citation impacts and citation environments of Chinese journals in mathematics,0.10929679870605469,#9edae5
-9.805674,-5.0048594,neighbor,7624311,İran ve Türkiye’nin Dünya Bilgibilim Literatürüne Katkıları: Karşılaştırmalı Bir Çalışma,0.10935115814208984,#9edae5
-8.16179,1.5029447,neighbor,7624311,Reviewers,0.10958480834960938,#9edae5
9.263064,3.0374105,neighbor,7624311,What's being said near “Martha”? Exploring name entities in literary text collections,0.10966581106185913,#9edae5
-9.396433,4.6546884,neighbor,7624311,Computing Effect Size Measures with ViSta - The Visual Statistics System,0.10971677303314209,#9edae5
7.214491,-8.172834,neighbor,7624311,Journal ranking based on social information,0.1097460389137268,#9edae5
-9.5772,1.2422842,neighbor,7624311,Reviewers,0.10984623432159424,#9edae5
-10.242204,-0.6969071,neighbor,7624311,Author Index,0.11001992225646973,#9edae5
-9.303229,-1.2180469,neighbor,7624311,Author Index,0.11001992225646973,#9edae5
5.9044046,-17.094923,neighbor,7624311,Using Citations to Generate surveys of Scientific Paradigms,0.11015588045120239,#9edae5
2.2842782,-14.870568,neighbor,7624311,The boomerang effect: retrieving scientific documents via the network of references and citations,0.11029064655303955,#9edae5
-4.141089,16.147114,neighbor,7624311,network: A Package for Managing Relational Data in R,0.11050671339035034,#9edae5
4.6010327,-16.854336,neighbor,7624311,Automatic acknowledgement indexing: expanding the semantics of contribution in the CiteSeer digital library,0.11053091287612915,#9edae5
1.4128959,-15.607991,neighbor,7624311,An evaluation of Bradfordizing Effects,0.11061030626296997,#9edae5
14.398276,-9.888562,neighbor,7624311,Structure and Dynamics of Research Collaboration in Computer Science,0.11071068048477173,#9edae5
3.050442,7.957547,neighbor,7624311,Visualising official statistics,0.11072874069213867,#9edae5
-2.4339933,-1.100388,neighbor,7624311,Building Pathfinders with Free Screen Capture Tools,0.11075663566589355,#9edae5
-6.0674567,-13.377173,neighbor,7624311,Telescopes in the mirror of scientometrics,0.11075937747955322,#9edae5
1.8803296,11.468404,neighbor,7624311,Browsing through an information visualization design space,0.11085838079452515,#9edae5
-1.7919317,1.6350999,neighbor,7624311,Sherlock: A Web Magnifying Glass For Microdata Files,0.11087185144424438,#9edae5
2.526602,5.0304933,neighbor,7624311,Topic maps for custom viewing of data,0.11101287603378296,#9edae5
3.2962625,17.600918,neighbor,7624311,SuperGraph Visualization,0.11132872104644775,#9edae5
14.062121,-8.12153,neighbor,7624311,A network approach for the scientific collaboration in the European Framework Programs,0.1114201545715332,#9edae5
6.5334077,6.3990197,neighbor,7624311,Discovering Knowledge Through Visual Analysis,0.11148488521575928,#9edae5
-1.5716499,9.023082,neighbor,7624311,Geovisualization to support the exploration of large health and demographic survey data,0.11151212453842163,#9edae5
-0.36630645,21.181442,neighbor,7624311,Extending Taxonomic Visualisation to Incorporate Synonymy and Structural Markers,0.11158877611160278,#9edae5
3.747846,9.496256,neighbor,7624311,Editors' introduction,0.11161148548126221,#9edae5
8.745871,-13.2685995,neighbor,7624311,Research Community Mining with Topic Identification,0.11172366142272949,#9edae5
-1.8377205,19.732891,neighbor,7624311,Visualising very large phylogenetic trees in three dimensional hyperbolic space,0.11175030469894409,#9edae5
6.869871,7.679826,neighbor,7624311,Challenges of evaluating the information visualisation experience,0.11177921295166016,#9edae5
-3.8749347,11.469715,neighbor,7624311,Lattice: Multivariate Data Visualization with R,0.11181586980819702,#9edae5
5.1896267,-3.6202815,query,786357,"YOLO9000: Better, Faster, Stronger",0.0,#9edae5
6.8975224,-3.5004065,neighbor,786357,Fast R-CNN,0.04204452037811279,#9edae5
-0.14580093,1.627224,neighbor,786357,LocNet: Improving Localization Accuracy for Object Detection,0.042798757553100586,#9edae5
2.3418322,-0.84514993,neighbor,786357,"You Only Look Once: Unified, Real-Time Object Detection",0.044824182987213135,#9edae5
-4.544129,-6.983853,neighbor,786357,"Scalable, High-Quality Object Detection",0.04611486196517944,#9edae5
8.1553545,-4.506177,neighbor,786357,Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors,0.04801130294799805,#9edae5
5.058826,-1.7190505,neighbor,786357,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.04829251766204834,#9edae5
-6.4824576,-7.2059383,neighbor,786357,Training Region-Based Object Detectors with Online Hard Example Mining,0.04995620250701904,#9edae5
-4.143387,-4.9103155,neighbor,786357,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.05042153596878052,#9edae5
4.3020287,0.14357482,neighbor,786357,Feature Pyramid Networks for Object Detection,0.05113875865936279,#9edae5
-2.6938853,-2.081944,neighbor,786357,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.05125391483306885,#9edae5
6.6514416,-4.964219,neighbor,786357,"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",0.051319420337677,#9edae5
-2.8379526,1.9160367,neighbor,786357,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.051853060722351074,#9edae5
6.4294286,-6.275307,neighbor,786357,PVANet: Lightweight Deep Neural Networks for Real-time Object Detection,0.05237746238708496,#9edae5
3.3794823,1.1458806,neighbor,786357,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.05268967151641846,#9edae5
5.9428043,-6.3458905,neighbor,786357,PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection,0.053899526596069336,#9edae5
-9.257214,-2.1805224,neighbor,786357,Object Detection Networks on Convolutional Feature Maps,0.05443918704986572,#9edae5
2.66548,-2.4364998,neighbor,786357,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.05492764711380005,#9edae5
1.3593241,0.78622353,neighbor,786357,Scalable Object Detection Using Deep Neural Networks,0.05580461025238037,#9edae5
-5.069452,5.1677327,neighbor,786357,Mid-level Elements for Object Detection,0.05592268705368042,#9edae5
-4.959527,-8.310295,neighbor,786357,What Makes for Effective Detection Proposals?,0.056420326232910156,#9edae5
6.975772,-1.2568715,neighbor,786357,Beyond Skip Connections: Top-Down Modulation for Object Detection,0.05673331022262573,#9edae5
0.5857447,9.789485,neighbor,786357,We Don’t Need No Bounding-Boxes: Training Object Class Detectors Using Only Human Verification,0.056932926177978516,#9edae5
5.786757,-7.76498,neighbor,786357,Recycle deep features for better object detection,0.05975210666656494,#9edae5
8.750742,-1.762858,neighbor,786357,Going deeper with convolutions,0.06028258800506592,#9edae5
-5.2407203,-8.933313,neighbor,786357,"How good are detection proposals, really?",0.06041902303695679,#9edae5
-4.559113,-4.276956,neighbor,786357,Keypoint Density-based Region Proposal for Fine-Grained Object Detection and Classification using Regions with Convolutional Neural Network Features,0.060782015323638916,#9edae5
4.0489144,-9.482861,neighbor,786357,Training a convolutional neural network for multi-class object detection using solely virtual world data,0.060979366302490234,#9edae5
-2.3710303,1.6640235,neighbor,786357,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.061361849308013916,#9edae5
-6.313734,7.222818,neighbor,786357,A Classification Leveraged Object Detector,0.061939358711242676,#9edae5
7.551019,4.0325775,neighbor,786357,1-HKUST: Object Detection in ILSVRC 2014,0.061968088150024414,#9edae5
0.9753879,3.3101768,neighbor,786357,ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks,0.062348902225494385,#9edae5
3.146399,-9.792679,neighbor,786357,Deep learning for class-generic object detection,0.06256103515625,#9edae5
-0.436284,-11.598034,neighbor,786357,LSDA: Large Scale Detection through Adaptation,0.06298786401748657,#9edae5
6.4728494,1.3028984,neighbor,786357,Crafting GBD-Net for Object Detection,0.0644499659538269,#9edae5
-6.688281,-4.9174547,neighbor,786357,R-CNN minus R,0.06465214490890503,#9edae5
-1.2891839,0.8398329,neighbor,786357,Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction,0.06528520584106445,#9edae5
-3.128682,-9.143799,neighbor,786357,End-to-End Training of Object Class Detectors for Mean Average Precision,0.06531643867492676,#9edae5
-2.6112509,-0.87629527,neighbor,786357,Object Detection via End-to-End Integration of Aspect Ratio and Context Aware Part-based Models and Fully Convolutional Networks,0.06550490856170654,#9edae5
-1.7614446,5.3023634,neighbor,786357,Co-localization with Category-Consistent Features and Geodesic Distance Propagation,0.06650781631469727,#9edae5
-6.1395135,-2.8615923,neighbor,786357,DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers,0.06653481721878052,#9edae5
-0.21852304,10.976718,neighbor,786357,Self Paced Deep Learning for Weakly Supervised Object Detection,0.06703805923461914,#9edae5
0.5672068,-0.17287354,neighbor,786357,Action-Driven Object Detection with Top-Down Visual Attentions,0.06736975908279419,#9edae5
-7.6545463,-3.9955661,neighbor,786357,G-CNN: An Iterative Grid Based Object Detector,0.06812930107116699,#9edae5
0.24716203,7.699452,neighbor,786357,Self-taught object localization with deep networks,0.06835848093032837,#9edae5
2.4153783,-3.1104345,neighbor,786357,UnitBox: An Advanced Object Detection Network,0.06839889287948608,#9edae5
-3.412502,-5.91629,neighbor,786357,HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection,0.06847262382507324,#9edae5
-5.2246695,-9.992722,neighbor,786357,Diagnosing state-of-the-art object proposal methods,0.06853467226028442,#9edae5
3.4964018,11.326605,neighbor,786357,Watch and learn: Semi-supervised learning of object detectors from videos,0.06854218244552612,#9edae5
-5.684596,0.15781948,neighbor,786357,CRAFT Objects from Images,0.06875193119049072,#9edae5
6.5326815,8.345394,neighbor,786357,ARTOS - Adaptive Real-Time Object Detection System,0.06972885131835938,#9edae5
9.572276,-1.1561298,neighbor,786357,Deep Residual Learning for Image Recognition,0.07000517845153809,#9edae5
-3.5811293,2.9187112,neighbor,786357,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.0700945258140564,#9edae5
5.7399716,13.49491,neighbor,786357,T-CNN: Tubelets With Convolutional Neural Networks for Object Detection From Videos,0.07022374868392944,#9edae5
-8.571415,1.2101085,neighbor,786357,StuffNet: Using ‘Stuff’ to Improve Object Detection,0.07045763731002808,#9edae5
-4.487083,8.828103,neighbor,786357,Ensemble of exemplar-SVMs for object detection and beyond,0.07079660892486572,#9edae5
6.5652995,-8.890387,neighbor,786357,Do More Dropouts in Pool5 Feature Maps for Better Object Detection,0.0709502100944519,#9edae5
8.691317,3.7111473,neighbor,786357,ImageNet Large Scale Visual Recognition Challenge,0.07098996639251709,#9edae5
3.0723464,1.7339749,neighbor,786357,Attentive Contexts for Object Detection,0.07099103927612305,#9edae5
-4.7303095,-2.8781004,neighbor,786357,Efficient object detection for high resolution images,0.0710572600364685,#9edae5
9.839831,-9.39361,neighbor,786357,"A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning",0.07106620073318481,#9edae5
6.023628,13.684747,neighbor,786357,Object Detection from Video Tubelets with Convolutional Neural Networks,0.07134908437728882,#9edae5
-3.2191854,7.5421896,neighbor,786357,Context Forest for efficient object detection with large mixture models,0.0717318058013916,#9edae5
-3.261936,10.19651,neighbor,786357,Fast self-supervised on-line training for object recognition specifically for robotic applications,0.07187414169311523,#9edae5
6.9100766,4.493072,neighbor,786357,Window-Object Relationship Guided Representation Learning for Generic Object Detections,0.07214593887329102,#9edae5
0.72805476,9.173126,neighbor,786357,Weakly Supervised Object Localization with Multi-Fold Multiple Instance Learning,0.07243496179580688,#9edae5
3.1879323,-12.27544,neighbor,786357,Factors in Finetuning Deep Model for Object Detection with Long-Tail Distribution,0.07256871461868286,#9edae5
2.0439131,-9.091652,neighbor,786357,Generic Object Detection with Dense Neural Patterns and Regionlets,0.07274556159973145,#9edae5
3.3095229,-13.221984,neighbor,786357,Object-centric Sampling for Fine-grained Image Classification,0.07274621725082397,#9edae5
-2.4773188,-4.6944942,neighbor,786357,Scale-Aware Pixelwise Object Proposal Networks,0.0729101300239563,#9edae5
-3.6205912,0.9622069,neighbor,786357,Learning to decompose for object detection and instance segmentation,0.07310295104980469,#9edae5
-0.07923188,-1.0324283,neighbor,786357,AttentionNet: Aggregating Weak Directions for Accurate Object Detection,0.07321619987487793,#9edae5
-5.876177,-4.9307485,neighbor,786357,Relief R-CNN: Utilizing Convolutional Features for Fast Object Detection,0.07356208562850952,#9edae5
-6.294041,8.418426,neighbor,786357,Detect2Rank: Combining Object Detectors Using Learning to Rank,0.07382339239120483,#9edae5
-1.8015918,-4.2010193,neighbor,786357,Subcategory-Aware Convolutional Neural Networks for Object Proposals and Detection,0.07421320676803589,#9edae5
-8.742027,-8.3248415,neighbor,786357,An active search strategy for efficient object class detection,0.07438534498214722,#9edae5
8.150713,-7.081245,neighbor,786357,Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection,0.07440584897994995,#9edae5
10.753546,-1.4332093,neighbor,786357,Rethinking the Inception Architecture for Computer Vision,0.07441246509552002,#9edae5
9.427727,-2.4271011,neighbor,786357,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.07472801208496094,#9edae5
-8.575201,4.9250693,neighbor,786357,Parallelized deformable part models with effective hypothesis pruning,0.07486516237258911,#9edae5
-6.3154855,5.8504357,neighbor,786357,Max-Margin Object Detection,0.07507705688476562,#9edae5
-4.598304,2.4756968,neighbor,786357,Learning to Segment Object Candidates,0.07510411739349365,#9edae5
2.3095639,-7.4619985,neighbor,786357,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.0751606822013855,#9edae5
4.7622833,12.754686,neighbor,786357,Context Matters: Refining Object Detection in Video with Recurrent Neural Networks,0.07521206140518188,#9edae5
2.4195359,3.6283925,neighbor,786357,Learning to detect and localize many objects from few examples,0.07531517744064331,#9edae5
3.2317047,9.798379,neighbor,786357,On learning to localize objects with minimal supervision,0.07533055543899536,#9edae5
-0.7801462,-2.9496865,neighbor,786357,DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization,0.07548165321350098,#9edae5
3.1180415,11.83642,neighbor,786357,Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection,0.07559037208557129,#9edae5
-5.0725217,0.7304752,neighbor,786357,Weakly Supervised Cascaded Convolutional Networks,0.07587385177612305,#9edae5
-7.3520412,9.524493,neighbor,786357,Joint learning for multi-class object detection,0.07598984241485596,#9edae5
11.75143,-5.6472425,neighbor,786357,Training a multi-exit cascade with linear asymmetric classification for efficient object detection,0.07615858316421509,#9edae5
-5.942024,2.3539348,neighbor,786357,Multistage Object Detection With Group Recursive Learning,0.07679468393325806,#9edae5
-6.441123,-2.0736482,neighbor,786357,DeepProposals: Hunting Objects and Actions by Cascading Deep Convolutional Layers,0.07693368196487427,#9edae5
3.6796303,6.304778,neighbor,786357,Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization,0.07694143056869507,#9edae5
-1.3670268,3.476731,neighbor,786357,Hypercolumns for object segmentation and fine-grained localization,0.07713109254837036,#9edae5
2.339017,-7.822773,neighbor,786357,DeepID-Net: Deformable deep convolutional neural networks for object detection,0.07713675498962402,#9edae5
1.2003655,4.3129034,neighbor,786357,ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization,0.07715725898742676,#9edae5
-7.346983,4.8025084,neighbor,786357,Regionlets for Generic Object Detection,0.07721740007400513,#9edae5
-6.408982,-10.408143,neighbor,786357,Object-Proposal Evaluation Protocol is ‘Gameable’,0.07728266716003418,#9edae5
-10.291458,6.9519444,neighbor,786357,Do We Need More Training Data?,0.0772860050201416,#9edae5
0.8578576,5.0697904,neighbor,786357,Learning Deep Features for Discriminative Localization,0.07740044593811035,#9edae5
-0.75667197,-0.2425947,query,8485068,Aggregated Residual Transformations for Deep Neural Networks,0.0,#9edae5
-1.7696208,0.6528033,neighbor,8485068,Deep Residual Learning for Image Recognition,0.038817644119262695,#9edae5
-0.32973468,1.5979455,neighbor,8485068,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.04923003911972046,#9edae5
4.170232,-3.63903,neighbor,8485068,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.05015522241592407,#9edae5
8.223025,-0.82969064,neighbor,8485068,Binarized Neural Networks on the ImageNet Classification Task,0.05067020654678345,#9edae5
0.41858837,-2.3926823,neighbor,8485068,DecomposeMe: Simplifying ConvNets for End-to-End Learning,0.05122941732406616,#9edae5
-3.239973,6.4208274,neighbor,8485068,Learning Identity Mappings with Residual Gates,0.051479876041412354,#9edae5
7.862265,-6.346761,neighbor,8485068,Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups,0.052559852600097656,#9edae5
-2.448911,1.6550239,neighbor,8485068,Deep Residual Networks with Exponential Linear Unit,0.0526200532913208,#9edae5
1.5906435,-2.4549606,neighbor,8485068,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.05396479368209839,#9edae5
-1.7084016,-1.7480593,neighbor,8485068,Deeply-Fused Nets,0.05454748868942261,#9edae5
1.3311387,-3.9751382,neighbor,8485068,Going deeper with convolutions,0.05463385581970215,#9edae5
8.191788,-2.0324852,neighbor,8485068,XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,0.05483722686767578,#9edae5
-4.2602243,2.1286771,neighbor,8485068,Resnet in Resnet: Generalizing Residual Architectures,0.05483967065811157,#9edae5
-2.5969214,2.8443809,neighbor,8485068,Deep Pyramidal Residual Networks,0.055780768394470215,#9edae5
-3.6685946,3.6961389,neighbor,8485068,Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks,0.05643022060394287,#9edae5
3.7945597,-5.4494996,neighbor,8485068,Refining Architectures of Deep Convolutional Neural Networks,0.0567660927772522,#9edae5
2.4917324,-4.0250382,neighbor,8485068,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.05679583549499512,#9edae5
0.5696306,-9.192159,neighbor,8485068,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",0.057144343852996826,#9edae5
-2.7958791,0.25978717,neighbor,8485068,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",0.05853086709976196,#9edae5
-8.20281,3.939319,neighbor,8485068,X-CNN: Cross-modal convolutional neural networks for sparse datasets,0.05891686677932739,#9edae5
-8.051303,1.4620799,neighbor,8485068,DisturbLabel: Regularizing CNN on the Loss Layer,0.058998167514801025,#9edae5
-4.4486947,3.3114474,neighbor,8485068,Residual Networks of Residual Networks: Multilevel Residual Networks,0.0591961145401001,#9edae5
-4.7152867,4.3357244,neighbor,8485068,Wide Residual Networks,0.05922895669937134,#9edae5
-5.2849064,-1.1944462,neighbor,8485068,The Shallow End: Empowering Shallower Deep-Convolutional Networks through Auxiliary Outputs,0.05952376127243042,#9edae5
4.181907,-2.8825421,neighbor,8485068,Enhanced image classification with a fast-learning shallow convolutional neural network,0.05992954969406128,#9edae5
9.632404,-0.37166128,neighbor,8485068,Binarized Neural Networks,0.059954941272735596,#9edae5
-1.3654442,5.6763544,neighbor,8485068,How far can we go without convolution: Improving fully-connected networks,0.05995738506317139,#9edae5
-10.921389,-5.382268,neighbor,8485068,Residual CNDS,0.06005585193634033,#9edae5
-0.70090455,-4.007521,neighbor,8485068,Xception: Deep Learning with Depthwise Separable Convolutions,0.060240328311920166,#9edae5
-0.099600874,2.6532104,neighbor,8485068,Parametric Exponential Linear Unit for Deep Convolutional Neural Networks,0.0604209303855896,#9edae5
1.1830691,-10.851326,neighbor,8485068,Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification,0.06045889854431152,#9edae5
-1.6879116,11.2427,neighbor,8485068,Committees of Deep Feedforward Networks Trained with Few Data,0.06101888418197632,#9edae5
10.243736,2.3019598,neighbor,8485068,Deep Fried Convnets,0.061040520668029785,#9edae5
-13.990147,-0.98800933,neighbor,8485068,Large-Margin kNN Classification Using a Deep Encoder Network,0.061552345752716064,#9edae5
-5.350191,5.633628,neighbor,8485068,Demystifying ResNet,0.061570703983306885,#9edae5
-3.8375323,6.5159106,neighbor,8485068,Identity Mappings in Deep Residual Networks,0.06158632040023804,#9edae5
10.085707,2.882381,neighbor,8485068,Fixed-Point Factorized Networks,0.06162375211715698,#9edae5
-3.7531612,-9.096366,neighbor,8485068,SimNets: A Generalization of Convolutional Networks,0.061751604080200195,#9edae5
-2.5160768,9.130473,neighbor,8485068,GradNets: Dynamic Interpolation Between Neural Architectures,0.061878740787506104,#9edae5
-10.648246,6.7610493,neighbor,8485068,Scheduled denoising autoencoders,0.062079012393951416,#9edae5
2.5889618,-2.2621057,neighbor,8485068,Rethinking the Inception Architecture for Computer Vision,0.06211930513381958,#9edae5
-10.735852,3.3290324,neighbor,8485068,Improving Deep Neural Networks with Probabilistic Maxout Units,0.062451183795928955,#9edae5
-9.006771,3.87519,neighbor,8485068,Swapout: Learning an ensemble of deep architectures,0.06263267993927002,#9edae5
3.3425105,9.295516,neighbor,8485068,How transferable are features in deep neural networks?,0.06265902519226074,#9edae5
-2.430437,-8.061355,neighbor,8485068,Group Equivariant Convolutional Networks,0.06270778179168701,#9edae5
9.834736,-1.8545882,neighbor,8485068,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,0.06278783082962036,#9edae5
4.0890183,-7.8552,neighbor,8485068,A HMAX with LLC for visual recognition,0.06297147274017334,#9edae5
5.435242,3.9816246,neighbor,8485068,Multi-scale Recognition with DAG-CNNs,0.0632469654083252,#9edae5
-2.917668,-4.2851872,neighbor,8485068,Scalable stacking and learning for building deep architectures,0.06336396932601929,#9edae5
1.9608086,-8.444285,neighbor,8485068,Learnable Pooling Regions for Image Classification,0.06367629766464233,#9edae5
1.7809703,-0.09685056,neighbor,8485068,Deep Epitomic Convolutional Neural Networks,0.06369435787200928,#9edae5
-8.816849,-0.11363961,neighbor,8485068,Diving deeper into mentee networks,0.06380623579025269,#9edae5
-10.817753,-3.3077524,neighbor,8485068,Deeply-Supervised Nets,0.06384092569351196,#9edae5
-0.23768875,-9.284347,neighbor,8485068,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.06425744295120239,#9edae5
4.0751624,-11.449797,neighbor,8485068,TI-POOLING: Transformation-Invariant Pooling for Feature Learning in Convolutional Neural Networks,0.06435102224349976,#9edae5
-5.5982847,8.631346,neighbor,8485068,Identity Matters in Deep Learning,0.06437933444976807,#9edae5
-1.4214616,-4.9567804,neighbor,8485068,Doubly Convolutional Neural Networks,0.06447374820709229,#9edae5
-2.0737119,-12.757762,neighbor,8485068,Hybrid multi-layer deep CNN/aggregator feature for image classification,0.06461697816848755,#9edae5
-5.7067704,9.354578,neighbor,8485068,Robust Large Margin Deep Neural Networks,0.0646849274635315,#9edae5
-4.515805,-5.6218033,neighbor,8485068,Convolutional Neural Fabrics,0.0647006630897522,#9edae5
-4.0508385,-9.315323,neighbor,8485068,Deep SimNets,0.06472063064575195,#9edae5
-5.485117,3.4807787,neighbor,8485068,Weighted residuals for very deep networks,0.06485337018966675,#9edae5
5.261955,1.3403878,neighbor,8485068,Towards Distortion-Predictable Embedding of Neural Networks,0.06502276659011841,#9edae5
-10.130572,-0.15357438,neighbor,8485068,Learning Compact Convolutional Neural Networks with Nested Dropout,0.06517654657363892,#9edae5
-11.426859,5.7804346,neighbor,8485068,Natural Neural Networks,0.06521081924438477,#9edae5
-11.558567,0.77126044,neighbor,8485068,Dropout Rademacher complexity of deep neural networks,0.06529158353805542,#9edae5
7.0495734,-2.2063112,neighbor,8485068,Local Binary Convolutional Neural Networks,0.06534731388092041,#9edae5
4.8637257,-11.24128,neighbor,8485068,Invariant backpropagation: how to train a transformation-invariant neural network,0.06537848711013794,#9edae5
7.10543,-6.485184,neighbor,8485068,Training CNNs with Low-Rank Filters for Efficient Image Classification,0.06553363800048828,#9edae5
11.664216,1.160681,neighbor,8485068,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size,0.06561022996902466,#9edae5
1.6956633,-7.723491,neighbor,8485068,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.0657915472984314,#9edae5
5.8836164,-4.111453,neighbor,8485068,Multi-column deep neural networks for image classification,0.06593078374862671,#9edae5
-7.3721366,-5.6216507,neighbor,8485068,Faster training of very deep networks via p-norm gates,0.06611675024032593,#9edae5
-10.657548,-4.2535534,neighbor,8485068,Training Deeper Convolutional Networks with Deep Supervision,0.06615948677062988,#9edae5
5.855859,5.0504375,neighbor,8485068,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.06653666496276855,#9edae5
-7.9969335,9.31968,neighbor,8485068,Representation Benefits of Deep Feedforward Networks,0.06660968065261841,#9edae5
-7.432228,6.3780746,neighbor,8485068,FractalNet: Ultra-Deep Neural Networks without Residuals,0.06661784648895264,#9edae5
1.1428255,6.3613415,neighbor,8485068,Network Morphism,0.0668979287147522,#9edae5
6.199393,6.1493435,neighbor,8485068,Feature Graph Architectures,0.06706726551055908,#9edae5
-6.5380745,-4.730048,neighbor,8485068,Highway Networks,0.06707638502120972,#9edae5
3.7100933,9.284248,neighbor,8485068,Transfer Learning Based on AdaBoost for Feature Selection from Multiple ConvNet Layer Features,0.06707984209060669,#9edae5
5.616058,-0.6476221,neighbor,8485068,Convolutional Kernel Networks,0.06712919473648071,#9edae5
11.545745,-2.2323968,neighbor,8485068,Adjustable Bounded Rectifiers: Towards Deep Binary Representations,0.06722038984298706,#9edae5
-6.713026,-0.615058,neighbor,8485068,DSD: Regularizing Deep Neural Networks with Dense-Sparse-Dense Training Flow,0.06723654270172119,#9edae5
4.750661,-1.6770357,neighbor,8485068,Feature Representation in Convolutional Neural Networks,0.06731700897216797,#9edae5
-5.7473373,-2.6481936,neighbor,8485068,Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks,0.06734263896942139,#9edae5
8.410345,-3.6015983,neighbor,8485068,MatConvNet: Convolutional Neural Networks for MATLAB,0.06737613677978516,#9edae5
-2.2929864,9.97618,neighbor,8485068,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,0.06738144159317017,#9edae5
-12.588056,-0.51530015,neighbor,8485068,Partitioning Large Scale Deep Belief Networks Using Dropout,0.06745666265487671,#9edae5
-6.6592107,-4.5744596,neighbor,8485068,Training Very Deep Networks,0.06749004125595093,#9edae5
2.1765125,3.1748707,neighbor,8485068,Do Convnets Learn Correspondence?,0.0674964189529419,#9edae5
6.674484,-7.4095955,neighbor,8485068,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.06767189502716064,#9edae5
11.78494,-6.5061064,neighbor,8485068,Network of Experts for Large-Scale Image Categorization,0.06778383255004883,#9edae5
-10.139012,3.1926866,neighbor,8485068,Maxout Networks,0.06786191463470459,#9edae5
-8.866861,-2.3970819,neighbor,8485068,Deep Clustered Convolutional Kernels,0.06789863109588623,#9edae5
-9.8225975,-1.3918364,neighbor,8485068,Post Training in Deep Learning with Last Kernel,0.06793475151062012,#9edae5
4.216227,-13.180924,neighbor,8485068,Geometry-Aware Deep Transform,0.06802237033843994,#9edae5
2.8697116,0.5191611,neighbor,8485068,Network In Network,0.06802403926849365,#9edae5
0.9887209,-5.800038,neighbor,8485068,About pyramid structure in convolutional neural networks,0.0680394172668457,#9edae5
10.42881,0.31017393,neighbor,8485068,Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations,0.06839191913604736,#9edae5
-1.138597,10.496948,query,9672033,Convolutional Neural Networks for Sentence Classification,0.0,#9edae5
-1.1937525,10.667648,neighbor,9672033,A Convolutional Neural Network for Modelling Sentences,0.03245675563812256,#9edae5
-7.7249064,4.9887004,neighbor,9672033,Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification,0.057625532150268555,#9edae5
-5.2055264,-7.9629574,neighbor,9672033,Adaptive Multi-Compositionality for Recursive Neural Models with Applications to Sentiment Analysis,0.05943739414215088,#9edae5
-7.9134493,4.609835,neighbor,9672033,Coooolll: A Deep Learning System for Twitter Sentiment Classification,0.06125819683074951,#9edae5
-4.714963,-7.0794573,neighbor,9672033,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,0.0613979697227478,#9edae5
-3.62107,-0.22502218,neighbor,9672033,Improvement of a Naive Bayes Sentiment Classifier Using MRS-Based Features,0.0653238296508789,#9edae5
-1.698823,13.158221,neighbor,9672033,Distributed Representations of Sentences and Documents,0.06656575202941895,#9edae5
-1.6288608,9.035006,neighbor,9672033,Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure,0.0672416090965271,#9edae5
0.055037603,11.107436,neighbor,9672033,Recurrent Convolutional Neural Networks for Discourse Compositionality,0.06895565986633301,#9edae5
-1.7410842,14.392644,neighbor,9672033,Semantic Vector Machines,0.07094013690948486,#9edae5
-0.6331936,12.627286,neighbor,9672033,"Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network",0.07124948501586914,#9edae5
-8.855744,0.94101727,neighbor,9672033,LT3: Sentiment Classification in User-Generated Content Using a Rich Feature Set,0.07185804843902588,#9edae5
-2.784407,-10.731553,neighbor,9672033,Multi-Domain Sentiment Relevance Classification with Automatic Representation Learning,0.07415604591369629,#9edae5
-7.3567996,1.5360086,neighbor,9672033,NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets,0.07472211122512817,#9edae5
-3.866603,-5.047095,neighbor,9672033,Dive deeper: Deep Semantics for Sentiment Analysis,0.07930558919906616,#9edae5
-2.183215,-6.644746,neighbor,9672033,A Statistical Parsing Framework for Sentiment Classification,0.07967162132263184,#9edae5
-0.58982897,-6.7196136,neighbor,9672033,Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization,0.07993918657302856,#9edae5
-9.20517,-0.14560673,neighbor,9672033,SemEval-2014 Task 9: Sentiment Analysis in Twitter,0.0800865888595581,#9edae5
-10.180097,3.8382442,neighbor,9672033,RTRGO: Enhancing the GU-MLT-LT System for Sentiment Analysis of Short Messages,0.08069217205047607,#9edae5
-5.172621,-6.474493,neighbor,9672033,An Empirical Study on the Effect of Negation Words on Sentiment,0.08078265190124512,#9edae5
-4.2644076,11.228098,neighbor,9672033,Natural Language Processing (Almost) from Scratch,0.08080494403839111,#9edae5
-8.358533,2.1483288,neighbor,9672033,Swiss-Chocolate: Sentiment Detection using Sparse SVMs and Part-Of-Speech n-Grams,0.08086144924163818,#9edae5
-7.3320613,-0.18664886,neighbor,9672033,GPLSI: Supervised Sentiment Analysis in Twitter using Skipgrams,0.080910325050354,#9edae5
-8.594558,0.2926315,neighbor,9672033,NRC-Canada-2014: Recent Improvements in the Sentiment Analysis of Tweets,0.08124220371246338,#9edae5
-1.9260304,-1.415503,neighbor,9672033,Sentiment Summarization: Evaluating and Learning User Preferences,0.08126199245452881,#9edae5
-12.520422,1.0157839,neighbor,9672033,CMUQ-Hybrid: Sentiment Classification By Feature Engineering and Parameter Tuning,0.08160507678985596,#9edae5
-7.0602355,-1.3363117,neighbor,9672033,Sentiment Analysis of Short Informal Texts,0.0828237533569336,#9edae5
1.991034,16.433065,neighbor,9672033,Deep learning,0.08362084627151489,#9edae5
-0.89325297,-7.6528416,neighbor,9672033,Adding Redundant Features for CRFs-based Sentence Sentiment Classification,0.0838821530342102,#9edae5
1.4121485,-4.7237,neighbor,9672033,Multi-aspect Sentiment Analysis with Topic Models,0.08425772190093994,#9edae5
-4.346559,1.5263215,neighbor,9672033,"Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis",0.0846867561340332,#9edae5
-2.0990443,2.068857,neighbor,9672033,Suicide Note Sentiment Classification: A Supervised Approach Augmented by Web Data,0.08475750684738159,#9edae5
-1.3653032,-3.6121688,neighbor,9672033,ReNew: A Semi-Supervised Framework for Generating Domain-Specific Lexicons and Sentiment Analysis,0.08478718996047974,#9edae5
-9.943014,0.99922484,neighbor,9672033,SU-FMI: System Description for SemEval-2014 Task 9 on Sentiment Analysis in Twitter,0.08491605520248413,#9edae5
-2.5224571,-2.7989874,neighbor,9672033,Determining the Sentiment of Opinions,0.08494675159454346,#9edae5
1.3655311,17.434969,neighbor,9672033,Do Deep Nets Really Need to be Deep?,0.0852975845336914,#9edae5
3.059577,8.765306,neighbor,9672033,Syntactic and Semantic Kernels for Short Text Pair Categorization,0.08614867925643921,#9edae5
-3.1284351,16.840918,neighbor,9672033,Factored Neural Language Models,0.0862969160079956,#9edae5
-3.876319,9.64659,neighbor,9672033,Simple Customization of Recursive Neural Networks for Semantic Relation Classification,0.08655118942260742,#9edae5
-6.1691184,-8.593968,neighbor,9672033,Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification,0.08656823635101318,#9edae5
4.0888357,-4.916524,neighbor,9672033,Blinov: Distributed Representations of Words for Aspect-Based Sentiment Analysis at SemEval 2014,0.08657598495483398,#9edae5
-11.165204,0.9522886,neighbor,9672033,NILC_USP: An Improved Hybrid System for Sentiment Analysis in Twitter Messages,0.0865788459777832,#9edae5
3.687891,7.643558,neighbor,9672033,Question classification using support vector machines,0.0866079330444336,#9edae5
-3.7939146,-11.287362,neighbor,9672033,Marginalized Denoising Autoencoders for Domain Adaptation,0.08687067031860352,#9edae5
-4.2778206,17.399153,neighbor,9672033,Efficient Estimation of Word Representations in Vector Space,0.08739769458770752,#9edae5
-6.5370417,16.430765,neighbor,9672033,Learning Multilingual Word Representations using a Bag-of-Words Autoencoder,0.08774876594543457,#9edae5
-9.527586,-2.002784,neighbor,9672033,SAIL: Sentiment Analysis using Semantic Similarity and Contrast Features,0.08778107166290283,#9edae5
-10.003271,0.22831692,neighbor,9672033,Senti.ue: Tweet Overall Sentiment Classification Approach for SemEval-2014 Task 9,0.08801412582397461,#9edae5
-8.065454,-2.2701457,neighbor,9672033,Columbia NLP: Sentiment Detection of Sentences and Subjective Phrases in Social Media,0.08840477466583252,#9edae5
-3.629341,16.011215,neighbor,9672033,A Classification Approach to Word Prediction,0.08841651678085327,#9edae5
3.241469,6.1047606,neighbor,9672033,Investigating Statistical Techniques for Sentence-Level Event Classification,0.08846580982208252,#9edae5
-5.5604506,0.7647073,neighbor,9672033,Classifying sentiment in microblogs: is brevity an advantage?,0.08860695362091064,#9edae5
-8.069298,-1.0780212,neighbor,9672033,Kea: Sentiment Analysis of Phrases Within Short Texts,0.08865952491760254,#9edae5
-10.594588,-0.5523987,neighbor,9672033,KUNLPLab:Sentiment Analysis on Twitter Data,0.08866721391677856,#9edae5
-0.65651006,5.0867805,neighbor,9672033,Automatic Detection and Analysis of Impressive Japanese Sentences Using Supervised Machine Learning,0.08867508172988892,#9edae5
2.7711134,15.853291,neighbor,9672033,The Attentive Perceptron,0.08891063928604126,#9edae5
-11.34604,-0.61266416,neighbor,9672033,AUEB: Two Stage Sentiment Analysis of Social Network Messages,0.08891606330871582,#9edae5
-5.3121467,3.283499,neighbor,9672033,Using Crowdsourcing to get Representations based on Regular Expressions,0.08891892433166504,#9edae5
-9.670358,-5.736768,neighbor,9672033,Emotions from Text: Machine Learning for Text-based Emotion Prediction,0.0889352560043335,#9edae5
0.4934654,12.557033,neighbor,9672033,Grounded Compositional Semantics for Finding and Describing Images with Sentences,0.08910346031188965,#9edae5
-4.519331,16.853027,neighbor,9672033,Learning Word Representations with Hierarchical Sparse Coding,0.08934956789016724,#9edae5
3.0508864,0.1179693,neighbor,9672033,Sense Sentiment Similarity: An Analysis,0.08948063850402832,#9edae5
-0.24348637,1.4159677,neighbor,9672033,Coarse-to-fine sentence-level emotion classification based on the intra-sentence features and sentential context,0.08948737382888794,#9edae5
-0.66995883,16.37242,neighbor,9672033,Regularization and nonlinearities for neural language models: when are they needed?,0.09020942449569702,#9edae5
-5.319772,15.52268,neighbor,9672033,Learning Grounded Meaning Representations with Autoencoders,0.09022730588912964,#9edae5
-3.463009,17.99042,neighbor,9672033,word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method,0.09030324220657349,#9edae5
-4.3093286,19.272207,neighbor,9672033,The Expressive Power of Word Embeddings,0.09034168720245361,#9edae5
-2.1004713,-6.1919293,neighbor,9672033,Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm,0.09068280458450317,#9edae5
-8.449727,-5.1800222,neighbor,9672033,UPAR7: A knowledge-based system for headline sentiment tagging,0.09103250503540039,#9edae5
4.4845605,7.604077,neighbor,9672033,Learning Question Classifiers,0.09105938673019409,#9edae5
4.5934877,-5.5698724,neighbor,9672033,SeemGo: Conditional Random Fields Labeling and Maximum Entropy Classification for Aspect Based Sentiment Analysis,0.09111326932907104,#9edae5
3.828476,7.1195703,neighbor,9672033,Question Classification with Support Vector Machines and Error Correcting Codes,0.0912209153175354,#9edae5
3.2887824,-4.8035755,neighbor,9672033,UNITOR: Aspect Based Sentiment Analysis with Structured Learning,0.09136301279067993,#9edae5
-6.0196857,16.722511,neighbor,9672033,Multilingual Distributed Representations without Word Alignment,0.09147709608078003,#9edae5
3.2190745,-6.429359,neighbor,9672033,SNAP: A Multi-Stage XML-Pipeline for Aspect Based Sentiment Analysis,0.09149104356765747,#9edae5
-6.763599,-5.200745,neighbor,9672033,Contextual Phrase-Level Polarity Analysis Using Lexical Affect Scoring and Syntactic N-Grams,0.09156423807144165,#9edae5
-11.82027,-1.7272922,neighbor,9672033,TUGAS: Exploiting unlabelled data for Twitter sentiment analysis,0.09164226055145264,#9edae5
-10.356109,2.1931038,neighbor,9672033,Biocom Usp: Tweet Sentiment Analysis with Adaptive Boosting Ensemble,0.09178096055984497,#9edae5
-11.893639,2.6562457,neighbor,9672033,JOINT_FORCES: Unite Competing Sentiment Classifiers with Random Forest,0.09255385398864746,#9edae5
-5.1345124,11.837184,neighbor,9672033,Weakly Supervised Natural Language Learning Without Redundant Views,0.0925636887550354,#9edae5
-4.856557,18.657925,neighbor,9672033,Word Embeddings through Hellinger PCA,0.09262597560882568,#9edae5
-2.047302,2.0052063,neighbor,9672033,Discovering Fine-grained Sentiment in Suicide Notes,0.09262967109680176,#9edae5
-3.0145655,14.083318,neighbor,9672033,Text segmentation with character-level text embeddings,0.09324812889099121,#9edae5
-1.7841231,-10.510731,neighbor,9672033,Learning sentiment classification model from labeled features,0.09327423572540283,#9edae5
-9.058004,-5.258957,neighbor,9672033,SemEval-2007 Task 14: Affective Text,0.09343504905700684,#9edae5
-0.2594905,-5.016632,neighbor,9672033,Sentiment Analysis of Conditional Sentences,0.09343701601028442,#9edae5
-5.5419784,17.767822,neighbor,9672033,Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds,0.09352481365203857,#9edae5
4.3572764,8.547272,neighbor,9672033,Automatic Feature Engineering for Answer Selection and Extraction,0.09359192848205566,#9edae5
3.1185894,0.09605383,neighbor,9672033,Leveraging Sentiment to Compute Word Similarity,0.09384828805923462,#9edae5
-3.1201532,11.06441,neighbor,9672033,A Deep Architecture for Semantic Parsing,0.09389132261276245,#9edae5
-1.1802958,14.982019,neighbor,9672033,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,0.0938987135887146,#9edae5
-9.45084,-0.7642588,neighbor,9672033,SemEval-2013 Task 2: Sentiment Analysis in Twitter,0.09397274255752563,#9edae5
-3.6762745,-2.0499096,neighbor,9672033,Feature Subsumption for Opinion Analysis,0.09416729211807251,#9edae5
3.3347008,-6.0691495,neighbor,9672033,UBham: Lexical Resources and Dependency Parsing for Aspect-Based Sentiment Analysis,0.09419530630111694,#9edae5
-2.9946835,-3.4456153,neighbor,9672033,SentiMerge: Combining Sentiment Lexicons in a Bayesian Framework,0.09449166059494019,#9edae5
2.268782,-6.225545,neighbor,9672033,INSIGHT Galway: Syntactic and Lexical Features for Aspect Based Sentiment Analysis,0.0945887565612793,#9edae5
2.7511473,-5.2023354,neighbor,9672033,USF: Chunking for Aspect-term Identification & Polarity Classification,0.09462350606918335,#9edae5
-2.1757038,-11.0745735,neighbor,9672033,Feature Ensemble Plus Sample Selection: Domain Adaptation for Sentiment Classification,0.0947115421295166,#9edae5
-0.5947566,-2.3914392,neighbor,9672033,Genre-Based Paragraph Classification for Sentiment Analysis,0.09491240978240967,#9edae5
1.8833561,16.838026,neighbor,9672033,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.09503638744354248,#9edae5
-0.30363354,-0.19853127,query,10716717,Feature Pyramid Networks for Object Detection,0.0,#1f77b4
-2.3219824,2.1962519,neighbor,10716717,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.036483168601989746,#1f77b4
-2.0315354,0.38643983,neighbor,10716717,Scalable Object Detection Using Deep Neural Networks,0.03826504945755005,#1f77b4
0.27238575,4.9206395,neighbor,10716717,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.03847736120223999,#1f77b4
-0.9933159,-0.7779575,neighbor,10716717,Fast Object Localization Using a CNN Feature Map Based Multi-Scale Search,0.04030734300613403,#1f77b4
0.15060063,1.8447889,neighbor,10716717,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.04179912805557251,#1f77b4
1.3408983,11.177649,neighbor,10716717,Mid-level Elements for Object Detection,0.042442262172698975,#1f77b4
1.2811447,0.3609134,neighbor,10716717,Object Detection Networks on Convolutional Feature Maps,0.04359477758407593,#1f77b4
2.2046366,-2.1203785,neighbor,10716717,Fast R-CNN,0.043914735317230225,#1f77b4
2.5630689,-8.254478,neighbor,10716717,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.044054508209228516,#1f77b4
2.3432393,-6.9887238,neighbor,10716717,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.044408202171325684,#1f77b4
-0.8930822,-2.8086197,neighbor,10716717,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.044733405113220215,#1f77b4
-6.557813,3.8886712,neighbor,10716717,Efficient object detection for high resolution images,0.04550677537918091,#1f77b4
3.6166892,-1.3979369,neighbor,10716717,PVANet: Lightweight Deep Neural Networks for Real-time Object Detection,0.04566025733947754,#1f77b4
3.5225058,-0.9824943,neighbor,10716717,PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection,0.04566293954849243,#1f77b4
6.1852818,2.0799968,neighbor,10716717,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.047039568424224854,#1f77b4
-3.2174363,-1.1658131,neighbor,10716717,ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks,0.04704028367996216,#1f77b4
4.0350456,10.939087,neighbor,10716717,Regionlets for Generic Object Detection,0.04704594612121582,#1f77b4
1.5683744,-11.72201,neighbor,10716717,Multi-scale Recognition with DAG-CNNs,0.04722326993942261,#1f77b4
-6.412731,5.4324946,neighbor,10716717,Keypoint Density-based Region Proposal for Fine-Grained Object Detection and Classification using Regions with Convolutional Neural Network Features,0.0477374792098999,#1f77b4
-4.2136073,7.3518586,neighbor,10716717,Scale-Aware Pixelwise Object Proposal Networks,0.04777121543884277,#1f77b4
-6.2212462,7.778454,neighbor,10716717,"Scalable, High-Quality Object Detection",0.047818779945373535,#1f77b4
0.613755,4.603672,neighbor,10716717,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.048023223876953125,#1f77b4
2.132979,-3.498525,neighbor,10716717,Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors,0.048074543476104736,#1f77b4
-4.1943984,0.113756485,neighbor,10716717,LocNet: Improving Localization Accuracy for Object Detection,0.04820358753204346,#1f77b4
-6.1009817,6.2748003,neighbor,10716717,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.04839491844177246,#1f77b4
0.46186212,3.175871,neighbor,10716717,Object Detection via End-to-End Integration of Aspect Ratio and Context Aware Part-based Models and Fully Convolutional Networks,0.04879635572433472,#1f77b4
-1.8990206,10.219779,neighbor,10716717,Crafting GBD-Net for Object Detection,0.048937857151031494,#1f77b4
-9.64763,5.2323995,neighbor,10716717,G-CNN: An Iterative Grid Based Object Detector,0.04961860179901123,#1f77b4
7.7624297,2.8195648,neighbor,10716717,Generic Object Detection with Dense Neural Patterns and Regionlets,0.04978477954864502,#1f77b4
-4.484793,-2.7732172,neighbor,10716717,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.0499953031539917,#1f77b4
2.5925946,11.941288,neighbor,10716717,Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection,0.05036735534667969,#1f77b4
-4.885381,-1.1744057,neighbor,10716717,"You Only Look Once: Unified, Real-Time Object Detection",0.050408780574798584,#1f77b4
-2.4819942,2.9093435,neighbor,10716717,Attentive Contexts for Object Detection,0.05063825845718384,#1f77b4
8.286624,-3.9442072,neighbor,10716717,Hierarchical part detection with deep neural networks,0.05290508270263672,#1f77b4
-8.115976,4.0624776,neighbor,10716717,DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers,0.052933573722839355,#1f77b4
-3.7544825,6.3829756,neighbor,10716717,Subcategory-Aware Convolutional Neural Networks for Object Proposals and Detection,0.05354505777359009,#1f77b4
4.86838,-0.45172006,neighbor,10716717,Recycle deep features for better object detection,0.0541151762008667,#1f77b4
-1.2129955,7.125954,neighbor,10716717,Weakly Supervised Cascaded Convolutional Networks,0.0544658899307251,#1f77b4
9.355815,7.3632755,neighbor,10716717,Deeply Supervised Salient Object Detection with Short Connections,0.054562509059906006,#1f77b4
8.647463,1.7190242,neighbor,10716717,Deep learning for class-generic object detection,0.054643988609313965,#1f77b4
3.83916,-9.762774,neighbor,10716717,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.054870784282684326,#1f77b4
6.5995727,2.1239896,neighbor,10716717,DeepID-Net: Deformable deep convolutional neural networks for object detection,0.055222928524017334,#1f77b4
4.2390237,9.813183,neighbor,10716717,Boosting Convolutional Features for Robust Object Proposals,0.05525857210159302,#1f77b4
8.880695,-7.903171,neighbor,10716717,Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition,0.05542021989822388,#1f77b4
-1.6096395,8.037282,neighbor,10716717,CRAFT Objects from Images,0.05542200803756714,#1f77b4
4.475424,13.066214,neighbor,10716717,The Fastest Deformable Part Model for Object Detection,0.055445194244384766,#1f77b4
-0.58818465,-11.670464,neighbor,10716717,Scene understanding based on Multi-Scale Pooling of deep learning features,0.05568516254425049,#1f77b4
4.835874,13.511487,neighbor,10716717,Parallelized deformable part models with effective hypothesis pruning,0.05597931146621704,#1f77b4
6.7208114,11.799703,neighbor,10716717,Scale-Aware Fast R-CNN for Pedestrian Detection,0.056218504905700684,#1f77b4
-3.0473347,12.715111,neighbor,10716717,1-HKUST: Object Detection in ILSVRC 2014,0.056858956813812256,#1f77b4
3.666349,-2.9931552,neighbor,10716717,"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",0.056919634342193604,#1f77b4
1.1743454,6.976496,neighbor,10716717,Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation,0.057062625885009766,#1f77b4
9.177327,-5.264794,neighbor,10716717,Part-Based R-CNNs for Fine-Grained Category Detection,0.05736422538757324,#1f77b4
0.6650656,9.928046,neighbor,10716717,A Classification Leveraged Object Detector,0.0579143762588501,#1f77b4
10.046614,7.5652804,neighbor,10716717,SuperCNN: A Superpixelwise Convolutional Neural Network for Salient Object Detection,0.0580446720123291,#1f77b4
10.481134,6.98262,neighbor,10716717,LCNN: Low-level Feature Embedded CNN for Salient Object Detection,0.0585293173789978,#1f77b4
-8.6194935,6.564696,neighbor,10716717,R-CNN minus R,0.05862867832183838,#1f77b4
8.086837,-9.070377,neighbor,10716717,Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition,0.05864804983139038,#1f77b4
-2.0864248,-5.4125857,neighbor,10716717,Hypercolumns for object segmentation and fine-grained localization,0.05886894464492798,#1f77b4
9.473503,-4.4967546,neighbor,10716717,DeePM: A Deep Part-Based Model for Object Detection and Semantic Part Localization,0.058935582637786865,#1f77b4
-10.909655,3.8436868,neighbor,10716717,Adaptive Object Detection Using Adjacency and Zoom Prediction,0.05933767557144165,#1f77b4
-5.9852953,0.3776201,neighbor,10716717,A novel method for object localization in digital images,0.05949997901916504,#1f77b4
2.1397765,5.4672327,neighbor,10716717,Learning to decompose for object detection and instance segmentation,0.05952638387680054,#1f77b4
9.786739,11.74111,neighbor,10716717,Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection,0.05962270498275757,#1f77b4
2.1087878,10.025285,neighbor,10716717,Max-Margin Object Detection,0.05968433618545532,#1f77b4
-3.9815226,13.516979,neighbor,10716717,ImageNet Large Scale Visual Recognition Challenge,0.05979996919631958,#1f77b4
-5.067592,-3.3294268,neighbor,10716717,UnitBox: An Advanced Object Detection Network,0.05983513593673706,#1f77b4
9.193556,-7.281001,neighbor,10716717,Part-Stacked CNN for Fine-Grained Visual Categorization,0.05995440483093262,#1f77b4
-7.818917,9.5809355,neighbor,10716717,Training Region-Based Object Detectors with Online Hard Example Mining,0.06009376049041748,#1f77b4
3.1790419,6.8622484,neighbor,10716717,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.06070435047149658,#1f77b4
-7.8300405,6.264173,neighbor,10716717,Relief R-CNN: Utilizing Convolutional Features for Fast Object Detection,0.06091111898422241,#1f77b4
-10.454837,-4.461322,neighbor,10716717,Object Detection from Video Tubelets with Convolutional Neural Networks,0.06105238199234009,#1f77b4
6.66278,-10.640522,neighbor,10716717,Modelling local deep convolutional neural network features to improve fine-grained image classification,0.061057865619659424,#1f77b4
-2.2603583,-3.3994372,neighbor,10716717,Learning Deep Features for Discriminative Localization,0.061087071895599365,#1f77b4
11.288523,4.2675014,neighbor,10716717,Object-centric Sampling for Fine-grained Image Classification,0.061167776584625244,#1f77b4
2.0496716,-9.813098,neighbor,10716717,Going deeper with convolutions,0.06128883361816406,#1f77b4
-8.93815,-0.20599014,neighbor,10716717,Deep Joint Task Learning for Generic Object Extraction,0.061797499656677246,#1f77b4
-3.4993088,3.7234242,neighbor,10716717,Context-Aware Semi-Local Feature Detector,0.061878740787506104,#1f77b4
5.779341,-8.95885,neighbor,10716717,Deep convolutional neural networks as generic feature extractors,0.061889827251434326,#1f77b4
8.144284,12.898964,neighbor,10716717,Pedestrian Detection with Unsupervised Multi-stage Feature Learning,0.061901748180389404,#1f77b4
6.064468,-0.84441406,neighbor,10716717,Do More Dropouts in Pool5 Feature Maps for Better Object Detection,0.062018394470214844,#1f77b4
-0.26565847,-12.099769,neighbor,10716717,"Scene Recognition with CNNs: Objects, Scales and Dataset Bias",0.062086522579193115,#1f77b4
2.5731971,2.651315,neighbor,10716717,Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction,0.06212562322616577,#1f77b4
-6.486429,8.29367,neighbor,10716717,What Makes for Effective Detection Proposals?,0.06237006187438965,#1f77b4
3.9482882,5.049513,neighbor,10716717,Convolutional feature masking for joint object and stuff segmentation,0.062398552894592285,#1f77b4
8.65157,0.7140301,neighbor,10716717,Training a convolutional neural network for multi-class object detection using solely virtual world data,0.062416136264801025,#1f77b4
-4.1952753,8.58422,neighbor,10716717,Object Proposal Generation Using Two-Stage Cascade SVMs,0.06276547908782959,#1f77b4
2.7484007,-10.553863,neighbor,10716717,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.0628930926322937,#1f77b4
-10.472921,-4.45392,neighbor,10716717,T-CNN: Tubelets With Convolutional Neural Networks for Object Detection From Videos,0.06325274705886841,#1f77b4
3.929087,-11.157534,neighbor,10716717,Enhanced image classification with a fast-learning shallow convolutional neural network,0.06330704689025879,#1f77b4
-4.2063813,2.2422261,neighbor,10716717,Context Forest for efficient object detection with large mixture models,0.06332176923751831,#1f77b4
0.35161042,6.5588546,neighbor,10716717,Learning to Segment Object Candidates,0.06363296508789062,#1f77b4
5.9970775,6.6305566,neighbor,10716717,Object Contour Detection with a Fully Convolutional Encoder-Decoder Network,0.06374555826187134,#1f77b4
10.944127,3.1821473,neighbor,10716717,Factors in Finetuning Deep Model for Object Detection with Long-Tail Distribution,0.06379008293151855,#1f77b4
-2.2423282,12.565159,neighbor,10716717,Window-Object Relationship Guided Representation Learning for Generic Object Detections,0.06403911113739014,#1f77b4
8.521887,13.695965,neighbor,10716717,"Convolutional Channel Features For Pedestrian, Face and Edge Detection",0.06404340267181396,#1f77b4
-8.4714985,3.2741947,neighbor,10716717,DeepProposals: Hunting Objects and Actions by Cascading Deep Convolutional Layers,0.06406402587890625,#1f77b4
11.182117,-3.3290825,neighbor,10716717,Deformable part models are convolutional neural networks,0.06451338529586792,#1f77b4
-1.0739772,5.6898727,neighbor,10716717,Multistage Object Detection With Group Recursive Learning,0.06463676691055298,#1f77b4
4.704788,-10.086934,neighbor,10716717,Feature Representation in Convolutional Neural Networks,0.06485188007354736,#1f77b4
-1.7477406,-2.789724,query,11212020,Neural Machine Translation by Jointly Learning to Align and Translate,0.0,#aec7e8
-1.0828773,-3.2029047,neighbor,11212020,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,0.045941710472106934,#aec7e8
-1.1148547,-1.9479258,neighbor,11212020,Recurrent Continuous Translation Models,0.051351964473724365,#aec7e8
1.0012779,-4.138653,neighbor,11212020,Semantic Vector Machines,0.06672543287277222,#aec7e8
-1.4074023,0.9139067,neighbor,11212020,Joint Space Neural Probabilistic Language Model for Statistical Machine Translation,0.07032793760299683,#aec7e8
-1.3452249,-9.266345,neighbor,11212020,Exploiting Similarities among Languages for Machine Translation,0.07078719139099121,#aec7e8
0.8382933,-8.545356,neighbor,11212020,Learning Multilingual Word Representations using a Bag-of-Words Autoencoder,0.07409268617630005,#aec7e8
0.32411247,-9.412253,neighbor,11212020,An Autoencoder Approach to Learning Bilingual Word Representations,0.07416480779647827,#aec7e8
-9.302685,-2.9960303,neighbor,11212020,Improved Statistical Machine Translation Using Monolingual Paraphrases,0.07573205232620239,#aec7e8
1.8424273,-9.247022,neighbor,11212020,Learning Bilingual Word Representations by Marginalizing Alignments,0.07592546939849854,#aec7e8
10.3316145,6.7445884,neighbor,11212020,Regularization and nonlinearities for neural language models: when are they needed?,0.07707983255386353,#aec7e8
2.3188643,8.537221,neighbor,11212020,Training Neural Network Language Models on Very Large Corpora,0.07857221364974976,#aec7e8
10.766328,10.400566,neighbor,11212020,Sequence Transduction with Recurrent Neural Networks,0.07874393463134766,#aec7e8
8.5758295,-1.0653515,neighbor,11212020,Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning,0.07931864261627197,#aec7e8
4.7269397,-4.185533,neighbor,11212020,Factored Neural Language Models,0.07955276966094971,#aec7e8
-8.955273,-10.480528,neighbor,11212020,Machine Translation Model based on Non-parallel Corpus and Semi-supervised Transductive Learning,0.08087301254272461,#aec7e8
-2.583207,-8.074147,neighbor,11212020,Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence,0.08249938488006592,#aec7e8
-9.790408,-9.303969,neighbor,11212020,Correction of Errors in a Modality Corpus Used for Machine Translation by Using Machine-learning Method,0.08258229494094849,#aec7e8
-4.7951655,-2.6792402,neighbor,11212020,Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach,0.083030104637146,#aec7e8
-0.795387,1.0397627,neighbor,11212020,Continuous Space Language Models for Statistical Machine Translation,0.08365219831466675,#aec7e8
-5.2251997,10.931385,neighbor,11212020,N-gram distribution based language model adaptation,0.08439594507217407,#aec7e8
-2.3809876,12.111963,neighbor,11212020,Rapid adaptation of n-gram language models using inter-word correlation for speech recognition,0.08487004041671753,#aec7e8
5.5759096,-8.668827,neighbor,11212020,Improving the Lexical Function Composition Model with Pathwise Optimized Elastic-Net Regression,0.08522969484329224,#aec7e8
10.170945,-3.9693356,neighbor,11212020,Composition of Conditional Random Fields for Transfer Learning,0.08554381132125854,#aec7e8
3.1035173,5.5741706,neighbor,11212020,Distributed Language Models,0.08559107780456543,#aec7e8
-2.605066,-6.799733,neighbor,11212020,Alignment by Agreement,0.08566838502883911,#aec7e8
-9.9934,-1.8455386,neighbor,11212020,Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment,0.08587765693664551,#aec7e8
-0.10168608,-0.7063461,neighbor,11212020,A Probabilistic Model of Machine Translation,0.08629447221755981,#aec7e8
1.121361,4.432221,neighbor,11212020,A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser Ney Smoothing,0.08640336990356445,#aec7e8
-0.057867657,7.6974764,neighbor,11212020,Eutrans: a speech-to-speech translator prototype,0.08645343780517578,#aec7e8
2.504983,1.2881945,neighbor,11212020,SHEF-Lite 2.0: Sparse Multi-task Gaussian Processes for Translation Quality Estimation,0.08667868375778198,#aec7e8
3.675574,-2.8004613,neighbor,11212020,Compositional Morphology for Word Representations and Language Modelling,0.08712971210479736,#aec7e8
-3.9994745,12.827368,neighbor,11212020,Integrating MAP and linear transformation for language model adaptation,0.08719819784164429,#aec7e8
-5.6702094,-2.7794297,neighbor,11212020,Does Syntactic Knowledge help English-Hindi SMT?,0.08754414319992065,#aec7e8
-10.99333,-7.8923807,neighbor,11212020,Anusaaraka: Machine Translation in Stages,0.08770036697387695,#aec7e8
-3.0054867,-5.975406,neighbor,11212020,Empirical Methods for Compound Splitting,0.08867871761322021,#aec7e8
-3.1976323,13.420264,neighbor,11212020,Language Model Adaptation with MAP Estimation and the Perceptron Algorithm,0.08876144886016846,#aec7e8
-0.53913987,10.592475,neighbor,11212020,Implementing vocal tract length normalization in the MLLR framework,0.0893520712852478,#aec7e8
-5.9557185,-12.0954275,neighbor,11212020,Full-text story alignment models for Chinese-English bilingual news corpora,0.08957785367965698,#aec7e8
1.7798469,9.560325,neighbor,11212020,Optimization of Neural Network Language Models for keyword search,0.08972960710525513,#aec7e8
-3.9707406,-1.1725351,neighbor,11212020,Hierarchical feature-based translation for scalable natural language understanding,0.08978271484375,#aec7e8
-0.91150963,15.2839575,neighbor,11212020,Investigations on joint-multigram models for grapheme-to-phoneme conversion,0.08978456258773804,#aec7e8
-10.009156,-4.875416,neighbor,11212020,Correction of Noisy Sentences using a Monolingual Corpus,0.08997082710266113,#aec7e8
7.689369,1.855038,neighbor,11212020,Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty,0.0901753306388855,#aec7e8
-8.450626,-7.822063,neighbor,11212020,Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation,0.09020715951919556,#aec7e8
-3.553284,10.8185005,neighbor,11212020,Language model adaptation using word clustering,0.0905609130859375,#aec7e8
4.497374,3.2706063,neighbor,11212020,Classes for fast maximum entropy training,0.0908888578414917,#aec7e8
-11.13784,-10.076611,neighbor,11212020,"Using a Support-Vector Machine for Japanese-to-English Translation of Tense, Aspect, and Modality",0.09098976850509644,#aec7e8
-7.1838393,-2.3808463,neighbor,11212020,Hybrid Simplification using Deep Semantics and Machine Translation,0.09156179428100586,#aec7e8
1.5865738,-7.4560046,neighbor,11212020,Distributed Word Representation Learning for Cross-Lingual Dependency Parsing,0.09195065498352051,#aec7e8
-4.8083577,8.493196,neighbor,11212020,Adapting language models for frequent fixed phrases by emphasizing n-gram subsets,0.09196007251739502,#aec7e8
6.9914894,-8.951559,neighbor,11212020,Adaptive Multi-Compositionality for Recursive Neural Models with Applications to Sentiment Analysis,0.0919647216796875,#aec7e8
6.17371,2.3956714,neighbor,11212020,Approximation Lasso Methods for Language Modeling,0.09200513362884521,#aec7e8
0.8064748,-11.34848,neighbor,11212020,Cross-Lingual Adaptation Using Structural Correspondence Learning,0.09237843751907349,#aec7e8
-2.6918995,10.109507,neighbor,11212020,Phoneme-to-Text Transcription System with an Infinite Vocabulary,0.09264189004898071,#aec7e8
-1.0842735,11.943698,neighbor,11212020,Empirically combining unnormalized NNLM and back-off N-gram for fast N-best rescoring in speech recognition,0.09273630380630493,#aec7e8
-2.2147737,5.038959,neighbor,11212020,Maximum entropy good-turing estimator for language modeling,0.09283053874969482,#aec7e8
10.08179,-1.8405228,neighbor,11212020,Weakly Supervised Natural Language Learning Without Redundant Views,0.09287905693054199,#aec7e8
-6.850468,5.7377014,neighbor,11212020,Word Confidence Estimation for SMT N-best List Re-ranking,0.0929374098777771,#aec7e8
-3.271239,3.595802,neighbor,11212020,Improvement of a Whole Sentence Maximum Entropy Language Model Using Grammatical Features,0.09303194284439087,#aec7e8
11.333872,-1.5775778,neighbor,11212020,Adversarial Evaluation for Models of Natural Language,0.09311962127685547,#aec7e8
-11.0005245,-0.60221744,neighbor,11212020,Learning to Say It Well: Reranking Realizations by Predicted Synthesis Quality,0.0935564637184143,#aec7e8
-6.2658024,4.5442467,neighbor,11212020,An Empirical Study on Multiple LVCSR Model Combination by Machine Learning,0.09360957145690918,#aec7e8
1.4959822,3.7465134,neighbor,11212020,Language Modeling with Power Low Rank Ensembles,0.09377014636993408,#aec7e8
-3.6980712,-11.495496,neighbor,11212020,Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval,0.09378892183303833,#aec7e8
7.626036,-1.1315668,neighbor,11212020,Effective Bilingual Constraints for Semi-Supervised Learning of Named Entity Recognizers,0.09388387203216553,#aec7e8
13.691214,9.942345,neighbor,11212020,Modelling the Probability Density of Markov Sources,0.09389722347259521,#aec7e8
0.376494,14.113685,neighbor,11212020,Coupling vs. unifying: modeling techniques for speech-to-speech translation,0.09399914741516113,#aec7e8
14.173854,8.548541,neighbor,11212020,Joint Training of Deep Boltzmann Machines,0.09407740831375122,#aec7e8
-2.7748306,8.788712,neighbor,11212020,Estimating Confusions in the ASR Channel for Improved Topic-based Language Model Adaptation,0.09408444166183472,#aec7e8
-6.2235823,12.075158,neighbor,11212020,Discriminative training on language model,0.09410566091537476,#aec7e8
5.4858027,-5.1482453,neighbor,11212020,word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method,0.09413546323776245,#aec7e8
2.6607513,-8.280573,neighbor,11212020,Multilingual Distributed Representations without Word Alignment,0.09418010711669922,#aec7e8
9.194175,4.9569454,neighbor,11212020,Training Connectionist Models for the Structured Language Model,0.09447169303894043,#aec7e8
11.26463,10.917295,neighbor,11212020,High-dimensional sequence transduction,0.09484922885894775,#aec7e8
10.411988,0.45938122,neighbor,11212020,Natural Language Processing (Almost) from Scratch,0.09493488073348999,#aec7e8
-8.485215,-11.728433,neighbor,11212020,Automatic Information Transfer between English and Chinese,0.09495174884796143,#aec7e8
12.618306,6.464985,neighbor,11212020,Improving neural networks by preventing co-adaptation of feature detectors,0.09529674053192139,#aec7e8
-7.12404,10.3968525,neighbor,11212020,Improving Language Model Size Reduction using Better Pruning Criteria,0.09532833099365234,#aec7e8
-4.820134,-13.727289,neighbor,11212020,Issues in Pre- and Post-translation Document Expansion: Untranslatable Cognates and Missegmented Words,0.09594857692718506,#aec7e8
-4.5004883,-11.778412,neighbor,11212020,Applying machine translation to two-stage cross-language information retrieval,0.09597760438919067,#aec7e8
8.9471245,0.47356045,neighbor,11212020,Semi-Supervised Learning and Domain Adaptation in Natural Language Processing,0.09600257873535156,#aec7e8
-2.630282,4.101503,neighbor,11212020,Using Perfect Sampling in Parameter Estimation of a Whole Sentence Maximum Entropy Language Model,0.09607279300689697,#aec7e8
-0.6878157,8.412994,neighbor,11212020,The RACAI Text-to-Speech Synthesis System,0.09626764059066772,#aec7e8
10.620366,8.152212,neighbor,11212020,A Clockwork RNN,0.09630542993545532,#aec7e8
2.3389902,10.383474,neighbor,11212020,Quantization-based language model compression,0.09647256135940552,#aec7e8
4.8594055,6.105269,neighbor,11212020,Learning Performance of a Machine Translation System: a Statistical and Computational Analysis,0.0966259241104126,#aec7e8
-5.532631,-6.773604,neighbor,11212020,SSMT:A Machine Translation Evaluation View To Paragraph-to-Sentence Semantic Similarity,0.09666621685028076,#aec7e8
3.9321127,-5.1610975,neighbor,11212020,Efficient Estimation of Word Representations in Vector Space,0.09671974182128906,#aec7e8
9.783976,9.01809,neighbor,11212020,Generating Sequences With Recurrent Neural Networks,0.09680932760238647,#aec7e8
0.19078086,-12.82828,neighbor,11212020,Polylingual Tree-Based Topic Models for Translation Domain Adaptation,0.09680944681167603,#aec7e8
-5.9545293,-5.753832,neighbor,11212020,Predicting the Fluency of Text with Shallow Structural Features: Case Studies of Machine Translation and Human-Written Text,0.0970984697341919,#aec7e8
-8.911934,-1.8631146,neighbor,11212020,Mutaphrase: Paraphrasing with FrameNet,0.09716302156448364,#aec7e8
13.729631,7.02476,neighbor,11212020,Do Deep Nets Really Need to be Deep?,0.09722214937210083,#aec7e8
-4.1235647,10.123293,neighbor,11212020,Unsupervised language model adaptation,0.09737074375152588,#aec7e8
9.170211,1.1411122,neighbor,11212020,Fast Easy Unsupervised Domain Adaptation with Marginalized Structured Dropout,0.09750300645828247,#aec7e8
4.6169467,-7.1628,neighbor,11212020,Evaluating Neural Word Representations in Tensor-Based Compositional Settings,0.09753364324569702,#aec7e8
4.481826,-5.9983015,neighbor,11212020,Learning Word Representations with Hierarchical Sparse Coding,0.0976024866104126,#aec7e8
7.9296947,-8.425861,neighbor,11212020,Convolutional Neural Networks for Sentence Classification,0.09779798984527588,#aec7e8
11.512352,7.8531184,neighbor,11212020,Advances in optimizing recurrent networks,0.09780055284500122,#aec7e8
8.13362,-6.837741,neighbor,11212020,"Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network",0.09799021482467651,#aec7e8
-4.0599246,7.429607,query,11758569,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,0.0,#aec7e8
-5.6405096,8.170399,neighbor,11758569,Adversarial Autoencoders,0.04437911510467529,#aec7e8
5.2443333,-4.3411436,neighbor,11758569,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.048960745334625244,#aec7e8
-5.1157765,9.285646,neighbor,11758569,Conditional Generative Adversarial Nets,0.05009043216705322,#aec7e8
-8.887006,-0.7009833,neighbor,11758569,A Generative Model for Deep Convolutional Learning,0.05640465021133423,#aec7e8
2.5774384,7.237738,neighbor,11758569,Adversarial Manipulation of Deep Representations,0.057323455810546875,#aec7e8
4.3694506,-3.8888946,neighbor,11758569,Unsupervised feature learning by augmenting single images,0.05901610851287842,#aec7e8
-7.0211473,11.782424,neighbor,11758569,Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks,0.05928581953048706,#aec7e8
6.098127,-4.657022,neighbor,11758569,Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks,0.06136232614517212,#aec7e8
-13.571197,9.269859,neighbor,11758569,Learning FRAME Models Using CNN Filters,0.0633360743522644,#aec7e8
-12.041542,7.5176287,neighbor,11758569,Symmetries and control in generative neural nets,0.06451416015625,#aec7e8
-7.06743,10.304568,neighbor,11758569,Generative adversarial networks,0.064816415309906,#aec7e8
-6.5744023,9.473057,neighbor,11758569,Generative Adversarial Nets,0.06590127944946289,#aec7e8
2.0683746,-3.8071373,neighbor,11758569,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.06648075580596924,#aec7e8
-9.291402,1.7262788,neighbor,11758569,Joint Training of Deep Auto-Encoders,0.06843733787536621,#aec7e8
-3.3522825,-2.3731685,neighbor,11758569,Scheduled denoising autoencoders,0.07017511129379272,#aec7e8
8.268851,-3.7447464,neighbor,11758569,Unsupervised network pretraining via encoding human design,0.07037824392318726,#aec7e8
-8.224462,-0.727633,neighbor,11758569,Generative Deep Deconvolutional Learning,0.07141858339309692,#aec7e8
-9.053361,4.0237927,neighbor,11758569,GSNs : Generative Stochastic Networks,0.07290303707122803,#aec7e8
11.422445,-2.9369352,neighbor,11758569,Unsupervised Visual Representation Learning by Context Prediction,0.07357728481292725,#aec7e8
9.633274,0.035793733,neighbor,11758569,CURL: Co-trained Unsupervised Representation Learning for Image Classification,0.07421022653579712,#aec7e8
-11.575795,-4.340631,neighbor,11758569,Learning to Linearize Under Uncertainty,0.07455796003341675,#aec7e8
3.1797073,8.110295,neighbor,11758569,Towards Deep Neural Network Architectures Robust to Adversarial Examples,0.07464241981506348,#aec7e8
-1.6707958,-4.809522,neighbor,11758569,Deeply-Supervised Nets,0.07482266426086426,#aec7e8
-12.140224,9.906329,neighbor,11758569,Generative Modeling of Convolutional Neural Networks,0.07489854097366333,#aec7e8
13.422579,-6.4183044,neighbor,11758569,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.07498997449874878,#aec7e8
-2.1423624,9.287193,neighbor,11758569,Distributional Smoothing with Virtual Adversarial Training,0.07577484846115112,#aec7e8
9.617391,-4.0055337,neighbor,11758569,Webly Supervised Learning of Convolutional Networks,0.07601791620254517,#aec7e8
11.069424,-8.412165,neighbor,11758569,Deformable part models are convolutional neural networks,0.07628786563873291,#aec7e8
-6.9071803,2.09707,neighbor,11758569,"Why are deep nets reversible: A simple theory, with implications for training",0.07668405771255493,#aec7e8
4.7157474,3.2921054,neighbor,11758569,Confusing Deep Convolution Networks by Relabelling,0.07671737670898438,#aec7e8
5.2349873,-10.865943,neighbor,11758569,Inverting Visual Representations with Convolutional Networks,0.07674890756607056,#aec7e8
8.514558,-12.319133,neighbor,11758569,Convolutional Kernel Networks,0.07708472013473511,#aec7e8
15.950951,-9.820286,neighbor,11758569,Exploring Invariances in Deep Convolutional Neural Networks Using Synthetic Images,0.07738351821899414,#aec7e8
2.346185,-7.4334383,neighbor,11758569,Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification,0.07798159122467041,#aec7e8
9.324445,-6.7425513,neighbor,11758569,Deep convolutional neural networks as generic feature extractors,0.07811838388442993,#aec7e8
-12.889536,0.62329847,neighbor,11758569,Mixed generative and supervised learning modes in Deep Predictive Coding Networks,0.07845526933670044,#aec7e8
6.956616,-7.2230825,neighbor,11758569,Modelling local deep convolutional neural network features to improve fine-grained image classification,0.07867342233657837,#aec7e8
-7.93943,9.0377245,neighbor,11758569,Generative Moment Matching Networks,0.07929098606109619,#aec7e8
2.1202283,8.182186,neighbor,11758569,DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks,0.07947754859924316,#aec7e8
-9.220416,5.7730255,neighbor,11758569,Scoring and Classifying with Gated Auto-Encoders,0.0796130895614624,#aec7e8
-4.3060637,-7.499506,neighbor,11758569,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.07968038320541382,#aec7e8
-2.9428096,-7.4206953,neighbor,11758569,Improving Deep Neural Networks with Probabilistic Maxout Units,0.07995396852493286,#aec7e8
3.4919531,-6.888891,neighbor,11758569,Unsupervised Feature Learning by Deep Sparse Coding,0.08016037940979004,#aec7e8
-10.573373,7.4898005,neighbor,11758569,A Group Theoretic Perspective on Unsupervised Deep Learning,0.08018839359283447,#aec7e8
-11.732191,-3.496071,neighbor,11758569,Deep Predictive Coding Networks,0.08020544052124023,#aec7e8
11.652427,-2.1099029,neighbor,11758569,Unsupervised Learning of Visual Representations Using Videos,0.08037161827087402,#aec7e8
-2.7050636,-3.9184413,neighbor,11758569,Natural Neural Networks,0.0810096263885498,#aec7e8
-5.5086617,11.548488,neighbor,11758569,"How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?",0.08116894960403442,#aec7e8
-8.155736,3.5095677,neighbor,11758569,Generative Class-conditional Autoencoders,0.08166438341140747,#aec7e8
-6.582769,5.2540355,neighbor,11758569,The Variational Fair Autoencoder,0.08181416988372803,#aec7e8
13.567737,-7.3628807,neighbor,11758569,From generic to specific deep representations for visual recognition,0.08215641975402832,#aec7e8
-10.253689,3.5434232,neighbor,11758569,Deep Directed Generative Autoencoders,0.08233147859573364,#aec7e8
13.806502,-9.9021225,neighbor,11758569,Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition,0.08320146799087524,#aec7e8
8.044198,-7.9248176,neighbor,11758569,Feature Representation in Convolutional Neural Networks,0.08361983299255371,#aec7e8
4.652665,-7.0224357,neighbor,11758569,Building high-level features using large scale unsupervised learning,0.08378088474273682,#aec7e8
3.5269246,-1.4679177,neighbor,11758569,Deep Representation Learning with Target Coding,0.08379209041595459,#aec7e8
10.18516,1.3341413,neighbor,11758569,Unsupervised Learning on Neural Network Outputs: With Application in Zero-Shot Learning,0.08384811878204346,#aec7e8
-0.56997293,2.7018905,neighbor,11758569,Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation,0.08395582437515259,#aec7e8
3.5411,9.346993,neighbor,11758569,"On the Claim for the Existence of ""Adversarial Examples"" in Deep Learning Neural Networks",0.08405578136444092,#aec7e8
-14.434677,2.9383388,neighbor,11758569,A Probabilistic Theory of Deep Learning,0.08442175388336182,#aec7e8
-12.536736,4.7838664,neighbor,11758569,Semi-supervised Learning with Deep Generative Models,0.08443814516067505,#aec7e8
-5.8124986,-7.7730007,neighbor,11758569,Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference,0.08464032411575317,#aec7e8
8.830793,-13.449541,neighbor,11758569,Learning Invariant Representations with Local Transformations,0.08491456508636475,#aec7e8
6.257348,-2.0680482,neighbor,11758569,A PCA-Based Convolutional Network,0.08528578281402588,#aec7e8
12.47671,-4.2343283,neighbor,11758569,Detector discovery in the wild: Joint multiple instance and representation learning,0.08598577976226807,#aec7e8
8.006637,-8.9222555,neighbor,11758569,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.08604294061660767,#aec7e8
-16.144268,7.4273777,neighbor,11758569,DRAW: A Recurrent Neural Network For Image Generation,0.08610206842422485,#aec7e8
17.600147,-5.9026127,neighbor,11758569,Deep Domain Confusion: Maximizing for Domain Invariance,0.08622938394546509,#aec7e8
-10.3218155,-1.3644242,neighbor,11758569,Learning with Hierarchical-Deep Models,0.0862722396850586,#aec7e8
-13.178719,10.454331,neighbor,11758569,Learning Generative Models with Visual Attention,0.08644753694534302,#aec7e8
10.408834,-5.7920694,neighbor,11758569,Deep learning for class-generic object detection,0.08650290966033936,#aec7e8
-2.8802752,-1.1323869,neighbor,11758569,Denoising autoencoder with modulated lateral connections learns invariant representations of natural images,0.08672195672988892,#aec7e8
-9.399552,11.713885,neighbor,11758569,NICE: Non-linear Independent Components Estimation,0.08712100982666016,#aec7e8
-11.565511,3.9063628,neighbor,11758569,Max-Margin Deep Generative Models,0.08713746070861816,#aec7e8
-2.9446862,-6.47545,neighbor,11758569,Maxout Networks,0.08757811784744263,#aec7e8
-3.9346054,-5.5031705,neighbor,11758569,Learning Compact Convolutional Neural Networks with Nested Dropout,0.08759647607803345,#aec7e8
15.0945215,-10.361181,neighbor,11758569,What Do Deep CNNs Learn About Objects?,0.08780300617218018,#aec7e8
-10.842452,2.484993,neighbor,11758569,Deep AutoRegressive Networks,0.08797645568847656,#aec7e8
-5.919962,-4.0966516,neighbor,11758569,Discriminative Recurrent Sparse Auto-Encoders,0.08831077814102173,#aec7e8
4.9440203,4.162198,neighbor,11758569,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,0.08852773904800415,#aec7e8
-0.9468977,-6.762794,neighbor,11758569,SimNets: A Generalization of Convolutional Networks,0.08881080150604248,#aec7e8
1.2630007,10.794582,neighbor,11758569,Censoring Representations with an Adversary,0.08885908126831055,#aec7e8
-16.143791,6.442185,neighbor,11758569,Learning Wake-Sleep Recurrent Attention Models,0.08897697925567627,#aec7e8
5.0119405,-11.338222,neighbor,11758569,Understanding deep image representations by inverting them,0.08899831771850586,#aec7e8
5.9209733,-13.345999,neighbor,11758569,Towards Distortion-Predictable Embedding of Neural Networks,0.08908742666244507,#aec7e8
10.626732,-12.496004,neighbor,11758569,Locally Scale-Invariant Convolutional Neural Networks,0.0891803503036499,#aec7e8
11.68015,-12.0377655,neighbor,11758569,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.0892258882522583,#aec7e8
4.2912145,2.1156142,neighbor,11758569,Learning from Noisy Labels with Deep Neural Networks,0.08925586938858032,#aec7e8
11.659518,-6.83951,neighbor,11758569,Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks,0.08930426836013794,#aec7e8
18.8536,-3.9418695,neighbor,11758569,Domain-Adversarial Neural Networks,0.08940327167510986,#aec7e8
2.2126784,9.766189,neighbor,11758569,Learning with a Strong Adversary,0.08948057889938354,#aec7e8
-0.48563445,-4.52117,neighbor,11758569,Semi-supervised Learning with Ladder Networks,0.08972889184951782,#aec7e8
-13.735736,0.89717454,neighbor,11758569,Modeling language and cognition with deep unsupervised learning: a tutorial overview,0.08989125490188599,#aec7e8
-10.559706,5.840096,neighbor,11758569,Towards universal neural nets: Gibbs machines and ACE,0.09007304906845093,#aec7e8
11.056472,-9.268991,neighbor,11758569,Spotlight the Negatives: A Generalized Discriminative Latent Model,0.09007471799850464,#aec7e8
17.653833,-6.6641464,neighbor,11758569,One-Shot Adaptation of Supervised Deep Convolutional Models,0.09007531404495239,#aec7e8
-5.2929664,-3.0149739,neighbor,11758569,A Winner-Take-All Method for Training Sparse Convolutional Autoencoders,0.09020090103149414,#aec7e8
-15.8018265,8.651905,neighbor,11758569,Generative Image Modeling Using Spatial LSTMs,0.09027236700057983,#aec7e8
-4.4538555,-1.9470286,neighbor,11758569,An introduction to deep learning,0.09037274122238159,#aec7e8
18.17855,-4.618527,neighbor,11758569,Domain Generalization for Object Recognition with Multi-task Autoencoders,0.09048855304718018,#aec7e8
3.0879498,3.54048,query,12670695,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,0.0,#aec7e8
4.134766,4.1318994,neighbor,12670695,Shoot to Know What: An Application of Deep Networks on Mobile Devices,0.04241269826889038,#aec7e8
0.6701995,-0.43595502,neighbor,12670695,DecomposeMe: Simplifying ConvNets for End-to-End Learning,0.04434335231781006,#aec7e8
-0.15305237,-1.2889975,neighbor,12670695,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.04903906583786011,#aec7e8
-0.55191827,-1.8033484,neighbor,12670695,Rethinking the Inception Architecture for Computer Vision,0.05042541027069092,#aec7e8
3.2883706,4.840943,neighbor,12670695,Convolutional Neural Networks for object recognition on mobile devices: A case study,0.05383497476577759,#aec7e8
2.4550636,-2.3685381,neighbor,12670695,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.054394423961639404,#aec7e8
-0.6825684,0.18885556,neighbor,12670695,Deep Residual Learning for Image Recognition,0.056561172008514404,#aec7e8
0.3242332,-3.0058203,neighbor,12670695,Enhanced image classification with a fast-learning shallow convolutional neural network,0.05838853120803833,#aec7e8
-9.206731,5.5684147,neighbor,12670695,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.058988332748413086,#aec7e8
1.52067,-1.5676637,neighbor,12670695,Going deeper with convolutions,0.05971086025238037,#aec7e8
5.8372564,3.6479661,neighbor,12670695,LCNN: Lookup-Based Convolutional Neural Network,0.06032359600067139,#aec7e8
5.799414,4.397513,neighbor,12670695,"Deep Learning for Consumer Devices and Services: Pushing the limits for machine learning, artificial intelligence, and computer vision.",0.06039696931838989,#aec7e8
-13.411973,6.681717,neighbor,12670695,Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors,0.06086111068725586,#aec7e8
-11.926989,6.680716,neighbor,12670695,Fast R-CNN,0.06130337715148926,#aec7e8
-2.111841,-3.0069258,neighbor,12670695,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.06138104200363159,#aec7e8
-13.787863,7.7688303,neighbor,12670695,Wide-residual-inception networks for real-time object detection,0.061960816383361816,#aec7e8
4.0394397,-5.326413,neighbor,12670695,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.06246906518936157,#aec7e8
3.3641076,-3.9146953,neighbor,12670695,Convolutional neural networks at constrained time cost,0.06301051378250122,#aec7e8
9.386904,-0.8604759,neighbor,12670695,Learning both Weights and Connections for Efficient Neural Network,0.0639144778251648,#aec7e8
4.1651692,-1.514748,neighbor,12670695,XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks,0.06409329175949097,#aec7e8
-13.343942,8.757072,neighbor,12670695,"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",0.06490975618362427,#aec7e8
-7.439702,6.362154,neighbor,12670695,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.06498950719833374,#aec7e8
-10.278304,-0.48626894,neighbor,12670695,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,0.06501150131225586,#aec7e8
7.427528,-1.0409355,neighbor,12670695,WRPN: Training and Inference using Wide Reduced-Precision Networks,0.06521385908126831,#aec7e8
11.156745,-7.0098577,neighbor,12670695,Fast Algorithms for Convolutional Neural Networks,0.06539326906204224,#aec7e8
8.851381,5.6338863,neighbor,12670695,DeepSense: A GPU-based Deep Convolutional Neural Network Framework on Commodity Mobile Devices,0.06639665365219116,#aec7e8
10.383156,4.6412687,neighbor,12670695,Fast and Energy-Efficient CNN Inference on IoT Devices,0.0666840672492981,#aec7e8
-9.279162,-0.9160766,neighbor,12670695,Clockwork Convnets for Video Semantic Segmentation,0.06686669588088989,#aec7e8
-1.3465958,-2.5107467,neighbor,12670695,A Taxonomy of Deep Convolutional Neural Nets for Computer Vision,0.06687873601913452,#aec7e8
2.893884,0.57466197,neighbor,12670695,Deep SimNets,0.06725990772247314,#aec7e8
-8.993675,6.4191875,neighbor,12670695,Scalable Object Detection Using Deep Neural Networks,0.06747251749038696,#aec7e8
3.9477541,-1.8570714,neighbor,12670695,Local Binary Convolutional Neural Networks,0.06792593002319336,#aec7e8
7.089518,-3.4848893,neighbor,12670695,Deep Fried Convnets,0.06827378273010254,#aec7e8
3.7420657,-5.582479,neighbor,12670695,Caffe: Convolutional Architecture for Fast Feature Embedding,0.06827563047409058,#aec7e8
8.057694,-2.7844524,neighbor,12670695,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size,0.06841546297073364,#aec7e8
-0.062473744,6.4541025,neighbor,12670695,NoScope: Optimizing Deep CNN-Based Queries over Video Streams at Scale,0.06846106052398682,#aec7e8
0.043653853,6.4092216,neighbor,12670695,CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data,0.06862950325012207,#aec7e8
-5.5463843,-3.2754817,neighbor,12670695,Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition,0.06928473711013794,#aec7e8
-7.241931,8.19576,neighbor,12670695,DeNet: Scalable Real-Time Object Detection with Directed Sparse Sampling,0.06937462091445923,#aec7e8
5.8341064,5.532346,neighbor,12670695,Deep Learning at Scale and at Ease,0.06972837448120117,#aec7e8
8.691857,-1.3849567,neighbor,12670695,DyVEDeep: Dynamic Variable Effort Deep Neural Networks,0.06997674703598022,#aec7e8
6.567485,-3.628724,neighbor,12670695,Fixed-Point Factorized Networks,0.07000619173049927,#aec7e8
-3.3127384,-3.3171635,neighbor,12670695,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.07000762224197388,#aec7e8
-8.079825,4.8393984,neighbor,12670695,Learning Deep Features for Discriminative Localization,0.07017159461975098,#aec7e8
1.1808486,-5.3103857,neighbor,12670695,Large-Scale Deep Learning on the YFCC100M Dataset,0.07023417949676514,#aec7e8
6.1406903,-6.4312873,neighbor,12670695,Training CNNs with Low-Rank Filters for Efficient Image Classification,0.07049262523651123,#aec7e8
11.043748,-0.6938768,neighbor,12670695,"FINN: A Framework for Fast, Scalable Binarized Neural Network Inference",0.07063955068588257,#aec7e8
4.9449825,-7.9991736,neighbor,12670695,ESPACE: Accelerating Convolutional Neural Networks via Eliminating Spatial and Channel Redundancy,0.0708765983581543,#aec7e8
-4.514439,-1.7249895,neighbor,12670695,Multi-scale Recognition with DAG-CNNs,0.07118523120880127,#aec7e8
-9.953619,7.232756,neighbor,12670695,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.07202261686325073,#aec7e8
-8.171648,6.71922,neighbor,12670695,ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks,0.07205039262771606,#aec7e8
-9.168817,-5.803375,neighbor,12670695,Designing Deep Convolutional Neural Networks for Continuous Object Orientation Estimation,0.07213979959487915,#aec7e8
8.920171,6.6432867,neighbor,12670695,GPGPU Accelerated Deep Object Classification on a Heterogeneous Mobile Platform,0.07234078645706177,#aec7e8
11.542363,3.8410354,neighbor,12670695,Energy-efficient ConvNets through approximate computing,0.07239675521850586,#aec7e8
-12.225413,7.255826,neighbor,12670695,PVANet: Lightweight Deep Neural Networks for Real-time Object Detection,0.07251155376434326,#aec7e8
-4.079643,7.733274,neighbor,12670695,Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey,0.07260477542877197,#aec7e8
9.604642,5.3921223,neighbor,12670695,CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android,0.07301318645477295,#aec7e8
5.281022,-0.45632964,neighbor,12670695,Quantization and Training of Low Bit-Width Convolutional Neural Networks for Object Detection,0.07301932573318481,#aec7e8
-10.666036,4.9600997,neighbor,12670695,Beyond Skip Connections: Top-Down Modulation for Object Detection,0.07306474447250366,#aec7e8
11.354612,3.9509246,neighbor,12670695,Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning,0.07307809591293335,#aec7e8
8.732203,-7.785326,neighbor,12670695,Compressing Convolutional Neural Networks in the Frequency Domain,0.0732346773147583,#aec7e8
-0.28548265,-3.6248093,neighbor,12670695,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.07328099012374878,#aec7e8
-3.7513425,-0.44809973,neighbor,12670695,Multi-Scale Dense Convolutional Networks for Efficient Prediction,0.07346838712692261,#aec7e8
-10.5222225,5.986689,neighbor,12670695,Feature Pyramid Networks for Object Detection,0.07367819547653198,#aec7e8
-5.6204715,5.513563,neighbor,12670695,Building high-level features using large scale unsupervised learning,0.07397079467773438,#aec7e8
1.9648283,-0.35059765,neighbor,12670695,Xception: Deep Learning with Depthwise Separable Convolutions,0.07408076524734497,#aec7e8
9.428208,2.148724,neighbor,12670695,Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices,0.07428789138793945,#aec7e8
-10.621171,7.55401,neighbor,12670695,Object Detection Networks on Convolutional Feature Maps,0.07464981079101562,#aec7e8
-3.8067229,0.6540792,neighbor,12670695,Conditional Deep Learning for energy-efficient and enhanced pattern recognition,0.07501238584518433,#aec7e8
10.653892,-6.73269,neighbor,12670695,Fast Training of Convolutional Networks through FFTs,0.07514959573745728,#aec7e8
-1.6693237,-3.5826423,neighbor,12670695,Feature Representation in Convolutional Neural Networks,0.07516419887542725,#aec7e8
10.119339,-0.28492558,neighbor,12670695,Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations,0.07523655891418457,#aec7e8
-6.834398,-0.20622212,neighbor,12670695,Places205-VGGNet Models for Scene Recognition,0.07538372278213501,#aec7e8
7.3341227,-1.2920462,neighbor,12670695,DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients,0.07551872730255127,#aec7e8
-2.1922638,2.6258934,neighbor,12670695,Spatially Adaptive Computation Time for Residual Networks,0.07554912567138672,#aec7e8
8.437328,0.046035122,neighbor,12670695,Training Deep Nets with Sublinear Memory Cost,0.07594305276870728,#aec7e8
8.654212,-5.398341,neighbor,12670695,Prune the Convolutional Neural Networks with Sparse Shrink,0.07631689310073853,#aec7e8
-0.64488465,-4.705792,neighbor,12670695,Application of Convolutional Neural Network for Image Classification on Pascal VOC Challenge 2012 dataset,0.07638770341873169,#aec7e8
10.007046,-5.0921597,neighbor,12670695,Faster CNNs with Direct Sparse Convolutions and Guided Pruning,0.07639110088348389,#aec7e8
11.481269,-4.2521186,neighbor,12670695,Parsimonious Inference on Convolutional Neural Networks: Learning and applying on-line kernel activation rules,0.07658994197845459,#aec7e8
-9.303355,1.4744725,neighbor,12670695,Dense CNN Learning with Equivalent Mappings,0.07672256231307983,#aec7e8
-11.415419,8.914251,neighbor,12670695,"YOLO9000: Better, Faster, Stronger",0.07678443193435669,#aec7e8
-14.7663,8.831358,neighbor,12670695,Shallow Networks for High-accuracy Road Object-detection,0.0768173336982727,#aec7e8
0.7345771,1.0268402,neighbor,12670695,Diving deeper into mentee networks,0.07695889472961426,#aec7e8
-10.630572,-0.18298003,neighbor,12670695,Deep structured features for semantic segmentation,0.07732105255126953,#aec7e8
6.8534827,-5.8716774,neighbor,12670695,Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups,0.07736861705780029,#aec7e8
-9.754624,0.20831726,neighbor,12670695,Fully convolutional networks for semantic segmentation,0.07739496231079102,#aec7e8
5.6311917,-7.6320353,neighbor,12670695,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.07769101858139038,#aec7e8
-13.616044,9.625897,neighbor,12670695,Evolving boxes for fast vehicle detection,0.07775139808654785,#aec7e8
-11.845102,5.0078545,neighbor,12670695,Learning Chained Deep Features and Classifiers for Cascade in Object Detection,0.07804012298583984,#aec7e8
-8.653056,7.6255074,neighbor,12670695,LocNet: Improving Localization Accuracy for Object Detection,0.0780569314956665,#aec7e8
-8.48413,-6.2955484,neighbor,12670695,Image-Based Localization Using Hourglass Networks,0.07813036441802979,#aec7e8
-1.7494149,0.0035565437,neighbor,12670695,Resfeats: Residual network based features for image classification,0.07824772596359253,#aec7e8
-3.5771089,-5.4903774,neighbor,12670695,On the Exploration of Convolutional Fusion Networks for Visual Recognition,0.07848691940307617,#aec7e8
9.39988,-5.128954,neighbor,12670695,Compact Deep Convolutional Neural Networks With Coarse Pruning,0.0786135196685791,#aec7e8
-7.7636957,-6.6707344,neighbor,12670695,Warped Convolutions: Efficient Invariance to Spatial Transformations,0.07870137691497803,#aec7e8
-0.857335,0.98375636,neighbor,12670695,Aggregated Residual Transformations for Deep Neural Networks,0.07903707027435303,#aec7e8
9.5203705,-2.4244812,neighbor,12670695,Compressing Neural Networks with the Hashing Trick,0.07909631729125977,#aec7e8
-4.91015,-4.3985567,neighbor,12670695,Bilinear CNN Models for Fine-Grained Visual Recognition,0.07928192615509033,#aec7e8
-9.579423,8.220912,neighbor,12670695,Keypoint Density-based Region Proposal for Fine-Grained Object Detection and Classification using Regions with Convolutional Neural Network Features,0.07959973812103271,#aec7e8
-0.56712294,8.0377445,query,12803511,Conditional Generative Adversarial Nets,0.0,#aec7e8
0.3472189,8.20939,neighbor,12803511,Generative Adversarial Nets,0.03980588912963867,#aec7e8
0.13074078,9.020802,neighbor,12803511,Generative adversarial networks,0.044538915157318115,#aec7e8
2.787677,3.7569225,neighbor,12803511,Deep Directed Generative Autoencoders,0.058535218238830566,#aec7e8
-5.034102,-2.2115898,neighbor,12803511,Semi-supervised Learning with Deep Generative Models,0.05877506732940674,#aec7e8
-5.2178907,6.5785995,neighbor,12803511,Recognizing Hand-written Digits Using Hierarchical Products of Experts,0.06420153379440308,#aec7e8
3.6719418,4.105092,neighbor,12803511,Joint Training of Deep Auto-Encoders,0.06653082370758057,#aec7e8
0.63156796,3.6762593,neighbor,12803511,Multimodal Transitions for Generative Stochastic Networks,0.07045197486877441,#aec7e8
1.9497025,4.213411,neighbor,12803511,Layer-wise learning of deep generative models,0.0711132287979126,#aec7e8
-5.535184,-2.1810114,neighbor,12803511,Asymptotic Analysis of Generative Semi-Supervised Learning,0.07728904485702515,#aec7e8
-6.275117,-3.1223931,neighbor,12803511,Stochastic Discriminative EM,0.07792806625366211,#aec7e8
1.0573181,-1.4918022,neighbor,12803511,Convolutional Factor Graphs as Probabilistic Models,0.08010870218276978,#aec7e8
-7.1877284,-2.2793472,neighbor,12803511,Stochastic feature mapping for PAC-Bayes classification,0.08086973428726196,#aec7e8
5.552006,6.186511,neighbor,12803511,How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation,0.08198446035385132,#aec7e8
-0.8074662,10.470166,neighbor,12803511,Learning Generative Models with Visual Attention,0.08238941431045532,#aec7e8
-0.5705024,-5.3872123,neighbor,12803511,Conditional Random Field Autoencoders for Unsupervised Structured Prediction,0.08324295282363892,#aec7e8
-6.1710777,-8.522113,neighbor,12803511,A Logic-based Approach to Generatively Defined Discriminative Modeling,0.08380216360092163,#aec7e8
2.7852147,2.854802,neighbor,12803511,Deep AutoRegressive Networks,0.08397603034973145,#aec7e8
-4.715299,-4.385746,neighbor,12803511,Efficient Methods for Unsupervised Learning of Probabilistic Models,0.08462649583816528,#aec7e8
-4.4578214,-1.9650284,neighbor,12803511,Exponential Family Hybrid Semi-Supervised Learning,0.08509576320648193,#aec7e8
0.5478007,11.91511,neighbor,12803511,NICE: Non-linear Independent Components Estimation,0.08549201488494873,#aec7e8
0.9362908,1.5889671,neighbor,12803511,Stochastic Back-propagation and Variational Inference in Deep Latent Gaussian Models,0.08583134412765503,#aec7e8
-0.33275756,4.349135,neighbor,12803511,Modelling the Probability Density of Markov Sources,0.08691394329071045,#aec7e8
-8.483916,-5.762667,neighbor,12803511,Refining Generative Language Models using Discriminative Learning,0.08719110488891602,#aec7e8
2.5377803,8.599567,neighbor,12803511,"On the Claim for the Existence of ""Adversarial Examples"" in Deep Learning Neural Networks",0.08768004179000854,#aec7e8
0.14752218,-6.4248714,neighbor,12803511,Learning Deep Structured Models,0.08768033981323242,#aec7e8
1.7593138,0.660557,neighbor,12803511,Learning the Structure of Deep Sparse Graphical Models,0.08779966831207275,#aec7e8
4.9535575,-3.0735688,neighbor,12803511,Conditional Restricted Boltzmann Machines for Structured Output Prediction,0.08787268400192261,#aec7e8
-10.349267,-6.5733666,neighbor,12803511,Adversarial Evaluation for Models of Natural Language,0.08826148509979248,#aec7e8
-2.4615107,3.1838262,neighbor,12803511,Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures,0.08894175291061401,#aec7e8
-1.5660689,-0.469953,neighbor,12803511,Auto-Encoding Variational Bayes,0.08926725387573242,#aec7e8
-6.586362,-1.479087,neighbor,12803511,Generative Prior Knowledge for Discriminative Classification,0.08964753150939941,#aec7e8
-5.745919,-0.11664913,neighbor,12803511,Statistical topic models for multi-label document classification,0.08984249830245972,#aec7e8
6.920645,4.919614,neighbor,12803511,Autoencoder Trees,0.09006398916244507,#aec7e8
-0.31462988,1.4588443,neighbor,12803511,Indian Buffet Process Deep Generative Models,0.09026771783828735,#aec7e8
7.321942,5.836483,neighbor,12803511,An introduction to deep learning,0.09032344818115234,#aec7e8
5.041174,0.48282397,neighbor,12803511,On Training Deep Boltzmann Machines,0.09075099229812622,#aec7e8
-3.662595,-3.607669,neighbor,12803511,Domain Knowledge Uncertainty and Probabilistic Parameter Constraints,0.09076917171478271,#aec7e8
0.5939522,-7.5432925,neighbor,12803511,Exploiting compositionality to explore a large space of model structures,0.09194856882095337,#aec7e8
8.970197,-0.6809664,neighbor,12803511,Deep Predictive Coding Networks,0.09201669692993164,#aec7e8
7.624386,2.54215,neighbor,12803511,Learning with Hierarchical-Deep Models,0.09237170219421387,#aec7e8
2.9578292,5.691503,neighbor,12803511,Generalized Denoising Auto-Encoders as Generative Models,0.0927966833114624,#aec7e8
-5.946076,-4.6650076,neighbor,12803511,Inference with Discriminative Posterior,0.09280532598495483,#aec7e8
10.040816,2.7899265,neighbor,12803511,Modeling language and cognition with deep unsupervised learning: a tutorial overview,0.09289181232452393,#aec7e8
-8.555193,-0.7525,neighbor,12803511,Learning Instance Concepts from Multiple-Instance Data with Bags as Distributions,0.09290117025375366,#aec7e8
5.725321,0.5032894,neighbor,12803511,Joint Training Deep Boltzmann Machines for Classification,0.09305745363235474,#aec7e8
10.476986,8.937718,neighbor,12803511,Unsupervised feature learning by augmenting single images,0.09496808052062988,#aec7e8
8.430691,1.7182659,neighbor,12803511,Maximally Informative Hierarchical Representations of High-Dimensional Data,0.09499883651733398,#aec7e8
3.3449314,-0.5848357,neighbor,12803511,Reweighted Wake-Sleep,0.09503746032714844,#aec7e8
0.6910217,0.74806195,neighbor,12803511,Deep Gaussian Processes,0.09538757801055908,#aec7e8
-5.676497,6.3277545,neighbor,12803511,A generative-discriminative hybrid for sequential data classification [image classification example],0.09561467170715332,#aec7e8
0.6694072,6.105634,neighbor,12803511,Bounding the Test Log-Likelihood of Generative Models,0.09571593999862671,#aec7e8
-9.194112,7.366301,neighbor,12803511,Deformable part models are convolutional neural networks,0.09588593244552612,#aec7e8
-9.822839,-2.6574726,neighbor,12803511,Discriminative Clustering via Generative Feature Mapping,0.09644752740859985,#aec7e8
7.9212027,-1.6670523,neighbor,12803511,Temporal Autoencoding Improves Generative Models of Time Series,0.09649735689163208,#aec7e8
10.227351,8.819163,neighbor,12803511,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.09680509567260742,#aec7e8
-6.1435413,-8.752911,neighbor,12803511,Viterbi training in PRISM,0.0969342589378357,#aec7e8
1.6801634,-1.2249428,neighbor,12803511,Deep Mixtures of Factor Analysers,0.09709674119949341,#aec7e8
-2.4728553,-8.148791,neighbor,12803511,Learning Markov Networks with Context-Specific Independences,0.09716653823852539,#aec7e8
0.91258115,10.513155,neighbor,12803511,Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs,0.09752684831619263,#aec7e8
3.5301702,-4.219496,neighbor,12803511,Approximated Structured Prediction for Learning Large Scale Graphical Models,0.09756588935852051,#aec7e8
8.742645,7.363278,neighbor,12803511,Scheduled denoising autoencoders,0.09768688678741455,#aec7e8
6.7986546,1.588993,neighbor,12803511,Learning Feature Hierarchies with Centered Deep Boltzmann Machines,0.09771281480789185,#aec7e8
-0.05614448,13.203393,neighbor,12803511,Modeling Images using Transformed Indian Buffet Processes,0.09795796871185303,#aec7e8
-10.503058,-2.7784572,neighbor,12803511,Generative Models that Discover Dependencies Between Data Sets,0.09806495904922485,#aec7e8
-5.2010126,-6.4666166,neighbor,12803511,Learning Graphical Models for Hypothesis Testing and Classification,0.09811419248580933,#aec7e8
-9.6995735,5.1147447,neighbor,12803511,A Bayesian generative model for learning semantic hierarchies,0.09837985038757324,#aec7e8
8.011838,8.277776,neighbor,12803511,Deeply-Supervised Nets,0.09842562675476074,#aec7e8
5.4042544,0.9621449,neighbor,12803511,Joint Training of Deep Boltzmann Machines,0.0988953709602356,#aec7e8
-2.371263,6.494237,neighbor,12803511,Learning Mixtures of Bernoulli Templates by Two-Round EM with Performance Guarantee,0.09989744424819946,#aec7e8
-3.5315807,9.001663,neighbor,12803511,Autonomous cleaning of corrupted scanned documents — A generative modeling approach,0.09998893737792969,#aec7e8
6.5813003,-6.170568,neighbor,12803511,Learning to Generate Networks,0.10004186630249023,#aec7e8
-4.6256766,-5.3729215,neighbor,12803511,Model-based machine learning,0.10016220808029175,#aec7e8
-10.321252,4.520611,neighbor,12803511,Unsupervised modeling and recognition of object categories with combination of visual contents and geometric similarity links,0.10023462772369385,#aec7e8
-2.3249753,-3.7307172,neighbor,12803511,Domain Adaptation for Statistical Classifiers,0.10064429044723511,#aec7e8
-2.8202367,3.3109863,neighbor,12803511,Learning Factored Representations in a Deep Mixture of Experts,0.10092496871948242,#aec7e8
-2.399483,-8.412353,neighbor,12803511,Closed-Form Learning of Markov Networks from Dependency Networks,0.10096877813339233,#aec7e8
-3.9174185,-6.8854456,neighbor,12803511,Exact Maximum Margin Structure Learning of Bayesian Networks,0.10115188360214233,#aec7e8
8.3397875,9.128399,neighbor,12803511,Improving Deep Neural Networks with Probabilistic Maxout Units,0.10120439529418945,#aec7e8
1.3362089,2.903034,neighbor,12803511,On the Equivalence between Deep NADE and Generative Stochastic Networks,0.10135173797607422,#aec7e8
-11.239051,4.627364,neighbor,12803511,Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice,0.10143518447875977,#aec7e8
5.05992,-0.7089295,neighbor,12803511,Deep Tempering,0.10145694017410278,#aec7e8
0.67214096,13.04316,neighbor,12803511,Mixtures of Conditional Gaussian Scale Mixtures Applied to Multiscale Image Representations,0.10146790742874146,#aec7e8
11.083743,2.8930967,neighbor,12803511,Supervised Generative Reconstruction: An Efficient Way To Flexibly Store and Recognize Patterns,0.10166949033737183,#aec7e8
1.1060044,13.817065,neighbor,12803511,Learning Multiscale Representations of Natural Scenes Using Dirichlet Processes,0.10167795419692993,#aec7e8
10.585194,6.2689686,neighbor,12803511,A Probabilistic WKL Rule for Incremental Feature Learning and Pattern Recognition,0.10178881883621216,#aec7e8
-7.010737,0.26909685,neighbor,12803511,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,0.10191178321838379,#aec7e8
8.046387,3.2242465,neighbor,12803511,Deep learning,0.1020248532295227,#aec7e8
6.8925157,-1.7252254,neighbor,12803511,Echo-State Conditional Restricted Boltzmann Machines,0.10208076238632202,#aec7e8
3.445332,-1.9329349,neighbor,12803511,Herded Gibbs Sampling,0.10212886333465576,#aec7e8
-1.6105082,11.403532,neighbor,12803511,Modeling the joint density of two images under a variety of transformations,0.10242980718612671,#aec7e8
6.63195,7.1914134,neighbor,12803511,Rate-Distortion Auto-Encoders,0.10250508785247803,#aec7e8
-5.1950846,1.5757321,neighbor,12803511,Autotagging music with conditional restricted Boltzmann machines,0.10276967287063599,#aec7e8
-1.3607622,-4.2558985,neighbor,12803511,Composition of Conditional Random Fields for Transfer Learning,0.10295194387435913,#aec7e8
-2.8452997,-9.87303,neighbor,12803511,Part-of-Speech Tagging using Virtual Evidence and Negative Training,0.10313272476196289,#aec7e8
-10.133604,3.1620524,neighbor,12803511,A revisit of Generative Model for Automatic Image Annotation using Markov Random Fields,0.10332214832305908,#aec7e8
-9.494789,6.5706973,neighbor,12803511,"Recursive compositional models: Representation, learning, and inference",0.10336571931838989,#aec7e8
-7.7252593,-3.4572778,neighbor,12803511,Bias-variance tradeoff in hybrid generative-discriminative models,0.10337448120117188,#aec7e8
8.787278,6.135457,neighbor,12803511,Switched linear encoding with rectified linear autoencoders,0.10342466831207275,#aec7e8
1.6506431,-5.184411,neighbor,12803511,Conditional Random Fields and Support Vector Machines: A Hybrid Approach,0.10360419750213623,#aec7e8
8.043002,6.5861025,neighbor,12803511,A Winner-Take-All Method for Training Sparse Convolutional Autoencoders,0.1038253903388977,#aec7e8
-3.7103317,0.17921518,query,13756489,Attention is All you Need,0.0,#ff7f0e
-2.483435,1.6275929,neighbor,13756489,Convolutional Sequence to Sequence Learning,0.04370272159576416,#ff7f0e
-1.5057777,4.253243,neighbor,13756489,Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU,0.044155001640319824,#ff7f0e
6.9078217,-0.03962932,neighbor,13756489,Recurrent Neural Machine Translation,0.04417741298675537,#ff7f0e
8.897429,3.9485729,neighbor,13756489,Effective Approaches to Attention-based Neural Machine Translation,0.046636223793029785,#ff7f0e
3.812889,-0.6811575,neighbor,13756489,Neural Machine Translation with Source-Side Latent Graph Parsing,0.047593533992767334,#ff7f0e
3.326558,-1.2581183,neighbor,13756489,Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,0.048135459423065186,#ff7f0e
6.8311787,-1.2649554,neighbor,13756489,Recurrent Continuous Translation Models,0.04834073781967163,#ff7f0e
-1.3631927,1.9245409,neighbor,13756489,Neural Machine Translation in Linear Time,0.04865998029708862,#ff7f0e
-8.300107,-5.105636,neighbor,13756489,DRAGNN: A Transition-based Framework for Dynamically Connected Neural Networks,0.04910808801651001,#ff7f0e
-6.753218,1.6492498,neighbor,13756489,Depth-Gated LSTM,0.04914671182632446,#ff7f0e
2.5612736,4.1569557,neighbor,13756489,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,0.05084043741226196,#ff7f0e
7.32416,0.7577256,neighbor,13756489,A GRU-Gated Attention Model for Neural Machine Translation,0.050891339778900146,#ff7f0e
-7.8572273,0.63621455,neighbor,13756489,Top-down Tree Long Short-Term Memory Networks,0.05101191997528076,#ff7f0e
-10.121207,5.437286,neighbor,13756489,Attention and Augmented Recurrent Neural Networks,0.05102008581161499,#ff7f0e
0.6175756,3.0230033,neighbor,13756489,Deep Neural Machine Translation with Linear Associative Unit,0.05121082067489624,#ff7f0e
-8.977416,-10.325099,neighbor,13756489,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,0.05129098892211914,#ff7f0e
6.547583,-2.7807336,neighbor,13756489,Classification-based RNN machine translation using GRUs,0.051387012004852295,#ff7f0e
11.261409,5.0292115,neighbor,13756489,Tree-to-Sequence Attentional Neural Machine Translation,0.05147165060043335,#ff7f0e
-6.135507,-1.1264782,neighbor,13756489,Neural Semantic Encoders,0.05152261257171631,#ff7f0e
4.801982,2.2506266,neighbor,13756489,Learning to Parse and Translate Improves Neural Machine Translation,0.05169963836669922,#ff7f0e
-9.967936,-12.18279,neighbor,13756489,Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles,0.05188167095184326,#ff7f0e
11.883818,2.9131134,neighbor,13756489,Supervised Attentions for Neural Machine Translation,0.05246013402938843,#ff7f0e
-7.4425416,-5.6796937,neighbor,13756489,Globally Normalized Transition-Based Neural Networks,0.05288439989089966,#ff7f0e
-9.466505,-6.383997,neighbor,13756489,Recurrent Neural Network Grammars,0.05321377515792847,#ff7f0e
12.380181,2.6037927,neighbor,13756489,Neural Machine Translation with Supervised Attention,0.05341154336929321,#ff7f0e
10.082343,3.0502663,neighbor,13756489,Neural Machine Translation with Recurrent Attention Modeling,0.05359506607055664,#ff7f0e
-6.5133247,4.533575,neighbor,13756489,Language Modeling with Gated Convolutional Networks,0.05359768867492676,#ff7f0e
-9.66898,3.72433,neighbor,13756489,Going Wider: Recurrent Neural Network With Parallel Cells,0.05382585525512695,#ff7f0e
9.391291,4.030654,neighbor,13756489,Learning When to Attend for Neural Machine Translation,0.05484408140182495,#ff7f0e
-7.8510346,-10.884187,neighbor,13756489,The Inside-Outside Recursive Neural Network model for Dependency Parsing,0.05510568618774414,#ff7f0e
3.1128132,-2.8145723,neighbor,13756489,Predicting Target Language CCG Supertags Improves Neural Machine Translation,0.05511671304702759,#ff7f0e
3.560991,5.2195096,neighbor,13756489,Transfer Learning for Low-Resource Neural Machine Translation,0.055125296115875244,#ff7f0e
-5.030102,4.1998715,neighbor,13756489,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,0.05525881052017212,#ff7f0e
6.281966,11.067826,neighbor,13756489,Sequence-to-Sequence Models Can Directly Translate Foreign Speech,0.055311620235443115,#ff7f0e
-1.3491367,5.5210576,neighbor,13756489,The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT,0.055387914180755615,#ff7f0e
4.401083,3.2088816,neighbor,13756489,Neural Machine Translation by Jointly Learning to Align and Translate,0.05542570352554321,#ff7f0e
-11.002067,-8.321889,neighbor,13756489,Joint RNN-Based Greedy Parsing and Word Composition,0.05569863319396973,#ff7f0e
5.5756392,5.2336836,neighbor,13756489,Mutual Information and Diverse Decoding Improve Neural Machine Translation,0.05572092533111572,#ff7f0e
5.8522224,-2.793574,neighbor,13756489,On the Properties of Neural Machine Translation: Encoder–Decoder Approaches,0.0562017560005188,#ff7f0e
9.379051,-2.283148,neighbor,13756489,genCNN: A Convolutional Architecture for Word Sequence Prediction,0.05623519420623779,#ff7f0e
-10.428781,-13.267166,neighbor,13756489,Grammar as a Foreign Language,0.05630296468734741,#ff7f0e
-3.2898743,-6.6735487,neighbor,13756489,Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks,0.05667334794998169,#ff7f0e
-9.402852,2.6521475,neighbor,13756489,Recurrent Highway Networks,0.05719536542892456,#ff7f0e
4.3722196,6.8617673,neighbor,13756489,Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder,0.057389140129089355,#ff7f0e
1.506011,4.458123,neighbor,13756489,Iterative Refinement for Machine Translation,0.05748546123504639,#ff7f0e
6.101307,3.7166932,neighbor,13756489,Linguistic Input Features Improve Neural Machine Translation,0.05758029222488403,#ff7f0e
-10.558399,5.535193,neighbor,13756489,Recurrent Neural Network Regularization,0.05761396884918213,#ff7f0e
-10.058661,-10.968043,neighbor,13756489,Shift-Reduce Constituent Parsing with Neural Lookahead Features,0.057969868183135986,#ff7f0e
-0.10216386,0.27283692,neighbor,13756489,A Character-level Decoder without Explicit Segmentation for Neural Machine Translation,0.05813676118850708,#ff7f0e
6.1763835,1.4665798,neighbor,13756489,Encoding Source Language with Convolutional Neural Network for Machine Translation,0.058174192905426025,#ff7f0e
10.539627,0.6538746,neighbor,13756489,A Coverage Embedding Model for Neural Machine Translation,0.058399736881256104,#ff7f0e
-11.037905,-9.863636,neighbor,13756489,A* CCG Parsing with a Supertag and Dependency Factored Model,0.05848515033721924,#ff7f0e
-11.242344,3.970685,neighbor,13756489,Scaling recurrent neural network language models,0.058512985706329346,#ff7f0e
-0.23553087,-1.6659573,neighbor,13756489,Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks,0.058524906635284424,#ff7f0e
-10.469994,2.833994,neighbor,13756489,Fast-Slow Recurrent Neural Networks,0.05858123302459717,#ff7f0e
3.273589,5.8952537,neighbor,13756489,Improving Neural Machine Translation Models with Monolingual Data,0.059349775314331055,#ff7f0e
-7.9642777,4.294322,neighbor,13756489,Gated Feedback Recurrent Neural Networks,0.05941307544708252,#ff7f0e
-0.47584733,7.713497,neighbor,13756489,Unfolding and Shrinking Neural Machine Translation Ensembles,0.059507787227630615,#ff7f0e
0.627619,2.2028186,neighbor,13756489,Neural Transformation Machine: A New Architecture for Sequence-to-Sequence Learning,0.05951732397079468,#ff7f0e
-9.294621,-11.362085,neighbor,13756489,Incremental Parsing with Minimal Features Using Bi-Directional LSTM,0.059606850147247314,#ff7f0e
3.4266746,9.354503,neighbor,13756489,MT/IE: Cross-lingual Open Information Extraction with Neural Sequence-to-Sequence Models,0.059712886810302734,#ff7f0e
9.608979,6.938925,neighbor,13756489,Agreement-Based Joint Training for Bidirectional Attention-Based Neural Machine Translation,0.059916794300079346,#ff7f0e
10.174396,6.6000156,neighbor,13756489,Incorporating Structural Alignment Biases into an Attentional Neural Translation Model,0.06004130840301514,#ff7f0e
-9.599196,-7.3099413,neighbor,13756489,Neural CRF Parsing,0.060677528381347656,#ff7f0e
10.682901,4.1596704,neighbor,13756489,Temporal Attention Model for Neural Machine Translation,0.06073266267776489,#ff7f0e
-8.946743,3.7714489,neighbor,13756489,Quasi-Recurrent Neural Networks,0.06078225374221802,#ff7f0e
-6.938013,-10.639628,neighbor,13756489,Bi-directional Attention with Agreement for Dependency Parsing,0.06091797351837158,#ff7f0e
3.0042787,1.9941069,neighbor,13756489,Neural Machine Translation with External Phrase Memory,0.06104964017868042,#ff7f0e
-10.263856,0.9602702,neighbor,13756489,Long-Short Range Context Neural Networks for Language Modeling,0.06109881401062012,#ff7f0e
4.323117,0.41830057,neighbor,13756489,Modeling Source Syntax for Neural Machine Translation,0.06115812063217163,#ff7f0e
4.8684363,7.074279,neighbor,13756489,"Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",0.061164140701293945,#ff7f0e
-5.5518417,-10.177381,neighbor,13756489,Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages,0.06117373704910278,#ff7f0e
1.240458,6.3048267,neighbor,13756489,Vocabulary Selection Strategies for Neural Machine Translation,0.06127816438674927,#ff7f0e
-2.8282628,-8.490115,neighbor,13756489,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,0.0615229606628418,#ff7f0e
-0.031031094,9.38363,neighbor,13756489,Chunk-Based Bi-Scale Decoder for Neural Machine Translation,0.06152939796447754,#ff7f0e
-6.9700627,-2.9441504,neighbor,13756489,Linguistic Knowledge as Memory for Recurrent Neural Networks,0.06152939796447754,#ff7f0e
-7.186936,0.06385019,neighbor,13756489,A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs,0.06175839900970459,#ff7f0e
10.666312,1.1548092,neighbor,13756489,Modeling Coverage for Neural Machine Translation,0.061853110790252686,#ff7f0e
7.2866383,5.8868055,neighbor,13756489,Can neural machine translation do simultaneous translation?,0.062043607234954834,#ff7f0e
8.497192,1.7873632,neighbor,13756489,Context Gates for Neural Machine Translation,0.06215626001358032,#ff7f0e
-0.2701708,6.4770236,neighbor,13756489,Massive Exploration of Neural Machine Translation Architectures,0.06256884336471558,#ff7f0e
1.6678348,2.0737007,neighbor,13756489,Memory-enhanced Decoder for Neural Machine Translation,0.06259685754776001,#ff7f0e
7.4119205,8.342213,neighbor,13756489,Target-Bidirectional Neural Models for Machine Transliteration,0.06261295080184937,#ff7f0e
-7.661843,-9.26516,neighbor,13756489,"Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing",0.06264698505401611,#ff7f0e
-9.237851,-8.663075,neighbor,13756489,Stack-propagation: Improved Representation Learning for Syntax,0.06273335218429565,#ff7f0e
-8.170071,-8.561376,neighbor,13756489,SyntaxNet Models for the CoNLL 2017 Shared Task,0.06299680471420288,#ff7f0e
-9.12108,-12.940382,neighbor,13756489,Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary Tasks,0.06304854154586792,#ff7f0e
-12.178096,-10.344813,neighbor,13756489,Training with Exploration Improves a Greedy Stack LSTM Parser,0.06308257579803467,#ff7f0e
-4.298578,-10.744125,neighbor,13756489,An Efficient Cross-lingual Model for Sentence Classification Using Convolutional Neural Network,0.06324225664138794,#ff7f0e
-9.533499,-3.1862402,neighbor,13756489,Online Segment to Segment Neural Transduction,0.06337869167327881,#ff7f0e
-11.837655,-6.5374904,neighbor,13756489,Language to Logical Form with Neural Attention,0.06341826915740967,#ff7f0e
-7.836205,4.8646965,neighbor,13756489,Gated Recurrent Neural Tensor Network,0.06357991695404053,#ff7f0e
14.691792,1.4565814,neighbor,13756489,Online and Linear-Time Attention by Enforcing Monotonic Alignments,0.06367737054824829,#ff7f0e
0.7816381,-2.5047271,neighbor,13756489,RIGA at SemEval-2016 Task 8: Impact of Smatch Extensions and Character-Level Neural Translation on AMR Parsing Accuracy,0.06371897459030151,#ff7f0e
-11.964667,-8.944759,neighbor,13756489,Neural Shift-Reduce CCG Semantic Parsing,0.06378424167633057,#ff7f0e
14.337042,1.9089346,neighbor,13756489,Unsupervised Pretraining for Sequence to Sequence Learning,0.06381303071975708,#ff7f0e
-1.2238023,-1.9008971,neighbor,13756489,Jointly learning to align and convert graphemes to phonemes with neural attention models,0.06392312049865723,#ff7f0e
7.6409826,3.4741201,neighbor,13756489,Does Neural Machine Translation Benefit from Larger Context?,0.06398791074752808,#ff7f0e
-3.356274,-7.3680615,neighbor,13756489,Multi-Task Cross-Lingual Sequence Tagging from Scratch,0.06403803825378418,#ff7f0e
5.4860044,6.6764684,neighbor,13756489,Enabling Multi-Source Neural Machine Translation By Concatenating Source Sentences In Multiple Languages,0.06408292055130005,#ff7f0e
-8.357541,-4.415126,query,15238391,Playing Atari with Deep Reinforcement Learning,0.0,#ffbb78
-9.181527,-3.6896842,neighbor,15238391,Reinforcement learning for the soccer dribbling task,0.04980236291885376,#ffbb78
-9.879529,-1.2301776,neighbor,15238391,Investigating Contingency Awareness Using Atari 2600 Games,0.050976574420928955,#ffbb78
-11.602295,1.301661,neighbor,15238391,Designing a Reinforcement Learning-based Adaptive AI for Large-Scale Strategy Games,0.05368995666503906,#ffbb78
-12.206634,2.1289403,neighbor,15238391,CLASSQ-L: A Q-Learning Algorithm for Adversarial Real-Time Strategy Games,0.05547970533370972,#ffbb78
-7.398664,-7.8495836,neighbor,15238391,Deep auto-encoder neural networks in reinforcement learning,0.05577373504638672,#ffbb78
-15.021733,2.4563143,neighbor,15238391,Learning Companion Behaviors Using Reinforcement Learning in Games,0.056894659996032715,#ffbb78
-0.5705336,-8.428038,neighbor,15238391,Reinforcement Learning in Robotics: Applications and Real-World Challenges,0.05731463432312012,#ffbb78
-11.336156,0.31739178,neighbor,15238391,The Arcade Learning Environment: An Evaluation Platform for General Agents,0.0580981969833374,#ffbb78
6.938545,0.32210886,neighbor,15238391,A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning,0.05871844291687012,#ffbb78
-8.492615,2.9691217,neighbor,15238391,Relational Reinforcement Learning in Infinite Mario,0.05919969081878662,#ffbb78
-2.7312665,4.4090667,neighbor,15238391,Learning to Play Using Low-Complexity Rule-Based Policies: Illustrations through Ms. Pac-Man,0.06046241521835327,#ffbb78
1.5793599,6.4290404,neighbor,15238391,Value Function Approximation in Reinforcement Learning Using the Fourier Basis,0.06069493293762207,#ffbb78
3.142437,0.6701793,neighbor,15238391,Scaling Up Reinforcement Learning through Targeted Exploration,0.06104731559753418,#ffbb78
0.4724364,-7.12148,neighbor,15238391,Reinforcement learning in robotics: A survey,0.061425745487213135,#ffbb78
-9.035122,-1.8041143,neighbor,15238391,The Self Organization of Context for Learning in MultiAgent Games,0.061628103256225586,#ffbb78
-5.1831856,-9.943132,neighbor,15238391,Biologically plausible reinforcement learning of continuous actions,0.06196814775466919,#ffbb78
7.477843,3.7016246,neighbor,15238391,Bayesian Learning of Noisy Markov Decision Processes,0.062247276306152344,#ffbb78
9.841463,4.305716,neighbor,15238391,Continuous Inverse Optimal Control with Locally Optimal Examples,0.06241220235824585,#ffbb78
4.3656535,9.085535,neighbor,15238391,Reinforcement Learning by Value Gradients,0.06292945146560669,#ffbb78
0.9401983,-5.006911,neighbor,15238391,RTMBA: A Real-Time Model-Based Reinforcement Learning Architecture for robot control,0.063540518283844,#ffbb78
2.3715777,-5.2440133,neighbor,15238391,Relational Reinforcement Learning with Continuous Actions by Combining Behavioural Cloning and Locally Weighted Regression,0.06375628709793091,#ffbb78
4.7237573,2.4980502,neighbor,15238391,What good are actions? Accelerating learning using learned action priors,0.06390315294265747,#ffbb78
2.644768,-1.7104049,neighbor,15238391,Safe Exploration of State and Action Spaces in Reinforcement Learning,0.06403154134750366,#ffbb78
6.378748,3.086818,neighbor,15238391,Monte Carlo Bayesian Reinforcement Learning,0.06414294242858887,#ffbb78
5.8894157,0.5831907,neighbor,15238391,Linear Off-Policy Actor-Critic,0.0641445517539978,#ffbb78
-10.009671,-2.9806654,neighbor,15238391,On Experiences in a Complex and Competitive Gaming Domain: Reinforcement Learning Meets RoboCup,0.06417566537857056,#ffbb78
0.869984,-8.464284,neighbor,15238391,Robot Skill Learning: From Reinforcement Learning to Evolution Strategies,0.06434661149978638,#ffbb78
-3.8142416,-13.379767,neighbor,15238391,Control Task for Reinforcement Learning with Known Optimal Solution for Discrete and Continuous Actions,0.06450331211090088,#ffbb78
9.597276,-4.1877193,neighbor,15238391,Automatic shaping and decomposition of reward functions,0.06524640321731567,#ffbb78
-1.9603813,-9.724916,neighbor,15238391,Exploring Deep and Recurrent Architectures for Optimal Control,0.06547820568084717,#ffbb78
3.6676023,2.1410973,neighbor,15238391,A Bayesian Sampling Approach to Exploration in Reinforcement Learning,0.06569743156433105,#ffbb78
-5.0942335,-3.2905726,neighbor,15238391,Multi-timescale nexting in a reinforcement learning robot,0.06612098217010498,#ffbb78
9.378159,0.12509729,neighbor,15238391,Exploring compact reinforcement-learning representations with linear regression,0.06666606664657593,#ffbb78
5.3195543,10.055158,neighbor,15238391,The Optimal Reward Baseline for Gradient-Based Reinforcement Learning,0.06689190864562988,#ffbb78
8.799908,3.900189,neighbor,15238391,Improving the efficiency of Bayesian inverse reinforcement learning,0.06695109605789185,#ffbb78
0.60106856,3.1156492,neighbor,15238391,Reinforcement Learning via AIXI Approximation,0.06695455312728882,#ffbb78
5.7714677,6.5410595,neighbor,15238391,Exploring parameter space in reinforcement learning,0.0675579309463501,#ffbb78
-9.579467,3.0650477,neighbor,15238391,An Object-Oriented Approach to Reinforcement Learning in an Action Game,0.06797069311141968,#ffbb78
0.42610157,4.220605,neighbor,15238391,Feature Reinforcement Learning in Practice,0.068031907081604,#ffbb78
-13.022172,3.5762677,neighbor,15238391,Learning Policies for First Person Shooter Games Using Inverse Reinforcement Learning,0.0681108832359314,#ffbb78
0.06255035,-11.145226,neighbor,15238391,Fast Reinforcement Learning for Vision-guided Mobile Robots,0.0682675838470459,#ffbb78
1.5143614,0.8086893,neighbor,15238391,T-Learning,0.06879860162734985,#ffbb78
2.7238715,-9.222614,neighbor,15238391,Socially guided intrinsic motivation for robot learning of motor skills,0.0688621997833252,#ffbb78
4.771704,-3.9429288,neighbor,15238391,Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping,0.06929576396942139,#ffbb78
1.634157,-0.9109175,neighbor,15238391,Direct Uncertainty Estimation in Reinforcement Learning,0.0698348879814148,#ffbb78
10.845245,3.9537299,neighbor,15238391,Inverse reinforcement learning with Gaussian process,0.07035630941390991,#ffbb78
4.598569,0.17378898,neighbor,15238391,Reducing Commitment to Tasks with Off-Policy Hierarchical Reinforcement Learning,0.07043242454528809,#ffbb78
2.7212088,3.278258,neighbor,15238391,Cover tree Bayesian reinforcement learning,0.07083392143249512,#ffbb78
-15.158744,1.6809227,neighbor,15238391,Agent Learning using Action-Dependent Learning Rates in Computer Role-Playing Games,0.07086962461471558,#ffbb78
-5.296681,-0.28153276,neighbor,15238391,Scaling Up Multi-agent Reinforcement Learning in Complex Domains,0.07091599702835083,#ffbb78
-2.8657799,-0.12656657,neighbor,15238391,Correction: Temporal-Difference Reinforcement Learning with Distributed Representations,0.07093900442123413,#ffbb78
-1.3297242,0.79794586,neighbor,15238391,Predictive State Temporal Difference Learning,0.07105964422225952,#ffbb78
-13.499472,1.1210147,neighbor,15238391,Examining Extended Dynamic Scripting in a Tactical Game Framework,0.07130461931228638,#ffbb78
10.698425,-1.9850674,neighbor,15238391,Reinforcement Learning in Partially Observable Markov Decision Processes using Hybrid Probabilistic Logic Programs,0.0713263750076294,#ffbb78
-13.967031,3.721513,neighbor,15238391,DRE-Bot: A hierarchical First Person Shooter bot using multiple Sarsa(λ) reinforcement learners,0.07152754068374634,#ffbb78
-3.388353,-1.2067229,neighbor,15238391,Autonomous reinforcement of behavioral sequences in neural dynamics,0.07153010368347168,#ffbb78
7.858559,0.6380263,neighbor,15238391,Kernel-Based Reinforcement Learning on Representative States,0.07175654172897339,#ffbb78
3.904527,-7.4579234,neighbor,15238391,Reinforcement Learning or Active Inference?,0.0720100998878479,#ffbb78
6.058729,9.189962,neighbor,15238391,Policy gradient methods,0.07205325365066528,#ffbb78
-12.090755,-0.1734473,neighbor,15238391,Modeling Unit Classes as Agents in Real-Time Strategy Games,0.07213956117630005,#ffbb78
3.691701,4.8005404,neighbor,15238391,Reinforcement Learning with a Gaussian mixture model,0.07234686613082886,#ffbb78
9.445362,8.296138,neighbor,15238391,Constrained reinforcement learning from intrinsic and extrinsic rewards,0.07235962152481079,#ffbb78
-2.776823,7.236433,neighbor,15238391,Multi-Agent Learning with Policy Prediction,0.07248616218566895,#ffbb78
-7.759913,6.097386,neighbor,15238391,Reinforcement learning from human reward: Discounting in episodic tasks,0.07256734371185303,#ffbb78
5.7453923,-2.6476245,neighbor,15238391,Efficient Reinforcement Learning in Deterministic Systems with Value Function Generalization,0.0726156234741211,#ffbb78
-8.28129,6.2359576,neighbor,15238391,Reinforcement learning in professional basketball players,0.07264399528503418,#ffbb78
-7.4418406,3.1830227,neighbor,15238391,The Thing that we Tried Didn't Work very Well: Deictic Representation in Reinforcement Learning,0.0726667046546936,#ffbb78
-6.691345,-7.751286,neighbor,15238391,Closed-Loop Learning of Visual Control Policies,0.07278239727020264,#ffbb78
9.067224,0.52298653,neighbor,15238391,Structured Kernel-Based Reinforcement Learning,0.0728217363357544,#ffbb78
-0.4093644,-8.113718,neighbor,15238391,Challenges for the policy representation when applying reinforcement learning in robotics,0.07295382022857666,#ffbb78
-0.9676482,3.6037326,neighbor,15238391,Universal Reinforcement Learning,0.07300132513046265,#ffbb78
3.260686,-4.619466,neighbor,15238391,CORL: A Continuous-state Offset-dynamics Reinforcement Learner,0.07303416728973389,#ffbb78
-13.378242,-0.410932,neighbor,15238391,Reinforcement Learning for Spatial Reasoning in Strategy Games,0.07309937477111816,#ffbb78
-4.765198,-0.702533,neighbor,15238391,Integrating Temporal Difference Methods and Self-Organizing Neural Networks for Reinforcement Learning With Delayed Evaluative Feedback,0.07310926914215088,#ffbb78
-6.110884,-10.182081,neighbor,15238391,Scaled free-energy based reinforcement learning for robust and efficient learning in high-dimensional state spaces,0.07312268018722534,#ffbb78
12.464815,-0.36556625,neighbor,15238391,Policy Improvement for POMDPs Using Normalized Importance Sampling,0.07320922613143921,#ffbb78
-1.8353802,-7.985214,neighbor,15238391,Multi-Task Policy Search,0.07334935665130615,#ffbb78
-16.133995,3.3509076,neighbor,15238391,Self-Play and Using an Expert to Learn to Play Backgammon with Temporal Difference Learning,0.07343834638595581,#ffbb78
-2.6178694,-5.603128,neighbor,15238391,APRIL: Active Preference-learning based Reinforcement Learning,0.07344788312911987,#ffbb78
3.950858,-1.8244723,neighbor,15238391,Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration,0.07371091842651367,#ffbb78
-0.2955823,-10.097416,neighbor,15238391,Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning,0.07379430532455444,#ffbb78
3.8972287,8.960545,neighbor,15238391,"The Local Optimality of Reinforcement Learning by Value Gradients, and its Relationship to Policy Gradient Learning",0.07397711277008057,#ffbb78
5.0238385,7.9225645,neighbor,15238391,Efficient Gradient Estimation for Motor Control Learning,0.07405680418014526,#ffbb78
-2.799275,6.671266,neighbor,15238391,A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics,0.07416033744812012,#ffbb78
7.193433,-1.0526228,neighbor,15238391,Value Function Approximation in Zero-Sum Markov Games,0.07416737079620361,#ffbb78
-3.4237058,-2.434737,neighbor,15238391,Reinforcement Learning Using a Continuous Time Actor-Critic Framework with Spiking Neurons,0.07417309284210205,#ffbb78
6.0525956,7.5480065,neighbor,15238391,Efficient Sample Reuse in Policy Gradients with Parameter-Based Exploration,0.07423853874206543,#ffbb78
3.8172846,-10.366353,neighbor,15238391,Sequence Learning by Backward Chaining in Synthetic Characters,0.07483994960784912,#ffbb78
1.6392835,7.205303,neighbor,15238391,Adaptive Bases for Reinforcement Learning,0.07487940788269043,#ffbb78
-8.507387,-0.03224223,neighbor,15238391,Learning for Adaptive Real-time Search,0.07498568296432495,#ffbb78
7.4118323,-3.0302975,neighbor,15238391,Risk-Sensitive Reinforcement Learning Applied to Control under Constraints,0.07506203651428223,#ffbb78
-9.999775,1.0034226,neighbor,15238391,IMPLANT: An Integrated MDP and POMDP Learning AgeNT for Adaptive Games,0.07511723041534424,#ffbb78
-3.5803392,7.003906,neighbor,15238391,Incremental policy learning: an equilibrium selection algorithm for reinforcement learning agents with common interests,0.07516735792160034,#ffbb78
-3.2052748,-12.069286,neighbor,15238391,Learning to reach by reinforcement learning using a receptive field based function approximation approach with continuous actions,0.07519853115081787,#ffbb78
2.031848,-2.2106209,neighbor,15238391,Searching for Plannable Domains can Speed up Reinforcement Learning,0.07519972324371338,#ffbb78
9.301934,5.14128,neighbor,15238391,On the Performance of Maximum Likelihood Inverse Reinforcement Learning,0.07535445690155029,#ffbb78
9.617696,-2.2760923,neighbor,15238391,Feature Reinforcement Learning: Part I. Unstructured MDPs,0.07541030645370483,#ffbb78
-5.5804315,-7.457308,neighbor,15238391,Exploration and Exploitation in Visuomotor Prediction of Autonomous Agents,0.07548362016677856,#ffbb78
9.889448,3.3238099,neighbor,15238391,Probabilistic inverse reinforcement learning in unknown environments,0.07553350925445557,#ffbb78
-0.264873,-6.0789647,neighbor,15238391,Metric State Space Reinforcement Learning for a Vision-Capable Mobile Robot,0.07570570707321167,#ffbb78
-2.5464482,0.3396018,query,1629541,Fully convolutional networks for semantic segmentation,0.0,#ffbb78
-0.23787767,-2.885101,neighbor,1629541,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.04468280076980591,#ffbb78
-12.789648,3.3630257,neighbor,1629541,Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation,0.0549318790435791,#ffbb78
-6.762043,-4.485053,neighbor,1629541,Recurrent Convolutional Neural Networks for Scene Parsing,0.055440664291381836,#ffbb78
2.024263,2.0215175,neighbor,1629541,Deformable part models are convolutional neural networks,0.05996561050415039,#ffbb78
13.620412,-2.9927661,neighbor,1629541,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.060487568378448486,#ffbb78
6.564097,-2.8343582,neighbor,1629541,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.06127995252609253,#ffbb78
9.665553,-2.2878478,neighbor,1629541,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.06150335073471069,#ffbb78
-13.529811,3.852112,neighbor,1629541,Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials,0.061926960945129395,#ffbb78
-0.85146445,-1.4136136,neighbor,1629541,Layered object detection for multi-class segmentation,0.06259644031524658,#ffbb78
-4.579558,2.5717456,neighbor,1629541,Learning Rich Features from RGB-D Images for Object Detection and Segmentation,0.06276684999465942,#ffbb78
5.3197584,-3.295729,neighbor,1629541,Scalable Object Detection Using Deep Neural Networks,0.06279945373535156,#ffbb78
-2.5595098,-1.9113541,neighbor,1629541,Semantic segmentation using regions and parts,0.06291484832763672,#ffbb78
13.224092,-3.9733455,neighbor,1629541,Going deeper with convolutions,0.06456667184829712,#ffbb78
-9.430892,-1.8186014,neighbor,1629541,Scene Segmentation with Low-Dimensional Semantic Representations and Conditional Random Fields,0.06524872779846191,#ffbb78
-5.5690694,2.6322622,neighbor,1629541,Constrained Parametric Proposals and Pooling Methods for Semantic Segmentation in RGB-D Images,0.06606584787368774,#ffbb78
12.667591,-0.7292862,neighbor,1629541,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.06640583276748657,#ffbb78
-13.146905,1.9070567,neighbor,1629541,Semantic Segmentation with Same Topic Constraints,0.06715637445449829,#ffbb78
-2.0078306,-2.5403128,neighbor,1629541,Semantic contours from inverse detectors,0.06903702020645142,#ffbb78
-15.4703865,-3.4345222,neighbor,1629541,Learning from Weak and Noisy Labels for Semantic Segmentation,0.06952136754989624,#ffbb78
-5.128173,3.4828877,neighbor,1629541,Indoor Semantic Segmentation using depth information,0.07007080316543579,#ffbb78
-18.692446,1.7728208,neighbor,1629541,Image Segmentation by Cascaded Region Agglomeration,0.07167357206344604,#ffbb78
12.942539,-2.368064,neighbor,1629541,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.07218384742736816,#ffbb78
2.4364097,0.2991323,neighbor,1629541,Part-Based R-CNNs for Fine-Grained Category Detection,0.07227921485900879,#ffbb78
-0.27313867,-4.3290367,neighbor,1629541,"Codemaps - Segment, Classify and Search Objects Locally",0.07234853506088257,#ffbb78
12.808567,-7.719725,neighbor,1629541,Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps,0.07235211133956909,#ffbb78
-14.405784,2.7649195,neighbor,1629541,Harmony potentials for joint classification and segmentation,0.0732908844947815,#ffbb78
10.1783905,-3.7233014,neighbor,1629541,Fast image scanning with deep max-pooling convolutional neural networks,0.07364463806152344,#ffbb78
5.8675046,0.98078555,neighbor,1629541,Deep learning for class-generic object detection,0.07463687658309937,#ffbb78
-6.764658,-2.323048,neighbor,1629541,Context by region ancestry,0.07472443580627441,#ffbb78
3.1970694,-4.649374,neighbor,1629541,Self-taught object localization with deep networks,0.07534867525100708,#ffbb78
4.529538,-0.18507954,neighbor,1629541,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.0755617618560791,#ffbb78
-9.040391,-3.9772036,neighbor,1629541,PartBook for image parsing,0.07597404718399048,#ffbb78
0.12404151,3.4865508,neighbor,1629541,Learning a Hierarchical Deformable Template for Rapid Deformable Object Parsing,0.07604032754898071,#ffbb78
-10.374602,-2.3372219,neighbor,1629541,Dense Semantic Image Segmentation with Objects and Attributes,0.07639908790588379,#ffbb78
16.243237,-1.2418201,neighbor,1629541,SimNets: A Generalization of Convolutional Networks,0.07655632495880127,#ffbb78
11.6464,-5.2392836,neighbor,1629541,Do Convnets Learn Correspondence?,0.07673043012619019,#ffbb78
14.75757,-3.369755,neighbor,1629541,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.07682716846466064,#ffbb78
11.288941,0.35217962,neighbor,1629541,Multi-scale Orderless Pooling of Deep Convolutional Activation Features,0.07701390981674194,#ffbb78
-10.244627,-5.103961,neighbor,1629541,Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency,0.0772320032119751,#ffbb78
0.5924148,4.2592525,neighbor,1629541,"Recursive compositional models: Representation, learning, and inference",0.07759970426559448,#ffbb78
-0.9118925,-6.5378604,neighbor,1629541,BING: Binarized normed gradients for objectness estimation at 300fps,0.07822370529174805,#ffbb78
14.2523,-1.5521011,neighbor,1629541,Caffe: Convolutional Architecture for Fast Feature Embedding,0.07846808433532715,#ffbb78
-13.424119,0.45384642,neighbor,1629541,Multiclass Image Segmentation Based on Pixel and Segment Level,0.07862097024917603,#ffbb78
10.187676,-1.5021875,neighbor,1629541,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.07961606979370117,#ffbb78
-8.047561,-4.0360208,neighbor,1629541,Context Driven Scene Parsing with Attention to Rare Classes,0.07987189292907715,#ffbb78
-5.7610426,-0.9165056,neighbor,1629541,Learning to Find Object Boundaries Using Motion Cues,0.08022463321685791,#ffbb78
-19.050148,0.044762958,neighbor,1629541,A benchmark for semantic image segmentation,0.0806652307510376,#ffbb78
13.217751,0.36786392,neighbor,1629541,Learnable Pooling Regions for Image Classification,0.08128809928894043,#ffbb78
9.630534,2.6695356,neighbor,1629541,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.08157187700271606,#ffbb78
15.905397,1.1459904,neighbor,1629541,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.08249729871749878,#ffbb78
-16.039568,6.9040823,neighbor,1629541,Fast Edge Detection Using Structured Forests,0.08295321464538574,#ffbb78
-12.108683,6.202571,neighbor,1629541,Learning a Loopy Model For Semantic Segmentation Exactly,0.0833234190940857,#ffbb78
-16.380215,3.4764729,neighbor,1629541,Non-Parametric Probabilistic Image Segmentation,0.0843513011932373,#ffbb78
-4.5119195,-6.472119,neighbor,1629541,Enriching Visual Knowledge Bases via Object Discovery and Segmentation,0.08453226089477539,#ffbb78
12.054677,2.8696907,neighbor,1629541,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.08519542217254639,#ffbb78
-0.075836614,-7.0641017,neighbor,1629541,Measuring the Objectness of Image Windows,0.08523154258728027,#ffbb78
7.09057,2.5784957,neighbor,1629541,Fine-grained object recognition with Gnostic Fields,0.08530354499816895,#ffbb78
6.9644938,0.36662987,neighbor,1629541,ImageNet Large Scale Visual Recognition Challenge,0.08565878868103027,#ffbb78
14.929405,3.7499635,neighbor,1629541,Deconvolutional networks,0.08575868606567383,#ffbb78
15.781673,-4.5156136,neighbor,1629541,Multi-column deep neural networks for image classification,0.08580577373504639,#ffbb78
-18.894981,3.7246008,neighbor,1629541,Segmentation of Natural Images by Texture and Boundary Compression,0.08592736721038818,#ffbb78
-16.642591,-0.5481492,neighbor,1629541,Image Co-segmentation via Consistent Functional Maps,0.0865904688835144,#ffbb78
16.397652,-2.577629,neighbor,1629541,Deep Epitomic Convolutional Neural Networks,0.08662354946136475,#ffbb78
14.318729,2.0270834,neighbor,1629541,Differentiable Pooling for Hierarchical Feature Learning,0.08691149950027466,#ffbb78
-0.18656701,0.6535885,neighbor,1629541,Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts,0.08694779872894287,#ffbb78
16.852528,0.014090115,neighbor,1629541,Deeply-Supervised Nets,0.08730292320251465,#ffbb78
-15.889016,1.5580857,neighbor,1629541,Image Segmentation by Discounted Cumulative Ranking on Maximal Cliques,0.087338387966156,#ffbb78
-13.476988,7.0102005,neighbor,1629541,Structured Learning of Sum-of-Submodular Higher Order Energy Functions,0.08744418621063232,#ffbb78
-16.744318,4.8154187,neighbor,1629541,Data-driven tree-structured Bayesian network for image segmentation,0.08776772022247314,#ffbb78
3.0292854,-2.1249328,neighbor,1629541,Pedestrian Detection with Unsupervised Multi-stage Feature Learning,0.08810156583786011,#ffbb78
-2.2687283,-4.06396,neighbor,1629541,Object category detection by incorporating mid-level grouping cues,0.08822202682495117,#ffbb78
-13.208693,-3.2714422,neighbor,1629541,Semantic Graph Construction for Weakly-Supervised Image Parsing,0.0884091854095459,#ffbb78
-11.855343,-2.1690497,neighbor,1629541,Semantic context modeling with maximal margin Conditional Random Fields for automatic image annotation,0.08846735954284668,#ffbb78
-18.018513,2.2378573,neighbor,1629541,Image Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration,0.0885114073753357,#ffbb78
-7.639193,-5.7122283,neighbor,1629541,A Hierarchical and Contextual Model for Aerial Image Parsing,0.08855730295181274,#ffbb78
0.67326474,-8.225623,neighbor,1629541,The Secrets of Salient Object Segmentation,0.08863288164138794,#ffbb78
-15.091851,-2.9598894,neighbor,1629541,Representative Discovery of Structure Cues for Weakly-Supervised Image Segmentation,0.08896106481552124,#ffbb78
-12.608266,-0.03543658,neighbor,1629541,Theme-Based Multi-class Object Recognition and Segmentation,0.08899086713790894,#ffbb78
-19.797138,2.1268265,neighbor,1629541,SEEDS: Superpixels Extracted Via Energy-Driven Sampling,0.08916455507278442,#ffbb78
7.856451,-7.724421,neighbor,1629541,Two-Stream Convolutional Networks for Action Recognition in Videos,0.08928173780441284,#ffbb78
-7.2020617,-0.41935557,neighbor,1629541,Geometric Context from Videos,0.08939963579177856,#ffbb78
1.5010089,-5.30181,neighbor,1629541,Visual chunking: A list prediction framework for region-based object detection,0.08946603536605835,#ffbb78
-5.1532297,-0.43828085,neighbor,1629541,Segmentation According to Natural Examples: Learning Static Segmentation from Motion Segmentation,0.08953350782394409,#ffbb78
-11.095189,2.9842532,neighbor,1629541,Parsing World's Skylines Using Shape-Constrained MRFs,0.08956372737884521,#ffbb78
0.37666446,1.8774683,neighbor,1629541,Building Part-Based Object Detectors via 3D Geometry,0.08959966897964478,#ffbb78
3.5477514,-1.9074464,neighbor,1629541,Joint Deep Learning for Pedestrian Detection,0.08980584144592285,#ffbb78
-10.741324,5.6588864,neighbor,1629541,Efficient Structured Prediction with Latent Variables for General Graphical Models,0.08984231948852539,#ffbb78
1.375721,5.279829,neighbor,1629541,Complexity of Representation and Inference in Compositional Models with Part Sharing,0.08985108137130737,#ffbb78
14.069243,-5.648437,neighbor,1629541,Visualizing and Understanding Convolutional Networks,0.08993405103683472,#ffbb78
-8.586371,0.30055648,neighbor,1629541,The Shape-Time Random Field for Semantic Video Labeling,0.09012818336486816,#ffbb78
14.742169,2.7803092,neighbor,1629541,Regularized Hierarchical Feature Learning with Non-negative Sparsity and Selectivity for Image Classification,0.09058338403701782,#ffbb78
1.5704588,-2.296015,neighbor,1629541,Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection,0.09066927433013916,#ffbb78
-9.923054,7.256504,neighbor,1629541,SpeedMachines: Anytime Structured Prediction,0.09068447351455688,#ffbb78
8.947584,-6.7975445,neighbor,1629541,Recurrent Models of Visual Attention,0.09069675207138062,#ffbb78
9.802987,1.4460509,neighbor,1629541,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,0.09079533815383911,#ffbb78
6.405344,-0.8942724,neighbor,1629541,1-HKUST: Object Detection in ILSVRC 2014,0.09086817502975464,#ffbb78
-11.532891,-6.131456,neighbor,1629541,Fusion Based Holistic Road Scene Understanding,0.09105920791625977,#ffbb78
-11.137535,1.1575912,neighbor,1629541,Recursive Neural Networks Based on PSO for Image Parsing,0.09106510877609253,#ffbb78
-4.1143045,-2.9459026,neighbor,1629541,Implicit spatial inference with sparse local features,0.09128332138061523,#ffbb78
5.1046844,3.4242082,neighbor,1629541,Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep Object Recognition,0.09132307767868042,#ffbb78
-1.2460122,-5.49131,query,167217261,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,0.0,#2ca02c
-1.1832902,-5.3685913,neighbor,167217261,Universally Slimmable Networks and Improved Training Techniques,0.04028820991516113,#2ca02c
-3.752234,-3.2063978,neighbor,167217261,SCANN: Synthesis of Compact and Accurate Neural Networks,0.04470944404602051,#2ca02c
-5.76275,-3.282464,neighbor,167217261,The Art of Getting Deep Neural Networks in Shape,0.045238614082336426,#2ca02c
2.946676,0.22395396,neighbor,167217261,Learning Efficient Convolutional Networks through Network Slimming,0.04550957679748535,#2ca02c
-0.82473,-3.3233025,neighbor,167217261,ImageNet Training in Minutes,0.04595977067947388,#2ca02c
11.853522,-1.6021022,neighbor,167217261,ShuffleNASNets: Efficient CNN models through modified Efficient Neural Architecture Search,0.04661291837692261,#2ca02c
-4.441124,2.744308,neighbor,167217261,DyVEDeep: Dynamic Variable Effort Deep Neural Networks,0.04739260673522949,#2ca02c
-8.853318,-3.88304,neighbor,167217261,Convolutional neural networks at constrained time cost,0.048036038875579834,#2ca02c
-7.0293274,-3.6877928,neighbor,167217261,Exploring the Design Space of Deep Convolutional Neural Networks at Large Scale,0.0480688214302063,#2ca02c
-1.9872247,1.7797855,neighbor,167217261,Low-Memory Neural Network Training: A Technical Report,0.04822510480880737,#2ca02c
-3.155991,0.31993744,neighbor,167217261,Full Deep Neural Network Training On A Pruned Weight Budget,0.0483323335647583,#2ca02c
-6.5890174,-0.33759415,neighbor,167217261,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size,0.04835247993469238,#2ca02c
-0.41332597,-1.7541014,neighbor,167217261,Deep Fried Convnets,0.04880708456039429,#2ca02c
7.5476823,3.6575596,neighbor,167217261,A Survey of Model Compression and Acceleration for Deep Neural Networks,0.04882001876831055,#2ca02c
3.1076808,5.140914,neighbor,167217261,PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning,0.048943400382995605,#2ca02c
7.50899,4.624136,neighbor,167217261,SlimNets: An Exploration of Deep Model Compression and Acceleration,0.04907882213592529,#2ca02c
7.2884746,2.1659896,neighbor,167217261,ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression,0.04919171333312988,#2ca02c
10.926538,2.0844984,neighbor,167217261,MBS: Macroblock Scaling for CNN Model Reduction,0.04987049102783203,#2ca02c
-4.8105373,-0.60967916,neighbor,167217261,Learning both Weights and Connections for Efficient Neural Network,0.05001473426818848,#2ca02c
12.3522415,-5.1618886,neighbor,167217261,FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search,0.05017608404159546,#2ca02c
2.8438435,0.55427516,neighbor,167217261,Training CNNs with Selective Allocation of Channels,0.050535500049591064,#2ca02c
1.1853441,-6.0462613,neighbor,167217261,Truncating Wide Networks Using Binary Tree Architectures,0.05067569017410278,#2ca02c
-1.2294283,6.4517636,neighbor,167217261,DARC: Differentiable ARchitecture Compression,0.05075496435165405,#2ca02c
-4.374481,-7.697504,neighbor,167217261,Towards Principled Design of Deep Convolutional Networks: Introducing SimpNet,0.05077016353607178,#2ca02c
-2.7283514,9.480757,neighbor,167217261,Optimize Deep Convolutional Neural Network with Ternarized Weights and High Accuracy,0.05091512203216553,#2ca02c
8.107151,-2.9769483,neighbor,167217261,ANTNets: Mobile Convolutional Neural Networks for Resource Efficient Image Classification,0.05092144012451172,#2ca02c
-6.8597264,6.2976093,neighbor,167217261,Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference,0.051013290882110596,#2ca02c
2.8436713,3.2499638,neighbor,167217261,Improving the Resolution of CNN Feature Maps Efficiently with Multisampling,0.05130112171173096,#2ca02c
-8.187009,2.38415,neighbor,167217261,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.051338016986846924,#2ca02c
-4.1025915,10.637249,neighbor,167217261,Ternary Neural Networks with Fine-Grained Quantization,0.05141091346740723,#2ca02c
10.582446,-4.4774427,neighbor,167217261,MnasNet: Platform-Aware Neural Architecture Search for Mobile,0.05144304037094116,#2ca02c
1.7413976,-1.5500947,neighbor,167217261,Memory-Efficient Implementation of DenseNets,0.051614463329315186,#2ca02c
7.4910407,-6.1769333,neighbor,167217261,Fast Training of Convolutional Neural Networks via Kernel Rescaling,0.05165815353393555,#2ca02c
6.1867766,1.6597961,neighbor,167217261,Structured Pruning for Efficient ConvNets via Incremental Regularization,0.0520327091217041,#2ca02c
3.476503,-6.5151916,neighbor,167217261,Deep Anchored Convolutional Neural Networks,0.05220472812652588,#2ca02c
0.3624161,0.5854476,neighbor,167217261,PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration,0.0523640513420105,#2ca02c
8.495412,4.605244,neighbor,167217261,DeepRebirth: Accelerating Deep Neural Network Execution on Mobile Devices,0.05240136384963989,#2ca02c
-4.077266,-2.724474,neighbor,167217261,NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm,0.05249619483947754,#2ca02c
-2.8977826,-1.377177,neighbor,167217261,Smallify: Learning Network Size while Training,0.05257570743560791,#2ca02c
-1.7264051,-7.273415,neighbor,167217261,Growing a Brain: Fine-Tuning by Increasing Model Capacity,0.05262792110443115,#2ca02c
1.5864068,9.337054,neighbor,167217261,ELASTIC: Improving CNNs With Dynamic Scaling Policies,0.05305987596511841,#2ca02c
-5.2507944,-8.197919,neighbor,167217261,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,0.05327868461608887,#2ca02c
-5.1629133,10.21857,neighbor,167217261,HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision,0.053366005420684814,#2ca02c
-5.819295,1.9108831,neighbor,167217261,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,0.05357563495635986,#2ca02c
-5.6648307,6.9020777,neighbor,167217261,WRPN: Training and Inference using Wide Reduced-Precision Networks,0.053646087646484375,#2ca02c
2.3146045,-6.7554793,neighbor,167217261,Elastic Neural Networks for Classification,0.05371582508087158,#2ca02c
0.91034245,1.5595628,neighbor,167217261,MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks,0.053858816623687744,#2ca02c
2.2925813,-8.195155,neighbor,167217261,Learning Transferable Architectures for Scalable Image Recognition,0.05396616458892822,#2ca02c
3.1213636,-2.6101704,neighbor,167217261,SC-Conv: Sparse-Complementary Convolution for Efficient Model Utilization on CNNs,0.053995490074157715,#2ca02c
-5.859768,-0.09802655,neighbor,167217261,SqueezeNext: Hardware-Aware Neural Network Design,0.05402827262878418,#2ca02c
4.3998446,-0.8313541,neighbor,167217261,Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups,0.05403316020965576,#2ca02c
-1.3941548,-9.908044,neighbor,167217261,FreezeOut: Accelerate Training by Progressively Freezing Layers,0.054191529750823975,#2ca02c
-6.356457,-4.9454455,neighbor,167217261,Automated Search for Configurations of Deep Neural Network Architectures,0.05449187755584717,#2ca02c
2.9314275,-10.54661,neighbor,167217261,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.05460721254348755,#2ca02c
4.2665744,-6.428552,neighbor,167217261,Doubly Convolutional Neural Networks,0.05466336011886597,#2ca02c
-6.2274423,2.587603,neighbor,167217261,Superneurons: dynamic GPU memory management for training deep neural networks,0.05492246150970459,#2ca02c
9.005763,6.324901,neighbor,167217261,Deep Net Triage: Assessing the Criticality of Network Layers by Structural Compression,0.054956912994384766,#2ca02c
-4.484476,7.3199058,neighbor,167217261,ADaPTION: Toolbox and Benchmark for Training Convolutional Neural Networks with Reduced Numerical Precision Weights and Activation,0.05503690242767334,#2ca02c
-11.450272,0.43374863,neighbor,167217261,Towards Efficient Model Compression via Learned Global Ranking,0.055053889751434326,#2ca02c
5.9904695,0.18037271,neighbor,167217261,Building Efficient ConvNets using Redundant Feature Pruning,0.05506008863449097,#2ca02c
0.839124,-3.1510267,neighbor,167217261,FastNet: An Efficient Architecture for Smart Devices,0.055098772048950195,#2ca02c
12.507292,-2.0158477,neighbor,167217261,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,0.05519181489944458,#2ca02c
2.9788857,5.5457134,neighbor,167217261,"Piggyback: Adding Multiple Tasks to a Single, Fixed Network by Learning to Mask",0.05519777536392212,#2ca02c
5.918612,4.431069,neighbor,167217261,Accelerating Convolutional Neural Networks via Activation Map Compression,0.0553058385848999,#2ca02c
5.094487,-7.170003,neighbor,167217261,Evenly Cascaded Convolutional Networks,0.055321455001831055,#2ca02c
-0.8931508,4.112598,neighbor,167217261,Learning Network Architectures of Deep CNNs Under Resource Constraints,0.05538833141326904,#2ca02c
-8.273033,7.9244356,neighbor,167217261,Incomplete Dot Products for Dynamic Computation Scaling in Neural Network Inference,0.05544745922088623,#2ca02c
-3.2572443,10.480123,neighbor,167217261,Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights,0.05547815561294556,#2ca02c
-8.775258,5.590521,neighbor,167217261,Ristretto: A Framework for Empirical Study of Resource-Efficient Inference in Convolutional Neural Networks,0.05550438165664673,#2ca02c
3.7771192,-10.425747,neighbor,167217261,ZipNet: ZFNet-level Accuracy with 48× Fewer Parameters,0.05569779872894287,#2ca02c
-3.2376435,3.738642,neighbor,167217261,HydraNets: Specialized Dynamic Architectures for Efficient Inference,0.05588918924331665,#2ca02c
-6.3690414,6.1027994,neighbor,167217261,Rethinking Numerical Representations for Deep Neural Networks,0.05597269535064697,#2ca02c
10.125552,2.0348577,neighbor,167217261,BRIEF: Backward Reduction of CNNs with Information Flow Analysis,0.05605936050415039,#2ca02c
11.99316,-4.5251145,neighbor,167217261,Single-Path NAS: Device-Aware Efficient ConvNet Design,0.056169986724853516,#2ca02c
-8.262932,-5.9987154,neighbor,167217261,Effective Building Block Design for Deep Convolutional Neural Networks using Search,0.05630028247833252,#2ca02c
-1.9530989,-1.106951,neighbor,167217261,ClosNets: Batchless DNN Training with On-Chip a Priori Sparse Neural Topologies,0.05634486675262451,#2ca02c
5.4046407,-3.999161,neighbor,167217261,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.05638706684112549,#2ca02c
7.837294,-9.966396,neighbor,167217261,A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation,0.05641096830368042,#2ca02c
-7.1761823,4.5813346,neighbor,167217261,Highly Efficient 8-bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe,0.05646699666976929,#2ca02c
5.5225887,9.614095,neighbor,167217261,CondenseNet: An Efficient DenseNet Using Learned Group Convolutions,0.056531548500061035,#2ca02c
3.122487,7.2202187,neighbor,167217261,Learning Identity Mappings with Residual Gates,0.05684995651245117,#2ca02c
7.584467,1.1074796,neighbor,167217261,Synaptic Strength For Convolutional Neural Network,0.05692720413208008,#2ca02c
-0.40202215,4.4019923,neighbor,167217261,Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks,0.057012319564819336,#2ca02c
5.600503,9.948652,neighbor,167217261,Sequentially Aggregated Convolutional Networks,0.05713778734207153,#2ca02c
6.4915648,-4.2873106,neighbor,167217261,On the Reduction of Computational Complexity of Deep Convolutional Neural Networks †,0.05719560384750366,#2ca02c
0.12747894,7.3028684,neighbor,167217261,GradNets: Dynamic Interpolation Between Neural Architectures,0.05732297897338867,#2ca02c
4.9218946,-10.018427,neighbor,167217261,Training Better CNNs Requires to Rethink ReLU,0.05735105276107788,#2ca02c
-1.233858,1.6484809,neighbor,167217261,SQuantizer: Simultaneous Learning for Both Sparse and Low-precision Neural Networks,0.057403624057769775,#2ca02c
-7.451483,-2.7120228,neighbor,167217261,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,0.057462871074676514,#2ca02c
3.2870836,-5.235457,neighbor,167217261,The Shallow End: Empowering Shallower Deep-Convolutional Networks through Auxiliary Outputs,0.057572364807128906,#2ca02c
5.8236666,-3.1955862,neighbor,167217261,Quantized CNN: A Unified Approach to Accelerate and Compress Convolutional Networks,0.05762249231338501,#2ca02c
12.533201,-4.1818986,neighbor,167217261,Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours,0.05767089128494263,#2ca02c
-3.380471,-4.6962705,neighbor,167217261,CompNet: Neural networks growing via the compact network morphism,0.05767667293548584,#2ca02c
6.8150315,5.7610636,neighbor,167217261,Compressing Neural Networks with the Hashing Trick,0.057787418365478516,#2ca02c
2.5434222,-1.6135408,neighbor,167217261,Faster CNNs with Direct Sparse Convolutions and Guided Pruning,0.05780982971191406,#2ca02c
-3.430467,12.756161,neighbor,167217261,Network Sketching: Exploiting Binary Structure in Deep CNNs,0.057813405990600586,#2ca02c
-0.1463187,-8.6911125,neighbor,167217261,Incremental Training of Deep Convolutional Neural Networks,0.05782586336135864,#2ca02c
4.623043,2.2483912,neighbor,167217261,Structured Pruning of Neural Networks With Budget-Aware Regularization,0.05784684419631958,#2ca02c
-4.977607,9.648795,neighbor,167217261,Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search,0.05798673629760742,#2ca02c
-5.3096623,5.1020117,neighbor,167217261,Using Pre-trained Full-Precision Models to Speed Up Training Binary Networks For Mobile Devices,0.05820655822753906,#2ca02c
5.795694,6.9096193,query,198953378,RoBERTa: A Robustly Optimized BERT Pretraining Approach,0.0,#98df8a
5.2977247,8.820753,neighbor,198953378,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,0.04930877685546875,#98df8a
9.16602,6.188066,neighbor,198953378,To Tune or Not To Tune? How About the Best of Both Worlds?,0.05843907594680786,#98df8a
1.6133276,-3.4736526,neighbor,198953378,On the State of the Art of Evaluation in Neural Language Models,0.059148192405700684,#98df8a
4.9168863,9.778985,neighbor,198953378,Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling,0.06360197067260742,#98df8a
2.9607308,-1.4912076,neighbor,198953378,Large Memory Layers with Product Keys,0.06389087438583374,#98df8a
12.2863865,6.0380807,neighbor,198953378,Cross-lingual Language Model Pretraining,0.06393301486968994,#98df8a
5.1322904,-4.010107,neighbor,198953378,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,0.06463956832885742,#98df8a
-5.0570774,1.053487,neighbor,198953378,Attention is All you Need,0.06507277488708496,#98df8a
-2.8982804,7.886976,neighbor,198953378,Training with Exploration Improves a Greedy Stack LSTM Parser,0.06511098146438599,#98df8a
3.5419977,10.189585,neighbor,198953378,XLNet: Generalized Autoregressive Pretraining for Language Understanding,0.0651625394821167,#98df8a
-8.653146,-0.625959,neighbor,198953378,Non-Autoregressive Neural Machine Translation,0.06523627042770386,#98df8a
9.001653,-5.7390695,neighbor,198953378,Shifting Mean Activation Towards Zero with Bipolar Activation Functions,0.06565070152282715,#98df8a
3.6725895,-5.7248254,neighbor,198953378,Language Models with Pre-Trained (GloVe) Word Embeddings,0.06591653823852539,#98df8a
1.7494286,3.154129,neighbor,198953378,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,0.06666046380996704,#98df8a
-4.8076706,-3.964756,neighbor,198953378,"Grow and Prune Compact, Fast, and Accurate LSTMs",0.06670832633972168,#98df8a
-2.2368093,-7.1901727,neighbor,198953378,Auto-Sizing Neural Networks: With Applications to n-gram Language Models,0.06672275066375732,#98df8a
6.7940617,5.4840155,neighbor,198953378,mlr Tutorial,0.06728190183639526,#98df8a
0.45592108,-5.1790137,neighbor,198953378,Improved Language Modeling by Decoding the Past,0.06752973794937134,#98df8a
-10.510431,-2.3226461,neighbor,198953378,Sockeye: A Toolkit for Neural Machine Translation,0.06772536039352417,#98df8a
1.3795471,4.520058,neighbor,198953378,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",0.06778478622436523,#98df8a
6.409172,9.517016,neighbor,198953378,Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks,0.06801313161849976,#98df8a
-2.82418,-10.181688,neighbor,198953378,Classes for fast maximum entropy training,0.06852954626083374,#98df8a
-1.2073873,-9.20976,neighbor,198953378,Strategies for Training Large Vocabulary Neural Language Models,0.06861937046051025,#98df8a
7.5724063,-7.788739,neighbor,198953378,Breaking the Activation Function Bottleneck through Adaptive Parameterization,0.06865477561950684,#98df8a
2.0402513,1.3134598,neighbor,198953378,Language Models with Transformers,0.06897985935211182,#98df8a
6.869941,-7.018491,neighbor,198953378,Adaptive Input Representations for Neural Language Modeling,0.06907474994659424,#98df8a
-3.2499082,12.824668,neighbor,198953378,"Data Programming: Creating Large Training Sets, Quickly",0.0691496729850769,#98df8a
8.553185,-1.3729848,neighbor,198953378,"Baseline: A Library for Rapid Modeling, Experimentation and Development of Deep Learning Algorithms targeting NLP",0.0693696141242981,#98df8a
9.974198,-7.9339557,neighbor,198953378,Fast Parametric Learning with Activation Memorization,0.06938934326171875,#98df8a
0.70021874,-0.70200044,neighbor,198953378,Character-Level Language Modeling with Deeper Self-Attention,0.0694548487663269,#98df8a
7.99752,-9.879586,neighbor,198953378,Parameter Re-Initialization through Cyclical Batch Size Schedules,0.06952565908432007,#98df8a
10.306262,9.20486,neighbor,198953378,Variational Pretraining for Semi-supervised Text Classification,0.06964635848999023,#98df8a
-2.947331,13.270768,neighbor,198953378,Snorkel: rapid training data creation with weak supervision,0.06975865364074707,#98df8a
3.62968,3.26388,neighbor,198953378,Latent Universal Task-Specific BERT,0.06983578205108643,#98df8a
-11.388527,-1.499859,neighbor,198953378,OpenNMT: Open-Source Toolkit for Neural Machine Translation,0.06993842124938965,#98df8a
-13.1191435,-2.7907906,neighbor,198953378,Marian: Fast Neural Machine Translation in C++,0.06995552778244019,#98df8a
-0.42099544,1.2484754,neighbor,198953378,Adaptive Attention Span in Transformers,0.0701901912689209,#98df8a
-3.3474967,8.9393215,neighbor,198953378,Good-Enough Compositional Data Augmentation,0.07064509391784668,#98df8a
-0.037877705,-12.999684,neighbor,198953378,Learning to Screen for Fast Softmax Inference on Large Vocabulary Neural Networks,0.07072412967681885,#98df8a
-8.604702,-5.0151467,neighbor,198953378,RETURNN as a Generic Flexible Neural Toolkit with Application to Translation and Speech Recognition,0.07073640823364258,#98df8a
-0.8116406,7.8128448,neighbor,198953378,Scalable Syntax-Aware Language Models Using Knowledge Distillation,0.0708160400390625,#98df8a
0.976815,-7.0862374,neighbor,198953378,Analysing Dropout and Compounding Errors in Neural Language Models,0.0709071159362793,#98df8a
3.9306326,5.819228,neighbor,198953378,Microsoft Icecaps: An Open-Source Toolkit for Conversation Modeling,0.07091516256332397,#98df8a
1.4490187,8.001459,neighbor,198953378,Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis,0.07112407684326172,#98df8a
9.282811,-3.856359,neighbor,198953378,Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks,0.07115405797958374,#98df8a
-3.828057,3.310217,neighbor,198953378,Learning What’s Easy: Fully Differentiable Neural Easy-First Taggers,0.07142692804336548,#98df8a
8.983561,4.4071517,neighbor,198953378,On transfer learning using a MAC model variant,0.07151222229003906,#98df8a
0.2930919,2.3616135,neighbor,198953378,Dynamic Evaluation of Transformer Language Models,0.07158029079437256,#98df8a
1.9871955,12.596051,neighbor,198953378,Improving Context Aware Language Models,0.07189542055130005,#98df8a
4.689145,12.147977,neighbor,198953378,No Training Required: Exploring Random Encoders for Sentence Classification,0.07202744483947754,#98df8a
-8.735509,0.12543593,neighbor,198953378,Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input,0.07220005989074707,#98df8a
10.735394,5.887834,neighbor,198953378,Fine-tuning of Language Models with Discriminator,0.07220393419265747,#98df8a
-0.5216016,-12.246205,neighbor,198953378,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,0.07234704494476318,#98df8a
-7.409441,0.013122823,neighbor,198953378,Syntactically Supervised Transformers for Faster Neural Machine Translation,0.07235783338546753,#98df8a
-4.2550874,0.4708048,neighbor,198953378,Universal Transformers,0.072468101978302,#98df8a
-2.181795,-4.9082427,neighbor,198953378,Training Language Models Using Target-Propagation,0.07254505157470703,#98df8a
-12.664811,-0.60468525,neighbor,198953378,SYSTRAN's Pure Neural Machine Translation Systems,0.07255899906158447,#98df8a
3.3227813,11.659326,neighbor,198953378,Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding,0.07267379760742188,#98df8a
11.6370125,-4.3039813,neighbor,198953378,CoastalCPH at SemEval-2016 Task 11: The importance of designing your Neural Networks right,0.07270985841751099,#98df8a
-6.4286575,-13.020399,neighbor,198953378,One Epoch Is All You Need,0.0727342963218689,#98df8a
-10.20205,-8.379466,neighbor,198953378,A Stable and Effective Learning Strategy for Trainable Greedy Decoding,0.07284700870513916,#98df8a
-6.83142,1.7254748,neighbor,198953378,"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",0.0729256272315979,#98df8a
1.9540833,-5.072315,neighbor,198953378,Improving Language Modeling using Densely Connected Recurrent Neural Networks,0.07308119535446167,#98df8a
-5.7805405,10.546877,neighbor,198953378,Automatic Machine Learning by Pipeline Synthesis using Model-Based Reinforcement Learning and a Grammar,0.07313454151153564,#98df8a
-13.552755,-4.451156,neighbor,198953378,Marian: Cost-effective High-Quality Neural Machine Translation in C++,0.07317191362380981,#98df8a
-9.650043,-5.319418,neighbor,198953378,Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU,0.07319039106369019,#98df8a
1.4975055,-1.9590582,neighbor,198953378,An Analysis of Neural Language Modeling at Multiple Scales,0.07339388132095337,#98df8a
10.6145315,10.136559,neighbor,198953378,Unsupervised Domain Adaptation of Contextualized Embeddings: A Case Study in Early Modern English,0.07368546724319458,#98df8a
5.9370594,2.2437625,neighbor,198953378,Efficient Transfer Learning for Neural Network Language Models,0.07380348443984985,#98df8a
13.520008,5.8021064,neighbor,198953378,Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language,0.07390272617340088,#98df8a
0.9350217,8.171863,neighbor,198953378,Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis,0.07406657934188843,#98df8a
-0.7977219,-3.495748,neighbor,198953378,"Calibration, Entropy Rates, and Memory in Language Models",0.07416558265686035,#98df8a
5.0717406,-1.0256697,neighbor,198953378,A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs,0.07423567771911621,#98df8a
7.3052545,11.377451,neighbor,198953378,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,0.07434231042861938,#98df8a
-8.451359,-2.8675125,neighbor,198953378,Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs,0.074413001537323,#98df8a
4.042841,-11.351226,neighbor,198953378,PROPS: Probabilistic personalization of black-box sequence models,0.07453298568725586,#98df8a
-13.0394,-5.5117636,neighbor,198953378,Training Tips for the Transformer Model,0.07466471195220947,#98df8a
-4.6507015,-6.8589473,neighbor,198953378,Information-Weighted Neural Cache Language Models for ASR,0.07479310035705566,#98df8a
-11.744225,-4.392741,neighbor,198953378,Scaling Neural Machine Translation,0.07481187582015991,#98df8a
-0.50355184,-1.336883,neighbor,198953378,Tabula Nearly Rasa: Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text,0.07490271329879761,#98df8a
-0.9990976,2.4235768,neighbor,198953378,Visualizing Attention in Transformer-Based Language models,0.07491058111190796,#98df8a
-14.417445,-7.1121197,neighbor,198953378,Microsoft’s Submission to the WMT2018 News Translation Task: How I Learned to Stop Worrying and Love the Data,0.07491946220397949,#98df8a
12.7671995,-3.4962847,neighbor,198953378,Making Classical Machine Learning Pipelines Differentiable: A Neural Translation Approach,0.0750424861907959,#98df8a
-2.7429047,-7.8007298,neighbor,198953378,Large Margin Neural Language Model,0.0752294659614563,#98df8a
-9.66069,-9.695289,neighbor,198953378,An Empirical Investigation of Global and Local Normalization for Recurrent Neural Sequence Models Using a Continuous Relaxation to Beam Search,0.07536697387695312,#98df8a
2.4353874,-8.439658,neighbor,198953378,Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations,0.07537919282913208,#98df8a
3.6351323,-4.5340853,neighbor,198953378,Language Modeling with Gated Convolutional Networks,0.07543796300888062,#98df8a
-2.5395708,-12.203054,neighbor,198953378,The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family,0.07554817199707031,#98df8a
0.41198182,-8.59984,neighbor,198953378,BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies,0.075702965259552,#98df8a
7.5706882,-1.2147506,neighbor,198953378,Rethinking Complex Neural Network Architectures for Document Classification,0.07582706212997437,#98df8a
-6.32857,6.8124766,neighbor,198953378,Bayesian Layers: A Module for Neural Network Uncertainty,0.07585668563842773,#98df8a
-10.967363,-5.9410787,neighbor,198953378,The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT,0.07590419054031372,#98df8a
8.58238,0.8590842,neighbor,198953378,Deep Learning for Classical Japanese Literature,0.07591485977172852,#98df8a
-7.583888,-5.891013,neighbor,198953378,Transfer Learning for Speech Recognition on a Budget,0.07597684860229492,#98df8a
8.885974,-8.877593,neighbor,198953378,Memory-based Parameter Adaptation,0.075980544090271,#98df8a
-11.042141,3.5439413,neighbor,198953378,Improved training for online end-to-end speech recognition systems,0.0760718584060669,#98df8a
9.826975,-1.3573128,neighbor,198953378,DeepBase: Deep Inspection of Neural Networks,0.07612842321395874,#98df8a
-10.471299,-3.3749819,neighbor,198953378,Transfer Learning for Low-Resource Neural Machine Translation,0.07613658905029297,#98df8a
-6.1984143,-2.741965,neighbor,198953378,Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks,0.07615745067596436,#98df8a
3.2095816,1.1430134,neighbor,198953378,Pre-Training With Whole Word Masking for Chinese BERT,0.07617068290710449,#98df8a
-2.6129234,7.847681,query,204838007,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,0.0,#98df8a
-1.6109045,8.852995,neighbor,204838007,Fine-tuned Language Models for Text Classification,0.032663822174072266,#98df8a
-1.304165,7.4525614,neighbor,204838007,Transfer Learning in Natural Language Processing,0.033804476261138916,#98df8a
-1.1437393,9.281796,neighbor,204838007,Universal Language Model Fine-tuning for Text Classification,0.03640782833099365,#98df8a
-7.1480355,-1.8708012,neighbor,204838007,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,0.03980386257171631,#98df8a
-0.67660654,7.7667527,neighbor,204838007,Evolution of transfer learning in natural language processing,0.040628135204315186,#98df8a
-5.0051446,7.657062,neighbor,204838007,Parameter-Efficient Transfer Learning for NLP,0.04303717613220215,#98df8a
-8.475729,-2.378344,neighbor,204838007,Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks,0.04850566387176514,#98df8a
-0.7453389,2.1438758,neighbor,204838007,Universal Sentence Encoder,0.04938548803329468,#98df8a
-1.7970656,5.584525,neighbor,204838007,Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?,0.04947704076766968,#98df8a
0.7415823,0.5735101,neighbor,204838007,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,0.04977536201477051,#98df8a
7.028522,-6.6749196,neighbor,204838007,Leveraging Pre-trained Checkpoints for Sequence Generation Tasks,0.05007600784301758,#98df8a
-5.6267138,-9.950708,neighbor,204838007,Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension,0.05028742551803589,#98df8a
-0.8962383,0.21777335,neighbor,204838007,Sentence embeddings in NLI with iterative refinement encoders,0.051716744899749756,#98df8a
0.72712886,11.106335,neighbor,204838007,Low-Shot Classification: A Comparison of Classical and Deep Transfer Machine Learning Approaches,0.05254083871841431,#98df8a
-2.1660838,10.450743,neighbor,204838007,A Practitioners' Guide to Transfer Learning for Text Classification using Convolutional Neural Networks,0.05300396680831909,#98df8a
5.087058,-8.748449,neighbor,204838007,Efficient Adaptation of Pretrained Transformers for Abstractive Summarization,0.05315124988555908,#98df8a
-1.1634967,3.3467865,neighbor,204838007,USEing Transfer Learning in Retrieval of Statistical Data,0.053272366523742676,#98df8a
-6.1715364,-8.086939,neighbor,204838007,Learning and Knowledge Transfer with Memory Networks for Machine Comprehension,0.05385476350784302,#98df8a
-7.0307245,-10.753633,neighbor,204838007,Question Answering through Transfer Learning from Large Fine-grained Supervision Data,0.054085493087768555,#98df8a
3.367547,-8.749486,neighbor,204838007,MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models,0.05424147844314575,#98df8a
6.933377,-3.5612671,neighbor,204838007,Large-Scale Transfer Learning for Natural Language Generation,0.05426967144012451,#98df8a
-1.0648179,5.4459896,neighbor,204838007,Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks,0.054778099060058594,#98df8a
-0.24921581,9.172424,neighbor,204838007,Improving Universal Language Model Fine-Tuning using Attention Mechanism,0.05505102872848511,#98df8a
-8.149953,3.7632756,neighbor,204838007,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",0.055809080600738525,#98df8a
8.128191,4.5961018,neighbor,204838007,"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT",0.05590313673019409,#98df8a
-9.270733,3.911444,neighbor,204838007,Reweighted Proximal Pruning for Large-Scale Language Representation,0.0559234619140625,#98df8a
1.014676,1.07721,neighbor,204838007,Multi-task Learning for Universal Sentence Embeddings: A Thorough Evaluation using Transfer and Auxiliary Tasks,0.05613285303115845,#98df8a
5.4455895,-10.586552,neighbor,204838007,Deep Transfer Reinforcement Learning for Text Summarization,0.05769026279449463,#98df8a
-6.4104567,-1.3951802,neighbor,204838007,Multi-Task Deep Neural Networks for Natural Language Understanding,0.0578005313873291,#98df8a
1.0450233,1.8343818,neighbor,204838007,"Learning Robust, Transferable Sentence Representations for Text Classification",0.05808347463607788,#98df8a
-1.0040174,11.020496,neighbor,204838007,From Accuracy to Versatility: Analysing Text Classification Models Regarding Transfer Learning,0.05810809135437012,#98df8a
-5.9453955,10.760631,neighbor,204838007,Transfer Fine-Tuning: A BERT Case Study,0.058116614818573,#98df8a
-1.3306049,-3.497886,neighbor,204838007,Universal Text Representation from BERT: An Empirical Study,0.05815744400024414,#98df8a
5.6923437,-8.015006,neighbor,204838007,Sample Efficient Text Summarization Using a Single Pre-Trained Transformer,0.05830603837966919,#98df8a
-7.633302,-10.505177,neighbor,204838007,Cross-Lingual Transfer Learning for Question Answering,0.058441221714019775,#98df8a
-4.2257223,-7.29041,neighbor,204838007,Unifying Question Answering and Text Classification via Span Extraction,0.05845606327056885,#98df8a
-3.5482452,-3.0322013,neighbor,204838007,StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,0.05869966745376587,#98df8a
-4.880748,1.6659813,neighbor,204838007,Testing the Generalization Power of Neural Network Models across NLI Benchmarks,0.05942189693450928,#98df8a
7.9251814,-4.1412063,neighbor,204838007,Cross-Lingual Natural Language Generation via Pre-Training,0.05969351530075073,#98df8a
-9.775307,2.7783177,neighbor,204838007,How to Fine-Tune BERT for Text Classification?,0.05970633029937744,#98df8a
3.9216864,-6.2801647,neighbor,204838007,Reducing Transformer Depth on Demand with Structured Dropout,0.05991923809051514,#98df8a
6.3355393,-5.373655,neighbor,204838007,Unified Language Model Pre-training for Natural Language Understanding and Generation,0.0605846643447876,#98df8a
2.8093936,10.63413,neighbor,204838007,Dropping Networks for Transfer Learning,0.06069529056549072,#98df8a
0.87284005,7.7242785,neighbor,204838007,Efficient Transfer Learning for Neural Network Language Models,0.060965538024902344,#98df8a
-5.623088,6.5630198,neighbor,204838007,HUBERT Untangles BERT to Improve Transfer across NLP Tasks,0.061106324195861816,#98df8a
6.5927672,5.746341,neighbor,204838007,Choosing Transfer Languages for Cross-Lingual Learning,0.06111431121826172,#98df8a
8.815778,4.592957,neighbor,204838007,Massively Multilingual Transfer for NER,0.0612148642539978,#98df8a
6.7516274,-8.557611,neighbor,204838007,Unsupervised Pretraining for Sequence to Sequence Learning,0.06141549348831177,#98df8a
-0.44861606,1.0049556,neighbor,204838007,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,0.061725497245788574,#98df8a
7.2010694,4.4148464,neighbor,204838007,Multi-Source Cross-Lingual Model Transfer: Learning What to Share,0.061818063259124756,#98df8a
-1.8981525,-7.3726444,neighbor,204838007,Attention-Based Convolutional Neural Network for Machine Comprehension,0.061865031719207764,#98df8a
-7.708334,-11.521775,neighbor,204838007,Learning Transferable Features For Open-Domain Question Answering,0.06204134225845337,#98df8a
-8.131102,-0.90977895,neighbor,204838007,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,0.0624735951423645,#98df8a
-7.2100635,-3.306583,neighbor,204838007,Learning and Evaluating General Linguistic Intelligence,0.06275069713592529,#98df8a
6.8490086,0.84822893,neighbor,204838007,XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering,0.06284695863723755,#98df8a
-1.5320255,13.063506,neighbor,204838007,Source Free Transfer Learning for Text Classification,0.06307244300842285,#98df8a
-7.387047,5.865671,neighbor,204838007,UER: An Open-Source Toolkit for Pre-training Models,0.06307381391525269,#98df8a
-7.484613,8.180048,neighbor,204838007,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,0.06308603286743164,#98df8a
3.7528343,3.1469676,neighbor,204838007,Story Ending Prediction by Transferable BERT,0.06326675415039062,#98df8a
-6.8423376,3.521215,neighbor,204838007,Transformer to CNN: Label-scarce distillation for efficient text classification,0.06332665681838989,#98df8a
5.881322,-7.003187,neighbor,204838007,Pre-trained language model representations for language generation,0.06354552507400513,#98df8a
10.925641,1.9975468,neighbor,204838007,Transfer Learning for Low-Resource Neural Machine Translation,0.06356948614120483,#98df8a
-2.4371424,-7.90424,neighbor,204838007,A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data,0.06370633840560913,#98df8a
10.26135,1.7830825,neighbor,204838007,Multilingual NMT with a Language-Independent Attention Bridge,0.06415599584579468,#98df8a
-4.0846906,4.008191,neighbor,204838007,Extrapolation in NLP,0.06422144174575806,#98df8a
-6.9755983,-12.094648,neighbor,204838007,ISS-MULT: Intelligent Sample Selection for Multi-Task Learning in Question Answering,0.06451612710952759,#98df8a
4.1390357,0.74404067,neighbor,204838007,XNLI: Evaluating Cross-lingual Sentence Representations,0.06455516815185547,#98df8a
3.4143157,5.7032733,neighbor,204838007,Tree Transformer: Integrating Tree Structures into Self-Attention,0.06469929218292236,#98df8a
4.3005953,2.1483877,neighbor,204838007,Semi-Supervised Sequence Modeling with Cross-View Training,0.06478339433670044,#98df8a
-2.5041637,4.6759,neighbor,204838007,MoRTy: Unsupervised Learning of Task-specialized Word Embeddings by Autoencoding,0.0654299259185791,#98df8a
-8.555451,5.9673676,neighbor,204838007,Visualizing and Understanding the Effectiveness of BERT,0.06555706262588501,#98df8a
-2.9727895,-9.639805,neighbor,204838007,Revealing the Importance of Semantic Retrieval for Machine Reading at Scale,0.06565302610397339,#98df8a
-1.1218531,-1.4276167,neighbor,204838007,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,0.06567186117172241,#98df8a
-1.3033521,-8.637695,neighbor,204838007,Phase Conductor on Multi-layered Attentions for Machine Comprehension,0.06576335430145264,#98df8a
-2.603998,-6.4516196,neighbor,204838007,Learning Answer-Entailing Structures for Machine Comprehension,0.06592041254043579,#98df8a
-2.3781009,1.0214367,neighbor,204838007,Pitfalls in the Evaluation of Sentence Embeddings,0.06609171628952026,#98df8a
-9.817892,-3.6940534,neighbor,204838007,A Re-Ranker Scheme For Integrating Large Scale NLU Models,0.06627476215362549,#98df8a
-8.03162,2.8606396,neighbor,204838007,TinyBERT: Distilling BERT for Natural Language Understanding,0.06641864776611328,#98df8a
-3.8272462,-8.264105,neighbor,204838007,Span Selection Pre-training for Question Answering,0.0664413571357727,#98df8a
-3.3827772,10.227021,neighbor,204838007,How Transferable are Neural Networks in NLP Applications?,0.06646865606307983,#98df8a
1.0012633,5.132478,neighbor,204838007,Contextualized Word Representations for Self-Attention Network,0.0665510892868042,#98df8a
-3.7480445,2.8795002,neighbor,204838007,On the Effective Use of Pretraining for Natural Language Inference,0.06660091876983643,#98df8a
-5.199486,-1.5771562,neighbor,204838007,Multi-task learning to improve natural language understanding,0.06669139862060547,#98df8a
7.8958263,2.904248,neighbor,204838007,Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework,0.0667961835861206,#98df8a
7.7246103,-5.9163547,neighbor,204838007,MASS: Masked Sequence to Sequence Pre-training for Language Generation,0.0668984055519104,#98df8a
4.701678,-7.865452,neighbor,204838007,Extractive Summarization with Very Deep Pretrained Language Model,0.06696528196334839,#98df8a
7.456805,10.895976,neighbor,204838007,IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and Translation,0.06697237491607666,#98df8a
0.9746865,3.0874612,neighbor,204838007,Direct Network Transfer: Transfer Learning of Sentence Embeddings for Semantic Similarity,0.06699633598327637,#98df8a
8.557479,-5.8668394,neighbor,204838007,Distilling the Knowledge of BERT for Text Generation,0.06752121448516846,#98df8a
1.678064,-11.57298,neighbor,204838007,Multi-Task Learning for Coherence Modeling,0.0675663948059082,#98df8a
1.729451,-11.793444,neighbor,204838007,A Cross-Domain Transferable Neural Coherence Model,0.06784886121749878,#98df8a
-4.953934,-11.114315,neighbor,204838007,Multilingual Question Answering from Formatted Text applied to Conversational Agents,0.0678870677947998,#98df8a
11.922209,2.2436948,neighbor,204838007,A Simple and Effective Approach to Automatic Post-Editing with Transfer Learning,0.06789684295654297,#98df8a
-3.570056,-4.807826,neighbor,204838007,WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia,0.0679430365562439,#98df8a
-9.522022,-1.0558084,neighbor,204838007,AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning,0.06805676221847534,#98df8a
3.5029194,8.519944,neighbor,204838007,Composition of Conditional Random Fields for Transfer Learning,0.06817799806594849,#98df8a
-5.7404213,3.3228621,neighbor,204838007,Rethinking Complex Neural Network Architectures for Document Classification,0.0681924819946289,#98df8a
3.0708742,-3.5188794,neighbor,204838007,Split and Rephrase: Better Evaluation and Stronger Baselines,0.0683169960975647,#98df8a
6.3979554,3.0442362,neighbor,204838007,Bridging the domain gap in cross-lingual document classification,0.0683409571647644,#98df8a
7.405122,10.950051,neighbor,204838007,Unsupervised Evaluation Metrics and Learning Criteria for Non-Parallel Textual Transfer,0.0685197114944458,#98df8a
-3.2533274,-1.9239416,query,204960716,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",0.0,#98df8a
-2.572746,-1.0551263,neighbor,204960716,Denoising based Sequence-to-Sequence Pre-training for Text Generation,0.02676248550415039,#98df8a
1.1983417,0.9965278,neighbor,204960716,MASS: Masked Sequence to Sequence Pre-training for Language Generation,0.03813380002975464,#98df8a
4.481061,8.381852,neighbor,204960716,Neural Text Generation: A Practical Guide,0.03928220272064209,#98df8a
2.7466671,-2.6504617,neighbor,204960716,Pre-trained language model representations for language generation,0.040992558002471924,#98df8a
3.5443227,-0.32099226,neighbor,204960716,Unified Language Model Pre-training for Natural Language Understanding and Generation,0.042543113231658936,#98df8a
1.0206968,-0.5617645,neighbor,204960716,Leveraging Pre-trained Checkpoints for Sequence Generation Tasks,0.044072747230529785,#98df8a
-4.7856736,13.144591,neighbor,204960716,Unsupervised Natural Language Generation with Denoising Autoencoders,0.04464089870452881,#98df8a
3.5220332,-4.2314367,neighbor,204960716,Sample Efficient Text Summarization Using a Single Pre-Trained Transformer,0.04584205150604248,#98df8a
1.6544673,-11.878932,neighbor,204960716,SEQˆ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression,0.04719191789627075,#98df8a
-3.5233963,-8.200048,neighbor,204960716,The Neural Noisy Channel,0.04724776744842529,#98df8a
-2.6650496,3.4210606,neighbor,204960716,Unsupervised Pretraining for Sequence to Sequence Learning,0.048034489154815674,#98df8a
-4.887675,9.614064,neighbor,204960716,Revisiting Self-Training for Neural Sequence Generation,0.048955678939819336,#98df8a
-10.052009,0.22525239,neighbor,204960716,Simplifying Sentences with Sequence to Sequence Models,0.04940694570541382,#98df8a
-5.885578,1.1753914,neighbor,204960716,Levenshtein Transformer,0.04957723617553711,#98df8a
-6.694747,-0.46872076,neighbor,204960716,XL-Editor: Post-editing Sentences with XLNet,0.04993605613708496,#98df8a
-6.885875,4.898376,neighbor,204960716,Token-level and sequence-level loss smoothing for RNN language models,0.04994708299636841,#98df8a
5.1683545,5.125649,neighbor,204960716,Keeping Notes: Conditional Natural Language Generation with a Scratchpad Encoder,0.05006974935531616,#98df8a
-5.790186,8.097033,neighbor,204960716,Generative Bridging Network in Neural Sequence Prediction,0.05036044120788574,#98df8a
4.045822,-12.801529,neighbor,204960716,Unsupervised Sentence Compression using Denoising Auto-Encoders,0.0504494309425354,#98df8a
-7.216699,2.7711859,neighbor,204960716,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,0.05165451765060425,#98df8a
0.32125178,-3.3515682,neighbor,204960716,Reducing Transformer Depth on Demand with Structured Dropout,0.05182534456253052,#98df8a
-2.29093,4.8238378,neighbor,204960716,Classical Structured Prediction Losses for Sequence to Sequence Learning,0.05268204212188721,#98df8a
0.29548562,7.3474417,neighbor,204960716,Deep Reinforcement Learning for Sequence-to-Sequence Models,0.05275481939315796,#98df8a
-2.644235,17.764101,neighbor,204960716,MaskGAN: Better Text Generation via Filling in the ______,0.05292212963104248,#98df8a
8.432225,4.5318213,neighbor,204960716,Few-Shot NLG with Pre-Trained Language Model,0.05338865518569946,#98df8a
-1.6472636,0.41977763,neighbor,204960716,"Encode, Tag, Realize: High-Precision Text Editing",0.05351865291595459,#98df8a
4.119521,2.0950246,neighbor,204960716,Cross-Lingual Natural Language Generation via Pre-Training,0.0541379451751709,#98df8a
3.4223163,6.327034,neighbor,204960716,Deep Learning Approaches to Text Production,0.05466914176940918,#98df8a
-1.3144614,20.563704,neighbor,204960716,Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation,0.0547107458114624,#98df8a
1.1169114,-6.0833507,neighbor,204960716,MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models,0.05514037609100342,#98df8a
2.9974093,11.558235,neighbor,204960716,Neural Response Generation via GAN with an Approximate Embedding Layer,0.05543088912963867,#98df8a
-8.787219,9.19642,neighbor,204960716,Sequence Level Training with Recurrent Neural Networks,0.055496931076049805,#98df8a
-4.529827,17.94143,neighbor,204960716,TextKD-GAN: Text Generation Using Knowledge Distillation and Generative Adversarial Networks,0.0558696985244751,#98df8a
4.5302587,12.535605,neighbor,204960716,A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation,0.05598634481430054,#98df8a
-12.159604,-0.42944637,neighbor,204960716,Sequence-to-sequence Pre-training with Data Augmentation for Sentence Rewriting,0.055999815464019775,#98df8a
8.601887,2.6002226,neighbor,204960716,A Simple Recipe towards Reducing Hallucination in Neural Surface Realisation,0.0560985803604126,#98df8a
-0.8600618,19.753584,neighbor,204960716,Natural Language Generation with Neural Variational Models,0.05652499198913574,#98df8a
-4.5653296,20.075312,neighbor,204960716,BFGAN: Backward and Forward Generative Adversarial Networks for Lexically Constrained Sentence Generation,0.056610047817230225,#98df8a
9.478996,13.02293,neighbor,204960716,Towards Coherent and Cohesive Long-form Text Generation,0.05661064386367798,#98df8a
-8.657311,-12.737669,neighbor,204960716,Query-Regression Networks for Machine Comprehension,0.05682796239852905,#98df8a
2.4515173,-7.649614,neighbor,204960716,Extractive Summarization with Very Deep Pretrained Language Model,0.05690878629684448,#98df8a
-4.5083966,1.3412254,neighbor,204960716,Sequence Generation: From Both Sides to the Middle,0.0569537878036499,#98df8a
-5.1562896,5.544994,neighbor,204960716,Sentence-wise Smooth Regularization for Sequence to Sequence Learning,0.057023823261260986,#98df8a
-14.2869835,-2.983181,neighbor,204960716,Corpora Generation for Grammatical Error Correction,0.05711257457733154,#98df8a
-8.882894,-5.2606816,neighbor,204960716,DeepNorm-A Deep Learning Approach to Text Normalization,0.05727797746658325,#98df8a
3.2436109,-5.925403,neighbor,204960716,Efficient Adaptation of Pretrained Transformers for Abstractive Summarization,0.057308197021484375,#98df8a
3.771493,9.64657,neighbor,204960716,DeepCopy: Grounded Response Generation with Hierarchical Pointer Networks,0.05731046199798584,#98df8a
2.9957,12.862646,neighbor,204960716,NEXUS Network: Connecting the Preceding and the Following in Dialogue Generation,0.05731821060180664,#98df8a
0.5635964,8.447812,neighbor,204960716,Generating Text with Deep Reinforcement Learning,0.05735480785369873,#98df8a
6.0097027,-5.1650934,neighbor,204960716,Neural Abstractive Text Summarization with Sequence-to-Sequence Models,0.05764120817184448,#98df8a
-10.690332,1.2937639,neighbor,204960716,Prior Attention for Style-aware Sequence-to-Sequence Models,0.057828545570373535,#98df8a
-8.206426,13.484134,neighbor,204960716,CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling,0.05783432722091675,#98df8a
-0.48426357,16.356735,neighbor,204960716,Differentiated Distribution Recovery for Neural Text Generation,0.057868242263793945,#98df8a
-1.9508382,7.9446945,neighbor,204960716,Learning to Decode for Future Success,0.05790668725967407,#98df8a
10.780303,5.526173,neighbor,204960716,Mixture Content Selection for Diverse Sequence Generation,0.05792093276977539,#98df8a
1.0476327,-14.11488,neighbor,204960716,Sentence Compression by Deletion with LSTMs,0.05825716257095337,#98df8a
1.3305956,2.060429,neighbor,204960716,Distilling the Knowledge of BERT for Text Generation,0.058351218700408936,#98df8a
2.7205067,-13.186892,neighbor,204960716,Deleter: Leveraging BERT to Perform Unsupervised Successive Text Compression,0.058396339416503906,#98df8a
7.082883,8.097172,neighbor,204960716,Towards Content Transfer through Grounded Text Generation,0.058428049087524414,#98df8a
-3.5652225,18.772556,neighbor,204960716,Adversarial Text Generation Without Reinforcement Learning,0.05850297212600708,#98df8a
-8.839768,8.151893,neighbor,204960716,genCNN: A Convolutional Architecture for Word Sequence Prediction,0.05856430530548096,#98df8a
-1.1764964,9.159755,neighbor,204960716,An Actor-Critic Algorithm for Sequence Prediction,0.058636486530303955,#98df8a
-12.422322,4.549275,neighbor,204960716,Cold Fusion: Training Seq2Seq Models Together with Language Models,0.05868351459503174,#98df8a
-10.042889,-7.3645897,neighbor,204960716,Syntactically Supervised Transformers for Faster Neural Machine Translation,0.05927884578704834,#98df8a
-10.694408,-1.2841986,neighbor,204960716,EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing,0.05954021215438843,#98df8a
1.175717,19.01019,neighbor,204960716,Professor Forcing: A New Algorithm for Training Recurrent Networks,0.05965834856033325,#98df8a
-3.747581,16.87083,neighbor,204960716,Long Text Generation via Adversarial Training with Leaked Information,0.05966532230377197,#98df8a
-13.908996,-6.1527495,neighbor,204960716,Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks,0.0596998929977417,#98df8a
-2.0460231,15.672113,neighbor,204960716,"Neural Text Generation: Past, Present and Beyond",0.05980795621871948,#98df8a
7.0357533,-6.56028,neighbor,204960716,Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization,0.059939682483673096,#98df8a
-9.831501,10.29683,neighbor,204960716,Sequence Modeling with Unconstrained Generation Order,0.06016814708709717,#98df8a
-2.6869404,13.512511,neighbor,204960716,A Semi-Supervised Approach for Low-Resourced Text Generation,0.06019127368927002,#98df8a
9.520545,10.264373,neighbor,204960716,Unifying Human and Statistical Evaluation for Natural Language Generation,0.06024003028869629,#98df8a
-1.7786335,-9.165681,neighbor,204960716,Online Segment to Segment Neural Transduction,0.060438573360443115,#98df8a
9.541794,12.268427,neighbor,204960716,"A bird's-eye view on coherence, and a worm's-eye view on cohesion",0.06046169996261597,#98df8a
-11.783784,-7.62621,neighbor,204960716,Multi-channel Encoder for Neural Machine Translation,0.06046861410140991,#98df8a
0.85481286,-9.921701,neighbor,204960716,Text Summarization as Tree Transduction by Top-Down TreeLSTM,0.06059432029724121,#98df8a
9.218972,14.684344,neighbor,204960716,Learning to Write with Cooperative Discriminators,0.06060183048248291,#98df8a
5.020021,0.9033895,neighbor,204960716,Large-Scale Transfer Learning for Natural Language Generation,0.060781657695770264,#98df8a
4.218584,-9.9407625,neighbor,204960716,Deconvolutional Paragraph Representation Learning,0.06085860729217529,#98df8a
8.526932,-3.8435445,neighbor,204960716,Sequential Copying Networks,0.06091088056564331,#98df8a
6.5068493,-9.971861,neighbor,204960716,SummAE: Zero-Shot Abstractive Text Summarization using Length-Agnostic Auto-Encoders,0.06097906827926636,#98df8a
-8.18146,-6.8434925,neighbor,204960716,SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,0.06102406978607178,#98df8a
-10.867628,-8.580547,neighbor,204960716,Attention is All you Need,0.06109076738357544,#98df8a
-6.230327,18.414738,neighbor,204960716,Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets,0.061157822608947754,#98df8a
-15.472534,-5.789385,neighbor,204960716,Synthetic and Natural Noise Both Break Neural Machine Translation,0.061296939849853516,#98df8a
-5.7821884,-3.3223157,neighbor,204960716,Still not systematic after all these years: On the compositional skills of sequence-to-sequence recurrent networks,0.061336517333984375,#98df8a
-8.519793,5.81416,neighbor,204960716,Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs,0.061380863189697266,#98df8a
3.6747248,-7.5266776,neighbor,204960716,Text Summarization with Pretrained Encoders,0.061472296714782715,#98df8a
6.3816185,-8.006973,neighbor,204960716,A Deep Reinforced Model for Abstractive Summarization,0.06155657768249512,#98df8a
6.604254,5.0355906,neighbor,204960716,"Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data",0.06155693531036377,#98df8a
9.086548,5.4785085,neighbor,204960716,End-to-End Content and Plan Selection for Data-to-Text Generation,0.061740219593048096,#98df8a
-9.129592,-11.959154,neighbor,204960716,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,0.061840593814849854,#98df8a
-2.9556653,-4.1299624,neighbor,204960716,Pre-Trained Text Embeddings for Enhanced Text-to-Speech Synthesis,0.061846137046813965,#98df8a
6.520242,11.056215,neighbor,204960716,"A Simple, Fast Diverse Decoding Algorithm for Neural Generation",0.061877429485321045,#98df8a
8.325066,-8.415224,neighbor,204960716,Generating Wikipedia by Summarizing Long Sequences,0.06191974878311157,#98df8a
-14.005913,-4.880391,neighbor,204960716,Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task,0.06193363666534424,#98df8a
-11.498932,-5.7598896,neighbor,204960716,Efficient Bidirectional Neural Machine Translation,0.06193667650222778,#98df8a
12.928844,4.721512,neighbor,204960716,KERMIT: Generative Insertion-Based Modeling for Sequences,0.06200462579727173,#98df8a
5.0983934,-6.4512954,neighbor,204960716,Multi-stage Pretraining for Abstractive Summarization,0.062212586402893066,#98df8a
-0.88902456,-0.57986796,query,206594738,"You Only Look Once: Unified, Real-Time Object Detection",0.0,#d62728
0.31251398,-0.30012876,neighbor,206594738,Scalable Object Detection Using Deep Neural Networks,0.041725873947143555,#d62728
0.9350569,1.3270129,neighbor,206594738,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.042772769927978516,#d62728
-1.5726935,-2.6329832,neighbor,206594738,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.044342219829559326,#d62728
0.5625782,1.3869505,neighbor,206594738,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.04456973075866699,#d62728
-4.7219825,-2.420347,neighbor,206594738,"Scalable, High-Quality Object Detection",0.04910576343536377,#d62728
-3.712254,-1.9709525,neighbor,206594738,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.04967331886291504,#d62728
-2.6194708,-2.7361717,neighbor,206594738,Fast R-CNN,0.050298869609832764,#d62728
-2.759388,3.1668441,neighbor,206594738,Mid-level Elements for Object Detection,0.05047327280044556,#d62728
6.6833043,-8.128009,neighbor,206594738,Data-driven 3D Voxel Patterns for object category recognition,0.05291992425918579,#d62728
-5.6624503,-1.6206725,neighbor,206594738,What Makes for Effective Detection Proposals?,0.052949368953704834,#d62728
-0.28606257,-2.3490822,neighbor,206594738,Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction,0.0536801815032959,#d62728
-6.0731316,-1.2323635,neighbor,206594738,"How good are detection proposals, really?",0.05440855026245117,#d62728
-0.44470307,-5.62466,neighbor,206594738,Deep learning for class-generic object detection,0.054461896419525146,#d62728
-3.3412569,2.518296,neighbor,206594738,Max-Margin Object Detection,0.055149734020233154,#d62728
8.980382,4.4335704,neighbor,206594738,Context Forest for efficient object detection with large mixture models,0.05570441484451294,#d62728
-4.186032,-1.0452828,neighbor,206594738,R-CNN minus R,0.05591118335723877,#d62728
1.091839,2.3897245,neighbor,206594738,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.056066155433654785,#d62728
0.8938279,-4.021318,neighbor,206594738,1-HKUST: Object Detection in ILSVRC 2014,0.05622398853302002,#d62728
4.352981,0.11488748,neighbor,206594738,Looking out of the window: object localization by joint analysis of all windows in the image,0.05791783332824707,#d62728
-2.9822648,-3.7589974,neighbor,206594738,Object Detection Networks on Convolutional Feature Maps,0.05799603462219238,#d62728
2.7639496,-2.6403034,neighbor,206594738,Attention for Fine-Grained Categorization,0.05875051021575928,#d62728
8.61141,8.701003,neighbor,206594738,A novel method for object localization in digital images,0.05898410081863403,#d62728
6.8793006,5.108796,neighbor,206594738,Ensemble of exemplar-SVMs for object detection and beyond,0.05940890312194824,#d62728
-1.8887895,-6.747858,neighbor,206594738,DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,0.05957996845245361,#d62728
-1.6003368,-6.8050065,neighbor,206594738,DeepID-Net: Deformable deep convolutional neural networks for object detection,0.059830665588378906,#d62728
5.3082714,-1.2480348,neighbor,206594738,Self-taught object localization with deep networks,0.060262978076934814,#d62728
12.168191,-2.8726895,neighbor,206594738,ARTOS - Adaptive Real-Time Object Detection System,0.06033891439437866,#d62728
-0.08623407,4.535004,neighbor,206594738,Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts,0.06033974885940552,#d62728
9.189494,-1.9338872,neighbor,206594738,On learning to localize objects with minimal supervision,0.06065666675567627,#d62728
-0.6149042,-6.5464573,neighbor,206594738,Generic Object Detection with Dense Neural Patterns and Regionlets,0.06069767475128174,#d62728
-1.9233807,-0.29683924,neighbor,206594738,Detecting people in Cubist art,0.06069892644882202,#d62728
4.63811,-4.871211,neighbor,206594738,Learning Rich Features from RGB-D Images for Object Detection and Segmentation,0.060821533203125,#d62728
8.427434,3.0883367,neighbor,206594738,An empirical study of context in object detection,0.06130945682525635,#d62728
-1.5453928,3.0969298,neighbor,206594738,Regionlets for Generic Object Detection,0.06138408184051514,#d62728
8.969334,0.9971885,neighbor,206594738,Object Detectors Emerge in Deep Scene CNNs,0.06162673234939575,#d62728
6.7581835,-8.317388,neighbor,206594738,3D object class detection in the wild,0.06172311305999756,#d62728
-2.8265803,1.4599612,neighbor,206594738,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.06183063983917236,#d62728
10.457013,-2.119822,neighbor,206594738,Watch and learn: Semi-supervised learning of object detectors from videos,0.06195110082626343,#d62728
-3.971043,7.7037354,neighbor,206594738,Measuring the Objectness of Image Windows,0.06266653537750244,#d62728
0.22134334,8.408363,neighbor,206594738,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.06289410591125488,#d62728
2.747538,-1.6977106,neighbor,206594738,Part-Based R-CNNs for Fine-Grained Category Detection,0.06329196691513062,#d62728
-7.5532737,-1.0759078,neighbor,206594738,An active search strategy for efficient object class detection,0.06402570009231567,#d62728
2.2120638,0.16151102,neighbor,206594738,Hypercolumns for object segmentation and fine-grained localization,0.06423288583755493,#d62728
-5.0745955,-5.4240885,neighbor,206594738,LSDA: Large Scale Detection through Adaptation,0.06429576873779297,#d62728
7.4949064,1.3732755,neighbor,206594738,Unsupervised Visual Representation Learning by Context Prediction,0.06496471166610718,#d62728
4.6672826,1.594769,neighbor,206594738,Visual chunking: A list prediction framework for region-based object detection,0.06516671180725098,#d62728
2.1253662,-11.989179,neighbor,206594738,Finding action tubes,0.0652434229850769,#d62728
-0.9630669,1.9609436,neighbor,206594738,Boosting Convolutional Features for Robust Object Proposals,0.06531602144241333,#d62728
1.3582911,-11.294889,neighbor,206594738,R-CNNs for Pose Estimation and Action Detection,0.06532090902328491,#d62728
-5.0603857,1.0674111,neighbor,206594738,Detect2Rank: Combining Object Detectors Using Learning to Rank,0.06601661443710327,#d62728
1.5359819,3.5390189,neighbor,206594738,Layered object detection for multi-class segmentation,0.0663108229637146,#d62728
8.251519,-1.6513754,neighbor,206594738,Weakly-supervised Discovery of Visual Pattern Configurations,0.06633299589157104,#d62728
-3.849709,9.045844,neighbor,206594738,Salient Object Detection: A Benchmark,0.06712043285369873,#d62728
-6.019757,3.0201063,neighbor,206594738,Shared Random Ferns for Efficient Detection of Multiple Categories,0.06720751523971558,#d62728
-3.8489568,9.052936,neighbor,206594738,Salient object detection: A survey,0.06769949197769165,#d62728
10.274894,8.186744,neighbor,206594738,Object Detection in Real Images,0.06771916151046753,#d62728
1.5332557,5.990022,neighbor,206594738,Object category detection by incorporating mid-level grouping cues,0.06775534152984619,#d62728
8.343575,8.167956,neighbor,206594738,Fast concurrent object localization and recognition,0.06794971227645874,#d62728
6.4968524,-1.8660114,neighbor,206594738,Weakly Supervised Object Localization with Multi-Fold Multiple Instance Learning,0.06820428371429443,#d62728
1.2812525,-4.387623,neighbor,206594738,ImageNet Large Scale Visual Recognition Challenge,0.06855589151382446,#d62728
-6.8008337,7.6696877,neighbor,206594738,Detection of Partially Visible Objects,0.06863552331924438,#d62728
0.7244262,8.386358,neighbor,206594738,Situational object boundary detection,0.06882065534591675,#d62728
-1.7045252,6.0775332,neighbor,206594738,Context-Aware Semi-Local Feature Detector,0.06886333227157593,#d62728
-5.6966114,-3.1760864,neighbor,206594738,DeepBox: Learning Objectness with Convolutional Networks,0.06919288635253906,#d62728
-2.392832,4.2441792,neighbor,206594738,Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection,0.06927448511123657,#d62728
6.679451,-2.6696756,neighbor,206594738,Detector discovery in the wild: Joint multiple instance and representation learning,0.06950134038925171,#d62728
2.3273919,-7.029302,neighbor,206594738,Deformable part models are convolutional neural networks,0.06994247436523438,#d62728
5.17805,4.824881,neighbor,206594738,"Recursive compositional models: Representation, learning, and inference",0.07020503282546997,#d62728
-9.613722,4.2796035,neighbor,206594738,Taking a deeper look at pedestrians,0.07032489776611328,#d62728
-4.52849,-10.041959,neighbor,206594738,Transferring Rich Feature Hierarchies for Robust Visual Tracking,0.07039427757263184,#d62728
0.9132132,4.6906266,neighbor,206594738,Semantic segmentation using regions and parts,0.07046830654144287,#d62728
9.54548,10.408273,neighbor,206594738,Faster and Better: A Machine Learning Approach to Corner Detection,0.07051354646682739,#d62728
5.729917,-6.746309,neighbor,206594738,Learning Deep Object Detectors from 3D Models,0.07083523273468018,#d62728
6.8940277,0.713224,neighbor,206594738,Multiple Object Recognition with Visual Attention,0.07090151309967041,#d62728
-9.7541065,4.5310144,neighbor,206594738,Pedestrian Detection with Unsupervised Multi-stage Feature Learning,0.0710897445678711,#d62728
5.357264,9.185388,neighbor,206594738,Learning From a Small Number of Training Examples by Exploiting Object Categories,0.07121092081069946,#d62728
9.271035,-8.291921,neighbor,206594738,Occlusion Reasoning for Object Detectionunder Arbitrary Viewpoint,0.07124698162078857,#d62728
2.4374046,1.7292945,neighbor,206594738,Deep Joint Task Learning for Generic Object Extraction,0.07142937183380127,#d62728
0.79933065,-0.87292075,neighbor,206594738,Object-centric Sampling for Fine-grained Image Classification,0.072238028049469,#d62728
6.332515,4.457674,neighbor,206594738,Composite Models of Objects and Scenes for Category Recognition,0.07245469093322754,#d62728
3.3547041,3.9954343,neighbor,206594738,Enriching Visual Knowledge Bases via Object Discovery and Segmentation,0.0725746750831604,#d62728
7.3663807,-7.342814,neighbor,206594738,Multi-View Priors for Learning Detectors from Sparse Viewpoint Data,0.07281088829040527,#d62728
1.0080448,-10.959653,neighbor,206594738,Deep Poselets for Human Detection,0.07281988859176636,#d62728
11.193758,-1.9615335,neighbor,206594738,"An unsupervised, online learning framework for moving object detection",0.07303071022033691,#d62728
-4.1184335,4.2791314,neighbor,206594738,Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection,0.0730857253074646,#d62728
10.134955,-8.22739,neighbor,206594738,Occlusion Coherence: Localizing Occluded Faces with a Hierarchical Deformable Part Model,0.07337439060211182,#d62728
-7.5436206,6.4849114,neighbor,206594738,Optimized Pedestrian Detection for Multiple and Occluded People,0.07354849576950073,#d62728
-2.718011,-5.419006,neighbor,206594738,Do More Dropouts in Pool5 Feature Maps for Better Object Detection,0.0739852786064148,#d62728
-0.24841164,9.342275,neighbor,206594738,Holistically-Nested Edge Detection,0.07399159669876099,#d62728
1.8001317,8.324563,neighbor,206594738,Learning to Find Object Boundaries Using Motion Cues,0.07418394088745117,#d62728
5.228415,-7.254691,neighbor,206594738,Building Part-Based Object Detectors via 3D Geometry,0.07427823543548584,#d62728
-10.37313,4.395467,neighbor,206594738,Pedestrian detection aided by deep learning semantic tasks,0.0744025707244873,#d62728
9.159341,4.0215683,neighbor,206594738,Using the forest to see the trees: exploiting context for visual object detection and localization,0.07448261976242065,#d62728
10.681623,8.279298,neighbor,206594738,Implicit spatial inference with sparse local features,0.0750267505645752,#d62728
-3.8265085,6.4879403,neighbor,206594738,BING: Binarized normed gradients for objectness estimation at 300fps,0.0750306248664856,#d62728
5.507772,7.072585,neighbor,206594738,Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation,0.07504856586456299,#d62728
5.396853,7.4571185,neighbor,206594738,Learning an Alphabet of Shape and Appearance for Multi-Class Object Detection,0.07507437467575073,#d62728
11.927766,8.894639,neighbor,206594738,Fast Rotation Invariant Object Detection with Gradient based Detection Models,0.07512819766998291,#d62728
5.9362664,-9.2851,neighbor,206594738,A coarse-to-fine model for 3D pose estimation and sub-category recognition,0.0752449631690979,#d62728
-4.4897923,-10.007234,neighbor,206594738,Tracking with deep neural networks,0.07538747787475586,#d62728
-1.2754134,4.038013,query,207238980,node2vec: Scalable Feature Learning for Networks,0.0,#ff9896
-1.3029885,3.4628084,neighbor,207238980,Learning Structural Features of Nodes in Large-Scale Networks for Link Prediction,0.035707831382751465,#ff9896
0.30907395,4.0739827,neighbor,207238980,LINE: Large-scale Information Network Embedding,0.05222219228744507,#ff9896
1.3087664,2.9203858,neighbor,207238980,DeepWalk: online learning of social representations,0.05564457178115845,#ff9896
-2.1326299,10.650604,neighbor,207238980,Classifying Network Data with Deep Kernel Machines,0.05727487802505493,#ff9896
-2.8406563,9.315436,neighbor,207238980,Diffusion-Convolutional Neural Networks,0.06247520446777344,#ff9896
-2.0649526,8.585107,neighbor,207238980,Learning Convolutional Neural Networks for Graphs,0.06419551372528076,#ff9896
0.19382972,6.402811,neighbor,207238980,subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs,0.06481307744979858,#ff9896
-7.119769,2.4724133,neighbor,207238980,Swivel: Improving Embeddings by Noticing What's Missing,0.06500083208084106,#ff9896
-0.6654911,-1.5535883,neighbor,207238980,Transductive Classification on Heterogeneous Information Networks with Edge Betweenness-based Normalization,0.07076132297515869,#ff9896
4.1597934,-4.482747,neighbor,207238980,Discriminative Nonparametric Latent Feature Relational Models with Data Augmentation,0.07136678695678711,#ff9896
0.6560484,9.432938,neighbor,207238980,Propagation kernels: efficient graph kernels from propagated information,0.0717475414276123,#ff9896
-11.258774,1.639592,neighbor,207238980,Multi-Task Metric Learning on Network Data,0.07186073064804077,#ff9896
4.3215203,-2.4082406,neighbor,207238980,LRBM: A Restricted Boltzmann Machine Based Approach for Representation Learning on Linked Data,0.07187944650650024,#ff9896
2.6879873,0.50025296,neighbor,207238980,Joint Inference of Multiple Label Types in Large Networks,0.07333016395568848,#ff9896
10.3553295,-5.224331,neighbor,207238980,New perspectives and methods in link prediction,0.07390528917312622,#ff9896
5.6049356,-2.8080077,neighbor,207238980,An Infinite Latent Attribute Model for Network Data,0.07410025596618652,#ff9896
10.835358,6.0352397,neighbor,207238980,Learning Scale Free Network by Node Specific Degree Prior,0.07499390840530396,#ff9896
-10.018188,9.910236,neighbor,207238980,"Sparse, guided feature connections in an Abstract Deep Network",0.07509618997573853,#ff9896
-9.232587,10.50537,neighbor,207238980,Feature Graph Architectures,0.07509911060333252,#ff9896
7.6202865,-7.4171968,neighbor,207238980,Link Prediction via Sparse Gaussian Graphical Model,0.07584095001220703,#ff9896
-8.364734,8.879708,neighbor,207238980,Self-informed neural network structure learning,0.07630902528762817,#ff9896
-1.2588737,8.73967,neighbor,207238980,Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction,0.07641208171844482,#ff9896
9.128756,-2.2304115,neighbor,207238980,OMNI-Prop: Seamless Node Classification on Arbitrary Label Correlation,0.07690441608428955,#ff9896
5.401749,9.428468,neighbor,207238980,Marginalized Denoising for Link Prediction and Multi-Label Learning,0.07720702886581421,#ff9896
-7.652603,-4.9663267,neighbor,207238980,Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases,0.07749354839324951,#ff9896
-7.232451,-3.6953242,neighbor,207238980,Context-Dependent Knowledge Graph Embedding,0.07754194736480713,#ff9896
-0.35426322,-5.2430973,neighbor,207238980,Transforming Graph Representations for Statistical Relational Learning,0.07756763696670532,#ff9896
4.6056046,-4.5269938,neighbor,207238980,Max-Margin Nonparametric Latent Feature Models for Link Prediction,0.07797521352767944,#ff9896
-7.3396983,-2.589441,neighbor,207238980,Semantically Smooth Knowledge Graph Embedding,0.07807528972625732,#ff9896
-3.8479059,-5.2288766,neighbor,207238980,A Minimalistic Approach to Sum-Product Network Learning for Real Applications,0.078280508518219,#ff9896
10.08177,-3.3715258,neighbor,207238980,"Discriminative Link Prediction Using Local Links, Node Features and Community Structure",0.0783892273902893,#ff9896
-11.14918,8.42263,neighbor,207238980,Omnigraph: Rich Representation and Graph Kernel Learning,0.07860589027404785,#ff9896
13.245558,-3.4393244,neighbor,207238980,A Regularization Approach for Prediction of Edges and Node Features in Dynamic Graphs,0.07864254713058472,#ff9896
-0.9919062,6.1402373,neighbor,207238980,Deep Neural Networks for Learning Graph Representations,0.07885950803756714,#ff9896
-9.136698,5.5537686,neighbor,207238980,Structural Learning with Amortized Inference,0.07925456762313843,#ff9896
-2.5462732,7.4919395,neighbor,207238980,Deep Convolutional Networks on Graph-Structured Data,0.07938164472579956,#ff9896
9.414176,-4.008871,neighbor,207238980,Network flows and the link prediction problem,0.08002692461013794,#ff9896
2.0377166,3.7870564,neighbor,207238980,Comprehend DeepWalk as Matrix Factorization,0.08022117614746094,#ff9896
11.986235,-1.3875524,neighbor,207238980,Unsupervised Feature Selection on Networks: A Generative View,0.08024674654006958,#ff9896
0.25362337,1.2541438,neighbor,207238980,Better Together: Combining Language and Social Interactions into a Shared Representation,0.08037668466567993,#ff9896
11.449049,-2.7363188,neighbor,207238980,Supervised random walks: predicting and recommending links in social networks,0.08038514852523804,#ff9896
-11.419027,2.2149227,neighbor,207238980,Learning Neighborhoods for Metric Learning,0.0805732011795044,#ff9896
4.7483664,10.846154,neighbor,207238980,Robust Semi-Supervised Classification for Multi-Relational Graphs,0.08064579963684082,#ff9896
3.111213,10.883446,neighbor,207238980,Semisupervised Classification Through the Bag-of-Paths Group Betweenness,0.08088243007659912,#ff9896
8.89209,3.8542666,neighbor,207238980,Hidden geometric correlations in real multiplex networks,0.08093869686126709,#ff9896
8.826596,8.543678,neighbor,207238980,Extending the Modelling Capacity of Gaussian Conditional Random Fields while Learning Faster,0.08104890584945679,#ff9896
9.329365,-7.150313,neighbor,207238980,Link Prediction in Complex Networks: A Mutual Information Perspective,0.08114320039749146,#ff9896
9.4792185,-5.7930627,neighbor,207238980,Link prediction in complex networks: a clustering perspective,0.08133131265640259,#ff9896
-1.3679327,-4.5722733,neighbor,207238980,Single Network Relational Transductive Learning,0.08140146732330322,#ff9896
2.0388906,10.956533,neighbor,207238980,Semi-supervised Learning with Density Based Distances,0.0815696120262146,#ff9896
-2.986254,14.1605835,neighbor,207238980,Learning to Generate Networks,0.08161270618438721,#ff9896
-0.3603456,-5.407849,neighbor,207238980,Transforming Graph Data for Statistical Relational Learning,0.08164399862289429,#ff9896
-0.24598083,10.709336,neighbor,207238980,Tree-Based Kernel for Graphs With Continuous Attributes,0.08166033029556274,#ff9896
0.6847656,10.661913,neighbor,207238980,Generalized Shortest Path Kernel on Graphs,0.08167403936386108,#ff9896
2.676549,9.137688,neighbor,207238980,Metric learning pairwise kernel for graph inference,0.0817074179649353,#ff9896
7.983165,-5.5086436,neighbor,207238980,Multi-relational Link Prediction in Heterogeneous Information Networks,0.08185458183288574,#ff9896
-8.646481,-2.5456662,neighbor,207238980,Knowledge Graph Embedding by Translating on Hyperplanes,0.08216285705566406,#ff9896
-2.7879837,-1.1869948,neighbor,207238980,Text Classification with Heterogeneous Information Network Kernels,0.08216631412506104,#ff9896
-2.3354242,-0.58872473,neighbor,207238980,Classifying networked entities with modularity kernels,0.08225065469741821,#ff9896
-4.1071815,4.7473373,neighbor,207238980,Discriminative Embeddings of Latent Variable Models for Structured Data,0.08251947164535522,#ff9896
0.65804255,-7.2831473,neighbor,207238980,NED: An Inter-Graph Node Metric Based On Edit Distance,0.08260947465896606,#ff9896
1.8560491,1.2619778,neighbor,207238980,Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks,0.08270883560180664,#ff9896
-8.717194,5.88624,neighbor,207238980,Learning Max-Margin Tree Predictors,0.08272945880889893,#ff9896
9.093462,1.067767,neighbor,207238980,Cross-validation estimate of the number of clusters in a network,0.08310145139694214,#ff9896
10.008306,1.8455439,neighbor,207238980,Partitioning Networks with Node Attributes by Compressing Information Flow,0.0831795334815979,#ff9896
1.0929896,-4.062206,neighbor,207238980,Adaptive Neighborhood Graph Construction for Inference in Multi-Relational Networks,0.0831993818283081,#ff9896
-7.9368014,1.7363676,neighbor,207238980,Entity Embeddings of Categorical Variables,0.0832929015159607,#ff9896
-6.733597,6.573932,neighbor,207238980,Normalized Hierarchical SVM,0.08345162868499756,#ff9896
1.0314552,-2.0352948,neighbor,207238980,Link Prediction-Based Multi-label Classification on Networked Data,0.0838618278503418,#ff9896
4.297385,10.080519,neighbor,207238980,Multi-Label Learning on Tensor Product Graph,0.08457314968109131,#ff9896
6.7672334,-2.689491,neighbor,207238980,Representing higher-order dependencies in networks,0.08465468883514404,#ff9896
-10.01153,-2.8242986,neighbor,207238980,Complex Embeddings for Simple Link Prediction,0.084786057472229,#ff9896
2.8826547,13.178329,neighbor,207238980,Learning from networked examples in a k-partite graph,0.08490675687789917,#ff9896
-4.7545905,2.0861313,neighbor,207238980,Inducing Language Networks from Continuous Space Word Representations,0.08542972803115845,#ff9896
7.9153266,2.9637609,neighbor,207238980,Detection of core–periphery structure in networks using spectral methods and geodesic paths,0.08549356460571289,#ff9896
8.243167,-9.288089,neighbor,207238980,Link Prediction via Convex Nonnegative Matrix Factorization on Multiscale Blocks,0.08550548553466797,#ff9896
-8.564979,-0.69484633,neighbor,207238980,Convex Co-embedding,0.08553826808929443,#ff9896
-3.925501,-5.888982,neighbor,207238980,Learning Relational Sum-Product Networks,0.08557653427124023,#ff9896
10.414042,-7.7642174,neighbor,207238980,Predicting missing links via significant paths,0.08560478687286377,#ff9896
10.599231,6.8377433,neighbor,207238980,Learning graphical models with hubs,0.08567512035369873,#ff9896
-5.7514877,7.2726636,neighbor,207238980,Sublinear Models for Graphs,0.08593165874481201,#ff9896
11.370381,3.3342655,neighbor,207238980,Learning Latent Block Structure in Weighted Networks,0.08594542741775513,#ff9896
9.313096,-7.8491616,neighbor,207238980,Link prediction in complex networks: A local naïve Bayes model,0.08604764938354492,#ff9896
-5.5445795,9.679124,neighbor,207238980,Neural Network Matrix Factorization,0.0860862135887146,#ff9896
9.861745,2.9825072,neighbor,207238980,Exploring the structural regularities in networks,0.08624374866485596,#ff9896
5.3199472,7.0018106,neighbor,207238980,A Boosting Approach to Learning Graph Representations,0.08634036779403687,#ff9896
-8.680462,-3.9052892,neighbor,207238980,Knowlege Graph Embedding by Flexible Translation,0.08649122714996338,#ff9896
-7.7423377,12.109273,neighbor,207238980,Deeply-Fused Nets,0.08659368753433228,#ff9896
10.515501,3.3874297,neighbor,207238980,Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction,0.08659803867340088,#ff9896
8.023148,-8.174621,neighbor,207238980,Link prediction via matrix completion,0.08661460876464844,#ff9896
1.5542693,-8.346067,neighbor,207238980,DELTACON: A Principled Massive-Graph Similarity Function,0.08692580461502075,#ff9896
-7.1985044,3.5763063,neighbor,207238980,Locally Non-linear Embeddings for Extreme Multi-label Learning,0.08696836233139038,#ff9896
-6.2588186,11.915798,neighbor,207238980,On Deep Representation Learning from Noisy Web Images,0.08700752258300781,#ff9896
-8.093813,-3.189742,neighbor,207238980,From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction,0.0871891975402832,#ff9896
-10.352895,3.1412826,neighbor,207238980,Deep Metric Learning via Lifted Structured Feature Embedding,0.08739310503005981,#ff9896
11.558803,-5.1771803,neighbor,207238980,Exploiting longer cycles for link prediction in signed networks,0.08755654096603394,#ff9896
7.161542,2.0528107,neighbor,207238980,SNAP,0.0875844955444336,#ff9896
7.285211,4.268583,neighbor,207238980,Geometric Network Comparisons,0.08760350942611694,#ff9896
-4.12534,-7.3276553,neighbor,207238980,Mean-Field Networks,0.08763128519058228,#ff9896
-8.614168,11.743128,neighbor,207238980,Highway Networks,0.08768045902252197,#ff9896
2.0727613,0.7463667,query,211096730,A Simple Framework for Contrastive Learning of Visual Representations,0.0,#ff9896
1.4484597,-6.7934012,neighbor,211096730,Multi-task Self-Supervised Visual Learning,0.03727364540100098,#ff9896
2.6508026,-0.6318608,neighbor,211096730,Selfie: Self-supervised Pretraining for Image Embedding,0.03941929340362549,#ff9896
0.83111733,1.6540555,neighbor,211096730,Momentum Contrast for Unsupervised Visual Representation Learning,0.04261857271194458,#ff9896
-5.0207767,13.890933,neighbor,211096730,Data-Efficient Image Recognition with Contrastive Predictive Coding,0.04486113786697388,#ff9896
-0.7492636,-4.432331,neighbor,211096730,Billion-scale semi-supervised learning for image classification,0.04697650671005249,#ff9896
-0.24777062,-8.788827,neighbor,211096730,Revisiting Self-Supervised Visual Representation Learning,0.04747045040130615,#ff9896
-3.6406093,11.493515,neighbor,211096730,Progressive Recurrent Learning for Visual Recognition,0.050251543521881104,#ff9896
10.792656,-1.9859829,neighbor,211096730,Self-Training With Noisy Student Improves ImageNet Classification,0.050675809383392334,#ff9896
-11.285647,-3.2058966,neighbor,211096730,Big Transfer (BiT): General Visual Representation Learning,0.05092489719390869,#ff9896
0.4460001,11.12328,neighbor,211096730,Master's Thesis : Deep Learning for Visual Recognition,0.05099767446517944,#ff9896
4.1039042,2.142161,neighbor,211096730,See Better Before Looking Closer: Weakly Supervised Data Augmentation Network for Fine-Grained Visual Classification,0.05118203163146973,#ff9896
-12.661785,7.944271,neighbor,211096730,Learning to Learn Image Classifiers with Informative Visual Analogy,0.051203012466430664,#ff9896
-3.6409206,7.443081,neighbor,211096730,Deep Residual Learning for Image Recognition,0.051496803760528564,#ff9896
-11.200509,-2.5513856,neighbor,211096730,A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark,0.05164456367492676,#ff9896
11.469882,-0.8520591,neighbor,211096730,Adversarial Examples Improve Image Recognition,0.051831960678100586,#ff9896
4.562842,-2.9780471,neighbor,211096730,"A critical analysis of self-supervision, or what we can learn from a single image",0.051835477352142334,#ff9896
-1.4116745,-7.665048,neighbor,211096730,Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey,0.05250579118728638,#ff9896
-10.851148,5.3932967,neighbor,211096730,Adapting Grad-CAM for Embedding Networks,0.053153812885284424,#ff9896
3.3780315,5.2662387,neighbor,211096730,DisturbLabel: Regularizing CNN on the Loss Layer,0.053547680377960205,#ff9896
-6.597884,6.130388,neighbor,211096730,Natural Neural Networks,0.05392974615097046,#ff9896
0.2309158,14.2136,neighbor,211096730,Global-and-local attention networks for visual recognition,0.054490506649017334,#ff9896
-2.7442925,-5.7182574,neighbor,211096730,Leveraging Large-Scale Uncurated Data for Unsupervised Pre-training of Visual Features,0.055003464221954346,#ff9896
4.978901,13.884638,neighbor,211096730,Stand-Alone Self-Attention in Vision Models,0.05566585063934326,#ff9896
-6.7112894,-6.0042443,neighbor,211096730,ClusterFit: Improving Generalization of Visual Representations,0.05574941635131836,#ff9896
-13.023631,8.24481,neighbor,211096730,Learning to Learn Image Classifiers With Visual Analogy,0.055890798568725586,#ff9896
3.4479678,-6.553617,neighbor,211096730,Image Enhanced Rotation Prediction for Self-Supervised Learning,0.05605173110961914,#ff9896
-0.07498457,-1.74892,neighbor,211096730,HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning,0.05608946084976196,#ff9896
1.7542118,-7.9629083,neighbor,211096730,Scaling and Benchmarking Self-Supervised Visual Representation Learning,0.0562250018119812,#ff9896
1.2669519,-1.3994135,neighbor,211096730,EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning,0.05626577138900757,#ff9896
-9.974571,-1.2421921,neighbor,211096730,Learning multiple visual domains with residual adapters,0.05641144514083862,#ff9896
10.613845,2.1210966,neighbor,211096730,Self-ensembling for domain adaptation,0.05683213472366333,#ff9896
-3.353137,-5.8483844,neighbor,211096730,Unsupervised Pre-Training of Image Features on Non-Curated Data,0.057120680809020996,#ff9896
-0.7947865,11.958676,neighbor,211096730,Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,0.057586729526519775,#ff9896
-0.11245377,-7.096897,neighbor,211096730,S4L: Self-Supervised Semi-Supervised Learning,0.05760079622268677,#ff9896
-5.104235,4.542017,neighbor,211096730,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.05769217014312744,#ff9896
5.4060354,-11.657351,neighbor,211096730,Boosting Supervision with Self-Supervision for Few-shot Learning,0.05802285671234131,#ff9896
8.287734,1.2136265,neighbor,211096730,AutoAugment: Learning Augmentation Strategies From Data,0.058299124240875244,#ff9896
-0.3988531,9.17879,neighbor,211096730,Enhanced image classification with a fast-learning shallow convolutional neural network,0.0584869384765625,#ff9896
-0.21398754,8.253944,neighbor,211096730,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.058822035789489746,#ff9896
4.6961913,-4.255826,neighbor,211096730,Towards a Hypothesis on Visual Transformation based Self-Supervision,0.05883914232254028,#ff9896
-4.5653496,3.6319702,neighbor,211096730,Swish: a Self-Gated Activation Function,0.05921822786331177,#ff9896
-1.2895029,5.378474,neighbor,211096730,Bag of Tricks for Image Classification with Convolutional Neural Networks,0.0592229962348938,#ff9896
-0.3577096,15.200544,neighbor,211096730,Saccader: Improving Accuracy of Hard Attention Models for Vision,0.0593145489692688,#ff9896
0.50570184,4.7057247,neighbor,211096730,Competitive Learning Enriches Learning Representation and Accelerates the Fine-tuning of CNNs,0.059391021728515625,#ff9896
4.2746296,14.494469,neighbor,211096730,Attention Augmented Convolutional Networks,0.05940431356430054,#ff9896
-2.065104,7.855358,neighbor,211096730,On Pre-Trained Image Features and Synthetic Images for Deep Learning,0.0595664381980896,#ff9896
7.924926,-1.0133702,neighbor,211096730,Unsupervised Data Augmentation for Consistency Training,0.05960887670516968,#ff9896
-2.974784,-1.7053927,neighbor,211096730,Unsupervised feature learning by augmenting single images,0.0601004958152771,#ff9896
6.4402213,11.447798,neighbor,211096730,Self-Quotient Image based CNN: A Basic Image Processing assisting Convolutional Neural Network,0.0601116418838501,#ff9896
15.744077,2.8732083,neighbor,211096730,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,0.060369014739990234,#ff9896
4.955046,-12.007965,neighbor,211096730,Boosting Few-Shot Visual Learning With Self-Supervision,0.060610294342041016,#ff9896
-8.197719,-0.5357391,neighbor,211096730,From generic to specific deep representations for visual recognition,0.06062018871307373,#ff9896
15.439939,1.6372077,neighbor,211096730,Improved Techniques for Training GANs,0.06064653396606445,#ff9896
-11.770473,-5.059082,neighbor,211096730,Do Better ImageNet Models Transfer Better?,0.060801923274993896,#ff9896
-5.4828496,6.928753,neighbor,211096730,Identity Connections in Residual Nets Improve Noise Stability,0.060845375061035156,#ff9896
8.923574,1.6328459,neighbor,211096730,AutoAugment: Learning Augmentation Policies from Data,0.060947537422180176,#ff9896
-2.9895196,4.004388,neighbor,211096730,"Piggyback: Adding Multiple Tasks to a Single, Fixed Network by Learning to Mask",0.06103318929672241,#ff9896
-7.255412,-0.076526925,neighbor,211096730,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.06106299161911011,#ff9896
4.964525,5.9467688,neighbor,211096730,Gradient Regularization Improves Accuracy of Discriminative Models,0.06107145547866821,#ff9896
-6.1419225,8.313824,neighbor,211096730,Resnet in Resnet: Generalizing Residual Architectures,0.06107807159423828,#ff9896
-15.345933,2.7841938,neighbor,211096730,Self-Paced Learning with Adaptive Deep Visual Embeddings,0.06139653921127319,#ff9896
-2.7455118,-3.9705954,neighbor,211096730,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.06140327453613281,#ff9896
18.356403,0.86348665,neighbor,211096730,Closed-Loop GAN for continual Learning,0.06147003173828125,#ff9896
-1.4853615,-10.376642,neighbor,211096730,Unsupervised Representation Learning with Prior-Free and Adversarial Mechanism Embedded Autoencoders,0.06173396110534668,#ff9896
-3.8389845,8.924676,neighbor,211096730,Dual Path Networks,0.06179159879684448,#ff9896
-10.994888,1.6159819,neighbor,211096730,Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data,0.06179696321487427,#ff9896
-8.0877905,-6.942459,neighbor,211096730,Exploring the Limits of Weakly Supervised Pretraining,0.06216031312942505,#ff9896
13.486291,6.575192,neighbor,211096730,Spatial PixelCNN: Generating Images from Patches,0.06222128868103027,#ff9896
-5.9005013,9.454005,neighbor,211096730,Res2Net: A New Multi-Scale Backbone Architecture,0.06226527690887451,#ff9896
-8.914329,-4.801948,neighbor,211096730,All Together Now! The Benefits of Adaptively Fusing Pre-trained Deep Representations,0.06227099895477295,#ff9896
9.312487,0.28516096,neighbor,211096730,Adversarial AutoAugment,0.062291860580444336,#ff9896
-1.4035523,1.0839914,neighbor,211096730,Scheduled denoising autoencoders,0.06229960918426514,#ff9896
6.5684834,12.278905,neighbor,211096730,Self-Representation Convolutional Neural Networks,0.0623321533203125,#ff9896
6.727035,2.2636974,neighbor,211096730,Data Augmentation Using Random Image Cropping and Patching for Deep CNNs,0.06258583068847656,#ff9896
1.3220196,-10.387767,neighbor,211096730,Self-supervised visual feature learning with curriculum,0.06266903877258301,#ff9896
14.494652,2.151089,neighbor,211096730,Conditional Transferring Features: Scaling GANs to Thousands of Classes with 30% Less High-Quality Data for Training,0.06284260749816895,#ff9896
-3.427308,6.206444,neighbor,211096730,Aggregated Residual Transformations for Deep Neural Networks,0.06305289268493652,#ff9896
2.1789505,10.829482,neighbor,211096730,CUImage: A Neverending Learning Platform on a Convolutional Knowledge Graph of Billion Web Images,0.06306999921798706,#ff9896
6.8919225,0.15231094,neighbor,211096730,Augmentation Invariant Training,0.06325167417526245,#ff9896
-12.487319,0.80544686,neighbor,211096730,Multifaceted Analysis of Fine-Tuning in a Deep Model for Visual Recognition,0.0632886290550232,#ff9896
-5.390044,-7.3279953,neighbor,211096730,Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning,0.06334996223449707,#ff9896
-8.404909,2.7316086,neighbor,211096730,L*ReLU: Piece-wise Linear Activation Functions for Deep Fine-grained Visual Categorization,0.06341791152954102,#ff9896
7.4478374,3.0464253,neighbor,211096730,The Effectiveness of Data Augmentation in Image Classification using Deep Learning,0.06360578536987305,#ff9896
1.7585893,-4.9023776,neighbor,211096730,Self-labelling via simultaneous clustering and representation learning,0.06364035606384277,#ff9896
-8.216524,-1.5700716,neighbor,211096730,Learning Finer-class Networks for Universal Representations,0.0636749267578125,#ff9896
-3.5329835,-1.6362038,neighbor,211096730,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks,0.06378024816513062,#ff9896
15.134223,0.059721835,neighbor,211096730,Implicit competitive regularization in GANs,0.06402182579040527,#ff9896
-2.096749,14.323013,neighbor,211096730,DIANet: Dense-and-Implicit Attention Network,0.0640561580657959,#ff9896
-14.732293,2.1871014,neighbor,211096730,Sample Balancing for Deep Learning-Based Visual Recognition,0.0642474889755249,#ff9896
14.403382,3.194408,neighbor,211096730,Self-Supervised GANs via Auxiliary Rotation Loss,0.06428176164627075,#ff9896
12.664279,-4.265791,neighbor,211096730,Adversarial TCAV - Robust and Effective Interpretation of Intermediate Layers in Neural Networks,0.06430518627166748,#ff9896
-3.2554321,12.967264,neighbor,211096730,Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition,0.06436043977737427,#ff9896
17.652264,0.7952975,neighbor,211096730,Memory Replay GANs: learning to generate images from new categories without forgetting,0.06446850299835205,#ff9896
14.653513,6.213467,neighbor,211096730,"Less Memory, Faster Speed: Refining Self-Attention Module for Image Reconstruction",0.06448990106582642,#ff9896
0.37039715,6.669489,neighbor,211096730,Learning Convolutional Neural Networks with Deep Part Embeddings,0.06469053030014038,#ff9896
8.014024,-3.274206,neighbor,211096730,Negative sampling in semi-supervised learning,0.06469857692718506,#ff9896
2.5348282,15.425534,neighbor,211096730,Weakly-Supervised Spatial Context Networks,0.0648077130317688,#ff9896
11.067707,-3.9645622,neighbor,211096730,Self-Referenced Deep Learning,0.06494557857513428,#ff9896
-7.524648,11.703142,neighbor,211096730,Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors,0.0650627613067627,#ff9896
3.79246,-8.404672,neighbor,211096730,Learning Image Representations by Completing Damaged Jigsaw Puzzles,0.0652206540107727,#ff9896
9.051377,9.322626,query,216080778,YOLOv4: Optimal Speed and Accuracy of Object Detection,0.0,#9467bd
8.050003,9.683794,neighbor,216080778,"YOLO9000: Better, Faster, Stronger",0.037651121616363525,#9467bd
-6.826615,-4.772912,neighbor,216080778,Rethinking Classification and Localization for Cascade R-CNN,0.043571293354034424,#9467bd
5.7947817,2.4654307,neighbor,216080778,Fast R-CNN,0.04414397478103638,#9467bd
-7.516505,-6.192308,neighbor,216080778,Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training,0.045103371143341064,#9467bd
6.2756476,3.6867588,neighbor,216080778,Training-Time-Friendly Network for Real-Time Object Detection,0.04599970579147339,#9467bd
-3.9046564,-8.516564,neighbor,216080778,Acquisition of Localization Confidence for Accurate Object Detection,0.04647409915924072,#9467bd
1.3144516,2.277508,neighbor,216080778,Light-Head R-CNN: In Defense of Two-Stage Object Detector,0.04739713668823242,#9467bd
-2.544749,2.0569088,neighbor,216080778,FCOS: Fully Convolutional One-Stage Object Detection,0.047751784324645996,#9467bd
2.2269514,-8.654428,neighbor,216080778,"Empirical Upper Bound, Error Diagnosis and Invariance Analysis of Modern Object Detectors",0.048293888568878174,#9467bd
-2.1480696,0.7665779,neighbor,216080778,RON: Reverse Connection with Objectness Prior Networks for Object Detection,0.048461735248565674,#9467bd
-1.0778034,0.10102918,neighbor,216080778,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.04862403869628906,#9467bd
-1.7884694,-5.6691403,neighbor,216080778,Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors,0.04939550161361694,#9467bd
6.53218,11.615863,neighbor,216080778,Fast YOLO: A Fast You Only Look Once System for Real-time Embedded Object Detection in Video,0.04951906204223633,#9467bd
-3.8139863,8.25316,neighbor,216080778,Learning Rich Features at High-Speed for Single-Shot Object Detection,0.04954570531845093,#9467bd
1.0442952,3.855094,neighbor,216080778,MegDet: A Large Mini-Batch Object Detector,0.04972481727600098,#9467bd
-6.6268926,-3.4675372,neighbor,216080778,Cascade R-CNN: Delving Into High Quality Object Detection,0.049868643283843994,#9467bd
2.6146612,0.48594162,neighbor,216080778,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.05074518918991089,#9467bd
5.7626133,-6.0909324,neighbor,216080778,An Analysis of Pre-Training on Object Detection,0.05086702108383179,#9467bd
4.068205,2.6747825,neighbor,216080778,Bag of Freebies for Training Object Detection Neural Networks,0.05131584405899048,#9467bd
-4.1046042,4.2388964,neighbor,216080778,"You Only Look Once: Unified, Real-Time Object Detection",0.05142182111740112,#9467bd
-3.4141624,-9.096855,neighbor,216080778,Bounding Box Regression With Uncertainty for Accurate Object Detection,0.05159956216812134,#9467bd
-3.670536,10.264014,neighbor,216080778,Consistent Optimization for Single-Shot Object Detection,0.052290260791778564,#9467bd
6.032103,-6.2200413,neighbor,216080778,Rethinking ImageNet Pre-Training,0.05278491973876953,#9467bd
9.225808,4.8133125,neighbor,216080778,Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors,0.052804768085479736,#9467bd
-2.1616035,8.383956,neighbor,216080778,DSOD: Learning Deeply Supervised Object Detectors from Scratch,0.05289280414581299,#9467bd
-6.421997,-2.6827157,neighbor,216080778,Cascade R-CNN: High Quality Object Detection and Instance Segmentation,0.052933454513549805,#9467bd
-3.2736123,3.4095273,neighbor,216080778,Object Detection and Localization in Natural Scenes Through Single-Step and Two-Step Models,0.05293834209442139,#9467bd
5.5429792,0.40654722,neighbor,216080778,Beyond Skip Connections: Top-Down Modulation for Object Detection,0.053340137004852295,#9467bd
2.2103646,-8.48434,neighbor,216080778,Empirical Upper Bound in Object Detection and More,0.05337780714035034,#9467bd
3.632633,5.532297,neighbor,216080778,PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection,0.05363965034484863,#9467bd
10.517841,9.709473,neighbor,216080778,YOLOv3: An Incremental Improvement,0.053671181201934814,#9467bd
-11.551627,1.4213504,neighbor,216080778,SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization,0.05393052101135254,#9467bd
-8.946261,-3.8882816,neighbor,216080778,"Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection",0.053976356983184814,#9467bd
8.712921,4.6570773,neighbor,216080778,Light-Weight RetinaNet for Object Detection,0.05405300855636597,#9467bd
3.9090002,5.1896124,neighbor,216080778,PVANet: Lightweight Deep Neural Networks for Real-time Object Detection,0.054081737995147705,#9467bd
3.5687978,-2.6100883,neighbor,216080778,Focal Loss for Dense Object Detection,0.054168760776519775,#9467bd
-8.485101,7.93544,neighbor,216080778,MatrixNets: A New Scale and Aspect Ratio Aware Architecture for Object Detection,0.05418819189071655,#9467bd
-6.3331733,-1.441,neighbor,216080778,MMDetection: Open MMLab Detection Toolbox and Benchmark,0.054296255111694336,#9467bd
-7.476144,4.065831,neighbor,216080778,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.054418981075286865,#9467bd
2.988159,-10.02699,neighbor,216080778,Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming,0.05441915988922119,#9467bd
1.8456497,-5.382292,neighbor,216080778,Fast Efficient Object Detection Using Selective Attention,0.05487567186355591,#9467bd
-2.356318,8.441101,neighbor,216080778,Object Detection from Scratch with Deep Supervision,0.05492675304412842,#9467bd
-0.76186025,3.392818,neighbor,216080778,HAR-Net: Joint Learning of Hybrid Attention for Single-Stage Object Detection,0.05494111776351929,#9467bd
0.9915044,5.9515843,neighbor,216080778,Dual-Resolution Dual-Path Convolutional Neural Networks for Fast Object Detection,0.05504453182220459,#9467bd
-2.3498926,5.207105,neighbor,216080778,R-FCN-3000 at 30fps: Decoupling Detection and Classification,0.05512291193008423,#9467bd
4.1441736,7.800534,neighbor,216080778,"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",0.05518859624862671,#9467bd
4.6017504,8.197224,neighbor,216080778,Realtime Object Detection via Deep Learning-based Pipelines,0.055204808712005615,#9467bd
3.5883496,3.8938777,neighbor,216080778,Object Detection through CNN with Deep Learning,0.055355727672576904,#9467bd
-2.752422,9.747199,neighbor,216080778,ScratchDet: Training Single-Shot Object Detectors From Scratch,0.05540269613265991,#9467bd
-0.12870364,-7.496271,neighbor,216080778,Training Domain Specific Models for Energy-Efficient Object Detection,0.05548751354217529,#9467bd
-7.665017,6.6312056,neighbor,216080778,CornerNet-Lite: Efficient Keypoint based Object Detection,0.05566835403442383,#9467bd
-4.380911,0.78598815,neighbor,216080778,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.05608248710632324,#9467bd
-1.7479059,-0.955028,neighbor,216080778,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.05616527795791626,#9467bd
10.115738,-1.5010859,neighbor,216080778,"ESPNetv2: A Light-Weight, Power Efficient, and General Purpose Convolutional Neural Network",0.0562744140625,#9467bd
5.829647,4.6954303,neighbor,216080778,POD: Practical Object Detection With Scale-Sensitive Network,0.05638772249221802,#9467bd
-3.2402637,-9.7783375,neighbor,216080778,Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression,0.05640643835067749,#9467bd
-8.194915,-2.5850706,neighbor,216080778,Enriched Feature Guided Refinement Network for Object Detection,0.05640798807144165,#9467bd
-2.8092027,-4.8702197,neighbor,216080778,Training Region-Based Object Detectors with Online Hard Example Mining,0.056589603424072266,#9467bd
7.1614695,9.982986,neighbor,216080778,YOLO-LITE: A Real-Time Object Detection Algorithm Optimized for Non-GPU Computers,0.05661267042160034,#9467bd
-8.66732,-0.41575223,neighbor,216080778,CBNet: A Novel Composite Backbone Network Architecture for Object Detection,0.05687218904495239,#9467bd
1.5243503,1.5678294,neighbor,216080778,Rethinking Classification and Localization for Object Detection,0.056929826736450195,#9467bd
-9.400881,0.08178329,neighbor,216080778,"FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction",0.05700576305389404,#9467bd
-11.287232,2.8157837,neighbor,216080778,SNIPER: Efficient Multi-Scale Training,0.05721467733383179,#9467bd
-5.870894,-7.5016522,neighbor,216080778,Dynamic Balance Net: Correlation-enhanced Two-stage Object Detection Network with IoU-Loss,0.05728787183761597,#9467bd
-1.4954304,6.3085275,neighbor,216080778,Object detection at 200 Frames Per Second,0.05735987424850464,#9467bd
-8.59824,8.461328,neighbor,216080778,Matrix Nets: A New Deep Architecture for Object Detection,0.05758386850357056,#9467bd
-0.5402004,8.740631,neighbor,216080778,Using Robust Networks to Inform Lightweight Models in Semi-Supervised Learning for Object Detection,0.058119237422943115,#9467bd
-7.2567596,-0.4452982,neighbor,216080778,RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation,0.058130085468292236,#9467bd
-3.1364555,-1.5549473,neighbor,216080778,Multi-Task Self-Supervised Object Detection via Recycling of Bounding Box Annotations,0.05822855234146118,#9467bd
-1.4003264,2.458615,neighbor,216080778,Selective Convolutional Network: An Efficient Object Detector with Ignoring Background,0.05846059322357178,#9467bd
9.694287,-1.7930726,neighbor,216080778,MixConv: Mixed Depthwise Convolutional Kernels,0.05854058265686035,#9467bd
5.3200507,11.534552,neighbor,216080778,Real-time object detection algorithm based on improved YOLOv3,0.05857419967651367,#9467bd
-6.6565437,-10.3856125,neighbor,216080778,Soft-NMS — Improving Object Detection with One Line of Code,0.058625757694244385,#9467bd
-7.925835,5.852408,neighbor,216080778,CenterNet: Keypoint Triplets for Object Detection,0.058669865131378174,#9467bd
10.190087,0.10709244,neighbor,216080778,CSPNet: A New Backbone that can Enhance Learning Capability of CNN,0.05870199203491211,#9467bd
-4.4470086,5.560853,neighbor,216080778,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.05871152877807617,#9467bd
-2.3113077,-2.1773083,neighbor,216080778,"Scalable, High-Quality Object Detection",0.058728575706481934,#9467bd
-13.046156,1.315545,neighbor,216080778,NAS-FCOS: Fast Neural Architecture Search for Object Detection,0.05880630016326904,#9467bd
-5.2477226,0.14908649,neighbor,216080778,"Objects365: A Large-Scale, High-Quality Dataset for Object Detection",0.05893617868423462,#9467bd
-8.983745,5.3613544,neighbor,216080778,Bottom-Up Object Detection by Grouping Extreme and Center Points,0.05895441770553589,#9467bd
4.8023477,10.775448,neighbor,216080778,YOLO v3-Tiny: Object Detection and Recognition using one stage improved model,0.05917388200759888,#9467bd
6.982476,5.984233,neighbor,216080778,Tiny-DSOD: Lightweight Object Detection for Resource-Restricted Usages,0.05922853946685791,#9467bd
2.0271318,6.9368963,neighbor,216080778,Wide-residual-inception networks for real-time object detection,0.05930042266845703,#9467bd
4.8395953,-0.7756805,neighbor,216080778,FocalNet - Foveal Attention for Post-processing DNN Outputs,0.0593113899230957,#9467bd
8.389694,-0.62135357,neighbor,216080778,Deep Residual Learning for Image Recognition,0.059327125549316406,#9467bd
0.41808933,-2.670151,neighbor,216080778,End-to-end Deep Object Tracking with Circular Loss Function for Rotated Bounding Box,0.059381306171417236,#9467bd
-4.227029,-2.2896428,neighbor,216080778,Object Detection as a Positive-Unlabeled Problem,0.059387803077697754,#9467bd
-10.536903,-6.150161,neighbor,216080778,Grid R-CNN Plus: Faster and Better,0.05942898988723755,#9467bd
7.6374607,1.9467221,neighbor,216080778,Quantization Mimic: Towards Very Tiny CNN for Object Detection,0.059490859508514404,#9467bd
-5.2418485,-5.861754,neighbor,216080778,Revisiting Feature Alignment for One-stage Object Detection,0.05960637331008911,#9467bd
-4.618304,-3.8840115,neighbor,216080778,Overlap Sampler for Region-Based Object Detection,0.0597914457321167,#9467bd
-0.6035841,0.28448617,neighbor,216080778,R-FCN++: Towards Accurate Region-Based Fully Convolutional Networks for Object Detection,0.0598217248916626,#9467bd
-9.080591,6.2071395,neighbor,216080778,Objects as Points,0.059878528118133545,#9467bd
3.617872,-3.053104,neighbor,216080778,Libra R-CNN: Towards Balanced Learning for Object Detection,0.06018286943435669,#9467bd
0.66543233,-0.4430827,neighbor,216080778,Object Detection Networks on Convolutional Feature Maps,0.060240089893341064,#9467bd
-12.174614,-2.079983,neighbor,216080778,Auto-Context R-CNN,0.06034952402114868,#9467bd
-4.782421,2.1817062,neighbor,216080778,Scalable Object Detection Using Deep Neural Networks,0.06037187576293945,#9467bd
2.0101016,11.485799,neighbor,216080778,Guiding the Creation of Deep Learning-based Object Detectors,0.060442566871643066,#9467bd
-9.448206,-9.048052,neighbor,216080778,An Implementation of Faster RCNN with Study for Region Sampling,0.06048166751861572,#9467bd
6.8165364,13.283103,neighbor,216080778,"SlimYOLOv3: Narrower, Faster and Better for Real-Time UAV Applications",0.06048309803009033,#9467bd
-12.135651,0.7203217,query,218889832,End-to-End Object Detection with Transformers,0.0,#9467bd
-8.829134,0.04455732,neighbor,218889832,Cascade R-CNN: High Quality Object Detection and Instance Segmentation,0.042788028717041016,#9467bd
-11.361613,4.024712,neighbor,218889832,RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation,0.04381650686264038,#9467bd
-2.3461504,1.6664004,neighbor,218889832,RON: Reverse Connection with Objectness Prior Networks for Object Detection,0.04458421468734741,#9467bd
11.897245,3.0956428,neighbor,218889832,FreeAnchor: Learning to Match Anchors for Visual Object Detection,0.044662296772003174,#9467bd
-0.9547789,-1.6336857,neighbor,218889832,DATNet: Dense Auxiliary Tasks for Object Detection,0.044666826725006104,#9467bd
-6.383872,3.47057,neighbor,218889832,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.04485291242599487,#9467bd
-4.094606,-3.4340272,neighbor,218889832,Learning Rich Features at High-Speed for Single-Shot Object Detection,0.04499334096908569,#9467bd
2.1684394,0.30168298,neighbor,218889832,Detective: An Attentive Recurrent Model for Sparse Object Detection,0.04515397548675537,#9467bd
1.4134424,1.9702802,neighbor,218889832,"You Only Look Once: Unified, Real-Time Object Detection",0.04587048292160034,#9467bd
-2.0474854,0.40051863,neighbor,218889832,FCOS: Fully Convolutional One-Stage Object Detection,0.046475350856781006,#9467bd
-0.6492741,-6.589581,neighbor,218889832,Focal Loss for Dense Object Detection,0.046478331089019775,#9467bd
-10.468477,2.1655796,neighbor,218889832,MMDetection: Open MMLab Detection Toolbox and Benchmark,0.046652793884277344,#9467bd
-10.838421,4.8491693,neighbor,218889832,Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation,0.04727810621261597,#9467bd
-1.6022936,-2.6442056,neighbor,218889832,DSOD: Learning Deeply Supervised Object Detectors from Scratch,0.0474621057510376,#9467bd
-9.26573,6.904157,neighbor,218889832,"Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection",0.04762458801269531,#9467bd
-1.7798338,-2.846703,neighbor,218889832,Object Detection from Scratch with Deep Supervision,0.04770851135253906,#9467bd
-10.085202,-2.1879716,neighbor,218889832,Cheaper Pre-training Lunch: An Efficient Paradigm for Object Detection,0.0479278564453125,#9467bd
-0.8429378,8.182213,neighbor,218889832,Acquisition of Localization Confidence for Accurate Object Detection,0.04849880933761597,#9467bd
1.8628082,-4.871599,neighbor,218889832,Object Detection through CNN with Deep Learning,0.048966288566589355,#9467bd
11.246677,2.7345881,neighbor,218889832,MetaAnchor: Learning to Detect Objects with Customized Anchors,0.048993051052093506,#9467bd
-10.0415945,0.31602913,neighbor,218889832,Training Region-Based Object Detectors with Online Hard Example Mining,0.04901599884033203,#9467bd
-7.394877,0.18895315,neighbor,218889832,CBNet: A Novel Composite Backbone Network Architecture for Object Detection,0.049064457416534424,#9467bd
6.0342417,4.777375,neighbor,218889832,CenterNet: Keypoint Triplets for Object Detection,0.04908090829849243,#9467bd
-2.9805257,-3.3475416,neighbor,218889832,Single-Shot Object Detection with Enriched Semantics,0.04923456907272339,#9467bd
-8.428688,-1.3045576,neighbor,218889832,Cascade R-CNN: Delving Into High Quality Object Detection,0.049260497093200684,#9467bd
-4.9032884,-1.4542472,neighbor,218889832,"Scalable, High-Quality Object Detection",0.04936128854751587,#9467bd
-7.941399,2.9384706,neighbor,218889832,"Objects365: A Large-Scale, High-Quality Dataset for Object Detection",0.04946625232696533,#9467bd
-7.467743,-9.069773,neighbor,218889832,YOLOv4: Optimal Speed and Accuracy of Object Detection,0.0496709942817688,#9467bd
3.5223773,0.075541474,neighbor,218889832,DeNet: Scalable Real-Time Object Detection with Directed Sparse Sampling,0.049726903438568115,#9467bd
6.874584,3.765686,neighbor,218889832,Bottom-Up Object Detection by Grouping Extreme and Center Points,0.05010342597961426,#9467bd
7.4938354,7.29292,neighbor,218889832,Matrix Nets: A New Deep Architecture for Object Detection,0.05019116401672363,#9467bd
4.340477,3.566598,neighbor,218889832,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.05023068189620972,#9467bd
-15.831371,-0.24478948,neighbor,218889832,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.0503118634223938,#9467bd
-13.580459,3.6888545,neighbor,218889832,Mask R-CNN,0.050672054290771484,#9467bd
1.8288833,-6.156557,neighbor,218889832,Bag of Freebies for Training Object Detection Neural Networks,0.050746142864227295,#9467bd
1.2068168,-1.8679243,neighbor,218889832,Object detection at 200 Frames Per Second,0.05086606740951538,#9467bd
7.1981406,-4.2547927,neighbor,218889832,Object Detection as a Positive-Unlabeled Problem,0.05087435245513916,#9467bd
-2.2746599,-5.850401,neighbor,218889832,LapNet : Automatic Balanced Loss and Optimal Assignment for Real-Time Dense Object Detection,0.0508996844291687,#9467bd
1.2406656,3.697533,neighbor,218889832,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.05129730701446533,#9467bd
-9.134154,-5.0690284,neighbor,218889832,Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training,0.0513271689414978,#9467bd
-13.2875805,8.633682,neighbor,218889832,Triply Supervised Decoder Networks for Joint Detection and Segmentation,0.051445186138153076,#9467bd
-6.8302193,-1.5255842,neighbor,218889832,Enriched Feature Guided Refinement Network for Object Detection,0.05147832632064819,#9467bd
0.5896551,1.9062495,neighbor,218889832,Object Detection and Localization in Natural Scenes Through Single-Step and Two-Step Models,0.05171757936477661,#9467bd
-5.6519012,8.675965,neighbor,218889832,A Simple Semi-Supervised Learning Framework for Object Detection,0.051842331886291504,#9467bd
-13.6589365,3.7434678,neighbor,218889832,Mask R-CNN,0.051947832107543945,#9467bd
-5.221819,0.45851538,neighbor,218889832,EfficientDet: Scalable and Efficient Object Detection,0.05199277400970459,#9467bd
0.09788046,0.36704797,neighbor,218889832,R-FCN-3000 at 30fps: Decoupling Detection and Classification,0.05202764272689819,#9467bd
-6.0663457,3.6893768,neighbor,218889832,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.05222076177597046,#9467bd
-11.508648,-1.8411205,neighbor,218889832,Training Domain Specific Models for Energy-Efficient Object Detection,0.0522422194480896,#9467bd
7.378074,6.7901754,neighbor,218889832,MatrixNets: A New Scale and Aspect Ratio Aware Architecture for Object Detection,0.052259743213653564,#9467bd
-9.429752,8.76506,neighbor,218889832,Learning to Track Any Object,0.052355825901031494,#9467bd
7.67286,-2.5748322,neighbor,218889832,Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection,0.05246257781982422,#9467bd
-6.5842643,-2.9605324,neighbor,218889832,SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization,0.052466630935668945,#9467bd
3.1105769,-3.3976278,neighbor,218889832,Deep Learning for Generic Object Detection: A Survey,0.05247277021408081,#9467bd
7.946603,3.6864688,neighbor,218889832,RepPoints: Point Set Representation for Object Detection,0.052651822566986084,#9467bd
-9.972166,5.5965657,neighbor,218889832,TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection,0.05293148756027222,#9467bd
3.0797696,-8.384185,neighbor,218889832,"Empirical Upper Bound, Error Diagnosis and Invariance Analysis of Modern Object Detectors",0.05315214395523071,#9467bd
-3.3817215,-1.224683,neighbor,218889832,Detecting 11K Classes: Large Scale Object Detection Without Fine-Grained Bounding Boxes,0.05328410863876343,#9467bd
-5.3920984,0.8086718,neighbor,218889832,Feature Pyramid Networks for Object Detection,0.05336815118789673,#9467bd
-3.5467823,-5.1355553,neighbor,218889832,Consistent Optimization for Single-Shot Object Detection,0.05365550518035889,#9467bd
-0.84359545,8.809301,neighbor,218889832,Bounding Box Regression With Uncertainty for Accurate Object Detection,0.05396085977554321,#9467bd
12.089994,1.6310726,neighbor,218889832,Learning From Noisy Anchors for One-Stage Object Detection,0.05406820774078369,#9467bd
-3.0618806,2.2667499,neighbor,218889832,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.054227590560913086,#9467bd
-15.071201,-2.6443546,neighbor,218889832,Soft-NMS — Improving Object Detection with One Line of Code,0.054268717765808105,#9467bd
-1.4977859,3.6583114,neighbor,218889832,Deep Regionlets for Object Detection,0.054322123527526855,#9467bd
-11.559554,-3.9925988,neighbor,218889832,An Analysis of Pre-Training on Object Detection,0.05434685945510864,#9467bd
10.46097,3.6781464,neighbor,218889832,Dynamic Anchor Selection for Improving Object Localization,0.054554522037506104,#9467bd
-4.3800564,3.4056497,neighbor,218889832,Object Detection via End-to-End Integration of Aspect Ratio and Context Aware Part-based Models and Fully Convolutional Networks,0.054621756076812744,#9467bd
10.511852,2.411102,neighbor,218889832,Anchor Box Optimization for Object Detection,0.05466228723526001,#9467bd
-6.856183,7.1919436,neighbor,218889832,Weakly- and Semi-Supervised Object Detection with Expectation-Maximization Algorithm,0.05466306209564209,#9467bd
-4.1792245,2.1489303,neighbor,218889832,Feature Selective Networks for Object Detection,0.054746270179748535,#9467bd
-1.1573559,-7.5700436,neighbor,218889832,AP-Loss for Accurate One-Stage Object Detection,0.05487704277038574,#9467bd
-0.60658365,-3.4624062,neighbor,218889832,Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids,0.05501663684844971,#9467bd
-7.9782257,-3.6689804,neighbor,218889832,Rethinking Classification and Localization for Cascade R-CNN,0.055045485496520996,#9467bd
-6.289197,-5.1670985,neighbor,218889832,Revisiting Feature Alignment for One-stage Object Detection,0.05506300926208496,#9467bd
8.319254,-0.34075385,neighbor,218889832,Training Object Class Detectors with Click Supervision,0.05508154630661011,#9467bd
3.7537239,9.938548,neighbor,218889832,Zero Shot Detection,0.05511915683746338,#9467bd
0.6621743,7.1135745,neighbor,218889832,Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction,0.05522513389587402,#9467bd
-13.500829,6.3021607,neighbor,218889832,Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation,0.05523324012756348,#9467bd
-3.3549068,5.440017,neighbor,218889832,HAR-Net: Joint Learning of Hybrid Attention for Single-Stage Object Detection,0.055478572845458984,#9467bd
3.1078014,-8.370554,neighbor,218889832,Empirical Upper Bound in Object Detection and More,0.055489540100097656,#9467bd
7.0985837,4.703534,neighbor,218889832,Objects as Points,0.05549401044845581,#9467bd
6.029047,5.6172576,neighbor,218889832,CornerNet-Lite: Efficient Keypoint based Object Detection,0.05562227964401245,#9467bd
-1.50008,2.535476,neighbor,218889832,DeRPN: Taking a further step toward more general object detection,0.05563777685165405,#9467bd
11.40807,4.591833,neighbor,218889832,FoveaBox: Beyound Anchor-Based Object Detection,0.0558130145072937,#9467bd
2.4748397,-4.1982274,neighbor,218889832,Object Detection in 20 Years: A Survey,0.055877745151519775,#9467bd
-12.2080965,5.39581,neighbor,218889832,Overlap Sampler for Region-Based Object Detection,0.05598574876785278,#9467bd
3.7253046,11.160334,neighbor,218889832,One-Shot Object Detection without Fine-Tuning,0.05600684881210327,#9467bd
-6.761247,5.4548397,neighbor,218889832,Weakly Supervised Object Detection via Object-Specific Pixel Gradient,0.056023240089416504,#9467bd
-3.1527777,5.8668346,neighbor,218889832,Attention CoupleNet: Fully Convolutional Attention Coupling Network for Object Detection,0.05604630708694458,#9467bd
1.2136811,6.1763587,neighbor,218889832,BiDet: An Efficient Binarized Object Detector,0.056072354316711426,#9467bd
6.951931,-1.7541327,neighbor,218889832,LoANs: Weakly Supervised Object Detection with Localizer Assessor Networks,0.0561826229095459,#9467bd
-1.2115334,9.541288,neighbor,218889832,Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression,0.05619019269943237,#9467bd
-6.958182,-8.922006,neighbor,218889832,"YOLO9000: Better, Faster, Stronger",0.05631256103515625,#9467bd
9.92511,4.6327014,neighbor,218889832,MAOD: An Efficient Anchor-Free Object Detector Based on MobileDet,0.05640298128128052,#9467bd
-12.442758,7.271123,neighbor,218889832,DASNet: Reducing Pixel-level Annotations for Instance and Semantic Segmentation,0.056404829025268555,#9467bd
-15.780706,-1.6052147,neighbor,218889832,Learning Non-maximum Suppression,0.05649232864379883,#9467bd
-7.960201,-5.7287073,neighbor,218889832,Dynamic Balance Net: Correlation-enhanced Two-stage Object Detection Network with IoU-Loss,0.05651295185089111,#9467bd
-2.4143348,11.114625,neighbor,218889832,Learning to Separate: Detecting Heavily-Occluded Objects in Urban Scenes,0.056550025939941406,#9467bd
2.8097358,2.2439418,neighbor,218889832,Action-Driven Object Detection with Top-Down Visual Attentions,0.05659592151641846,#9467bd
4.262579,-5.977341,query,21889700,A Unified Approach to Interpreting Model Predictions,0.0,#9467bd
3.9840047,-6.609939,neighbor,21889700,An unexpected unity among methods for interpreting model predictions,0.03897547721862793,#9467bd
-0.14254446,-7.444794,neighbor,21889700,“Why Should I Trust You?”: Explaining the Predictions of Any Classifier,0.06561285257339478,#9467bd
1.639478,-9.22413,neighbor,21889700,Towards A Rigorous Science of Interpretable Machine Learning,0.07736837863922119,#9467bd
0.63398594,-4.259657,neighbor,21889700,Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable,0.07834458351135254,#9467bd
4.797622,-7.746549,neighbor,21889700,Explaining Classification Models Built on High-Dimensional Sparse Data,0.07876324653625488,#9467bd
9.621656,-4.5824265,neighbor,21889700,Evaluating computational models of explanation using human judgments,0.07913267612457275,#9467bd
14.513827,-2.9618042,neighbor,21889700,A Framework to Adjust Dependency Measure Estimates for Chance,0.07934170961380005,#9467bd
10.017421,9.496943,neighbor,21889700,Learning Important Features Through Propagating Activation Differences,0.08100926876068115,#9467bd
-0.12698391,-9.140179,neighbor,21889700,"European Union Regulations on Algorithmic Decision-Making and a ""Right to Explanation""",0.08213502168655396,#9467bd
-8.699093,-1.7222229,neighbor,21889700,Efficient Decomposed Learning for Structured Prediction,0.08237743377685547,#9467bd
11.326292,-7.7786484,neighbor,21889700,"Most Relevant Explanation: Properties, Algorithms, and Evaluations",0.08279889822006226,#9467bd
14.604835,-6.836343,neighbor,21889700,Discovering Structure in High-Dimensional Data Through Correlation Explanation,0.08319765329360962,#9467bd
-4.0933805,0.65617293,neighbor,21889700,MOB-ESP and other Improvements in Probability Estimation,0.08323222398757935,#9467bd
9.579083,8.789325,neighbor,21889700,Interpretation of Prediction Models Using the Input Gradient,0.08376359939575195,#9467bd
2.7056928,-9.273561,neighbor,21889700,The mythos of model interpretability,0.08414876461029053,#9467bd
-1.196377,-7.292658,neighbor,21889700,How to Explain Individual Classification Decisions,0.08539658784866333,#9467bd
-5.8154125,-4.1995964,neighbor,21889700,Reinterpreting Importance-Weighted Autoencoders,0.08565336465835571,#9467bd
-5.1123266,9.936804,neighbor,21889700,The Feature Importance Ranking Measure,0.08610832691192627,#9467bd
0.93444824,-10.739488,neighbor,21889700,100% Classification Accuracy Considered Harmful: The Normalized Information Transfer Factor Explains the Accuracy Paradox,0.08615988492965698,#9467bd
-7.197877,-10.365814,neighbor,21889700,Ranking Sentiment Explanations for Review Summarization Using Dual Decomposition,0.08631139993667603,#9467bd
-1.2765433,-12.451024,neighbor,21889700,Methods and Models for Interpretable Linear Classification,0.08660447597503662,#9467bd
12.942732,0.85012907,neighbor,21889700,Causal Inference on Multivariate and Mixed-Type Data,0.08672034740447998,#9467bd
-5.0431285,1.9038823,neighbor,21889700,Predicting accurate probabilities with a ranking loss,0.08687996864318848,#9467bd
-1.1278902,-10.17022,neighbor,21889700,"Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation",0.08704864978790283,#9467bd
5.7939053,-10.539985,neighbor,21889700,TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning,0.0874817967414856,#9467bd
10.301825,0.58273447,neighbor,21889700,Origo: causal inference by compression,0.08772754669189453,#9467bd
11.814006,-7.601251,neighbor,21889700,Most Relevant Explanation in Bayesian Networks,0.0882793664932251,#9467bd
-1.3000482,2.872189,neighbor,21889700,Large-scale probabilistic predictors with and without guarantees of validity,0.08838021755218506,#9467bd
-6.73498,-5.895482,neighbor,21889700,Unified Framework for Quantification,0.08844268321990967,#9467bd
-6.4887934,8.762598,neighbor,21889700,Prediction Error Reduction Function as a Variable Importance Score,0.08847439289093018,#9467bd
2.0530343,-13.628127,neighbor,21889700,Machine Learning Model Interpretability for Precision Medicine,0.08898615837097168,#9467bd
-0.55536824,5.122126,neighbor,21889700,"Statistical Model Building, Machine Learning, and the Ah-Ha Moment",0.08916884660720825,#9467bd
-2.7519293,5.6189322,neighbor,21889700,Combining predictions from linear models when training and test inputs differ,0.08926308155059814,#9467bd
-9.882457,8.688799,neighbor,21889700,Interpreting tree ensembles with inTrees,0.08962041139602661,#9467bd
11.206715,10.233638,neighbor,21889700,Gradients of Counterfactuals,0.08968627452850342,#9467bd
-0.303188,8.205658,neighbor,21889700,Learning Optimized Or's of And's,0.09027671813964844,#9467bd
-6.57039,2.3560028,neighbor,21889700,Ranking and combining multiple predictors without labeled data,0.09028822183609009,#9467bd
13.992335,0.980681,neighbor,21889700,Inferring deterministic causal relations,0.09030342102050781,#9467bd
-4.6261177,10.413904,neighbor,21889700,Feature Importance Measure for Non-linear Learning Algorithms,0.0904238224029541,#9467bd
3.1446073,-10.431629,neighbor,21889700,Meaningful Models: Utilizing Conceptual Structure to Improve Machine Learning Interpretability,0.09045255184173584,#9467bd
15.540859,-1.6435316,neighbor,21889700,Learning from Pairwise Marginal Independencies,0.09053915739059448,#9467bd
5.340558,-0.005208682,neighbor,21889700,"Algorithmic statistics, prediction and machine learning",0.09069746732711792,#9467bd
-12.184596,-0.7060326,neighbor,21889700,Probabilistic Formulations of Regression with Mixed Guidance,0.09078198671340942,#9467bd
-4.839956,3.0263917,neighbor,21889700,Discovering Characterization Rules from Rankings,0.09079080820083618,#9467bd
8.50326,-6.918374,neighbor,21889700,Balancing Conflicting Factors in Argument Interpretation,0.09079587459564209,#9467bd
8.907046,-3.0080543,neighbor,21889700,Formalizing Neurath’s Ship: Approximate Algorithms for Online Causal Learning,0.09090381860733032,#9467bd
-0.4101715,1.9319347,neighbor,21889700,"Edward: A library for probabilistic modeling, inference, and criticism",0.09107542037963867,#9467bd
6.1374025,-5.025419,neighbor,21889700,On Shapley Value for Measuring Importance of Dependent Inputs,0.09110862016677856,#9467bd
13.754383,-7.6301436,neighbor,21889700,Toward Interpretable Topic Discovery via Anchored Correlation Explanation,0.09119319915771484,#9467bd
13.872071,-0.16437529,neighbor,21889700,From Dependence to Causation,0.09127461910247803,#9467bd
0.9627014,-6.656987,neighbor,21889700,Programs as Black-Box Explanations,0.09147006273269653,#9467bd
11.215447,8.737306,neighbor,21889700,Axiomatic Attribution for Deep Networks,0.0916326642036438,#9467bd
-4.02113,6.3490624,neighbor,21889700,Cross-conformal predictors,0.09171491861343384,#9467bd
-0.15900193,3.8031619,neighbor,21889700,Black-box $\alpha$-divergence Minimization,0.09187185764312744,#9467bd
6.9888735,-10.592755,neighbor,21889700,Learning how to explain neural networks: PatternNet and PatternAttribution,0.0919293761253357,#9467bd
5.5846825,6.2491994,neighbor,21889700,Extracting Credible Dependencies for Averaged One-Dependence Estimator Analysis,0.09201222658157349,#9467bd
0.6383137,-3.1510782,neighbor,21889700,Learning to Learn: Algorithmic Inspirations from Human Problem Solving,0.0920448899269104,#9467bd
-8.109324,-2.8945177,neighbor,21889700,Piecewise training for structured prediction,0.09212517738342285,#9467bd
2.7148106,9.336859,neighbor,21889700,Formula-Based Probabilistic Inference,0.09213054180145264,#9467bd
-10.284996,2.903985,neighbor,21889700,The optimal crowd learning machine,0.09220629930496216,#9467bd
-2.009545,-1.0873387,neighbor,21889700,Using cognitive models to combine probability estimates,0.0923810601234436,#9467bd
10.101809,-0.9933316,neighbor,21889700,Inferring causal structure: a quantum advantage,0.09238630533218384,#9467bd
-10.099596,1.5243425,neighbor,21889700,The offset tree for learning with partial labels,0.09268301725387573,#9467bd
-8.305551,2.6884863,neighbor,21889700,Scalable Semi-Supervised Classifier Aggregation,0.0926862359046936,#9467bd
4.627659,11.153009,neighbor,21889700,Visualizing and understanding Sum-Product Networks,0.09288269281387329,#9467bd
4.228558,10.770226,neighbor,21889700,Learning Relational Sum-Product Networks,0.09288299083709717,#9467bd
4.57458,-3.2157423,neighbor,21889700,Improved Measures of Integrated Information,0.09301143884658813,#9467bd
5.383592,0.6377426,neighbor,21889700,"Information, learning and falsification",0.0930601954460144,#9467bd
0.36283097,5.6443424,neighbor,21889700,Predictive Hypothesis Identification,0.09326958656311035,#9467bd
-6.892747,6.7604065,neighbor,21889700,Feature-Weighted Linear Stacking,0.09327924251556396,#9467bd
1.0331491,-13.79139,neighbor,21889700,Automatically explaining machine learning prediction results: a demonstration on type 2 diabetes risk prediction,0.09333282709121704,#9467bd
-10.248938,0.4929558,neighbor,21889700,Interactive Learning from Multiple Noisy Labels,0.09355217218399048,#9467bd
-13.247779,0.5258445,neighbor,21889700,Max-Ordinal Learning,0.09359604120254517,#9467bd
-7.169241,-9.193151,neighbor,21889700,Automatic Aggregation by Joint Modeling of Aspects and Values,0.09360754489898682,#9467bd
13.138214,2.3890018,neighbor,21889700,Entropic Causal Inference,0.09371858835220337,#9467bd
-11.643846,0.6313936,neighbor,21889700,Quantity Makes Quality: Learning with Partial Views,0.0937957763671875,#9467bd
2.0152009,-2.4435728,neighbor,21889700,Sampling Assumptions in Inductive Generalization,0.09388506412506104,#9467bd
11.95184,-0.29413903,neighbor,21889700,Causal Discovery Using Proxy Variables,0.0939372181892395,#9467bd
16.231876,0.4732422,neighbor,21889700,A Causal Framework for Discovering and Removing Direct and Indirect Discrimination,0.09400022029876709,#9467bd
-9.046307,-2.6422534,neighbor,21889700,Structure Regularization for Structured Prediction,0.09408283233642578,#9467bd
16.812048,-2.9008253,neighbor,21889700,Learning Module Networks,0.09411561489105225,#9467bd
1.6917335,6.4822736,neighbor,21889700,Inferential Models for Linear Regression,0.09425991773605347,#9467bd
1.0488774,4.2225413,neighbor,21889700,An objective prior that unifies objective Bayes and information-based inference,0.09426403045654297,#9467bd
-3.0985465,-0.7574769,neighbor,21889700,Aggregating Learned Probabilistic Beliefs,0.09430217742919922,#9467bd
8.628483,0.8455785,neighbor,21889700,"Zipf’s Law Arises Naturally When There Are Underlying, Unobserved Variables",0.0944364070892334,#9467bd
-7.5716634,7.5766153,neighbor,21889700,Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions,0.09445178508758545,#9467bd
2.193705,2.1681273,neighbor,21889700,Venn-Abers Predictors,0.09461402893066406,#9467bd
9.354004,3.0896566,neighbor,21889700,On directed information and gambling,0.09472274780273438,#9467bd
11.718852,1.5627756,neighbor,21889700,Causal Inference by Stochastic Complexity,0.09480077028274536,#9467bd
-0.22904092,0.7266263,neighbor,21889700,Reweighted Data for Robust Probabilistic Models,0.09488677978515625,#9467bd
4.858636,2.9960573,neighbor,21889700,Exchangeable Variable Models,0.0950518250465393,#9467bd
14.759208,2.16389,neighbor,21889700,Distinguishing Cause from Effect Using Observational Data: Methods and Benchmarks,0.095059335231781,#9467bd
-9.165721,8.918606,neighbor,21889700,Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach,0.09508633613586426,#9467bd
-6.713749,-7.893698,neighbor,21889700,Modeling Annotators: A Generative Approach to Learning from Annotator Rationales,0.09513610601425171,#9467bd
10.45723,-4.1919413,neighbor,21889700,A Quantum Probability Model of Causal Reasoning,0.09519082307815552,#9467bd
-7.8491173,-0.9198353,neighbor,21889700,Normalized Hierarchical SVM,0.09530764818191528,#9467bd
-1.6294824,-5.2695537,neighbor,21889700,Predictions as Statements and Decisions,0.09538203477859497,#9467bd
-8.131049,5.3754635,neighbor,21889700,Across-Model Collective Ensemble Classification,0.09541469812393188,#9467bd
13.352765,-2.1268375,neighbor,21889700,Seeking relationships in big data: a Bayesian perspective,0.09566354751586914,#9467bd
5.0103474,6.443248,neighbor,21889700,Semi-naive Exploitation of One-Dependence Estimators,0.09567761421203613,#9467bd
-7.970685,-3.6125035,query,218971783,Language Models are Few-Shot Learners,0.0,#c5b0d5
-3.292976,-1.5479953,neighbor,218971783,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,0.047594428062438965,#c5b0d5
-8.603627,7.3579135,neighbor,218971783,Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks,0.048017263412475586,#c5b0d5
-10.877374,-6.5902414,neighbor,218971783,Few-Shot NLG with Pre-Trained Language Model,0.04954248666763306,#c5b0d5
-2.575344,11.53522,neighbor,218971783,Towards one-shot learning for rare-word translation with external experts,0.05111682415008545,#c5b0d5
-0.38625702,3.4507432,neighbor,218971783,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,0.05271720886230469,#c5b0d5
-1.4370725,-1.1419169,neighbor,218971783,Fine-tuned Language Models for Text Classification,0.05375713109970093,#c5b0d5
-12.382864,0.66076225,neighbor,218971783,REALM: Retrieval-Augmented Language Model Pre-Training,0.053971946239471436,#c5b0d5
-5.9768577,1.4804592,neighbor,218971783,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,0.0540623664855957,#c5b0d5
-9.747871,-3.498449,neighbor,218971783,Zero-shot Text Classification With Generative Language Models,0.05444997549057007,#c5b0d5
3.900078,5.044338,neighbor,218971783,LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning,0.05467724800109863,#c5b0d5
-12.530044,4.271464,neighbor,218971783,Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference,0.054755330085754395,#c5b0d5
6.0761776,1.9409745,neighbor,218971783,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,0.054894089698791504,#c5b0d5
4.302166,4.540332,neighbor,218971783,Generation-Distillation for Efficient Natural Language Understanding in Low-Data Settings,0.05497938394546509,#c5b0d5
-13.838211,-6.432614,neighbor,218971783,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,0.05524474382400513,#c5b0d5
-8.5300455,-0.88371575,neighbor,218971783,Evaluating Machines by their Real-World Language Use,0.05556321144104004,#c5b0d5
-8.285083,0.11153147,neighbor,218971783,Learning and Evaluating General Linguistic Intelligence,0.05649799108505249,#c5b0d5
4.1107836,2.879651,neighbor,218971783,Reweighted Proximal Pruning for Large-Scale Language Representation,0.05657505989074707,#c5b0d5
2.5632217,-8.0794,neighbor,218971783,Generalization through Memorization: Nearest Neighbor Language Models,0.057299137115478516,#c5b0d5
-1.3572747,-0.6954835,neighbor,218971783,Universal Language Model Fine-tuning for Text Classification,0.05730646848678589,#c5b0d5
-7.5446196,-5.2206316,neighbor,218971783,Fine-Tuning a Transformer-Based Language Model to Avoid Generating Non-Normative Text,0.05768316984176636,#c5b0d5
-6.128487,3.7780342,neighbor,218971783,Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks,0.058001577854156494,#c5b0d5
-2.5874214,6.8053026,neighbor,218971783,Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks,0.0580064058303833,#c5b0d5
0.6963844,5.9987283,neighbor,218971783,Selecting Informative Contexts Improves Language Model Fine-tuning,0.058023035526275635,#c5b0d5
5.901289,-3.5635498,neighbor,218971783,General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference,0.058118581771850586,#c5b0d5
2.0189369,-5.4511857,neighbor,218971783,Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer,0.058275043964385986,#c5b0d5
-6.3887806,7.9781575,neighbor,218971783,Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining,0.0588110089302063,#c5b0d5
5.8811707,-5.599813,neighbor,218971783,Extrapolation in NLP,0.058917105197906494,#c5b0d5
8.2518425,6.6529274,neighbor,218971783,Structured Pruning of Large Language Models,0.059298813343048096,#c5b0d5
6.350278,6.522739,neighbor,218971783,AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search,0.059410810470581055,#c5b0d5
-4.6105604,1.9159162,neighbor,218971783,Bootstrapping NLU Models with Multi-task Learning,0.05952572822570801,#c5b0d5
3.217847,1.0229228,neighbor,218971783,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,0.05957895517349243,#c5b0d5
-5.3355947,0.38983724,neighbor,218971783,Adversarial NLI: A New Benchmark for Natural Language Understanding,0.05973660945892334,#c5b0d5
6.375701,-3.8907588,neighbor,218971783,On the Effective Use of Pretraining for Natural Language Inference,0.060927391052246094,#c5b0d5
-6.2380266,7.022786,neighbor,218971783,When does MAML Work the Best? An Empirical Study on Model-Agnostic Meta-Learning in NLP Applications,0.06109124422073364,#c5b0d5
-6.0363493,2.430996,neighbor,218971783,jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models,0.06126934289932251,#c5b0d5
-12.609412,1.5279979,neighbor,218971783,Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model,0.06133180856704712,#c5b0d5
1.0267509,5.0063667,neighbor,218971783,Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,0.0614168643951416,#c5b0d5
-5.4003105,-8.977238,neighbor,218971783,Evaluating Commonsense in Pre-trained Language Models,0.06157028675079346,#c5b0d5
10.838469,-1.206658,neighbor,218971783,Exploring the Limits of Language Modeling,0.061716437339782715,#c5b0d5
6.8461704,7.256572,neighbor,218971783,LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression,0.06191438436508179,#c5b0d5
3.2602944,-4.398906,neighbor,218971783,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,0.06195509433746338,#c5b0d5
10.573972,5.4534984,neighbor,218971783,Lite Transformer with Long-Short Range Attention,0.062088608741760254,#c5b0d5
8.710031,3.9394617,neighbor,218971783,Reducing Transformer Depth on Demand with Structured Dropout,0.06215250492095947,#c5b0d5
2.5604548,5.8792143,neighbor,218971783,Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation,0.06215941905975342,#c5b0d5
-0.64724314,0.7903039,neighbor,218971783,Parameter-Efficient Transfer Learning for NLP,0.062159836292266846,#c5b0d5
5.1631837,3.2572083,neighbor,218971783,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",0.06219393014907837,#c5b0d5
0.6396169,-3.032996,neighbor,218971783,Reverse Transfer Learning: Can Word Embeddings Trained for Different NLP Tasks Improve Neural Language Models?,0.062222063541412354,#c5b0d5
-8.777826,3.6441624,neighbor,218971783,Improving BERT With Self-Supervised Attention,0.062379658222198486,#c5b0d5
-0.33140865,-4.896793,neighbor,218971783,USEing Transfer Learning in Retrieval of Statistical Data,0.06241482496261597,#c5b0d5
-9.4826145,-9.721014,neighbor,218971783,Paraphrasing with Large Language Models,0.06251668930053711,#c5b0d5
-11.660656,-2.973359,neighbor,218971783,Explaining Question Answering Models through Text Generation,0.06275254487991333,#c5b0d5
-13.5210495,-8.107035,neighbor,218971783,POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training,0.06277763843536377,#c5b0d5
7.9305773,-4.3040257,neighbor,218971783,Learning to Compute Word Embeddings On the Fly,0.06279224157333374,#c5b0d5
-10.2202835,7.449492,neighbor,218971783,Learning to Control Latent Representations for Few-Shot Learning of Named Entities,0.06285208463668823,#c5b0d5
-8.875016,9.870746,neighbor,218971783,Fast Cross-domain Data Augmentation through Neural Sentence Editing,0.06300777196884155,#c5b0d5
11.549506,-1.7251927,neighbor,218971783,Towards Zero-shot Language Modeling,0.06314092874526978,#c5b0d5
-2.3217697,-4.8260508,neighbor,218971783,Efficient Transfer Learning for Neural Network Language Models,0.0631413459777832,#c5b0d5
0.31792042,4.775016,neighbor,218971783,Visualizing and Understanding the Effectiveness of BERT,0.06342071294784546,#c5b0d5
3.6238425,-1.7971472,neighbor,218971783,From English To Foreign Languages: Transferring Pre-trained Language Models,0.06345432996749878,#c5b0d5
7.664854,-0.8518348,neighbor,218971783,"Baseline: A Library for Rapid Modeling, Experimentation and Development of Deep Learning Algorithms targeting NLP",0.0635603666305542,#c5b0d5
-15.094622,-2.9267936,neighbor,218971783,Semantically Equivalent Adversarial Rules for Debugging NLP models,0.06362855434417725,#c5b0d5
-5.0999646,-8.713524,neighbor,218971783,"Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",0.06365472078323364,#c5b0d5
-11.76015,-1.5595759,neighbor,218971783,TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection,0.06371963024139404,#c5b0d5
-2.644537,-2.5111172,neighbor,218971783,Exploring and Predicting Transferability across NLP Tasks,0.06373143196105957,#c5b0d5
0.90058094,11.628587,neighbor,218971783,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,0.06380409002304077,#c5b0d5
-2.6259584,9.1805725,neighbor,218971783,Unsupervised Domain Clusters in Pretrained Language Models,0.06394517421722412,#c5b0d5
2.062983,2.5893762,neighbor,218971783,Incorporating BERT into Neural Machine Translation,0.06399351358413696,#c5b0d5
4.9335604,-0.42633623,neighbor,218971783,What the [MASK]? Making Sense of Language-Specific BERT Models,0.06404399871826172,#c5b0d5
-3.3167012,3.3577642,neighbor,218971783,What do you learn from context? Probing for sentence structure in contextualized word representations,0.06409478187561035,#c5b0d5
7.0504994,-6.438991,neighbor,218971783,Adaptively Sparse Transformers,0.06410408020019531,#c5b0d5
-2.0708525,0.31560922,neighbor,218971783,FineText: Text Classification via Attention-based Language Model Fine-tuning,0.06411445140838623,#c5b0d5
8.801642,-0.96140933,neighbor,218971783,Rethinking Complex Neural Network Architectures for Document Classification,0.06413644552230835,#c5b0d5
-3.6770556,11.130573,neighbor,218971783,Meta-Learning for Few-Shot NMT Adaptation,0.06414705514907837,#c5b0d5
7.5285583,3.3108015,neighbor,218971783,WaLDORf: Wasteless Language-model Distillation On Reading-comprehension,0.06415355205535889,#c5b0d5
6.28033,3.019241,neighbor,218971783,TinyBERT: Distilling BERT for Natural Language Understanding,0.06430739164352417,#c5b0d5
-9.433523,-7.02632,neighbor,218971783,Do Massively Pretrained Language Models Make Better Storytellers?,0.06453698873519897,#c5b0d5
-11.172569,-9.025035,neighbor,218971783,Leveraging Pre-trained Checkpoints for Sequence Generation Tasks,0.06463217735290527,#c5b0d5
2.9510674,-1.8081276,neighbor,218971783,"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT",0.06465959548950195,#c5b0d5
1.1611028,10.288735,neighbor,218971783,Masked Language Model Scoring,0.06485599279403687,#c5b0d5
-7.218829,0.919924,neighbor,218971783,Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?,0.06486225128173828,#c5b0d5
-5.3754935,-2.0071275,neighbor,218971783,Learning and Knowledge Transfer with Memory Networks for Machine Comprehension,0.06493568420410156,#c5b0d5
-0.8138526,-2.8029127,neighbor,218971783,Transfer Learning in Natural Language Processing,0.0650404691696167,#c5b0d5
10.38222,6.333357,neighbor,218971783,Ouroboros: On Accelerating Training of Transformer-Based Language Models,0.06505018472671509,#c5b0d5
-1.2391671,-3.356172,neighbor,218971783,Evolution of transfer learning in natural language processing,0.06514555215835571,#c5b0d5
5.889792,-0.6767934,neighbor,218971783,UER: An Open-Source Toolkit for Pre-training Models,0.06517118215560913,#c5b0d5
0.3850733,12.203354,neighbor,218971783,Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation,0.06524932384490967,#c5b0d5
7.8598695,1.1974165,neighbor,218971783,Transformer to CNN: Label-scarce distillation for efficient text classification,0.06527954339981079,#c5b0d5
-11.068976,-8.325221,neighbor,218971783,Unified Language Model Pre-training for Natural Language Understanding and Generation,0.06528174877166748,#c5b0d5
-10.028778,2.9402962,neighbor,218971783,Blockwise Self-Attention for Long Document Understanding,0.0652884840965271,#c5b0d5
-7.7635636,6.49007,neighbor,218971783,When Low Resource NLP Meets Unsupervised Language Model: Meta-Pretraining then Meta-Learning for Few-Shot Text Classification (Student Abstract),0.0653156042098999,#c5b0d5
2.196264,12.518524,neighbor,218971783,Controlling Text Complexity in Neural Machine Translation,0.06531929969787598,#c5b0d5
-2.7504878,10.022854,neighbor,218971783,Addressing Zero-Resource Domains Using Document-Level Context in Neural Machine Translation,0.06534886360168457,#c5b0d5
-11.338198,-7.393015,neighbor,218971783,Cross-Lingual Natural Language Generation via Pre-Training,0.06545966863632202,#c5b0d5
1.6115226,-5.104727,neighbor,218971783,SimpleTran: Transferring Pre-Trained Sentence Embeddings for Low Resource Text Classification,0.06557875871658325,#c5b0d5
-12.713359,-9.672826,neighbor,218971783,XL-Editor: Post-editing Sentences with XLNet,0.06560134887695312,#c5b0d5
-15.322219,-3.053487,neighbor,218971783,Universal Adversarial Triggers for Attacking and Analyzing NLP,0.06561034917831421,#c5b0d5
-10.630439,-4.3167534,neighbor,218971783,Neural Response Generation with Dynamic Vocabularies,0.06577682495117188,#c5b0d5
-14.661144,1.6306252,neighbor,218971783,K-BERT: Enabling Language Representation with Knowledge Graph,0.06584286689758301,#c5b0d5
-11.946621,3.0404615,neighbor,218971783,How Context Affects Language Models' Factual Predictions,0.06587761640548706,#c5b0d5
2.7116175,0.51007044,neighbor,218971783,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,0.06591367721557617,#c5b0d5
-0.64564633,-6.3251414,query,219955663,Denoising Diffusion Probabilistic Models,0.0,#c5b0d5
8.726498,-4.4283032,neighbor,219955663,Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling,0.0554729700088501,#c5b0d5
9.851342,-3.9242387,neighbor,219955663,Implicit Generation and Generalization in Energy-Based Models,0.060195982456207275,#c5b0d5
2.1706667,-9.357582,neighbor,219955663,Generative Imaging and Image Processing via Generative Encoder,0.060205817222595215,#c5b0d5
10.453649,-2.997129,neighbor,219955663,Generative Modeling by Estimating Gradients of the Data Distribution,0.06105053424835205,#c5b0d5
-8.367831,1.528437,neighbor,219955663,A Bayesian Perspective on the Deep Image Prior,0.06140667200088501,#c5b0d5
-1.198324,0.8890489,neighbor,219955663,Variational Inference using Implicit Distributions,0.061874449253082275,#c5b0d5
-3.034692,-3.210228,neighbor,219955663,Preventing Posterior Collapse with delta-VAEs,0.0626821517944336,#c5b0d5
1.3925474,0.20693147,neighbor,219955663,Learning Generative ConvNet with Continuous Latent Factors by Alternating Back-Propagation,0.0628519058227539,#c5b0d5
-6.445545,6.5843496,neighbor,219955663,Radial and Directional Posteriors for Bayesian Deep Learning,0.06356912851333618,#c5b0d5
-0.67642355,-1.2452302,neighbor,219955663,Improved Variational Inference with Inverse Autoregressive Flow,0.06358844041824341,#c5b0d5
-2.5308123,-1.3052716,neighbor,219955663,PixelVAE++: Improved PixelVAE with Discrete Prior,0.0636981725692749,#c5b0d5
7.0085006,4.976159,neighbor,219955663,Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,0.06378293037414551,#c5b0d5
-2.7380416,-0.46721503,neighbor,219955663,Variational Lossy Autoencoder,0.06410026550292969,#c5b0d5
3.8353598,0.4606932,neighbor,219955663,Probabilistic Auto-Encoder,0.06430810689926147,#c5b0d5
7.893664,-3.9315374,neighbor,219955663,LOGAN: Latent Optimisation for Generative Adversarial Networks,0.06462937593460083,#c5b0d5
7.497329,2.2917798,neighbor,219955663,FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models,0.0646396279335022,#c5b0d5
-7.6298194,-7.489465,neighbor,219955663,Variational image compression with a scale hyperprior,0.06489509344100952,#c5b0d5
-3.583131,2.958614,neighbor,219955663,The continuous Bernoulli: fixing a pervasive error in variational autoencoders,0.0657879114151001,#c5b0d5
-0.98194337,-7.994453,neighbor,219955663,Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration,0.06585335731506348,#c5b0d5
-6.5860496,-8.421943,neighbor,219955663,Deep Generative Models for Distribution-Preserving Lossy Compression,0.06595510244369507,#c5b0d5
-8.407766,8.448451,neighbor,219955663,Learnable Bernoulli Dropout for Bayesian Deep Learning,0.06599324941635132,#c5b0d5
-3.598309,4.7078505,neighbor,219955663,Reconsidering Analytical Variational Bounds for Output Layers of Deep Networks,0.06631439924240112,#c5b0d5
-6.620148,6.4005527,neighbor,219955663,Radial and Directional Posteriors for Bayesian Neural Networks,0.0663948655128479,#c5b0d5
3.0927787,2.3978198,neighbor,219955663,GSNs : Generative Stochastic Networks,0.06657141447067261,#c5b0d5
-5.0909266,6.161925,neighbor,219955663,Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes,0.06673425436019897,#c5b0d5
1.717737,2.9747167,neighbor,219955663,From Variational to Deterministic Autoencoders,0.06679040193557739,#c5b0d5
-5.5583596,1.9486731,neighbor,219955663,\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\pi $$\end{document}πVAE: a stochastic process prior for Bayesian deep,0.06685006618499756,#c5b0d5
-4.810624,-2.8359873,neighbor,219955663,Noise Contrastive Variational Autoencoders,0.06690436601638794,#c5b0d5
3.1447906,-6.3656697,neighbor,219955663,Wavelets to the Rescue: Improving Sample Quality of Latent Variable Deep Generative Models,0.06705641746520996,#c5b0d5
-4.701433,1.5918506,neighbor,219955663,Tutorial on Variational Autoencoders,0.06712234020233154,#c5b0d5
9.269493,-8.569974,neighbor,219955663,Autoregressive Quantile Networks for Generative Modeling,0.06722038984298706,#c5b0d5
3.464447,-10.343996,neighbor,219955663,Theory of Generative Deep Learning II:Probe Landscape of Empirical Error via Norm Based Capacity Control,0.06728112697601318,#c5b0d5
-1.1954188,9.8479395,neighbor,219955663,"Simple, Distributed, and Accelerated Probabilistic Programming",0.06728255748748779,#c5b0d5
-7.2610393,2.3005748,neighbor,219955663,Deep Prior,0.06805616617202759,#c5b0d5
-9.935919,-8.462681,neighbor,219955663,Practical Full Resolution Learned Lossless Image Compression,0.06822085380554199,#c5b0d5
-3.1124613,-10.371879,neighbor,219955663,Denoising Deep Boltzmann Machines: Compression for Deep Learning,0.06837344169616699,#c5b0d5
4.588001,-6.411157,neighbor,219955663,Generating Diverse High-Fidelity Images with VQ-VAE-2,0.06837505102157593,#c5b0d5
-8.516959,-7.3908796,neighbor,219955663,Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding,0.06860107183456421,#c5b0d5
-8.697548,1.1577487,neighbor,219955663,The Spectral Bias of the Deep Image Prior,0.06872034072875977,#c5b0d5
-9.381212,-9.083352,neighbor,219955663,HiLLoC: Lossless Image Compression with Hierarchical Latent Variable Models,0.06873512268066406,#c5b0d5
-1.1898099,9.562401,neighbor,219955663,Deep Probabilistic Programming,0.06880736351013184,#c5b0d5
-7.8300815,-8.513771,neighbor,219955663,Improving Inference for Neural Image Compression,0.06909698247909546,#c5b0d5
-5.563428,7.8554206,neighbor,219955663,Practical Deep Learning with Bayesian Principles,0.0692092776298523,#c5b0d5
2.3816125,3.4935627,neighbor,219955663,Batch norm with entropic regularization turns deterministic autoencoders into generative models,0.06926172971725464,#c5b0d5
0.9895168,-10.964178,neighbor,219955663,Correction by Projection: Denoising Images with Generative Adversarial Networks,0.06938987970352173,#c5b0d5
11.06571,-0.568009,neighbor,219955663,Learning Generative ConvNets via Multi-grid Modeling and Sampling,0.06956183910369873,#c5b0d5
6.7259164,-6.979093,neighbor,219955663,Analyzing and Improving the Image Quality of StyleGAN,0.06958115100860596,#c5b0d5
0.57794464,-9.328954,neighbor,219955663,SUNLayer: Stable denoising with generative networks,0.06975799798965454,#c5b0d5
-5.452304,0.32864848,neighbor,219955663,A variational autoencoder for probabilistic non-negative matrix factorisation,0.0697634220123291,#c5b0d5
4.4559226,-5.1566405,neighbor,219955663,Diverse Conditional Image Generation by Stochastic Regression with Latent Drop-Out Codes,0.06990188360214233,#c5b0d5
5.7124043,-6.368127,neighbor,219955663,Pioneer Networks: Progressively Growing Generative Autoencoder,0.06990247964859009,#c5b0d5
3.7107303,7.7345276,neighbor,219955663,A Probabilistic Framework for Deep Learning,0.0699511170387268,#c5b0d5
2.200757,-0.17960636,neighbor,219955663,Perceptual Generative Autoencoders,0.07002806663513184,#c5b0d5
4.8922205,-0.23169087,neighbor,219955663,Adaptive Density Estimation for Generative Models,0.07005780935287476,#c5b0d5
8.455685,0.2168592,neighbor,219955663,Generalized Energy Based Models,0.07005804777145386,#c5b0d5
10.641185,-0.06054822,neighbor,219955663,Cooperative Training of Descriptor and Generator Networks,0.07016205787658691,#c5b0d5
5.541151,-8.12029,neighbor,219955663,Low Distortion Block-Resampling with Spatially Stochastic Networks,0.07025259733200073,#c5b0d5
-2.3379383,0.1321181,neighbor,219955663,MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders,0.07026195526123047,#c5b0d5
-9.433282,9.848512,neighbor,219955663,Conditional Restricted Boltzmann Machines for Structured Output Prediction,0.07028138637542725,#c5b0d5
-3.1591487,2.197159,neighbor,219955663,Balancing Reconstruction Error and Kullback-Leibler Divergence in Variational Autoencoders,0.07047820091247559,#c5b0d5
-1.1036536,-10.504902,neighbor,219955663,Deep Decoder: Concise Image Representations from Untrained Non-convolutional Networks,0.07048976421356201,#c5b0d5
6.2647886,6.7555833,neighbor,219955663,InfoCNF: An Efficient Conditional Continuous Normalizing Flow with Adaptive Solvers,0.07052218914031982,#c5b0d5
6.9303727,-10.363486,neighbor,219955663,Optimal Transport Structure of CycleGAN for Unsupervised Learning for Inverse Problems,0.07052916288375854,#c5b0d5
4.9048057,-1.8242161,neighbor,219955663,Bayesian GAN,0.07060521841049194,#c5b0d5
-8.590533,-8.287316,neighbor,219955663,Hierarchical Quantized Autoencoders,0.07063812017440796,#c5b0d5
-0.18739524,-10.39352,neighbor,219955663,Rate-optimal denoising with deep neural networks,0.07079821825027466,#c5b0d5
5.5792637,-2.5400548,neighbor,219955663,Prescribed Generative Adversarial Networks,0.07085186243057251,#c5b0d5
-7.4926734,4.858525,neighbor,219955663,Variational Probability Flow for Biologically Plausible Training of Deep Neural Networks,0.07093596458435059,#c5b0d5
-10.232948,6.19854,neighbor,219955663,Improving Discrete Latent Representations With Differentiable Approximation Bridges,0.0710301399230957,#c5b0d5
6.374193,-6.238064,neighbor,219955663,Large Scale GAN Training for High Fidelity Natural Image Synthesis,0.07110899686813354,#c5b0d5
4.419925,7.40745,neighbor,219955663,A Bayesian Perspective of Convolutional Neural Networks through a Deconvolutional Generative Model,0.07128846645355225,#c5b0d5
-5.055663,-3.4011319,neighbor,219955663,Denoising Criterion for Variational Auto-Encoding Framework,0.07134836912155151,#c5b0d5
10.264415,-5.465041,neighbor,219955663,GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium,0.07159042358398438,#c5b0d5
-0.19817798,-11.201408,neighbor,219955663,Deep Image Prior,0.07165193557739258,#c5b0d5
1.5439469,4.3143287,neighbor,219955663,Variational Generative Stochastic Networks with Collaborative Shaping,0.07166999578475952,#c5b0d5
6.376735,-2.4039369,neighbor,219955663,Mixture Density Generative Adversarial Networks,0.07171565294265747,#c5b0d5
-3.2787738,7.2392406,neighbor,219955663,Scalable Uncertainty for Computer Vision With Functional Variational Inference,0.07181745767593384,#c5b0d5
-10.89916,4.2449827,neighbor,219955663,Proximal Mean-Field for Neural Network Quantization,0.07182377576828003,#c5b0d5
8.288446,-1.7563113,neighbor,219955663,Training Generative Reversible Networks,0.07182490825653076,#c5b0d5
3.903317,4.048303,neighbor,219955663,"Why are deep nets reversible: A simple theory, with implications for training",0.07183337211608887,#c5b0d5
8.235079,-6.017678,neighbor,219955663,Improved Consistency Regularization for GANs,0.07195347547531128,#c5b0d5
6.505586,-0.2394426,neighbor,219955663,Generative Adversarial Networks as Variational Training of Energy Based Models,0.07197070121765137,#c5b0d5
-0.12778105,-2.2004848,neighbor,219955663,Variational Autoencoders with Normalizing Flow Decoders,0.07200360298156738,#c5b0d5
2.9488988,1.6931973,neighbor,219955663,Improving Sampling from Generative Autoencoders with Markov Chains,0.07200759649276733,#c5b0d5
-1.4466993,6.024401,neighbor,219955663,Implicit Posterior Variational Inference for Deep Gaussian Processes,0.07201415300369263,#c5b0d5
-4.874052,8.763645,neighbor,219955663,Lightweight Probabilistic Deep Networks,0.0720529556274414,#c5b0d5
3.3023376,-2.1226194,neighbor,219955663,A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models,0.07206952571868896,#c5b0d5
0.9151326,2.0225906,neighbor,219955663,Flexible and accurate inference and learning for deep generative models,0.07210814952850342,#c5b0d5
6.270574,-4.346417,neighbor,219955663,Optimal Transport Based Generative Autoencoders,0.0721428394317627,#c5b0d5
-6.234284,8.452981,neighbor,219955663,A Comprehensive guide to Bayesian Convolutional Neural Network with Variational Inference,0.07216620445251465,#c5b0d5
0.6866002,7.7854285,neighbor,219955663,Deep and Hierarchical Implicit Models,0.07222872972488403,#c5b0d5
5.383596,-3.8598125,neighbor,219955663,NIPS 2016 Tutorial: Generative Adversarial Networks,0.07224714756011963,#c5b0d5
-1.8004353,5.5559907,neighbor,219955663,Variational Auto-encoded Deep Gaussian Processes,0.07230532169342041,#c5b0d5
6.5190144,-3.3975453,neighbor,219955663,Towards Understanding the Dynamics of Generative Adversarial Networks,0.072307288646698,#c5b0d5
-8.624386,7.801452,neighbor,219955663,Variational Bayesian Dropout With a Hierarchical Prior,0.07230830192565918,#c5b0d5
6.9616175,5.1420727,neighbor,219955663,MaCow: Masked Convolutional Generative Flow,0.0723186731338501,#c5b0d5
4.9756637,1.9386793,neighbor,219955663,Sinkhorn AutoEncoders,0.07234930992126465,#c5b0d5
-6.15212,-2.6232939,neighbor,219955663,Towards Deeper Understanding of Variational Autoencoding Models,0.07237577438354492,#c5b0d5
8.1003475,3.2739756,neighbor,219955663,Invertible Residual Networks,0.07237613201141357,#c5b0d5
-8.270135,-10.073476,neighbor,219955663,Towards Conceptual Compression,0.07244712114334106,#c5b0d5
5.7466345,1.4382416,query,231591445,Learning Transferable Visual Models From Natural Language Supervision,0.0,#c5b0d5
2.6902304,3.1003,neighbor,231591445,Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision,0.03360867500305176,#c5b0d5
4.154789,4.4163136,neighbor,231591445,Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions,0.037652432918548584,#c5b0d5
11.821637,0.47004578,neighbor,231591445,Enhancing Visual Embeddings through Weakly Supervised Captioning for Zero-Shot Learning,0.04027235507965088,#c5b0d5
3.6360855,5.0878983,neighbor,231591445,Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks,0.0414731502532959,#c5b0d5
0.40645707,6.7473054,neighbor,231591445,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks,0.043687283992767334,#c5b0d5
-7.0128098,-6.793698,neighbor,231591445,Attentive Linear Transformation for Image Captioning,0.04375898838043213,#c5b0d5
1.5705462,-4.0529346,neighbor,231591445,VirTex: Learning Visual Representations from Textual Annotations,0.044183969497680664,#c5b0d5
-0.704856,-3.5632756,neighbor,231591445,Learning Visual Representations with Caption Annotations,0.04475134611129761,#c5b0d5
-11.426231,-4.8272924,neighbor,231591445,Pointing Novel Objects in Image Captioning,0.04793274402618408,#c5b0d5
0.8733615,5.8494864,neighbor,231591445,VisualBERT: A Simple and Performant Baseline for Vision and Language,0.04868382215499878,#c5b0d5
11.131735,-3.1367528,neighbor,231591445,How Well Do Self-Supervised Models Transfer?,0.04874694347381592,#c5b0d5
13.3421135,2.197816,neighbor,231591445,Domain-Aware Visual Bias Eliminating for Generalized Zero-Shot Learning,0.04888266324996948,#c5b0d5
1.1638451,4.2907987,neighbor,231591445,UNITER: Learning UNiversal Image-TExt Representations,0.0491146445274353,#c5b0d5
-6.489517,-3.490007,neighbor,231591445,Text-Guided Attention Model for Image Captioning,0.04911619424819946,#c5b0d5
-6.656365,-7.597509,neighbor,231591445,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",0.0491294264793396,#c5b0d5
-8.670528,-1.1399765,neighbor,231591445,Leveraging Linguistically-aware Object Relations and NASNet for Image Captioning,0.04922395944595337,#c5b0d5
-12.683866,-5.9340525,neighbor,231591445,Object Captioning and Retrieval with Natural Language,0.049321770668029785,#c5b0d5
-4.929817,1.6918354,neighbor,231591445,Using Visual Feature Space as a Pivot Across Languages,0.04989206790924072,#c5b0d5
11.754811,-1.6722084,neighbor,231591445,Low-shot visual object recognition,0.050090789794921875,#c5b0d5
-12.09182,-8.12625,neighbor,231591445,Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering,0.05023300647735596,#c5b0d5
12.276926,-0.67920434,neighbor,231591445,Shaping Visual Representations with Language for Few-Shot Classification,0.050263047218322754,#c5b0d5
1.9441046,11.1950035,neighbor,231591445,Aligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks,0.05034869909286499,#c5b0d5
-5.628405,-4.264157,neighbor,231591445,Captioning with Language-Based Attention,0.05057340860366821,#c5b0d5
-2.2761922,1.0057536,neighbor,231591445,VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training,0.05058765411376953,#c5b0d5
-10.908333,-3.7944343,neighbor,231591445,Neural Baby Talk,0.0510903000831604,#c5b0d5
-8.226262,1.4903735,neighbor,231591445,Image Captioning with Pretrained Language Generators,0.05123722553253174,#c5b0d5
7.7653136,0.84644806,neighbor,231591445,Fast Parameter Adaptation for Few-shot Image Captioning and Visual Question Answering,0.05158495903015137,#c5b0d5
-0.71533465,5.7000527,neighbor,231591445,Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts,0.05166935920715332,#c5b0d5
-9.545299,-1.773316,neighbor,231591445,Dense Captioning with Joint Inference and Visual Context,0.05167508125305176,#c5b0d5
-13.152451,-0.1667221,neighbor,231591445,Deep image representations using caption generators,0.051997482776641846,#c5b0d5
-10.026382,-7.491205,neighbor,231591445,Attention Correctness in Neural Image Captioning,0.052224695682525635,#c5b0d5
10.145324,-2.4534502,neighbor,231591445,Representation learning from videos in-the-wild: An object-centric approach,0.052352964878082275,#c5b0d5
-8.047855,-2.6642952,neighbor,231591445,Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations,0.05240154266357422,#c5b0d5
1.8016179,-9.52047,neighbor,231591445,Visual Commonsense R-CNN,0.05243128538131714,#c5b0d5
-9.920807,-6.679702,neighbor,231591445,Areas of Attention for Image Captioning,0.05244183540344238,#c5b0d5
13.334861,-3.204627,neighbor,231591445,Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions,0.0525701642036438,#c5b0d5
-7.1496,-1.9411534,neighbor,231591445,Image Captioning with Semantic Attention,0.05258435010910034,#c5b0d5
-2.3015282,8.468748,neighbor,231591445,Multimodal Learning with Vision and Language,0.05282074213027954,#c5b0d5
-8.170062,-5.005688,neighbor,231591445,Constrained LSTM and Residual Attention for Image Captioning,0.0529400110244751,#c5b0d5
9.077832,3.5830426,neighbor,231591445,TextTopicNet - Self-Supervised Learning of Visual Features Through Embedding Images on Semantic Text Spaces,0.05297046899795532,#c5b0d5
-4.4454155,-6.6459017,neighbor,231591445,Image Captioning with Bidirectional Semantic Attention-Based Guiding of Long Short-Term Memory,0.05311119556427002,#c5b0d5
-1.5532666,5.057692,neighbor,231591445,Unified Vision-Language Pre-Training for Image Captioning and VQA,0.05323171615600586,#c5b0d5
-4.019971,4.1016726,neighbor,231591445,VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning,0.05342364311218262,#c5b0d5
-5.8225794,-2.1217337,neighbor,231591445,Learning to Caption Images with Two-Stream Attention and Sentence Auto-Encoder,0.05349475145339966,#c5b0d5
-7.875682,-5.666036,neighbor,231591445,Time-Dependent Pre-attention Model for Image Captioning,0.05350816249847412,#c5b0d5
3.604402,-1.9482839,neighbor,231591445,Large-Scale Representation Learning from Visually Grounded Untranscribed Speech,0.053678691387176514,#c5b0d5
3.142866,6.9419613,neighbor,231591445,ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph,0.05372631549835205,#c5b0d5
-10.123337,-4.5410204,neighbor,231591445,Parallel Image Captioning Using 2D Masked Convolution,0.05409860610961914,#c5b0d5
12.1617155,2.9486709,neighbor,231591445,Generative Model-driven Structure Aligning Discriminative Embeddings for Transductive Zero-shot Learning,0.054148733615875244,#c5b0d5
10.783425,2.8695712,neighbor,231591445,Learning Robust Visual-Semantic Embeddings,0.05423891544342041,#c5b0d5
-4.5524936,-1.157394,neighbor,231591445,From captions to visual concepts and back,0.05440801382064819,#c5b0d5
-8.4261055,-8.16984,neighbor,231591445,Normalized and Geometry-Aware Self-Attention Network for Image Captioning,0.05455988645553589,#c5b0d5
-7.142948,-10.686457,neighbor,231591445,Meshed-Memory Transformer for Image Captioning,0.05461341142654419,#c5b0d5
4.557867,6.8243775,neighbor,231591445,MiniVLM: A Smaller and Faster Vision-Language Model,0.054625630378723145,#c5b0d5
-11.161535,2.122953,neighbor,231591445,Context-Aware Captions from Context-Agnostic Supervision,0.05463266372680664,#c5b0d5
10.359283,-3.854106,neighbor,231591445,ClusterFit: Improving Generalization of Visual Representations,0.054658353328704834,#c5b0d5
-2.830427,-5.524823,neighbor,231591445,"Image Captioning using Deep Stacked LSTMs, Contextual Word Embeddings and Data Augmentation",0.05466490983963013,#c5b0d5
-8.051548,-3.6434984,neighbor,231591445,Connecting Language to Images: A Progressive Attention-Guided Network for Simultaneous Image Captioning and Language Grounding,0.05479151010513306,#c5b0d5
-5.090757,-3.2723463,neighbor,231591445,Bridging the Gap between Vision and Language Domains for Improved Image Captioning,0.054810941219329834,#c5b0d5
4.6755233,-2.1657145,neighbor,231591445,VideoBERT: A Joint Model for Video and Language Representation Learning,0.05482971668243408,#c5b0d5
-12.247457,-3.7380693,neighbor,231591445,Partially-Supervised Image Captioning,0.054863035678863525,#c5b0d5
-10.599863,-10.3433275,neighbor,231591445,"Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models",0.05486917495727539,#c5b0d5
-4.1296415,-6.04112,neighbor,231591445,Image Captioning with Deep Bidirectional LSTMs,0.05494004487991333,#c5b0d5
-10.41026,-0.21280381,neighbor,231591445,Dense Relational Image Captioning via Multi-task Triple-Stream Networks,0.054985105991363525,#c5b0d5
0.7033028,2.2878764,neighbor,231591445,Visual Natural Language Query Auto-Completion for Estimating Instance Probabilities,0.05498766899108887,#c5b0d5
-8.671097,6.7223644,neighbor,231591445,Contrastive Learning for Weakly Supervised Phrase Grounding,0.0550234317779541,#c5b0d5
-5.3704834,-10.83054,neighbor,231591445,End-to-End Video Captioning,0.05504578351974487,#c5b0d5
5.2984543,7.3873043,neighbor,231591445,VinVL: Making Visual Representations Matter in Vision-Language Models,0.055098533630371094,#c5b0d5
1.821691,-9.577437,neighbor,231591445,Visual Commonsense Representation Learning via Causal Inference,0.055531442165374756,#c5b0d5
1.730888,10.048334,neighbor,231591445,LXMERT: Learning Cross-Modality Encoder Representations from Transformers,0.055540621280670166,#c5b0d5
13.942813,0.1630747,neighbor,231591445,"Link the Head to the ""Beak"": Zero Shot Learning from Noisy Text Description at Part Precision",0.055552542209625244,#c5b0d5
1.150223,8.110911,neighbor,231591445,Multi-Task Learning of Hierarchical Vision-Language Representation,0.05560731887817383,#c5b0d5
12.982838,-2.04118,neighbor,231591445,Zero-Shot Learning from scratch (ZFS): leveraging local compositional representations,0.055650949478149414,#c5b0d5
-8.944305,1.7423086,neighbor,231591445,Auto-Encoding Scene Graphs for Image Captioning,0.05566138029098511,#c5b0d5
-15.488476,-7.2378936,neighbor,231591445,AttnGrounder: Talking to Cars with Attention,0.05578869581222534,#c5b0d5
12.826505,0.5327305,neighbor,231591445,Learning Deep Representations of Fine-Grained Visual Descriptions,0.05579257011413574,#c5b0d5
1.0446749,1.0548307,neighbor,231591445,Deep visual-semantic alignments for generating image descriptions,0.05580705404281616,#c5b0d5
10.822314,0.86428833,neighbor,231591445,Learning a Deep Embedding Model for Zero-Shot Learning,0.055999934673309326,#c5b0d5
-0.712101,10.460387,neighbor,231591445,WeaQA: Weak Supervision via Captions for Visual Question Answering,0.05601799488067627,#c5b0d5
-2.3025002,-6.838635,neighbor,231591445,Image Caption via Visual Attention Switch on DenseNet,0.05612385272979736,#c5b0d5
2.8706307,1.1642212,neighbor,231591445,Learning Visual N-Grams from Web Data,0.05617928504943848,#c5b0d5
1.1181284,10.682587,neighbor,231591445,Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks,0.0562131404876709,#c5b0d5
-6.1681175,-5.6031146,neighbor,231591445,Pseudo-3D Attention Transfer Network with Content-aware Strategy for Image Captioning,0.05631411075592041,#c5b0d5
-2.7928023,4.698558,neighbor,231591445,Learning to Collocate Neural Modules for Image Captioning,0.05631512403488159,#c5b0d5
-1.9062022,8.343013,neighbor,231591445,What Value Do Explicit High Level Concepts Have in Vision to Language Problems?,0.05639618635177612,#c5b0d5
4.0132966,9.135159,neighbor,231591445,Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training,0.056412339210510254,#c5b0d5
-3.7574062,0.8305513,neighbor,231591445,Unpaired Image Captioning by Language Pivoting,0.05650889873504639,#c5b0d5
6.720547,-2.9920464,neighbor,231591445,ON THE EFFECTIVENESS OF TASK GRANULARITY FOR TRANSFER LEARNING,0.056676626205444336,#c5b0d5
-7.1348515,2.6343853,neighbor,231591445,Translating Videos to Natural Language Using Deep Recurrent Neural Networks,0.056683480739593506,#c5b0d5
-1.2484825,-1.6271595,neighbor,231591445,Towards Unsupervised Image Captioning With Shared Multimodal Embeddings,0.05681347846984863,#c5b0d5
-14.260928,-3.3361642,neighbor,231591445,Learning to generalize to new compositions in image understanding,0.05682063102722168,#c5b0d5
0.15160379,7.9931164,neighbor,231591445,12-in-1: Multi-Task Vision and Language Representation Learning,0.05684918165206909,#c5b0d5
-4.0857296,-8.177308,neighbor,231591445,Image Cationing with Visual-Semantic LSTM,0.056873619556427,#c5b0d5
-3.3417122,-1.7102984,neighbor,231591445,Oracle Performance for Visual Captioning,0.05694293975830078,#c5b0d5
-6.707044,-9.953926,neighbor,231591445,CPTR: Full Transformer Network for Image Captioning,0.05694848299026489,#c5b0d5
12.504312,1.7683263,neighbor,231591445,Rethinking Zero-Shot Learning: A Conditional Visual Classification Perspective,0.056966185569763184,#c5b0d5
5.4995837,3.8668363,neighbor,231591445,Unsupervised Pre-Training for Detection Transformers,0.056968748569488525,#c5b0d5
15.425525,1.8870252,neighbor,231591445,Large-Scale Datasets for Going Deeper in Image Understanding,0.056976139545440674,#c5b0d5
-8.567801,6.975778,neighbor,231591445,Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation,0.05701082944869995,#c5b0d5
-12.7047415,-1.837827,neighbor,231591445,Boosting Image Captioning with Attributes,0.057011544704437256,#c5b0d5
0.14774697,-4.306567,query,235458009,LoRA: Low-Rank Adaptation of Large Language Models,0.0,#8c564b
0.6356821,-5.4081388,neighbor,235458009,Compacter: Efficient Low-Rank Hypercomplex Adapter Layers,0.037165939807891846,#8c564b
-0.8583859,-3.3599768,neighbor,235458009,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,0.03793692588806152,#8c564b
3.679973,-9.444272,neighbor,235458009,Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,0.04107069969177246,#8c564b
2.634394,-7.220604,neighbor,235458009,Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning,0.04368644952774048,#8c564b
2.725426,-10.325704,neighbor,235458009,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,0.0439106822013855,#8c564b
1.3087806,-6.1181436,neighbor,235458009,How fine can fine-tuning be? Learning efficient language models,0.044683992862701416,#8c564b
-5.7763095,-5.9813643,neighbor,235458009,Multi-scale Transformer Language Models,0.04615741968154907,#8c564b
0.5520447,12.110344,neighbor,235458009,"Simple, Scalable Adaptation for Neural Machine Translation",0.046344757080078125,#8c564b
3.394582,-10.392748,neighbor,235458009,Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach,0.047357380390167236,#8c564b
4.9309044,-1.3040751,neighbor,235458009,Knowledge Inheritance for Pre-trained Language Models,0.04743313789367676,#8c564b
-2.152456,-6.010638,neighbor,235458009,Structured Pruning of Large Language Models,0.048222124576568604,#8c564b
4.686184,4.514015,neighbor,235458009,UNKs Everywhere: Adapting Multilingual Language Models to New Scripts,0.04846608638763428,#8c564b
-1.8865684,-2.7506483,neighbor,235458009,Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,0.048604726791381836,#8c564b
-5.5552354,-2.1174517,neighbor,235458009,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,0.0489315390586853,#8c564b
4.634924,3.4302244,neighbor,235458009,Exploring the Data Efficiency of Cross-Lingual Post-Training in Pretrained Language Models,0.04918092489242554,#8c564b
3.822401,-8.341318,neighbor,235458009,Selecting Informative Contexts Improves Language Model Fine-tuning,0.049500465393066406,#8c564b
-4.239214,6.755199,neighbor,235458009,Towards Making the Most of BERT in Neural Machine Translation,0.04994434118270874,#8c564b
1.4167922,0.7856591,neighbor,235458009,Scaling End-to-End Models for Large-Scale Multilingual ASR,0.050095319747924805,#8c564b
1.7486907,9.840246,neighbor,235458009,Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data,0.05036276578903198,#8c564b
-5.3081865,5.19549,neighbor,235458009,DeLighT: Very Deep and Light-weight Transformer,0.05081254243850708,#8c564b
0.91716754,11.139113,neighbor,235458009,Rapid Adaptation of Neural Machine Translation to New Languages,0.051043152809143066,#8c564b
4.284504,-10.74819,neighbor,235458009,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,0.05105102062225342,#8c564b
6.688796,-11.869839,neighbor,235458009,MML: Maximal Multiverse Learning for Robust Fine-Tuning of Language Models,0.051130056381225586,#8c564b
4.682525,-9.232679,neighbor,235458009,Visualizing and Understanding the Effectiveness of BERT,0.0511818528175354,#8c564b
2.2230332,-8.951938,neighbor,235458009,Variational Information Bottleneck for Effective Low-Resource Fine-Tuning,0.051807701587677,#8c564b
-4.1244783,-10.214555,neighbor,235458009,"Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",0.05185204744338989,#8c564b
-5.784786,10.62788,neighbor,235458009,In-training Matrix Factorization for Parameter-frugal Neural Machine Translation,0.052116990089416504,#8c564b
0.39251193,13.402088,neighbor,235458009,Finding Sparse Structures for Domain Specific Neural Machine Translation,0.05224764347076416,#8c564b
4.356525,-14.162282,neighbor,235458009,Adversarial Training for Large Neural Language Models,0.05264967679977417,#8c564b
-7.295442,6.586664,neighbor,235458009,Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention,0.052667975425720215,#8c564b
1.4490745,5.818689,neighbor,235458009,Transfer training from smaller language model,0.052822232246398926,#8c564b
5.7464437,-5.299533,neighbor,235458009,How to Train BERT with an Academic Budget,0.053269386291503906,#8c564b
-2.0522327,-8.258342,neighbor,235458009,Reweighted Proximal Pruning for Large-Scale Language Representation,0.05329400300979614,#8c564b
-4.272203,-9.242741,neighbor,235458009,Rethinking embedding coupling in pre-trained language models,0.05350989103317261,#8c564b
-6.5042706,0.98158485,neighbor,235458009,Reservoir Transformers,0.05351072549819946,#8c564b
7.1790466,-5.055885,neighbor,235458009,Masked Language Model Scoring,0.053542137145996094,#8c564b
3.1207364,-12.54531,neighbor,235458009,Better Fine-Tuning by Reducing Representational Collapse,0.05391901731491089,#8c564b
-3.5387015,9.2132635,neighbor,235458009,Adaptive Sparse Transformer for Multilingual Translation,0.05397456884384155,#8c564b
-5.344087,6.7838964,neighbor,235458009,Scalable Transformers for Neural Machine Translation,0.05433434247970581,#8c564b
-6.4232736,-0.628604,neighbor,235458009,Ouroboros: On Accelerating Training of Transformer-Based Language Models,0.05450558662414551,#8c564b
3.3536222,8.189574,neighbor,235458009,Multilingual Translation with Extensible Multilingual Pretraining and Finetuning,0.05464452505111694,#8c564b
5.809985,-9.72419,neighbor,235458009,"On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",0.05489605665206909,#8c564b
8.84346,-2.0979416,neighbor,235458009,On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation,0.05490899085998535,#8c564b
4.3702188,-13.610649,neighbor,235458009,Domain Adversarial Fine-Tuning as an Effective Regularizer,0.055137693881988525,#8c564b
-1.6934084,-7.3952384,neighbor,235458009,Movement Pruning: Adaptive Sparsity by Fine-Tuning,0.05533653497695923,#8c564b
4.509168,4.881299,neighbor,235458009,When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models,0.055393755435943604,#8c564b
4.7887316,-2.221421,neighbor,235458009,LightPAFF: A Two-Stage Distillation Framework for Pre-training and Fine-tuning,0.05546516180038452,#8c564b
6.158045,-7.759003,neighbor,235458009,Masking as an Efficient Alternative to Finetuning for Pretrained Language Models,0.055487215518951416,#8c564b
1.89036,10.573476,neighbor,235458009,Adapting Multilingual Neural Machine Translation to Unseen Languages,0.05549895763397217,#8c564b
5.179736,3.0401325,neighbor,235458009,Unsupervised Cross-lingual Representation Learning at Scale,0.05551409721374512,#8c564b
2.8996134,-6.0987244,neighbor,235458009,IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization,0.05553889274597168,#8c564b
-9.299933,7.868111,neighbor,235458009,Sparse Attention with Linear Units,0.055702805519104004,#8c564b
6.687553,3.251154,neighbor,235458009,MultiFiT: Efficient Multi-lingual Language Model Fine-tuning,0.05572229623794556,#8c564b
1.8217394,-3.6012769,neighbor,235458009,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,0.05572474002838135,#8c564b
-5.163469,-1.6996509,neighbor,235458009,Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models,0.05593234300613403,#8c564b
-0.97721744,-9.948714,neighbor,235458009,A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios,0.05596435070037842,#8c564b
-6.4209924,8.9667425,neighbor,235458009,Not all parameters are born equal: Attention is mostly what you need,0.05596977472305298,#8c564b
0.09062885,14.382115,neighbor,235458009,Compact Personalized Models for Neural Machine Translation,0.05600684881210327,#8c564b
-0.30879653,12.572432,neighbor,235458009,Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey,0.05612075328826904,#8c564b
-4.3314385,-8.449859,neighbor,235458009,Emergent Properties of Finetuned Language Representation Models,0.05621105432510376,#8c564b
2.694558,2.880339,neighbor,235458009,Regularization Advantages of Multilingual Neural Language Models for Low Resource Domains,0.0562511682510376,#8c564b
1.0548998,3.6989686,neighbor,235458009,An Empirical Study of Transformer-Based Neural Language Model Adaptation,0.056255042552948,#8c564b
3.8060746,8.680684,neighbor,235458009,Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation,0.0564534068107605,#8c564b
-1.6707828,-10.373216,neighbor,235458009,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,0.05666482448577881,#8c564b
1.3147494,-11.4068575,neighbor,235458009,Investigating Transferability in Pretrained Language Models,0.056703805923461914,#8c564b
-8.678578,2.7913175,neighbor,235458009,Training Language Models Using Target-Propagation,0.056741952896118164,#8c564b
-7.643518,7.792879,neighbor,235458009,UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost,0.0570107102394104,#8c564b
-7.050297,3.9414604,neighbor,235458009,Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation,0.05708855390548706,#8c564b
-3.1423957,10.24171,neighbor,235458009,Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation,0.057327449321746826,#8c564b
-4.86189,7.840234,neighbor,235458009,Shallow-to-Deep Training for Neural Machine Translation,0.05735498666763306,#8c564b
5.1922994,-4.910755,neighbor,235458009,RoBERTa: A Robustly Optimized BERT Pretraining Approach,0.05742603540420532,#8c564b
-6.6467514,5.8386855,neighbor,235458009,Sharing Attention Weights for Fast Transformer,0.05742919445037842,#8c564b
-9.693798,5.661194,neighbor,235458009,A Tensorized Transformer for Language Modeling,0.05757272243499756,#8c564b
-1.0173478,-2.0593867,neighbor,235458009,Top-KAST: Top-K Always Sparse Training,0.05763816833496094,#8c564b
-6.426583,-3.9194293,neighbor,235458009,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,0.05768769979476929,#8c564b
-7.973599,-3.5448635,neighbor,235458009,Large Scale Language Modeling: Converging on 40GB of Text in Four Hours,0.0577053427696228,#8c564b
0.5918155,-7.6500196,neighbor,235458009,Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks,0.05779784917831421,#8c564b
-5.940165,7.713576,neighbor,235458009,Efficient Inference For Neural Machine Translation,0.057817816734313965,#8c564b
6.039031,-7.1181984,neighbor,235458009,To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks,0.05782192945480347,#8c564b
-8.276389,5.628252,neighbor,235458009,Improving Transformer Models by Reordering their Sublayers,0.05783534049987793,#8c564b
-6.3333735,-5.1254044,neighbor,235458009,An Analysis of Neural Language Modeling at Multiple Scales,0.05786097049713135,#8c564b
7.6481433,-9.466869,neighbor,235458009,AutoFreeze: Automatically Freezing Model Blocks to Accelerate Fine-tuning,0.05810892581939697,#8c564b
-4.6093683,2.1476932,neighbor,235458009,"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",0.058164775371551514,#8c564b
-1.4189651,6.956116,neighbor,235458009,Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation,0.05821007490158081,#8c564b
8.993261,-7.695118,neighbor,235458009,A Generalizable Approach to Learning Optimizers,0.058320581912994385,#8c564b
-7.1318603,10.812262,neighbor,235458009,Auto-Sizing Neural Networks: With Applications to n-gram Language Models,0.05850929021835327,#8c564b
5.0375137,0.4956342,neighbor,235458009,MergeDistill: Merging Language Models using Pre-trained Distillation,0.05857771635055542,#8c564b
-2.131898,-3.7297924,neighbor,235458009,"BASE Layers: Simplifying Training of Large, Sparse Models",0.05864185094833374,#8c564b
-8.178931,-5.3293066,neighbor,235458009,Fast LSTM Inference by Dynamic Decomposition on Cloud Systems,0.05870842933654785,#8c564b
0.22101642,9.135064,neighbor,235458009,Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges,0.058712542057037354,#8c564b
-1.5003958,8.413834,neighbor,235458009,Language Model Prior for Low-Resource Neural Machine Translation,0.058822572231292725,#8c564b
-0.122413725,8.019682,neighbor,235458009,Transfer Learning for Low-Resource Neural Machine Translation,0.05882912874221802,#8c564b
-3.025725,7.329067,neighbor,235458009,On the use of BERT for Neural Machine Translation,0.05890393257141113,#8c564b
-0.089027286,-12.125063,neighbor,235458009,Structure-inducing pre-training,0.05896347761154175,#8c564b
-0.9551474,13.274274,neighbor,235458009,Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation,0.0589635968208313,#8c564b
-4.99954,-3.9371836,neighbor,235458009,Large Memory Layers with Product Keys,0.05901443958282471,#8c564b
-3.8355777,-1.4573253,neighbor,235458009,FastMoE: A Fast Mixture-of-Expert Training System,0.05913221836090088,#8c564b
-6.3486624,7.140983,neighbor,235458009,Multi-Unit Transformers for Neural Machine Translation,0.05923062562942505,#8c564b
0.9212156,8.297795,neighbor,235458009,"Many-to-English Machine Translation Tools, Data, and Pretrained Models",0.059267640113830566,#8c564b
9.054781,-2.4108071,neighbor,235458009,AdapterHub: A Framework for Adapting Transformers,0.05929684638977051,#8c564b
0.97149867,2.491235,query,246426909,Training language models to follow instructions with human feedback,0.0,#8c564b
2.0573978,2.8325462,neighbor,246426909,Evaluating Machines by their Real-World Language Use,0.043891966342926025,#8c564b
6.1130977,-3.9407666,neighbor,246426909,Making Pre-trained Language Models Better Few-shot Learners,0.048050880432128906,#8c564b
5.836119,-7.273793,neighbor,246426909,Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity,0.04936981201171875,#8c564b
-2.6408277,3.2864623,neighbor,246426909,Putting Humans in the Natural Language Processing Loop: A Survey,0.04971092939376831,#8c564b
5.151674,6.8791313,neighbor,246426909,Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning,0.050232768058776855,#8c564b
-0.17677666,2.5210814,neighbor,246426909,Capturing Failures of Large Language Models via Human Cognitive Biases,0.05023324489593506,#8c564b
5.693908,-6.0182657,neighbor,246426909,Calibrate Before Use: Improving Few-Shot Performance of Language Models,0.05091416835784912,#8c564b
2.5515776,2.9039426,neighbor,246426909,TuringAdvice: A Generative and Dynamic Evaluation of Language Use,0.051003336906433105,#8c564b
3.8465827,-4.383492,neighbor,246426909,Generating Training Data with Language Models: Towards Zero-Shot Language Understanding,0.05115252733230591,#8c564b
11.256966,1.6833946,neighbor,246426909,Using natural language prompts for machine translation,0.05140489339828491,#8c564b
1.2064148,-0.35104406,neighbor,246426909,Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?,0.05145043134689331,#8c564b
1.2059928,-3.7288005,neighbor,246426909,Want To Reduce Labeling Cost? GPT-3 Can Help,0.05167055130004883,#8c564b
7.5903363,-3.7613835,neighbor,246426909,Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners,0.05176520347595215,#8c564b
10.021553,-4.8519006,neighbor,246426909,P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks,0.052162110805511475,#8c564b
-7.7632294,-1.6756215,neighbor,246426909,Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey,0.052247464656829834,#8c564b
2.0804691,4.578085,neighbor,246426909,LaMDA: Language Models for Dialog Applications,0.05241596698760986,#8c564b
7.7474756,-7.525637,neighbor,246426909,GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation,0.05243116617202759,#8c564b
8.151264,-1.1297922,neighbor,246426909,Reframing Instructional Prompts to GPTk’s Language,0.052750229835510254,#8c564b
11.5004,0.5407795,neighbor,246426909,MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators,0.05306762456893921,#8c564b
4.9171586,-3.3345041,neighbor,246426909,Finetuned Language Models Are Zero-Shot Learners,0.05344587564468384,#8c564b
12.204002,-1.8065524,neighbor,246426909,Black-Box Tuning for Language-Model-as-a-Service,0.053618788719177246,#8c564b
-8.692726,2.4506795,neighbor,246426909,Predicting Performance for Natural Language Processing Tasks,0.05402553081512451,#8c564b
4.7837267,-7.7895226,neighbor,246426909,Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm,0.05417299270629883,#8c564b
8.430781,-2.195119,neighbor,246426909,Do Prompts Solve NLP Tasks Using Natural Language?,0.054342448711395264,#8c564b
5.884258,7.9803267,neighbor,246426909,Fine-Tuning Language Models from Human Preferences,0.05447542667388916,#8c564b
-3.5176823,3.4428852,neighbor,246426909,Explanation-Based Human Debugging of NLP Models: A Survey,0.05459260940551758,#8c564b
1.9331961,5.662627,neighbor,246426909,A General Language Assistant as a Laboratory for Alignment,0.054679155349731445,#8c564b
-8.135412,2.1821563,neighbor,246426909,Towards More Fine-grained and Reliable NLP Performance Prediction,0.055043160915374756,#8c564b
2.4941266,-8.961268,neighbor,246426909,Reframing Human-AI Collaboration for Generating Free-Text Explanations,0.055068016052246094,#8c564b
-11.324189,3.6536334,neighbor,246426909,Learning from others' mistakes: Avoiding dataset biases without modeling them,0.05551546812057495,#8c564b
-9.848603,-0.6241197,neighbor,246426909,Contemporary NLP Modeling in Six Comprehensive Programming Assignments,0.05598217248916626,#8c564b
2.7068176,10.461934,neighbor,246426909,Reducing Non-Normative Text Generation from Language Models,0.05612170696258545,#8c564b
-7.443185,3.5035248,neighbor,246426909,ExplainaBoard: An Explainable Leaderboard for NLP,0.056556642055511475,#8c564b
-8.368583,-5.633055,neighbor,246426909,"On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",0.05661582946777344,#8c564b
-9.352386,5.789658,neighbor,246426909,LMdiff: A Visual Diff Tool to Compare Language Models,0.05681711435317993,#8c564b
-5.5618167,-4.315699,neighbor,246426909,Predictions For Pre-training Language Models,0.05694544315338135,#8c564b
-8.508256,-3.0009024,neighbor,246426909,How to Train BERT with an Academic Budget,0.05731332302093506,#8c564b
3.223939,1.0429468,neighbor,246426909,Teaching Models new APIs: Domain-Agnostic Simulators for Task Oriented Dialogue,0.057461678981781006,#8c564b
10.484196,-7.032196,neighbor,246426909,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,0.05755358934402466,#8c564b
5.738063,5.248635,neighbor,246426909,Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback,0.057708919048309326,#8c564b
9.467512,-2.5856862,neighbor,246426909,OpenPrompt: An Open-source Framework for Prompt-learning,0.057757556438446045,#8c564b
-4.9209805,0.36086074,neighbor,246426909,NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation,0.05780917406082153,#8c564b
4.459908,-2.16568,neighbor,246426909,Language Models are Few-Shot Learners,0.05781036615371704,#8c564b
-3.1549819,9.371519,neighbor,246426909,Assessing Discourse Relations in Language Generation from Pre-trained Language Models,0.05798327922821045,#8c564b
13.847419,-7.564334,neighbor,246426909,Jointly Improving Language Understanding and Generation with Quality-Weighted Weak Supervision of Automatic Labeling,0.058057188987731934,#8c564b
-1.3221493,-7.1741,neighbor,246426909,A Comparison of LSTM and BERT for Small Corpus,0.0584225058555603,#8c564b
-10.8010025,3.2349336,neighbor,246426909,Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models,0.05861771106719971,#8c564b
6.348271,-9.220613,neighbor,246426909,Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts,0.05866330862045288,#8c564b
12.774282,-4.579897,neighbor,246426909,Context-Tuning: Learning Contextualized Prompts for Natural Language Generation,0.05872476100921631,#8c564b
5.0607486,2.1306787,neighbor,246426909,Learning from Task Descriptions,0.05872488021850586,#8c564b
-2.5918367,-2.2610521,neighbor,246426909,Language Modelling via Learning to Rank,0.05873382091522217,#8c564b
2.7458756,10.259833,neighbor,246426909,Fine-Tuning a Transformer-Based Language Model to Avoid Generating Non-Normative Text,0.05873608589172363,#8c564b
11.57052,-5.792316,neighbor,246426909,HyperPrompt: Prompt-based Task-Conditioning of Transformers,0.05889219045639038,#8c564b
-7.842791,-4.744259,neighbor,246426909,Visualizing and Understanding the Effectiveness of BERT,0.059084177017211914,#8c564b
-11.275181,-2.990967,neighbor,246426909,AutoText: An End-to-End AutoAI Framework for Text,0.05921316146850586,#8c564b
2.5292442,-4.4521613,neighbor,246426909,Towards Zero-Label Language Learning,0.05924040079116821,#8c564b
10.471168,-7.797897,neighbor,246426909,WARP: Word-level Adversarial ReProgramming,0.059370219707489014,#8c564b
9.110545,-4.7480254,neighbor,246426909,PPT: Pre-trained Prompt Tuning for Few-shot Learning,0.05940979719161987,#8c564b
-6.698683,1.982592,neighbor,246426909,Dynabench: Rethinking Benchmarking in NLP,0.05962955951690674,#8c564b
1.4352642,11.258678,neighbor,246426909,Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets,0.05963021516799927,#8c564b
5.3099637,2.6949387,neighbor,246426909,The Turking Test: Can Language Models Understand Instructions?,0.059661924839019775,#8c564b
-1.261597,8.314365,neighbor,246426909,Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies,0.059767961502075195,#8c564b
-7.3094482,-0.61014265,neighbor,246426909,Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,0.05981093645095825,#8c564b
-12.786861,2.3016434,neighbor,246426909,Explaining and Improving Model Behavior with k Nearest Neighbor Representations,0.05993342399597168,#8c564b
-6.1361866,-1.4119451,neighbor,246426909,jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models,0.05994117259979248,#8c564b
-2.5759046,10.981083,neighbor,246426909,Language Model Evaluation in Open-ended Text Generation,0.059946537017822266,#8c564b
-3.1820421,9.43947,neighbor,246426909,Assessing Discourse Relations in Language Generation from GPT-2,0.06007874011993408,#8c564b
-7.711375,6.017458,neighbor,246426909,Show Your Work: Improved Reporting of Experimental Results,0.060206592082977295,#8c564b
13.22483,-6.1991096,neighbor,246426909,The Power of Prompt Tuning for Low-Resource Semantic Parsing,0.060247182846069336,#8c564b
5.757103,4.327471,neighbor,246426909,Interscript: A dataset for interactive learning of scripts through error feedback,0.060517728328704834,#8c564b
-13.425881,2.2888255,neighbor,246426909,Pathologies of Neural Models Make Interpretations Difficult,0.060608625411987305,#8c564b
-5.8355703,-6.318641,neighbor,246426909,Aligning the Pretraining and Finetuning Objectives of Language Models,0.0607951283454895,#8c564b
3.4768965,8.351839,neighbor,246426909,Red Teaming Language Models with Language Models,0.060799598693847656,#8c564b
2.098921,12.613729,neighbor,246426909,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,0.060855865478515625,#8c564b
-0.94213045,7.0981555,neighbor,246426909,Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data,0.06096947193145752,#8c564b
10.949663,-3.3367374,neighbor,246426909,"Protum: A New Method For Prompt Tuning Based on ""[MASK]""",0.06100583076477051,#8c564b
-2.7252169,11.967266,neighbor,246426909,Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models,0.06111687421798706,#8c564b
-6.25438,-3.2127404,neighbor,246426909,On the Influence of Masking Policies in Intermediate Pre-training,0.061181724071502686,#8c564b
-14.878484,0.7087489,neighbor,246426909,"Adversarially Constructed Evaluation Sets Are More Challenging, but May Not Be Fair",0.06128096580505371,#8c564b
-3.9127383,11.184598,neighbor,246426909,All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text,0.061338603496551514,#8c564b
6.390346,-2.6199024,neighbor,246426909,FLEX: Unifying Evaluation for Few-Shot NLP,0.061493098735809326,#8c564b
-4.7681494,10.91906,neighbor,246426909,BEAMetrics: A Benchmark for Language Generation Evaluation Evaluation,0.06155288219451904,#8c564b
-0.2943785,4.3738756,neighbor,246426909,What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?,0.061688363552093506,#8c564b
-9.421761,5.114035,neighbor,246426909,Principles and Interactive Tools for Evaluating and Improving the Behavior of Natural Language Processing models,0.06168961524963379,#8c564b
-6.196372,-5.2062273,neighbor,246426909,Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach,0.06170833110809326,#8c564b
-8.836347,-0.8470272,neighbor,246426909,"Baseline: A Library for Rapid Modeling, Experimentation and Development of Deep Learning Algorithms targeting NLP",0.06172215938568115,#8c564b
-6.7543674,3.9114258,neighbor,246426909,How not to Lie with a Benchmark: Rearranging NLP Leaderboards,0.06176483631134033,#8c564b
1.7606053,-5.091076,neighbor,246426909,"Generate, Annotate, and Learn: NLP with Synthetic Text",0.06178414821624756,#8c564b
2.9398875,-9.241415,neighbor,246426909,Few-Shot Self-Rationalization with Natural Language Prompts,0.06189197301864624,#8c564b
3.5214179,11.281106,neighbor,246426909,Reward modeling for mitigating toxicity in transformer-based language models,0.061924755573272705,#8c564b
4.878236,-4.8606,neighbor,246426909,Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections,0.06213873624801636,#8c564b
-1.2010403,0.37699455,neighbor,246426909,Analyzing the Limits of Self-Supervision in Handling Bias in Language,0.06226295232772827,#8c564b
-7.3742104,6.76891,neighbor,246426909,How Reliable are Model Diagnostics?,0.06230771541595459,#8c564b
8.359212,-9.771186,neighbor,246426909,Learning How to Ask: Querying LMs with Mixtures of Soft Prompts,0.06236904859542847,#8c564b
-11.121852,0.21288358,neighbor,246426909,"Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates",0.062396347522735596,#8c564b
7.332576,-5.07702,neighbor,246426909,Reconsidering the Past: Optimizing Hidden States in Language Models,0.06245434284210205,#8c564b
-9.163342,-2.401404,neighbor,246426909,On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜,0.06258374452590942,#8c564b
3.7745388,-0.49727607,neighbor,246426909,Language Models are Few-Shot Butlers,0.06265181303024292,#8c564b
-1.2554321,-7.7660007,neighbor,246426909,Translation vs. Dialogue: A Comparative Analysis of Sequence-to-Sequence Modeling,0.06290292739868164,#8c564b
8.682574,-5.993902,neighbor,246426909,Co-training Improves Prompt-based Learning for Large Language Models,0.06292837858200073,#8c564b
-7.087491,-4.403445,query,257219404,LLaMA: Open and Efficient Foundation Language Models,0.0,#8c564b
-5.817612,1.1987151,neighbor,257219404,Training Compute-Optimal Large Language Models,0.03381240367889404,#8c564b
-9.004508,-3.5444007,neighbor,257219404,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,0.04282635450363159,#8c564b
-6.070686,-5.6997895,neighbor,257219404,FPM: A Collection of Large-scale Foundation Pre-trained Language Models,0.04333919286727905,#8c564b
11.110209,2.686131,neighbor,257219404,ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models,0.043442487716674805,#8c564b
-6.4137316,-0.017697081,neighbor,257219404,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",0.04636591672897339,#8c564b
-3.5192919,-10.987746,neighbor,257219404,naab: A ready-to-use plug-and-play corpus for Farsi,0.046664416790008545,#8c564b
-0.117717944,-11.09631,neighbor,257219404,ALBETO and DistilBETO: Lightweight Spanish Language Models,0.04798591136932373,#8c564b
-6.0312624,-3.1925495,neighbor,257219404,Director: Generator-Classifiers For Supervised Language Modeling,0.04938298463821411,#8c564b
-3.8439221,4.1769543,neighbor,257219404,How to Train BERT with an Academic Budget,0.050383567810058594,#8c564b
9.985947,-3.273991,neighbor,257219404,SimpleBooks: Long-term dependency book dataset with simplified English vocabulary for word-level language modeling,0.050531089305877686,#8c564b
4.8130784,0.8464396,neighbor,257219404,Efficient pre-training objectives for Transformers,0.050556719303131104,#8c564b
0.81094843,-1.2593051,neighbor,257219404,On Losses for Modern Language Models,0.05102348327636719,#8c564b
3.908007,5.7357235,neighbor,257219404,schuBERT: Optimizing Elements of BERT,0.051703572273254395,#8c564b
-3.6937678,-7.8789015,neighbor,257219404,The birth of Romanian BERT,0.05212634801864624,#8c564b
6.3083243,-0.043499354,neighbor,257219404,Hierarchical Transformers Are More Efficient Language Models,0.05267220735549927,#8c564b
-6.190683,2.5466747,neighbor,257219404,Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing,0.05277830362319946,#8c564b
-3.3491504,-3.6467767,neighbor,257219404,CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model,0.05332857370376587,#8c564b
5.984846,-2.6144435,neighbor,257219404,Shortformer: Better Language Modeling using Shorter Inputs,0.05343508720397949,#8c564b
-11.885619,-3.6716623,neighbor,257219404,OpenGPT-2,0.05358421802520752,#8c564b
7.821413,2.6195822,neighbor,257219404,Simple Recurrence Improves Masked Language Models,0.05378592014312744,#8c564b
9.09404,-7.3307805,neighbor,257219404,Superbloom: Bloom filter meets Transformer,0.05389606952667236,#8c564b
12.159742,-5.64573,neighbor,257219404,"Spell Once, Summon Anywhere: A Two-Level Open-Vocabulary Language Model",0.05395948886871338,#8c564b
7.181122,-2.0020907,neighbor,257219404,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,0.05397289991378784,#8c564b
0.43383712,-4.226757,neighbor,257219404,Training Multilingual Pre-trained Language Model with Byte-level Subwords,0.05400317907333374,#8c564b
2.5344634,6.508771,neighbor,257219404,Q8BERT: Quantized 8Bit BERT,0.05413705110549927,#8c564b
-4.1475806,12.093837,neighbor,257219404,Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages,0.0546412467956543,#8c564b
4.729946,7.8063493,neighbor,257219404,Towards Fine-tuning Pre-trained Language Models with Integer Forward and Backward Propagation,0.054821670055389404,#8c564b
-0.9715466,-2.5223517,neighbor,257219404,Pre-Training With Whole Word Masking for Chinese BERT,0.0549168586730957,#8c564b
-13.917987,-2.3631914,neighbor,257219404,Toolformer: Language Models Can Teach Themselves to Use Tools,0.05498313903808594,#8c564b
5.8093195,2.8364203,neighbor,257219404,Primer: Searching for Efficient Transformers for Language Modeling,0.05529755353927612,#8c564b
-10.469844,-6.266573,neighbor,257219404,Show Your Work: Improved Reporting of Experimental Results,0.055438995361328125,#8c564b
-8.880513,1.66346,neighbor,257219404,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,0.05548018217086792,#8c564b
0.013218507,2.5753148,neighbor,257219404,"AxFormer: Accuracy-driven Approximation of Transformers for Faster, Smaller and more Accurate NLP Models",0.055703163146972656,#8c564b
9.726359,-0.8635076,neighbor,257219404,Language Modeling with Transformer,0.055770277976989746,#8c564b
-4.161825,6.5097623,neighbor,257219404,RoBERTa: A Robustly Optimized BERT Pretraining Approach,0.05590713024139404,#8c564b
-0.40880287,-4.7539334,neighbor,257219404,LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization,0.05604565143585205,#8c564b
11.370882,0.023582991,neighbor,257219404,Character-Level Language Modeling with Deeper Self-Attention,0.05611598491668701,#8c564b
-1.8317102,-2.5110376,neighbor,257219404,"BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark",0.05624234676361084,#8c564b
-5.301765,0.56072766,neighbor,257219404,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,0.05631381273269653,#8c564b
-3.7303455,-8.350478,neighbor,257219404,A Lite Romanian BERT: ALR-BERT,0.05632436275482178,#8c564b
7.6094704,0.8642614,neighbor,257219404,Transformer on a Diet,0.05634528398513794,#8c564b
8.903945,-6.172998,neighbor,257219404,Transformer Feed-Forward Layers Are Key-Value Memories,0.05640590190887451,#8c564b
7.996109,4.236946,neighbor,257219404,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,0.05663716793060303,#8c564b
-0.70971745,1.751007,neighbor,257219404,FastFormers: Highly Efficient Transformer Models for Natural Language Understanding,0.056676387786865234,#8c564b
-7.79086,-8.084005,neighbor,257219404,Nonparametric Masked Language Modeling,0.05676764249801636,#8c564b
6.753376,3.4302013,neighbor,257219404,Transformer Quality in Linear Time,0.05679881572723389,#8c564b
2.5215435,7.824768,neighbor,257219404,Prune Once for All: Sparse Pre-Trained Language Models,0.05680423974990845,#8c564b
-6.7518144,-1.1787193,neighbor,257219404,Keynote Talk 2 Training Large Language Models: Challenges and Opportunities,0.056813061237335205,#8c564b
-4.825457,12.265874,neighbor,257219404,Pre-trained transformer-based language models for Sundanese,0.05682229995727539,#8c564b
1.5469296,-7.612879,neighbor,257219404,Learning Dynamic BERT via Trainable Gate Variables and a Bi-modal Regularizer,0.05683577060699463,#8c564b
-8.048809,6.139379,neighbor,257219404,BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling,0.05683612823486328,#8c564b
-2.5951462,-11.949297,neighbor,257219404,Ungoliant: An Optimized Pipeline for the Generation of a Very Large-Scale Multilingual Web Corpus,0.05685555934906006,#8c564b
-4.2982802,2.1994941,neighbor,257219404,Training a T5 Using Lab-sized Resources,0.05694633722305298,#8c564b
-7.969547,8.18649,neighbor,257219404,Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning,0.05694925785064697,#8c564b
2.1141996,-5.0182343,neighbor,257219404,Multilingual Language Processing From Bytes,0.05695986747741699,#8c564b
6.4478507,4.9897084,neighbor,257219404,ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs,0.057102739810943604,#8c564b
-7.405346,6.2271247,neighbor,257219404,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,0.057128190994262695,#8c564b
-2.1777287,4.9109983,neighbor,257219404,Can depth-adaptive BERT perform better on binary classification tasks,0.05713045597076416,#8c564b
1.3427238,0.19967596,neighbor,257219404,Should You Mask 15% in Masked Language Modeling?,0.05718696117401123,#8c564b
11.086857,1.7304542,neighbor,257219404,Bridging the Gap for Tokenizer-Free Language Models,0.05724459886550903,#8c564b
1.4829032,-1.9121087,neighbor,257219404,MPNet: Masked and Permuted Pre-training for Language Understanding,0.057337939739227295,#8c564b
1.6219742,3.0979164,neighbor,257219404,Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning,0.05735623836517334,#8c564b
5.9734983,-1.2442671,neighbor,257219404,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,0.05735647678375244,#8c564b
12.2591915,-1.2960484,neighbor,257219404,Exploring the Limits of Language Modeling,0.05737292766571045,#8c564b
-12.36285,0.39155895,neighbor,257219404,Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints,0.05747288465499878,#8c564b
4.4662,-1.973252,neighbor,257219404,Improving language models by retrieving from trillions of tokens,0.05756109952926636,#8c564b
12.09745,2.2880743,neighbor,257219404,Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information,0.05756121873855591,#8c564b
3.9368935,9.35604,neighbor,257219404,Empirical Evaluation of Post-Training Quantization Methods for Language Tasks,0.057650625705718994,#8c564b
-2.456888,4.1585436,neighbor,257219404,Lessons Learned from Applying off-the-shelf BERT: There is no Silver Bullet,0.05768972635269165,#8c564b
-9.911605,-5.3625636,neighbor,257219404,LMentry: A Language Model Benchmark of Elementary Language Tasks,0.05776393413543701,#8c564b
-2.348375,-9.568702,neighbor,257219404,RobBERT: a Dutch RoBERTa-based Language Model,0.05781739950180054,#8c564b
-2.195746,-0.06491802,neighbor,257219404,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,0.05782473087310791,#8c564b
-8.350012,-2.0796905,neighbor,257219404,Understanding BLOOM: An empirical study on diverse NLP tasks,0.05782723426818848,#8c564b
-8.79356,0.11359,neighbor,257219404,PaLM: Scaling Language Modeling with Pathways,0.057883501052856445,#8c564b
8.281154,-0.32667223,neighbor,257219404,Multi-scale Transformer Language Models,0.05792427062988281,#8c564b
4.226799,-6.9445705,neighbor,257219404,What the [MASK]? Making Sense of Language-Specific BERT Models,0.0579984188079834,#8c564b
8.74055,-1.4576169,neighbor,257219404,Language Models with Transformers,0.058010876178741455,#8c564b
-4.103557,-3.5364661,neighbor,257219404,WeLM: A Well-Read Pre-trained Language Model for Chinese,0.05824708938598633,#8c564b
1.543186,8.474415,neighbor,257219404,Exploring Extreme Parameter Compression for Pre-trained Language Models,0.0582539439201355,#8c564b
6.0398006,-4.2890058,neighbor,257219404,N-Grammer: Augmenting Transformers with latent n-grams,0.05831646919250488,#8c564b
-9.605318,-0.34045014,neighbor,257219404,Scaling Instruction-Finetuned Language Models,0.05837982892990112,#8c564b
-9.061304,9.422607,neighbor,257219404,One-Size-Fits-All Multilingual Models,0.05838358402252197,#8c564b
-0.41115537,-5.975371,neighbor,257219404,MC-BERT: Efficient Language Pre-Training via a Meta Controller,0.058433473110198975,#8c564b
-4.657418,4.5642138,neighbor,257219404,Contemporary NLP Modeling in Six Comprehensive Programming Assignments,0.05846661329269409,#8c564b
-10.782182,-2.624092,neighbor,257219404,The Stack: 3 TB of permissively licensed source code,0.05849778652191162,#8c564b
3.8574033,7.2477617,neighbor,257219404,Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT,0.05850625038146973,#8c564b
-1.241085,5.560126,neighbor,257219404,EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets,0.05859416723251343,#8c564b
11.778991,-2.9417653,neighbor,257219404,On the State of the Art of Evaluation in Neural Language Models,0.05861848592758179,#8c564b
-1.8036697,-9.443103,neighbor,257219404,BERTje: A Dutch BERT Model,0.05871456861495972,#8c564b
2.9175265,3.24254,neighbor,257219404,Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection,0.05872875452041626,#8c564b
0.43644148,8.443699,neighbor,257219404,RefBERT: Compressing BERT by Referencing to Pre-computed Representations,0.05874878168106079,#8c564b
3.2380226,-10.120616,neighbor,257219404,PaLM: A Hybrid Parser and Language Model,0.058889150619506836,#8c564b
-1.7556895,-11.256088,neighbor,257219404,RigoBERTa: A State-of-the-Art Language Model For Spanish,0.05889385938644409,#8c564b
9.016232,-4.3630075,neighbor,257219404,Transformers: State-of-the-Art Natural Language Processing,0.05891096591949463,#8c564b
-5.897004,-11.585988,neighbor,257219404,PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Transformers for Patronizing and Condescending Language Detection,0.05898779630661011,#8c564b
8.883036,5.610887,neighbor,257219404,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,0.05912601947784424,#8c564b
-10.0180645,3.5199242,neighbor,257219404,FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference,0.05916798114776611,#8c564b
13.22062,3.0115912,neighbor,257219404,Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality,0.059169888496398926,#8c564b
-11.523774,3.9608996,neighbor,257219404,Elbert: Fast Albert with Confidence-Window Based Early Exit,0.059180378913879395,#8c564b
-5.965619,9.398778,neighbor,257219404,MonoByte: A Pool of Monolingual Byte-level Language Models,0.05919790267944336,#8c564b
-1.1345725,10.4175,query,259950998,Llama 2: Open Foundation and Fine-Tuned Chat Models,0.0,#c49c94
-0.25844228,10.631019,neighbor,259950998,Enhancing Chat Language Models by Scaling High-quality Instructional Conversations,0.033985018730163574,#c49c94
3.7355556,9.776419,neighbor,259950998,"ChatGPT: Fundamentals, Applications and Social Impacts",0.037181973457336426,#c49c94
-1.1298944,13.528567,neighbor,259950998,Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models,0.03826385736465454,#c49c94
-2.6182446,4.8494725,neighbor,259950998,Towards a Human-like Open-Domain Chatbot,0.03941124677658081,#c49c94
0.80249465,12.08171,neighbor,259950998,"ChatGPT may Pass the Bar Exam soon, but has a Long Way to Go for the LexGLUE benchmark",0.04119229316711426,#c49c94
-2.8981323,5.5920267,neighbor,259950998,Recipes for Building an Open-Domain Chatbot,0.04123067855834961,#c49c94
4.5386295,13.318706,neighbor,259950998,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,0.04151862859725952,#c49c94
-0.81302524,12.66103,neighbor,259950998,Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation,0.04154527187347412,#c49c94
1.2522217,-9.205051,neighbor,259950998,The Gutenberg Dialogue Dataset,0.04266953468322754,#c49c94
-3.5999088,2.9806697,neighbor,259950998,Deep Chit-Chat: Deep Learning for Chatbots,0.043301284313201904,#c49c94
-6.259645,0.7640046,neighbor,259950998,A scaled‐down neural conversational model for chatbots,0.043768107891082764,#c49c94
-1.5784786,-12.210835,neighbor,259950998,Beyond Goldfish Memory: Long-Term Open-Domain Conversation,0.04392576217651367,#c49c94
3.5893948,-1.5265275,neighbor,259950998,Automatic Evaluation and Moderation of Open-domain Dialogue Systems,0.044078290462493896,#c49c94
-3.76905,-4.7596617,neighbor,259950998,Improving Neural Conversational Models with Entropy-Based Data Filtering,0.04413485527038574,#c49c94
-2.2455509,7.390244,neighbor,259950998,Low-Resource Adaptation of Open-Domain Generative Chatbots,0.04436540603637695,#c49c94
-0.8382325,6.2188954,neighbor,259950998,Prompted LLMs as Chatbot Modules for Long Open-domain Conversation,0.044685959815979004,#c49c94
5.197098,-6.7704678,neighbor,259950998,AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models,0.044701457023620605,#c49c94
5.4877443,8.757637,neighbor,259950998,Is ChatGPT Equipped with Emotional Dialogue Capabilities?,0.04482996463775635,#c49c94
-11.446511,2.0981982,neighbor,259950998,Microsoft Icecaps: An Open-Source Toolkit for Conversation Modeling,0.04490166902542114,#c49c94
-2.8565807,11.029948,neighbor,259950998,XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,0.045346081256866455,#c49c94
2.2709904,-1.3939955,neighbor,259950998,ChatEval: A Tool for the Systematic Evaluation of Chatbots,0.04544287919998169,#c49c94
-0.20307118,7.3231635,neighbor,259950998,WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia,0.04577231407165527,#c49c94
-7.4482956,0.19928437,neighbor,259950998,Modeling Situations in Neural Chat Bots,0.04584145545959473,#c49c94
4.6492977,-0.9568655,neighbor,259950998,Talking with Machines: A Comprehensive Survey of Emergent Dialogue Systems,0.04588085412979126,#c49c94
-0.4967758,-5.964372,neighbor,259950998,Taming the Beast: Learning to Control Neural Conversational Models,0.04604619741439819,#c49c94
-3.211057,1.1716228,neighbor,259950998,The Second Conversational Intelligence Challenge (ConvAI2),0.0464823842048645,#c49c94
-3.8115795,-3.9495623,neighbor,259950998,Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances,0.046667397022247314,#c49c94
0.3693222,9.4609585,neighbor,259950998,Teaching Models new APIs: Domain-Agnostic Simulators for Task Oriented Dialogue,0.04667264223098755,#c49c94
1.5132223,0.074490435,neighbor,259950998,How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation,0.04689079523086548,#c49c94
6.821554,6.8118315,neighbor,259950998,Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback,0.047037720680236816,#c49c94
-0.3276526,2.5282981,neighbor,259950998,FinChat: Corpus and evaluation setup for Finnish chat conversations on everyday topics,0.047090888023376465,#c49c94
5.1095414,-2.071014,neighbor,259950998,Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark,0.04724764823913574,#c49c94
1.3944464,-2.91307,neighbor,259950998,ParlAI: A Dialog Research Software Platform,0.04740685224533081,#c49c94
4.4892354,12.614373,neighbor,259950998,ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning,0.04759639501571655,#c49c94
6.0352216,4.1769457,neighbor,259950998,XDAI: A Tuning-free Framework for Exploiting Pre-trained Language Models in Knowledge Grounded Dialogue Generation,0.04767942428588867,#c49c94
4.4419475,8.433699,neighbor,259950998,"A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development",0.047763943672180176,#c49c94
2.5856016,-8.994569,neighbor,259950998,Large Language Models Meet Harry Potter: A Bilingual Dataset for Aligning Dialogue Agents with Characters,0.04804956912994385,#c49c94
0.70184314,13.74554,neighbor,259950998,Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning,0.04822820425033569,#c49c94
6.4446044,14.144557,neighbor,259950998,Large Language Models,0.04827165603637695,#c49c94
2.1409643,-10.33471,neighbor,259950998,PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs,0.04860907793045044,#c49c94
-7.548802,-5.040906,neighbor,259950998,A Repository of Conversational Datasets,0.04917466640472412,#c49c94
2.8945036,8.927056,neighbor,259950998,a survey on GPT-3,0.049398601055145264,#c49c94
8.610786,9.41001,neighbor,259950998,Assessing the efficacy of large language models in generating accurate teacher responses,0.04939866065979004,#c49c94
-2.2574515,-1.7399468,neighbor,259950998,SMRTer Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multi-Reference Training,0.04945337772369385,#c49c94
-5.2188005,1.6145141,neighbor,259950998,Deep Learning Based Chatbot Models,0.04952484369277954,#c49c94
-1.3182324,17.65079,neighbor,259950998,Can we trust the evaluation on ChatGPT?,0.04954403638839722,#c49c94
-3.010761,-6.3772345,neighbor,259950998,An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation,0.04955399036407471,#c49c94
4.6461554,15.076539,neighbor,259950998,Incorporating Fairness in Large Scale NLU Systems,0.04956352710723877,#c49c94
6.0746055,12.777092,neighbor,259950998,Several categories of Large Language Models (LLMs): A Short Survey,0.049591779708862305,#c49c94
-5.3680196,6.4021664,neighbor,259950998,Sounding Board: A User-Centric and Content-Driven Social Chatbot,0.049608707427978516,#c49c94
-3.1448584,16.901388,neighbor,259950998,Bring Your Own Data! Self-Supervised Evaluation for Large Language Models,0.04967218637466431,#c49c94
4.9864078,-8.0598,neighbor,259950998,A Comparative Study on Language Models for Task-Oriented Dialogue Systems,0.04971814155578613,#c49c94
-1.3198311,-11.429161,neighbor,259950998,"Re³Dial: Retrieve, Reorganize and Rescale Conversations for Long-Turn Open-Domain Dialogue Pre-training",0.049723148345947266,#c49c94
5.686911,-8.695968,neighbor,259950998,Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System,0.049885571002960205,#c49c94
4.436653,-7.323164,neighbor,259950998,Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System,0.04993635416030884,#c49c94
-10.611266,-2.2427938,neighbor,259950998,"ChatGPT: Applications, Opportunities, and Threats",0.05002957582473755,#c49c94
-2.8881161,-8.327209,neighbor,259950998,Training End-to-End Dialogue Systems with the Ubuntu Dialogue Corpus,0.05013716220855713,#c49c94
-2.4287887,0.39155737,neighbor,259950998,TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents,0.05017954111099243,#c49c94
-4.9724317,3.6486108,neighbor,259950998,Experimental Research on Encoder-Decoder Architectures with Attention for Chatbots,0.05039137601852417,#c49c94
8.492303,-8.873001,neighbor,259950998,Fine-tuning techniques and data augmentation on transformer-based models for conversational texts and noisy user-generated content,0.05049490928649902,#c49c94
-8.220593,-8.444532,neighbor,259950998,Building a Swedish Open-Domain Conversational Language Model,0.05054432153701782,#c49c94
-6.409023,-1.2869262,neighbor,259950998,A Knowledge-Grounded Neural Conversation Model,0.0505681037902832,#c49c94
-1.5932825,3.013592,neighbor,259950998,Conversational Intelligence Challenge: Accelerating Research with Crowd Science and Open Source,0.050577759742736816,#c49c94
-6.05716,4.0594325,neighbor,259950998,A Deep Reinforcement Learning Chatbot,0.05072891712188721,#c49c94
6.8064537,-5.8445377,neighbor,259950998,AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,0.05073636770248413,#c49c94
-1.4193413,-9.551854,neighbor,259950998,A Large-Scale Chinese Short-Text Conversation Dataset,0.05074864625930786,#c49c94
-6.291301,7.677836,neighbor,259950998,Rewarding Chatbots for Real-World Engagement with Millions of Users,0.05089491605758667,#c49c94
-5.3697495,-3.0884013,neighbor,259950998,"Designing dialogue systems: A mean, grumpy, sarcastic chatbot in the browser",0.05107319355010986,#c49c94
-0.33249477,-0.86279273,neighbor,259950998,Learning an Unreferenced Metric for Online Dialogue Evaluation,0.05108577013015747,#c49c94
-7.52972,1.9636596,neighbor,259950998,Investigating Deep Learning for Predicting Multi-linguistic Interactions with a Chatterbot,0.051098644733428955,#c49c94
-0.83024615,16.452246,neighbor,259950998,AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback,0.05111783742904663,#c49c94
-5.1347227,11.196215,neighbor,259950998,MemoryBank: Enhancing Large Language Models with Long-Term Memory,0.051369428634643555,#c49c94
3.0292013,6.022025,neighbor,259950998,PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation,0.0514790415763855,#c49c94
-2.2936132,-4.2813573,neighbor,259950998,Probing Neural Dialog Models for Conversational Understanding,0.0514870285987854,#c49c94
3.2617018,2.1932797,neighbor,259950998,Local Knowledge Powered Conversational Agents,0.05156034231185913,#c49c94
8.135467,6.53604,neighbor,259950998,LaMDA: Language Models for Dialog Applications,0.051704466342926025,#c49c94
-11.083616,3.4074917,neighbor,259950998,DeepPavlov: Open-Source Library for Dialogue Systems,0.051713526248931885,#c49c94
8.072375,0.65289956,neighbor,259950998,Effortless Integration of Memory Management into Open-Domain Conversation Systems,0.05171924829483032,#c49c94
-0.35886437,-5.0209584,neighbor,259950998,Deep Learning for Conversational AI,0.0517544150352478,#c49c94
6.561411,3.7736647,neighbor,259950998,Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters,0.05175495147705078,#c49c94
-5.2376995,-5.8496003,neighbor,259950998,Viola: A Topic Agnostic Generate-and-Rank Dialogue System,0.05177021026611328,#c49c94
-10.452592,-3.5351572,neighbor,259950998,Learn What NOT to Learn: Towards Generative Safety in Chatbots,0.05183219909667969,#c49c94
7.616018,-4.972249,neighbor,259950998,PLACES: Prompting Language Models for Social Conversation Synthesis,0.05189073085784912,#c49c94
9.836477,-3.9174016,neighbor,259950998,LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models,0.05194246768951416,#c49c94
4.2444253,-2.878224,neighbor,259950998,"ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems",0.051965951919555664,#c49c94
0.5721879,15.261628,neighbor,259950998,How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources,0.052221059799194336,#c49c94
2.8883386,12.182837,neighbor,259950998,Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding,0.05227160453796387,#c49c94
-5.045686,-7.7290225,neighbor,259950998,Data-Efficient Methods for Dialogue Systems,0.05235785245895386,#c49c94
5.00177,5.6654725,neighbor,259950998,PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation,0.0525398850440979,#c49c94
2.0555222,-6.33978,neighbor,259950998,Pretraining the Noisy Channel Model for Task-Oriented Dialogue,0.05255448818206787,#c49c94
-2.8091478,-11.394563,neighbor,259950998,Coherent Dialogue with Attention-Based Language Models,0.052591919898986816,#c49c94
2.1016273,1.3245077,neighbor,259950998,XPersona: Evaluating Multilingual Personalized Chatbot,0.05268430709838867,#c49c94
4.241916,-5.768855,neighbor,259950998,An Empirical Study of Multitask Learning to Improve Open Domain Dialogue Systems,0.052721381187438965,#c49c94
-6.0974593,-7.6854916,neighbor,259950998,Plug-and-Play Conversational Models,0.052806317806243896,#c49c94
-4.1075644,-9.151091,neighbor,259950998,AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses,0.05282849073410034,#c49c94
-8.308368,5.4562745,neighbor,259950998,"Learning from Dialogue after Deployment: Feed Yourself, Chatbot!",0.05286496877670288,#c49c94
-1.0623919,-0.38803416,neighbor,259950998,PoE: A Panel of Experts for Generalized Automatic Dialogue Assessment,0.05307137966156006,#c49c94
1.1092478,15.856924,neighbor,259950998,Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance,0.05332905054092407,#c49c94
-5.310405,-0.19639744,neighbor,259950998,Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling,0.053344786167144775,#c49c94
-2.6069536,13.880793,neighbor,259950998,llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology,0.053374528884887695,#c49c94
-9.650274,-2.2728646,query,28695052,Proximal Policy Optimization Algorithms,0.0,#c49c94
-7.5199685,-1.8954542,neighbor,28695052,Expected Policy Gradients,0.037559449672698975,#c49c94
-10.967236,-2.291575,neighbor,28695052,Trust Region Policy Optimization,0.04080384969711304,#c49c94
-6.2133245,0.70718825,neighbor,28695052,Policy gradient methods,0.04319185018539429,#c49c94
-3.164405,-7.395361,neighbor,28695052,Linear Off-Policy Actor-Critic,0.044272661209106445,#c49c94
-6.465151,-6.1871967,neighbor,28695052,PGQ: Combining policy gradient and Q-learning,0.045860886573791504,#c49c94
-9.911658,1.524783,neighbor,28695052,Efficient Sample Reuse in Policy Gradients with Parameter-Based Exploration,0.04587280750274658,#c49c94
-7.501865,-3.9428408,neighbor,28695052,Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic,0.04608321189880371,#c49c94
-12.285746,-6.2080226,neighbor,28695052,Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces,0.04743748903274536,#c49c94
-6.9401507,0.10825959,neighbor,28695052,Policy Gradient Methods for Off-policy Control,0.048908889293670654,#c49c94
5.395106,-5.8591895,neighbor,28695052,OFFER: Off-Environment Reinforcement Learning,0.04978609085083008,#c49c94
-6.6171227,-3.3782337,neighbor,28695052,Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning,0.050309062004089355,#c49c94
-13.350375,-6.265859,neighbor,28695052,Finite-Sample Analysis of Proximal Gradient TD Algorithms,0.0505293607711792,#c49c94
7.2662764,1.1814584,neighbor,28695052,Model-Free Imitation Learning with Policy Optimization,0.05070602893829346,#c49c94
5.5119357,-3.9818704,neighbor,28695052,Experience Replay Using Transition Sequences,0.05091500282287598,#c49c94
0.42697662,9.839944,neighbor,28695052,Continuous Deep Q-Learning with Model-based Acceleration,0.05119287967681885,#c49c94
-4.8676887,12.74788,neighbor,28695052,Constrained Policy Optimization,0.0512126088142395,#c49c94
-12.198805,-2.5636685,neighbor,28695052,Trust-PCL: An Off-Policy Trust Region Method for Continuous Control,0.051660239696502686,#c49c94
-5.1332326,-2.0971591,neighbor,28695052,Stein Variational Policy Gradient,0.05180400609970093,#c49c94
-2.2894886,4.644072,neighbor,28695052,Exploring parameter space in reinforcement learning,0.0528179407119751,#c49c94
-2.1597023,-8.420209,neighbor,28695052,Investigating Practical Linear Temporal Difference Learning,0.05340367555618286,#c49c94
-1.3431672,12.188137,neighbor,28695052,Reinforcement Learning with Deep Energy-Based Policies,0.0538211464881897,#c49c94
0.7896924,-14.157327,neighbor,28695052,Approximate Bayes Optimal Policy Search using Neural Networks,0.054057538509368896,#c49c94
-1.0139921,-12.221361,neighbor,28695052,A Tutorial on Linear Function Approximators for Dynamic Programming and Reinforcement Learning,0.054057538509368896,#c49c94
-4.8563557,-3.5092828,neighbor,28695052,Easy Monotonic Policy Iteration,0.05417114496231079,#c49c94
-0.8194822,-2.980649,neighbor,28695052,Equivalence Between Policy Gradients and Soft Q-Learning,0.05442851781845093,#c49c94
-9.142948,-8.361539,neighbor,28695052,Sample Efficient Actor-Critic with Experience Replay,0.054657042026519775,#c49c94
-10.87666,2.7660072,neighbor,28695052,Efficient Gradient Estimation for Motor Control Learning,0.054743826389312744,#c49c94
-4.374973,-13.25586,neighbor,28695052,Integral Policy Iterations for Reinforcement Learning Problems in Continuous Time and Space,0.055001020431518555,#c49c94
-1.2180783,4.0470624,neighbor,28695052,Parameter Space Noise for Exploration,0.055125296115875244,#c49c94
5.010688,-2.069182,neighbor,28695052,Scaling Up Reinforcement Learning through Targeted Exploration,0.05555695295333862,#c49c94
-3.4287002,14.087384,neighbor,28695052,High-Dimensional Continuous Control Using Generalized Advantage Estimation,0.05590730905532837,#c49c94
6.2195883,11.01253,neighbor,28695052,Reinforcement Learning in Robotics: Applications and Real-World Challenges,0.05615246295928955,#c49c94
6.70697,5.8515396,neighbor,28695052,Hindsight Experience Replay,0.05621820688247681,#c49c94
-5.665962,13.7361355,neighbor,28695052,Guided Policy Search as Approximate Mirror Descent,0.05627179145812988,#c49c94
1.8746113,-10.685441,neighbor,28695052,Rollout sampling approximate policy iteration,0.056383371353149414,#c49c94
-1.6916338,9.18966,neighbor,28695052,Actor-critic versus direct policy search: a comparison based on sample complexity,0.05670219659805298,#c49c94
-2.3822377,-1.0242146,neighbor,28695052,Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies,0.056958913803100586,#c49c94
10.086545,-6.806224,neighbor,28695052,Inverse Reinforcement Learning through Policy Gradient Minimization,0.05748188495635986,#c49c94
4.7416606,11.301747,neighbor,28695052,Robot Skill Learning: From Reinforcement Learning to Evolution Strategies,0.057620465755462646,#c49c94
-8.338814,4.2734146,neighbor,28695052,Reinforcement Learning by Value Gradients,0.057718515396118164,#c49c94
6.1373525,-8.296832,neighbor,28695052,Efficient Probabilistic Performance Bounds for Inverse Reinforcement Learning,0.05774587392807007,#c49c94
6.8534636,15.945681,neighbor,28695052,Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret,0.057776033878326416,#c49c94
-4.0095696,-6.0158033,neighbor,28695052,Revisiting stochastic off-policy action-value gradients,0.05788576602935791,#c49c94
2.12062,-3.127342,neighbor,28695052,Taming the Noise in Reinforcement Learning via Soft Updates,0.0580480694770813,#c49c94
1.381576,3.4986563,neighbor,28695052,Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models,0.058068037033081055,#c49c94
3.7117743,-8.506185,neighbor,28695052,High-Confidence Off-Policy Evaluation,0.05811452865600586,#c49c94
-6.347154,-7.9447803,neighbor,28695052,Safe and Efficient Off-Policy Reinforcement Learning,0.05815058946609497,#c49c94
6.3169456,7.5890374,neighbor,28695052,Deep Reinforcement Learning from Human Preferences,0.0582161545753479,#c49c94
-4.53966,0.9431511,neighbor,28695052,Policy Gradient Methods for Reinforcement Learning with Function Approximation and Action-Dependent Baselines,0.05851888656616211,#c49c94
2.0881932,-0.14885707,neighbor,28695052,Off-Policy Shaping Ensembles in Reinforcement Learning,0.05862164497375488,#c49c94
0.47802016,10.75065,neighbor,28695052,Continuous control with deep reinforcement learning,0.05863136053085327,#c49c94
-13.0027485,0.7915232,neighbor,28695052,A K-fold Method for Baseline Estimation in Policy Gradient Algorithms,0.05867934226989746,#c49c94
-8.887122,4.8641186,neighbor,28695052,"The Local Optimality of Reinforcement Learning by Value Gradients, and its Relationship to Policy Gradient Learning",0.05882197618484497,#c49c94
-8.342582,-8.7753315,neighbor,28695052,The Reactor: A Sample-Efficient Actor-Critic Architecture,0.0589442253112793,#c49c94
8.078722,11.344756,neighbor,28695052,Information Theoretically Aided Reinforcement Learning for Embodied Agents,0.058944523334503174,#c49c94
-1.8257579,-4.0783143,neighbor,28695052,Policy Tree: Adaptive Representation for Policy Gradient,0.05896556377410889,#c49c94
1.9112513,-8.332506,neighbor,28695052,Improving Approximate Value Iteration with Complex Returns by Bounding,0.05950695276260376,#c49c94
3.4363167,-9.335936,neighbor,28695052,High Confidence Off-Policy Evaluation with Models,0.06036454439163208,#c49c94
2.7457793,4.5505743,neighbor,28695052,Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains,0.06060582399368286,#c49c94
1.3302866,6.7657514,neighbor,28695052,Playing Atari with Deep Reinforcement Learning,0.0606311559677124,#c49c94
10.702307,-2.4455202,neighbor,28695052,Relative Entropy Policy Search,0.06071889400482178,#c49c94
6.7944603,3.6267786,neighbor,28695052,V-MIN: Efficient Reinforcement Learning through Demonstrations and Relaxed Reward Demands,0.06117731332778931,#c49c94
-1.5715463,-10.380668,neighbor,28695052,Mixing-Time Regularized Policy Gradient,0.061244845390319824,#c49c94
1.485854,12.365815,neighbor,28695052,Reset-free guided policy search: Efficient deep reinforcement learning with stochastic initial states,0.06138467788696289,#c49c94
1.4746516,-15.095029,neighbor,28695052,Cover tree Bayesian reinforcement learning,0.061437010765075684,#c49c94
4.1324325,9.5217905,neighbor,28695052,Layered direct policy search for learning hierarchical skills,0.06147968769073486,#c49c94
-1.9821186,-12.85139,neighbor,28695052,Dynamic policy programming,0.06158852577209473,#c49c94
6.891689,-2.2525785,neighbor,28695052,Efficient Average Reward Reinforcement Learning Using Constant Shifting Values,0.06161147356033325,#c49c94
6.4218807,10.223147,neighbor,28695052,Reinforcement learning in robotics: A survey,0.06163817644119263,#c49c94
6.139583,2.2660606,neighbor,28695052,Imitation Learning with Demonstrations and Shaping Rewards,0.06168794631958008,#c49c94
5.4068317,-12.463843,neighbor,28695052,Policy Improvement for POMDPs Using Normalized Importance Sampling,0.06191682815551758,#c49c94
-5.998408,2.859525,neighbor,28695052,Gradient-based Reinforcement Planning in Policy-Search Methods,0.061918437480926514,#c49c94
5.696673,14.341521,neighbor,28695052,Robotic Search & Rescue via Online Multi-task Reinforcement Learning,0.06193196773529053,#c49c94
-2.4755502,15.149773,neighbor,28695052,Path integral guided policy search,0.06198155879974365,#c49c94
-3.9136987,-9.617099,neighbor,28695052,Multi-step Off-policy Learning Without Importance Sampling Ratios,0.06200695037841797,#c49c94
8.569931,-8.2138195,neighbor,28695052,Improving the efficiency of Bayesian inverse reinforcement learning,0.062026262283325195,#c49c94
4.0547104,14.751192,neighbor,28695052,Collective robot reinforcement learning with distributed asynchronous guided policy search,0.06204253435134888,#c49c94
-1.5138468,-7.5118365,neighbor,28695052,Multi-step Reinforcement Learning: A Unifying Algorithm,0.06218600273132324,#c49c94
-0.25255403,-1.6112959,neighbor,28695052,Deep Reinforcement Learning: An Overview,0.06228142976760864,#c49c94
9.776829,7.2597194,neighbor,28695052,SEAPoT-RL: Selective Exploration Algorithm for Policy Transfer in RL,0.06228703260421753,#c49c94
5.368659,6.531779,neighbor,28695052,Deep reward shaping from demonstrations,0.0622982382774353,#c49c94
10.91187,7.1491995,neighbor,28695052,Successor Features for Transfer in Reinforcement Learning,0.0624273419380188,#c49c94
2.2770586,5.2863555,neighbor,28695052,Angrier Birds: Bayesian reinforcement learning,0.06246715784072876,#c49c94
-9.782579,6.9853735,neighbor,28695052,Real-Time Reinforcement Learning of Constrained Markov Decision Processes with Weak Derivatives,0.06259554624557495,#c49c94
4.0892982,0.8988183,neighbor,28695052,Expressing Arbitrary Reward Functions as Potential-Based Advice,0.06261569261550903,#c49c94
9.591251,-9.5641575,neighbor,28695052,Inverse reinforcement learning with Gaussian process,0.06261932849884033,#c49c94
2.1225982,13.369477,neighbor,28695052,Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates,0.06262177228927612,#c49c94
9.204811,-7.382877,neighbor,28695052,Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics,0.06273478269577026,#c49c94
8.022974,16.575565,neighbor,28695052,Scaling life-long off-policy learning,0.06275177001953125,#c49c94
-3.3515306,16.955463,neighbor,28695052,Sparse Latent Space Policy Search,0.06278568506240845,#c49c94
-14.339683,3.8354957,neighbor,28695052,Contextual Relative Entropy Policy Search with Covariance Matrix Adaptation,0.06288045644760132,#c49c94
-8.756542,-10.702187,neighbor,28695052,Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning,0.06288355588912964,#c49c94
-6.7081165,12.296426,neighbor,28695052,State-Regularized Policy Search for Linearized Dynamical Systems,0.06288880109786987,#c49c94
9.655888,1.4210447,neighbor,28695052,Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration,0.063213050365448,#c49c94
-7.888407,2.3334212,neighbor,28695052,The Optimal Reward Baseline for Gradient-Based Reinforcement Learning,0.06325256824493408,#c49c94
-4.092675,9.69464,neighbor,28695052,Black-box data-efficient policy search for robotics,0.06354039907455444,#c49c94
-0.4929135,-0.049928278,neighbor,28695052,Value Function Approximation in Reinforcement Learning Using the Fourier Basis,0.06361502408981323,#c49c94
1.1955982,14.709612,neighbor,28695052,Deep predictive policy training using reinforcement learning,0.06377053260803223,#c49c94
-0.29067412,3.3039868,neighbor,28695052,Noisy Networks for Exploration,0.06384038925170898,#c49c94
2.7104557,-4.0454516,neighbor,28695052,Reinforcement Learning under Model Mismatch,0.06394076347351074,#c49c94
-2.7850058,-2.1825423,query,3144218,Semi-Supervised Classification with Graph Convolutional Networks,0.0,#c49c94
-3.1189442,-2.635214,neighbor,3144218,Learning Convolutional Neural Networks for Graphs,0.04006701707839966,#c49c94
9.62435,-1.4752971,neighbor,3144218,Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction,0.04809248447418213,#c49c94
-2.555547,-3.9834905,neighbor,3144218,Deep Convolutional Networks on Graph-Structured Data,0.053433239459991455,#c49c94
-4.2561135,-2.0633192,neighbor,3144218,Diffusion-Convolutional Neural Networks,0.056846797466278076,#c49c94
-3.003087,-0.6760349,neighbor,3144218,subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs,0.05744975805282593,#c49c94
-5.6900334,-4.761636,neighbor,3144218,Propagation kernels: efficient graph kernels from propagated information,0.06076323986053467,#c49c94
6.625206,-1.8060981,neighbor,3144218,Robust Semi-Supervised Classification for Multi-Relational Graphs,0.06384116411209106,#c49c94
-5.5383563,-1.8848797,neighbor,3144218,Classifying Network Data with Deep Kernel Machines,0.06652086973190308,#c49c94
-4.715252,1.5685831,neighbor,3144218,node2vec: Scalable Feature Learning for Networks,0.0673133134841919,#c49c94
-2.9652195,-4.365853,neighbor,3144218,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,0.06755632162094116,#c49c94
3.8980865,-1.4136645,neighbor,3144218,Semi-Supervised Collective Classification via Hybrid Label Regularization,0.06932187080383301,#c49c94
-5.8487353,-6.25179,neighbor,3144218,Graph Kernels via Functional Embedding,0.07121390104293823,#c49c94
-6.512517,3.9434903,neighbor,3144218,DeepWalk: online learning of social representations,0.071544349193573,#c49c94
2.991739,8.096028,neighbor,3144218,Omnigraph: Rich Representation and Graph Kernel Learning,0.07308012247085571,#c49c94
10.570492,-6.1481395,neighbor,3144218,Soft-Supervised Learning for Text Classification,0.07332336902618408,#c49c94
8.362998,-3.2917454,neighbor,3144218,Graph Construction with Label Information for Semi-Supervised Learning,0.07434910535812378,#c49c94
-1.422239,-5.369089,neighbor,3144218,Learning Deep Representations for Graph Clustering,0.0748034119606018,#c49c94
-7.0835485,-0.8278193,neighbor,3144218,Classifying networked entities with modularity kernels,0.07515162229537964,#c49c94
10.946298,-4.6379957,neighbor,3144218,Generalized Optimization Framework for Graph-based Semi-supervised Learning,0.0754041075706482,#c49c94
9.596449,-3.67547,neighbor,3144218,Large-Scale Graph-Based Semi-Supervised Learning via Tree Laplacian Solver,0.0768844485282898,#c49c94
-3.975275,-7.468697,neighbor,3144218,A Non-parametric Spectral Model for Graph Classification,0.07769155502319336,#c49c94
10.160078,-2.8300948,neighbor,3144218,Scaling Graph-based Semi Supervised Learning to Large Number of Labels Using Count-Min Sketch,0.07822912931442261,#c49c94
-6.95629,-5.048866,neighbor,3144218,Generalized Shortest Path Kernel on Graphs,0.07827150821685791,#c49c94
-5.72395,3.6788077,neighbor,3144218,Discriminative Deep Random Walk for Network Classification,0.07854562997817993,#c49c94
-0.9347449,-4.8227553,neighbor,3144218,Deep Neural Networks for Learning Graph Representations,0.07855051755905151,#c49c94
5.847813,6.561912,neighbor,3144218,A Minimalistic Approach to Sum-Product Network Learning for Real Applications,0.07882505655288696,#c49c94
-0.28300422,1.2287303,neighbor,3144218,Unsupervised modeling of object categories using link analysis techniques,0.07888251543045044,#c49c94
7.313379,-3.7688475,neighbor,3144218,Nonnegative Sparse and KNN graph for semi-supervised learning,0.07896941900253296,#c49c94
-6.602671,-4.3331523,neighbor,3144218,Tree-Based Kernel for Graphs With Continuous Attributes,0.07910782098770142,#c49c94
9.108911,-4.500933,neighbor,3144218,Semi-supervised Learning with Density Based Distances,0.07962620258331299,#c49c94
5.632291,3.38176,neighbor,3144218,Relational Similarity Machines,0.07984107732772827,#c49c94
-6.959627,-6.6405535,neighbor,3144218,Subgraph Matching Kernels for Attributed Graphs,0.08003705739974976,#c49c94
-5.5443287,2.491786,neighbor,3144218,LINE: Large-scale Information Network Embedding,0.0801398754119873,#c49c94
-6.97488,6.4599123,neighbor,3144218,Supervised random walks: predicting and recommending links in social networks,0.08029341697692871,#c49c94
4.015104,3.7608254,neighbor,3144218,Single Network Relational Transductive Learning,0.08039391040802002,#c49c94
-8.362942,-5.375982,neighbor,3144218,Subgraph Similarity Search in Large Graphs,0.08082389831542969,#c49c94
9.74477,1.7535093,neighbor,3144218,Tight Error Bounds for Structured Prediction,0.08113682270050049,#c49c94
2.1042488,2.363458,neighbor,3144218,A Boosting Approach to Learning Graph Representations,0.08131271600723267,#c49c94
7.881626,0.9120917,neighbor,3144218,Using Graphs of Classifiers to Impose Constraints on Semi-supervised Relation Extraction,0.08233356475830078,#c49c94
7.3262887,-5.8189144,neighbor,3144218,Deformed Graph Laplacian for Semisupervised Learning,0.08356720209121704,#c49c94
8.449368,7.670364,neighbor,3144218,Semantically Smooth Knowledge Graph Embedding,0.08398294448852539,#c49c94
7.9826584,-4.2452855,neighbor,3144218,Semi-supervised Data Representation via Affinity Graph Learning,0.08437877893447876,#c49c94
4.09047,7.4507074,neighbor,3144218,A semantic matching energy function for learning with multi-relational data,0.08486086130142212,#c49c94
5.2425785,-2.8810484,neighbor,3144218,Multi-Label Learning on Tensor Product Graph,0.08493036031723022,#c49c94
4.6597414,-6.576512,neighbor,3144218,Empirical stationary correlations for semi-supervised learning on graphs,0.08496755361557007,#c49c94
-7.161922,0.14386101,neighbor,3144218,Transductive Classification on Heterogeneous Information Networks with Edge Betweenness-based Normalization,0.0849754810333252,#c49c94
0.19380774,0.7084478,neighbor,3144218,Self-informed neural network structure learning,0.08521091938018799,#c49c94
6.9046845,-6.1682897,neighbor,3144218,ReLISH: Reliable Label Inference via Smoothness Hypothesis,0.08532696962356567,#c49c94
4.515301,4.6472406,neighbor,3144218,Transforming Graph Representations for Statistical Relational Learning,0.08568078279495239,#c49c94
-3.6759825,7.921771,neighbor,3144218,Exploring the structural regularities in networks,0.08592098951339722,#c49c94
6.3800545,-3.027756,neighbor,3144218,Graph-based semi-supervised learning with multi-label,0.08608704805374146,#c49c94
-1.1195586,4.125278,neighbor,3144218,Learning graphical models with hubs,0.08618885278701782,#c49c94
6.953464,-0.18481967,neighbor,3144218,Metric learning approach for graph-based label propagation,0.08635151386260986,#c49c94
8.73043,-5.5736156,neighbor,3144218,Semi-supervised learning with explicit relationship regularization,0.08646798133850098,#c49c94
3.632975,0.09984765,neighbor,3144218,Transductive Classification Methods for Mixed Graphs,0.08657288551330566,#c49c94
-4.97984,9.105972,neighbor,3144218,A Spin-Glass Model for Semi-Supervised Community Detection,0.0868343710899353,#c49c94
-7.6060467,7.7035785,neighbor,3144218,Exploiting longer cycles for link prediction in signed networks,0.08683526515960693,#c49c94
6.53428,7.827009,neighbor,3144218,Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases,0.08699870109558105,#c49c94
-4.4078274,8.693031,neighbor,3144218,Structure and inference in annotated networks,0.08764427900314331,#c49c94
7.5917854,9.076927,neighbor,3144218,Representation Learning of Knowledge Graphs with Entity Descriptions,0.08769893646240234,#c49c94
-7.6055527,4.975948,neighbor,3144218,On the network you keep: analyzing persons of interest using Cliqster,0.08773273229598999,#c49c94
-8.2205715,3.798899,neighbor,3144218,Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks,0.08782351016998291,#c49c94
4.449302,4.5805926,neighbor,3144218,Transforming Graph Data for Statistical Relational Learning,0.08804184198379517,#c49c94
5.264832,1.641597,neighbor,3144218,Metric learning pairwise kernel for graph inference,0.08868300914764404,#c49c94
-1.188788,-7.368769,neighbor,3144218,Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann Manifolds,0.08882063627243042,#c49c94
-4.5550947,1.7730445,neighbor,3144218,Learning Structural Features of Nodes in Large-Scale Networks for Link Prediction,0.08904004096984863,#c49c94
8.814177,8.681784,neighbor,3144218,Holographic Embeddings of Knowledge Graphs,0.08906620740890503,#c49c94
-11.213951,-1.008948,neighbor,3144218,Graph neural networks for ranking Web pages,0.08931618928909302,#c49c94
8.424461,-2.223464,neighbor,3144218,Regular graph construction for semi-supervised learning,0.08935338258743286,#c49c94
-4.314679,4.267498,neighbor,3144218,Discriminative Nonparametric Latent Feature Relational Models with Data Augmentation,0.08942502737045288,#c49c94
5.191086,-4.470249,neighbor,3144218,Semisupervised Classification Through the Bag-of-Paths Group Betweenness,0.08964431285858154,#c49c94
-4.1181498,-10.191364,neighbor,3144218,Learning Graph Matching,0.08992820978164673,#c49c94
3.8932364,-2.6203787,neighbor,3144218,Marginalized Denoising for Link Prediction and Multi-Label Learning,0.08993303775787354,#c49c94
-6.132031,-7.427253,neighbor,3144218,On Valid Optimal Assignment Kernels and Applications to Graph Classification,0.08995181322097778,#c49c94
-9.819122,-3.2926803,neighbor,3144218,DELTACON: A Principled Massive-Graph Similarity Function,0.09006106853485107,#c49c94
-2.5494747,6.8152475,neighbor,3144218,An Infinite Latent Attribute Model for Network Data,0.09028506278991699,#c49c94
5.484673,-5.972322,neighbor,3144218,Semi-Supervised Learning with Heterophily,0.09044963121414185,#c49c94
6.9676013,8.31889,neighbor,3144218,Knowledge Graph Completion with Adaptive Sparse Transfer Matrix,0.09052205085754395,#c49c94
-4.1053033,7.1411195,neighbor,3144218,Learning Latent Block Structure in Weighted Networks,0.0906115174293518,#c49c94
-6.404373,8.427931,neighbor,3144218,Discovering signature of social networks with application to community detection,0.0906745195388794,#c49c94
-0.16539621,-2.371182,neighbor,3144218,Continuous Conditional Random Fields for Efficient Regression in Large Fully Connected Graphs,0.09083431959152222,#c49c94
7.6462893,7.1184344,neighbor,3144218,Knowledge Graph Embedding by Translating on Hyperplanes,0.0908847451210022,#c49c94
1.346269,-6.7623878,neighbor,3144218,A Novel Regularized Principal Graph Learning Framework on Explicit Graph Representation,0.09092849493026733,#c49c94
3.4208713,-0.5439784,neighbor,3144218,Graph Based Classification Methods Using Inaccurate External Classifier Information,0.09093350172042847,#c49c94
-8.523974,-6.3733816,neighbor,3144218,The Structurally Smoothed Graphlet Kernel,0.09106999635696411,#c49c94
-3.1278086,9.933387,neighbor,3144218,Cross-validation estimate of the number of clusters in a network,0.0914376974105835,#c49c94
-5.4805894,-6.053812,neighbor,3144218,Graph Kernels,0.09155744314193726,#c49c94
-6.132396,6.3822637,neighbor,3144218,Unsupervised Feature Selection on Networks: A Generative View,0.092104971408844,#c49c94
5.838966,-9.189279,neighbor,3144218,Group and Graph Joint Sparsity for Linked Data Classification,0.09228450059890747,#c49c94
8.6209545,-7.04192,neighbor,3144218,Scaling Up Graph-Based Semisupervised Learning via Prototype Vector Machines,0.0923261046409607,#c49c94
-4.0306377,-10.173141,neighbor,3144218,Unsupervised Learning for Graph Matching,0.09239602088928223,#c49c94
11.445065,-0.91941196,neighbor,3144218,Semi-supervised Learning with Ladder Networks,0.09243232011795044,#c49c94
-7.4697833,-8.117391,neighbor,3144218,Graphlet-based lazy associative graph classification,0.09244722127914429,#c49c94
-8.821872,3.2673686,neighbor,3144218,Joint Inference of Multiple Label Types in Large Networks,0.09251070022583008,#c49c94
-1.5396848,4.3291183,neighbor,3144218,Learning Scale Free Network by Node Specific Degree Prior,0.09255123138427734,#c49c94
8.7473545,2.838541,neighbor,3144218,Structural Learning with Amortized Inference,0.09259587526321411,#c49c94
-3.0705214,8.323727,neighbor,3144218,Revealing the Hidden Language of Complex Networks,0.09290558099746704,#c49c94
-9.108319,5.9456344,neighbor,3144218,Data mapping by Restricted Boltzmann Machines for social circles detection,0.09307229518890381,#c49c94
7.9877315,7.648182,neighbor,3144218,From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction,0.09332060813903809,#c49c94
-0.93018144,7.026031,neighbor,3144218,Latent topics in graph-structured data,0.09337455034255981,#c49c94
-1.4370306,0.9090244,query,3162051,mixup: Beyond Empirical Risk Minimization,0.0,#e377c2
5.0923696,0.79390734,neighbor,3162051,EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples,0.046305716037750244,#e377c2
-0.18044709,-0.80142206,neighbor,3162051,Regularizing deep networks using efficient layerwise adversarial training,0.0470428466796875,#e377c2
7.1405907,1.2966985,neighbor,3162051,Towards Deep Learning Models Resistant to Adversarial Attacks,0.04854828119277954,#e377c2
-1.3496611,-5.6469474,neighbor,3162051,Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN,0.04874145984649658,#e377c2
-4.2191725,-3.3706925,neighbor,3162051,Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,0.04931175708770752,#e377c2
-1.9606591,-1.7113906,neighbor,3162051,Adversarial Dropout for Supervised and Semi-supervised Learning,0.05061447620391846,#e377c2
8.728056,-0.5579474,neighbor,3162051,Adversarial Robustness: Softmax versus Openmax,0.05074089765548706,#e377c2
10.117835,-0.43988112,neighbor,3162051,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods,0.05082458257675171,#e377c2
10.30414,4.4791107,neighbor,3162051,Discovering Adversarial Examples with Momentum,0.05084240436553955,#e377c2
5.807515,2.2982795,neighbor,3162051,Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test,0.05101478099822998,#e377c2
8.0111265,5.071698,neighbor,3162051,Ensemble Adversarial Training: Attacks and Defenses,0.051145076751708984,#e377c2
12.187531,2.7656028,neighbor,3162051,Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial Examples,0.051188647747039795,#e377c2
0.03904954,-5.59302,neighbor,3162051,Adversarial Transformation Networks: Learning to Generate Adversarial Examples,0.05122089385986328,#e377c2
-3.9107153,-9.367112,neighbor,3162051,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,0.051718711853027344,#e377c2
9.630431,-1.3690374,neighbor,3162051,Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models,0.05181586742401123,#e377c2
2.6201456,-2.2094479,neighbor,3162051,Towards Deep Neural Network Architectures Robust to Adversarial Examples,0.05209237337112427,#e377c2
-0.664799,-5.388201,neighbor,3162051,APE-GAN: Adversarial Perturbation Elimination with GAN,0.05239218473434448,#e377c2
-5.5636106,-11.483859,neighbor,3162051,Variance Regularizing Adversarial Learning,0.052522897720336914,#e377c2
3.9921002,2.0900483,neighbor,3162051,A Multi-strength Adversarial Training Method to Mitigate Adversarial Attacks,0.05266714096069336,#e377c2
2.0177662,-6.375857,neighbor,3162051,Are Accuracy and Robustness Correlated,0.05279010534286499,#e377c2
9.704791,-5.31287,neighbor,3162051,Stabilizing Adversarial Nets With Prediction Methods,0.05327385663986206,#e377c2
4.6072736,-1.726589,neighbor,3162051,DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks,0.05356156826019287,#e377c2
7.8533125,-0.065363176,neighbor,3162051,The Limitations of Deep Learning in Adversarial Settings,0.05371135473251343,#e377c2
7.269356,-0.846979,neighbor,3162051,Detecting Adversarial Samples from Artifacts,0.05372112989425659,#e377c2
-8.489938,-9.293915,neighbor,3162051,Improved Training of Wasserstein GANs,0.054266929626464844,#e377c2
-5.874498,-10.396953,neighbor,3162051,Dual Discriminator Generative Adversarial Nets,0.0546494722366333,#e377c2
8.979421,1.4255447,neighbor,3162051,Simple Black-Box Adversarial Perturbations for Deep Networks,0.054943203926086426,#e377c2
5.514197,-1.7181854,neighbor,3162051,Universal Adversarial Perturbations,0.055079102516174316,#e377c2
10.176371,4.7134376,neighbor,3162051,Boosting Adversarial Attacks with Momentum,0.05554366111755371,#e377c2
9.654763,1.9051465,neighbor,3162051,Machine Learning as an Adversarial Service: Learning Black-Box Adversarial Examples,0.056248605251312256,#e377c2
-1.1036415,-1.5487987,neighbor,3162051,Improving Back-Propagation by Adding an Adversarial Gradient,0.05644583702087402,#e377c2
0.9142838,-6.5093446,neighbor,3162051,Adversarial Diversity and Hard Positive Generation,0.05653727054595947,#e377c2
1.6042852,1.1267855,neighbor,3162051,Measuring Neural Net Robustness with Constraints,0.056937456130981445,#e377c2
-4.1696696,2.433907,neighbor,3162051,Swapout: Learning an ensemble of deep architectures,0.05714380741119385,#e377c2
1.2839386,-0.6413463,neighbor,3162051,Robust Convolutional Neural Networks under Adversarial Noise,0.05728602409362793,#e377c2
-4.5625677,-3.6180468,neighbor,3162051,Distributional Smoothing with Virtual Adversarial Training,0.05757021903991699,#e377c2
-4.112663,-1.436323,neighbor,3162051,Parseval Networks: Improving Robustness to Adversarial Examples,0.05789309740066528,#e377c2
-5.7105346,-9.818698,neighbor,3162051,Multi-Generator Generative Adversarial Nets,0.058219075202941895,#e377c2
1.9741892,-2.8565762,neighbor,3162051,Biologically inspired protection of deep networks from adversarial attacks,0.058380961418151855,#e377c2
3.7545125,-1.4438759,neighbor,3162051,Towards Robust Deep Neural Networks with BANG,0.0584980845451355,#e377c2
-3.9375412,-6.307228,neighbor,3162051,Adversarially Learned Inference,0.058764874935150146,#e377c2
-5.0582414,3.311319,neighbor,3162051,Maxout Networks,0.059075117111206055,#e377c2
-1.0641302,-7.317018,neighbor,3162051,Adversarial Images for Variational Autoencoders,0.059139907360076904,#e377c2
13.005067,0.2819351,neighbor,3162051,Extending Defensive Distillation,0.05950784683227539,#e377c2
-1.7690883,-7.980393,neighbor,3162051,Adversarial Examples for Generative Models,0.05955594778060913,#e377c2
-7.693619,-12.632883,neighbor,3162051,Towards Principled Methods for Training Generative Adversarial Networks,0.05972486734390259,#e377c2
-5.1941338,3.381901,neighbor,3162051,Improving Deep Neural Networks with Probabilistic Maxout Units,0.05981040000915527,#e377c2
-8.780001,-13.297263,neighbor,3162051,Generalization and Equilibrium in Generative Adversarial Nets (GANs),0.06044834852218628,#e377c2
-4.8590417,-12.259902,neighbor,3162051,Stabilizing GAN Training with Multiple Random Projections,0.060695111751556396,#e377c2
5.0686812,5.0846834,neighbor,3162051,Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks,0.06074833869934082,#e377c2
-6.0542374,4.7137074,neighbor,3162051,Information Dropout: Learning Optimal Representations Through Noisy Computation,0.061016082763671875,#e377c2
-3.3749623,-11.366441,neighbor,3162051,Softmax GAN,0.061128973960876465,#e377c2
-7.4257784,2.5060976,neighbor,3162051,Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach,0.06134063005447388,#e377c2
-6.908671,-10.921243,neighbor,3162051,Mode Regularized Generative Adversarial Networks,0.0613785982131958,#e377c2
10.572864,2.1260352,neighbor,3162051,MagNet: A Two-Pronged Defense against Adversarial Examples,0.061444222927093506,#e377c2
-6.852916,4.118081,neighbor,3162051,Regularizing Neural Networks by Penalizing Confident Output Distributions,0.0616152286529541,#e377c2
8.853907,6.060317,neighbor,3162051,Delving into Transferable Adversarial Examples and Black-box Attacks,0.06164830923080444,#e377c2
-7.2622576,-10.505497,neighbor,3162051,Unrolled Generative Adversarial Networks,0.06210613250732422,#e377c2
-7.8372893,-11.434134,neighbor,3162051,Stabilizing Training of Generative Adversarial Networks through Regularization,0.0623134970664978,#e377c2
12.704119,-0.45768318,neighbor,3162051,Towards Evaluating the Robustness of Neural Networks,0.06238788366317749,#e377c2
7.6811943,5.4319944,neighbor,3162051,Cascade Adversarial Machine Learning Regularized with a Unified Embedding,0.06244206428527832,#e377c2
6.6092052,-3.800255,neighbor,3162051,A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples,0.06279754638671875,#e377c2
12.029492,2.3255715,neighbor,3162051,Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks,0.0630180835723877,#e377c2
1.2518114,-5.2272573,neighbor,3162051,Synthesizing Robust Adversarial Examples,0.06356722116470337,#e377c2
7.977015,-2.0760367,neighbor,3162051,Detecting Adversarial Samples Using Density Ratio Estimates,0.06377464532852173,#e377c2
-3.9899094,-7.74153,neighbor,3162051,Least Squares Generative Adversarial Networks,0.06381916999816895,#e377c2
9.206821,-4.434214,neighbor,3162051,"On the Claim for the Existence of ""Adversarial Examples"" in Deep Learning Neural Networks",0.06388556957244873,#e377c2
10.142129,-2.853143,neighbor,3162051,Learning with a Strong Adversary,0.06391316652297974,#e377c2
0.65723187,-4.3392754,neighbor,3162051,Adversarial Manipulation of Deep Representations,0.06392580270767212,#e377c2
-7.7215257,2.2904491,neighbor,3162051,Deep Learning is Robust to Massive Label Noise,0.06409960985183716,#e377c2
-5.2715554,-6.2395253,neighbor,3162051,Associative Adversarial Networks,0.06412738561630249,#e377c2
-9.534843,-11.754303,neighbor,3162051,GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium,0.06423646211624146,#e377c2
5.4487343,-2.370794,neighbor,3162051,Analysis of universal adversarial perturbations,0.06425690650939941,#e377c2
-6.078348,-13.123156,neighbor,3162051,Dualing GANs,0.06427228450775146,#e377c2
-6.7760625,-11.593653,neighbor,3162051,Towards Understanding the Dynamics of Generative Adversarial Networks,0.06440597772598267,#e377c2
-5.6003366,-8.495991,neighbor,3162051,Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models,0.06461286544799805,#e377c2
5.64728,-0.14614177,neighbor,3162051,Fast Feature Fool: A data independent approach to universal adversarial perturbations,0.06477886438369751,#e377c2
4.6868987,-6.0844955,neighbor,3162051,Understanding deep learning requires rethinking generalization,0.06481927633285522,#e377c2
11.066232,-0.28145516,neighbor,3162051,Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong,0.06485462188720703,#e377c2
8.137568,1.8304366,neighbor,3162051,Efficient Defenses Against Adversarial Attacks,0.06498420238494873,#e377c2
6.743782,3.6892762,neighbor,3162051,A General Retraining Framework for Scalable Adversarial Classification,0.06499946117401123,#e377c2
-6.643932,-6.4444437,neighbor,3162051,Distributional Adversarial Networks,0.06506073474884033,#e377c2
-4.747249,-8.423995,neighbor,3162051,Improved Techniques for Training GANs,0.06514406204223633,#e377c2
-4.036515,-9.835136,neighbor,3162051,Triple Generative Adversarial Nets,0.06524759531021118,#e377c2
-4.6832137,4.6661015,neighbor,3162051,Identity Matters in Deep Learning,0.06525301933288574,#e377c2
-9.017035,-8.902928,neighbor,3162051,On the regularization of Wasserstein GANs,0.06535923480987549,#e377c2
2.093219,-0.09014488,neighbor,3162051,Improving the Robustness of Neural Networks Using K-Support Norm Based Adversarial Training,0.06538164615631104,#e377c2
-9.454151,-9.41491,neighbor,3162051,Wasserstein GAN,0.0653875470161438,#e377c2
-10.051643,-12.939162,neighbor,3162051,The Numerics of GANs,0.06557655334472656,#e377c2
10.902519,-1.9789227,neighbor,3162051,Technical Report on the CleverHans v2.1.0 Adversarial Examples Library,0.06563615798950195,#e377c2
-4.1455345,3.77612,neighbor,3162051,Learning Identity Mappings with Residual Gates,0.06565165519714355,#e377c2
8.244393,0.99473476,neighbor,3162051,DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples,0.06611251831054688,#e377c2
12.194327,5.234924,neighbor,3162051,ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models,0.066173255443573,#e377c2
-6.7342534,-4.9745007,neighbor,3162051,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching,0.06619977951049805,#e377c2
-3.43039,4.2276626,neighbor,3162051,Regularizing CNNs with Locally Constrained Decorrelations,0.06631207466125488,#e377c2
-6.5741873,2.4627898,neighbor,3162051,DisturbLabel: Regularizing CNN on the Loss Layer,0.06635069847106934,#e377c2
7.928939,2.5865822,neighbor,3162051,Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks,0.06636452674865723,#e377c2
-10.675542,-12.703094,neighbor,3162051,How to Train Your DRAGAN,0.0663793683052063,#e377c2
3.3482165,-3.1629107,neighbor,3162051,Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions,0.06646460294723511,#e377c2
9.381546,2.6724548,neighbor,3162051,Practical Black-Box Attacks against Machine Learning,0.06652790307998657,#e377c2
-5.561185,-0.08419331,query,3292002,Graph Attention Networks,0.0,#e377c2
-4.893165,1.0226033,neighbor,3292002,Semi-Supervised Classification with Graph Convolutional Networks,0.039584994316101074,#e377c2
-5.147667,-1.4985944,neighbor,3292002,Shift Aggregate Extract Networks,0.044599056243896484,#e377c2
-11.954386,-5.015561,neighbor,3292002,Deep Convolutional Networks on Graph-Structured Data,0.04722416400909424,#e377c2
0.18800917,-2.8573024,neighbor,3292002,Deep Feature Learning for Graphs,0.048309147357940674,#e377c2
-8.642152,-0.37421975,neighbor,3292002,Learning Convolutional Neural Networks for Graphs,0.05020338296890259,#e377c2
-7.0209513,-1.614806,neighbor,3292002,Graph Convolution: A High-Order and Adaptive Approach,0.05193936824798584,#e377c2
-3.7569103,0.47496027,neighbor,3292002,Variational Graph Auto-Encoders,0.05266505479812622,#e377c2
-7.801396,1.0867412,neighbor,3292002,Deep Graph Attention Model,0.053957223892211914,#e377c2
6.071104,-2.48954,neighbor,3292002,Watch Your Step: Learning Graph Embeddings Through Attention,0.054593026638031006,#e377c2
1.8727112,-2.3949413,neighbor,3292002,A Framework for Generalizing Graph-based Representation Learning Methods,0.056047797203063965,#e377c2
-13.085395,-3.5994625,neighbor,3292002,CayleyNets: Graph Convolutional Neural Networks With Complex Rational Spectral Filters,0.05610525608062744,#e377c2
-8.933633,0.17866655,neighbor,3292002,Classifying Graphs as Images with Convolutional Neural Networks,0.05629527568817139,#e377c2
6.0938416,-5.1286764,neighbor,3292002,Representation Learning on Graphs: Methods and Applications,0.05700153112411499,#e377c2
-2.1448808,3.2656076,neighbor,3292002,Neural Graph Machines: Learning Neural Networks Using Graphs,0.05726158618927002,#e377c2
4.501262,-3.512571,neighbor,3292002,HARP: Hierarchical Representation Learning for Networks,0.05895274877548218,#e377c2
-10.241294,-1.5252293,neighbor,3292002,Topology adaptive graph convolutional networks,0.05965214967727661,#e377c2
-11.640174,-1.8268417,neighbor,3292002,Robust Spatial Filtering With Graph Convolutional Neural Networks,0.06233876943588257,#e377c2
-0.5599765,-3.477243,neighbor,3292002,Transfer Learning for Deep Learning on Graph-Structured Data,0.06247943639755249,#e377c2
-12.338928,-3.5850399,neighbor,3292002,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,0.06347548961639404,#e377c2
3.6315792,-4.8891616,neighbor,3292002,Neural Embeddings of Graphs in Hyperbolic Space,0.06350290775299072,#e377c2
-8.561173,-5.402563,neighbor,3292002,Diffusion-Convolutional Neural Networks,0.06439340114593506,#e377c2
-3.7667162,-8.770105,neighbor,3292002,Graph Kernels via Functional Embedding,0.06518733501434326,#e377c2
3.6377728,-2.668837,neighbor,3292002,Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,0.06526678800582886,#e377c2
4.5145946,-1.8825326,neighbor,3292002,Learning Graph Representations with Embedding Propagation,0.06547093391418457,#e377c2
-1.612874,-7.1681757,neighbor,3292002,Propagation kernels: efficient graph kernels from propagated information,0.067920982837677,#e377c2
-3.6811502,3.3247962,neighbor,3292002,Graph Convolutional Networks for Classification with a Structured Label Space,0.06798994541168213,#e377c2
2.6768122,-2.7656987,neighbor,3292002,Representation Learning in Large Attributed Graphs,0.06819862127304077,#e377c2
-13.705238,-5.993372,neighbor,3292002,Distance Metric Learning Using Graph Convolutional Networks: Application to Functional Brain Networks,0.06851905584335327,#e377c2
-4.8382144,2.9432416,neighbor,3292002,Semi-supervised Learning for Convolutional Neural Networks via Online Graph Construction,0.06898188591003418,#e377c2
9.831194,-5.2089615,neighbor,3292002,subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs,0.0690261721611023,#e377c2
-5.0849743,-3.9968898,neighbor,3292002,Deriving Neural Architectures from Sequence and Graph Kernels,0.06915080547332764,#e377c2
7.2349963,-2.2236342,neighbor,3292002,LINE: Large-scale Information Network Embedding,0.06932204961776733,#e377c2
4.7295403,-6.8308926,neighbor,3292002,Joint Embedding of Graphs,0.07013028860092163,#e377c2
7.0143385,0.7092218,neighbor,3292002,node2vec: Scalable Feature Learning for Networks,0.07118791341781616,#e377c2
7.5637393,-7.460768,neighbor,3292002,Deep Neural Networks for Learning Graph Representations,0.07125043869018555,#e377c2
-12.594724,-1.195447,neighbor,3292002,Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs,0.07237905263900757,#e377c2
14.264903,-1.8760942,neighbor,3292002,Deep Generative Models for Relational Data with Side Information,0.07240110635757446,#e377c2
-10.176912,2.0274763,neighbor,3292002,Pixels to Graphs by Associative Embedding,0.07272863388061523,#e377c2
-8.512455,-3.47146,neighbor,3292002,Stochastic Training of Graph Convolutional Networks,0.07290256023406982,#e377c2
0.3753008,1.1503736,neighbor,3292002,Deep Collective Inference,0.07320791482925415,#e377c2
-11.220509,-0.8345201,neighbor,3292002,Dynamic Filters in Graph Convolutional Networks,0.07459336519241333,#e377c2
0.5324787,2.2225072,neighbor,3292002,Learning From Graph Neighborhoods Using LSTMs,0.07519721984863281,#e377c2
-1.0259663,4.4417634,neighbor,3292002,Bootstrapped Graph Diffusions: Exposing the Power of Nonlinearity,0.07585567235946655,#e377c2
6.360692,-12.380874,neighbor,3292002,Retrofitting Distributional Embeddings to Knowledge Graphs with Functional Relations,0.07592815160751343,#e377c2
8.731635,-8.3118725,neighbor,3292002,Learning Deep Representations for Graph Clustering,0.07601594924926758,#e377c2
-2.4833028,-7.854563,neighbor,3292002,Global Weisfeiler-Lehman Graph Kernels,0.07604384422302246,#e377c2
8.793933,1.6311251,neighbor,3292002,Universal network representation for heterogeneous information networks,0.0764428973197937,#e377c2
-6.334846,-12.280102,neighbor,3292002,Large Scale Graph Learning from Smooth Signals,0.0769537091255188,#e377c2
6.112921,-4.124538,neighbor,3292002,Learning Edge Representations via Low-Rank Asymmetric Projections,0.07704371213912964,#e377c2
13.847044,-3.195117,neighbor,3292002,RUM: network Representation learning throUgh Multi-level structural information preservation,0.07726889848709106,#e377c2
7.0943146,4.0845523,neighbor,3292002,Learning Structural Node Embeddings via Diffusion Wavelets,0.07741475105285645,#e377c2
14.083357,-5.905707,neighbor,3292002,Community Detection with Graph Neural Networks,0.07747048139572144,#e377c2
-1.9832344,8.506608,neighbor,3292002,Robust Semi-Supervised Classification for Multi-Relational Graphs,0.07778573036193848,#e377c2
3.4264734,-14.758136,neighbor,3292002,Modeling Relational Data with Graph Convolutional Networks,0.07783108949661255,#e377c2
-10.939846,-9.526291,neighbor,3292002,Can GAN Learn Topological Features of a Graph?,0.07787257432937622,#e377c2
7.1255937,5.4438024,neighbor,3292002,Weighted Network Estimation by the Use of Topological Graph Metrics,0.07808667421340942,#e377c2
-0.36424214,8.8833065,neighbor,3292002,Transductive Classification Methods for Mixed Graphs,0.07809525728225708,#e377c2
-1.1449047,9.395864,neighbor,3292002,Single Network Relational Transductive Learning,0.07828253507614136,#e377c2
-9.250201,3.4025362,neighbor,3292002,The More You Know: Using Knowledge Graphs for Image Classification,0.07844781875610352,#e377c2
-6.188052,-4.023131,neighbor,3292002,Deep Learning with Dynamic Computation Graphs,0.07845288515090942,#e377c2
2.8018517,0.106641866,neighbor,3292002,DeepWalk: online learning of social representations,0.07860684394836426,#e377c2
2.9066105,-13.664038,neighbor,3292002,Convolutional Neural Knowledge Graph Learning,0.07861781120300293,#e377c2
10.003864,-1.9766884,neighbor,3292002,CANE: Context-Aware Network Embedding for Relation Modeling,0.07870668172836304,#e377c2
-3.565779,-10.225722,neighbor,3292002,The Multiscale Laplacian Graph Kernel,0.07887423038482666,#e377c2
4.9669056,-0.4924747,neighbor,3292002,StarSpace: Embed All The Things!,0.07894003391265869,#e377c2
9.853233,-4.3791604,neighbor,3292002,Distributed Representation of Subgraphs,0.07901936769485474,#e377c2
12.839027,-2.996502,neighbor,3292002,Community-enhanced Network Representation Learning for Network Analysis,0.0791693925857544,#e377c2
2.1888509,0.1927525,neighbor,3292002,Discriminative Deep Random Walk for Network Classification,0.07930618524551392,#e377c2
13.66954,-5.167978,neighbor,3292002,Node Embedding via Word Embedding for Network Community Discovery,0.07934629917144775,#e377c2
12.62213,-3.355146,neighbor,3292002,A Unified Framework for Community Detection and Network Representation Learning,0.07953232526779175,#e377c2
3.4389095,-14.00411,neighbor,3292002,Convolutional 2D Knowledge Graph Embeddings,0.08004820346832275,#e377c2
-7.881207,5.985167,neighbor,3292002,Task Sensitive Feature Exploration and Learning for Multitask Graph Classification,0.08025479316711426,#e377c2
3.5917957,-7.7191105,neighbor,3292002,Shuffled Graph Classification: Theory and Connectome Applications,0.08073669672012329,#e377c2
5.4200525,-10.617907,neighbor,3292002,Knowledge Graph Completion via Complex Tensor Factorization,0.08077102899551392,#e377c2
7.9708877,0.328516,neighbor,3292002,A General Framework for Content-enhanced Network Representation Learning,0.08082115650177002,#e377c2
7.3045635,-13.027661,neighbor,3292002,A semantic matching energy function for learning with multi-relational data,0.08092021942138672,#e377c2
-3.1229265,7.1940594,neighbor,3292002,Deformed Graph Laplacian for Semisupervised Learning,0.08097606897354126,#e377c2
1.4430599,8.722117,neighbor,3292002,Transduction on Directed Graphs via Absorbing Random Walks,0.08101844787597656,#e377c2
-2.9305885,-9.7737665,neighbor,3292002,Graph Kernels,0.08114701509475708,#e377c2
-5.85797,-9.072441,neighbor,3292002,A Non-parametric Spectral Model for Graph Classification,0.08121871948242188,#e377c2
-1.2560841,0.15518211,neighbor,3292002,Collective Vertex Classification Using Recursive Neural Network,0.08147215843200684,#e377c2
-8.74304,-5.7905383,neighbor,3292002,Sparse Diffusion-Convolutional Neural Networks,0.08156853914260864,#e377c2
7.4385395,-5.353452,neighbor,3292002,LASAGNE: Locality and Structure Aware Graph Node Embedding,0.0817786455154419,#e377c2
4.292928,-12.958742,neighbor,3292002,Holographic Embeddings of Knowledge Graphs,0.08185940980911255,#e377c2
4.1582675,-12.002837,neighbor,3292002,Semantically Smooth Knowledge Graph Embedding,0.08194243907928467,#e377c2
-3.9220831,7.2262354,neighbor,3292002,Interpretable Graph-Based Semi-Supervised Learning via Flows,0.08223038911819458,#e377c2
2.5728447,4.8796873,neighbor,3292002,FLAG: Faster Learning on Anchor Graph with Label Predictor Optimization,0.0823097825050354,#e377c2
-2.1425884,-8.6837015,neighbor,3292002,A unifying view of explicit and implicit feature maps of graph kernels,0.08245646953582764,#e377c2
7.35137,-3.971862,neighbor,3292002,"A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications",0.08259451389312744,#e377c2
-6.717054,-11.856337,neighbor,3292002,Graph Learning from Data under Structural and Laplacian Constraints,0.08282274007797241,#e377c2
-5.477567,-13.489447,neighbor,3292002,A Novel Regularized Principal Graph Learning Framework on Explicit Graph Representation,0.08287584781646729,#e377c2
4.405695,1.9456116,neighbor,3292002,Omnigraph: Rich Representation and Graph Kernel Learning,0.0829210877418518,#e377c2
-10.998158,-3.1667254,neighbor,3292002,CONVOLUTIONAL NEURAL NETWORKS VIA NODE-VARYING GRAPH FILTERS,0.0829804539680481,#e377c2
15.283237,-3.4239042,neighbor,3292002,Exploring the structural regularities in networks,0.0830390453338623,#e377c2
-11.06021,-9.289403,neighbor,3292002,Learning Graph Topological Features via GAN,0.08304661512374878,#e377c2
9.2616625,-1.1229284,neighbor,3292002,Attributed Social Network Embedding,0.08316773176193237,#e377c2
-4.78803,4.8430185,neighbor,3292002,Auto-clustering Output Layer: Automatic Learning of Latent Annotations in Neural Networks,0.08340704441070557,#e377c2
-0.591251,-8.282176,neighbor,3292002,The All-Paths and Cycles Graph Kernel,0.08341646194458008,#e377c2
10.442048,-0.21146663,neighbor,3292002,Graph Embedding with Rich Information through Bipartite Heterogeneous Network,0.08355587720870972,#e377c2
-7.1135993,-3.7215693,neighbor,3292002,Dragon: A Computation Graph Virtual Machine Based Deep Learning Framework,0.08366626501083374,#e377c2
-2.3794456,-2.8141818,query,3488815,Towards Deep Learning Models Resistant to Adversarial Attacks,0.0,#f7b6d2
-1.4294099,-2.9687946,neighbor,3488815,Simple Black-Box Adversarial Perturbations for Deep Networks,0.028456032276153564,#f7b6d2
-0.50288683,-2.911826,neighbor,3488815,The Limitations of Deep Learning in Adversarial Settings,0.031130611896514893,#f7b6d2
-0.064268894,-2.016332,neighbor,3488815,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods,0.0350610613822937,#f7b6d2
-1.0239325,-6.0605507,neighbor,3488815,Learning Adversary-Resistant Deep Neural Networks,0.03515660762786865,#f7b6d2
-2.1606755,2.3434439,neighbor,3488815,Universal Adversarial Perturbations,0.037489473819732666,#f7b6d2
-2.9608002,5.6850715,neighbor,3488815,Robust Convolutional Neural Networks under Adversarial Noise,0.03796643018722534,#f7b6d2
2.35515,-1.8018805,neighbor,3488815,Learning with a Strong Adversary,0.038138628005981445,#f7b6d2
-2.9804606,0.29205784,neighbor,3488815,Ensemble Adversarial Training: Attacks and Defenses,0.038285255432128906,#f7b6d2
-2.5841947,4.0233097,neighbor,3488815,DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks,0.0383792519569397,#f7b6d2
-2.0603309,6.694742,neighbor,3488815,Towards Deep Neural Network Architectures Robust to Adversarial Examples,0.03871697187423706,#f7b6d2
-2.6237743,-1.8059144,neighbor,3488815,MagNet: A Two-Pronged Defense against Adversarial Examples,0.03884875774383545,#f7b6d2
-1.2004787,-6.0733275,neighbor,3488815,Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks,0.039235591888427734,#f7b6d2
-1.1578449,-2.2492592,neighbor,3488815,On Detecting Adversarial Perturbations,0.039504170417785645,#f7b6d2
-0.8930834,-4.9875917,neighbor,3488815,DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples,0.04024249315261841,#f7b6d2
-1.9099891,7.691094,neighbor,3488815,Towards Robust Deep Neural Networks with BANG,0.04068797826766968,#f7b6d2
0.3533168,-2.7313302,neighbor,3488815,Detecting Adversarial Samples from Artifacts,0.04094499349594116,#f7b6d2
-1.5495697,-4.068532,neighbor,3488815,A Multi-strength Adversarial Training Method to Mitigate Adversarial Attacks,0.04168140888214111,#f7b6d2
-4.2403445,-3.9819095,neighbor,3488815,Robust Deep Learning via Reverse Cross-Entropy Training and Thresholding Test,0.041835665702819824,#f7b6d2
-5.3245997,-4.797328,neighbor,3488815,A General Retraining Framework for Scalable Adversarial Classification,0.04198354482650757,#f7b6d2
-3.2166831,6.451861,neighbor,3488815,Regularizing deep networks using efficient layerwise adversarial training,0.04211688041687012,#f7b6d2
3.8165545,-1.716869,neighbor,3488815,A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples,0.043262600898742676,#f7b6d2
1.3323133,-6.8387647,neighbor,3488815,Practical Black-Box Attacks against Machine Learning,0.04332059621810913,#f7b6d2
1.7786936,3.6883068,neighbor,3488815,Are Accuracy and Robustness Correlated,0.04374939203262329,#f7b6d2
-5.4953585,4.5522647,neighbor,3488815,Measuring Neural Net Robustness with Constraints,0.04448592662811279,#f7b6d2
-1.554959,-0.77396894,neighbor,3488815,Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial Examples,0.044657349586486816,#f7b6d2
-0.5415835,-6.860682,neighbor,3488815,Adversary Resistant Deep Neural Networks with an Application to Malware Detection,0.045933663845062256,#f7b6d2
-1.5808533,-7.4848337,neighbor,3488815,Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,0.046394526958465576,#f7b6d2
-2.902552,-8.908257,neighbor,3488815,Towards Evaluating the Robustness of Neural Networks,0.04714071750640869,#f7b6d2
-3.0204973,2.7639096,neighbor,3488815,Analysis of universal adversarial perturbations,0.04722762107849121,#f7b6d2
-3.5014942,-9.809688,neighbor,3488815,Extending Defensive Distillation,0.0472409725189209,#f7b6d2
-8.675056,-1.5195526,neighbor,3488815,Adversarial Attacks on Neural Network Policies,0.047297120094299316,#f7b6d2
-1.0415614,-0.65799564,neighbor,3488815,Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks,0.04800689220428467,#f7b6d2
-1.8618358,10.010717,neighbor,3488815,Biologically inspired protection of deep networks from adversarial attacks,0.04808706045150757,#f7b6d2
0.29567593,-4.375441,neighbor,3488815,Crafting adversarial input sequences for recurrent neural networks,0.048464298248291016,#f7b6d2
-4.607524,-6.562615,neighbor,3488815,Can Intelligent Hyperparameter Selection Improve Resistance to Adversarial Examples?,0.048537254333496094,#f7b6d2
3.3565104,5.429125,neighbor,3488815,Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN,0.04863995313644409,#f7b6d2
0.34475088,5.5675073,neighbor,3488815,The Artificial Mind's Eye: Resisting Adversarials for Convolutional Neural Networks using Internal Projection,0.04925817251205444,#f7b6d2
2.4342709,5.3801084,neighbor,3488815,Adversarial Transformation Networks: Learning to Generate Adversarial Examples,0.05025249719619751,#f7b6d2
-2.8832457,-6.3269243,neighbor,3488815,MTDeep: Boosting the Security of Deep Neural Nets Against Adversarial Attacks with Moving Target Defense,0.05081188678741455,#f7b6d2
1.8746966,0.5438845,neighbor,3488815,"On the Claim for the Existence of ""Adversarial Examples"" in Deep Learning Neural Networks",0.05109685659408569,#f7b6d2
-4.666694,2.3772728,neighbor,3488815,Robustness of classifiers: from adversarial to random noise,0.05121660232543945,#f7b6d2
-4.316569,5.3288383,neighbor,3488815,Improving the Robustness of Neural Networks Using K-Support Norm Based Adversarial Training,0.051525235176086426,#f7b6d2
-3.5380118,-9.822976,neighbor,3488815,Defensive Distillation is Not Robust to Adversarial Examples,0.05180996656417847,#f7b6d2
-6.48757,-5.5101895,neighbor,3488815,AdversariaLib: An Open-source Library for the Security Evaluation of Machine Learning Algorithms Under Attack,0.05252569913864136,#f7b6d2
0.9550875,6.257503,neighbor,3488815,Adversarial Manipulation of Deep Representations,0.0526728630065918,#f7b6d2
1.15079,-1.2117153,neighbor,3488815,Technical Report on the CleverHans v2.1.0 Adversarial Examples Library,0.05277585983276367,#f7b6d2
-5.0964794,-1.22572,neighbor,3488815,Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong,0.053005099296569824,#f7b6d2
-3.4970658,0.05298262,neighbor,3488815,Delving into Transferable Adversarial Examples and Black-box Attacks,0.05350041389465332,#f7b6d2
-1.9642538,10.219301,neighbor,3488815,"Comment on ""Biologically inspired protection of deep networks from adversarial attacks""",0.05357229709625244,#f7b6d2
2.9906619,-6.422076,neighbor,3488815,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples,0.05369985103607178,#f7b6d2
3.1602993,1.1508343,neighbor,3488815,Stabilizing Adversarial Nets With Prediction Methods,0.05378091335296631,#f7b6d2
3.5293183,6.9643145,neighbor,3488815,Adversarial Images for Variational Autoencoders,0.05395102500915527,#f7b6d2
-5.3473296,2.18467,neighbor,3488815,Analysis of classifiers’ robustness to adversarial perturbations,0.05504274368286133,#f7b6d2
-0.6871835,0.7463946,neighbor,3488815,Detecting Adversarial Image Examples in Deep Neural Networks with Adaptive Noise Reduction,0.05734771490097046,#f7b6d2
-9.209617,-1.586821,neighbor,3488815,Delving into adversarial attacks on deep policies,0.057554662227630615,#f7b6d2
3.287144,-6.03993,neighbor,3488815,Blocking Transferability of Adversarial Examples in Black-Box Learning Systems,0.057700157165527344,#f7b6d2
-7.002904,6.782483,neighbor,3488815,Parseval Networks: Improving Robustness to Adversarial Examples,0.057717978954315186,#f7b6d2
-2.8025053,-4.381441,neighbor,3488815,Adversarial and Clean Data Are Not Twins,0.05869418382644653,#f7b6d2
4.0174155,-0.3644428,neighbor,3488815,A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples,0.05927532911300659,#f7b6d2
3.7611904,4.2013583,neighbor,3488815,Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,0.060370683670043945,#f7b6d2
-0.7739259,-8.965578,neighbor,3488815,LOTS about attacking deep features,0.06090909242630005,#f7b6d2
-4.493594,6.6404395,neighbor,3488815,Improving Back-Propagation by Adding an Adversarial Gradient,0.061335623264312744,#f7b6d2
-1.2042689,2.2612262,neighbor,3488815,A study of the effect of JPG compression on adversarial images,0.06266206502914429,#f7b6d2
1.0313152,1.0883597,neighbor,3488815,Assessing Threat of Adversarial Examples on Deep Neural Networks,0.06308716535568237,#f7b6d2
-3.2799325,8.272913,neighbor,3488815,Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions,0.06308883428573608,#f7b6d2
7.2475877,7.3157544,neighbor,3488815,LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks,0.06355303525924683,#f7b6d2
4.087616,-8.922429,neighbor,3488815,Certified Defenses for Data Poisoning Attacks,0.06371647119522095,#f7b6d2
-2.155681,5.4759717,neighbor,3488815,On the Robustness of Convolutional Neural Networks to Internal Architecture and Weight Perturbations,0.06381773948669434,#f7b6d2
6.3405747,1.2143906,neighbor,3488815,Safety Verification of Deep Neural Networks,0.0638967752456665,#f7b6d2
2.8695447,9.57138,neighbor,3488815,Dense Associative Memory Is Robust to Adversarial Inputs,0.06392592191696167,#f7b6d2
-3.4879801,3.091173,neighbor,3488815,Classification regions of deep neural networks,0.06486892700195312,#f7b6d2
4.96707,6.9072285,neighbor,3488815,Adversarial Examples for Generative Models,0.06490814685821533,#f7b6d2
-6.4675694,3.218583,neighbor,3488815,Ensemble Robustness of Deep Learning Algorithms,0.06492698192596436,#f7b6d2
-6.36471,-3.0024145,neighbor,3488815,Censoring Representations with an Adversary,0.0650254487991333,#f7b6d2
5.324724,-2.3982928,neighbor,3488815,Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation,0.06503599882125854,#f7b6d2
4.216914,-3.2567682,neighbor,3488815,Analyzing the Robustness of Nearest Neighbors to Adversarial Examples,0.06510072946548462,#f7b6d2
5.657143,6.0436354,neighbor,3488815,Towards Principled Methods for Training Generative Adversarial Networks,0.06513893604278564,#f7b6d2
1.4567493,-3.263213,neighbor,3488815,Detecting Adversarial Samples Using Density Ratio Estimates,0.0654301643371582,#f7b6d2
-9.51047,-1.6810155,neighbor,3488815,Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks,0.06578999757766724,#f7b6d2
-0.4967771,3.9959345,neighbor,3488815,Toward Robust Image Classification,0.06588089466094971,#f7b6d2
2.787393,-4.83771,neighbor,3488815,Adversarial examples in the physical world,0.06596106290817261,#f7b6d2
-7.3135185,-7.4504223,neighbor,3488815,Sparse Feature Attacks in Adversarial Learning,0.06604617834091187,#f7b6d2
4.3972597,-9.0829525,neighbor,3488815,Enhancing robustness of machine learning systems via data transformations,0.06651175022125244,#f7b6d2
-3.8070097,-2.5731711,neighbor,3488815,Learning to Protect Communications with Adversarial Neural Cryptography,0.06701338291168213,#f7b6d2
-5.683925,-7.469493,neighbor,3488815,Adequacy of the Gradient-Descent Method for Classifier Evasion Attacks,0.06721818447113037,#f7b6d2
3.893555,8.122197,neighbor,3488815,Associative Adversarial Networks,0.06749379634857178,#f7b6d2
1.7610353,-8.867714,neighbor,3488815,On the (Statistical) Detection of Adversarial Examples,0.0675080418586731,#f7b6d2
1.181986,7.319197,neighbor,3488815,SafetyNet: Detecting and Rejecting Adversarial Examples Robustly,0.06780660152435303,#f7b6d2
-4.140527,-10.773774,neighbor,3488815,On the Effectiveness of Defensive Distillation,0.06831473112106323,#f7b6d2
4.350835,5.347558,neighbor,3488815,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,0.06855762004852295,#f7b6d2
-5.359805,8.171936,neighbor,3488815,Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach,0.0685584545135498,#f7b6d2
7.204834,7.313108,neighbor,3488815,LOGAN: Membership Inference Attacks Against Generative Models,0.06878185272216797,#f7b6d2
5.28303,4.8981185,neighbor,3488815,Stabilizing GAN Training with Multiple Random Projections,0.06933492422103882,#f7b6d2
0.36121127,-6.0374837,neighbor,3488815,Generative Poisoning Attack Method Against Neural Networks,0.06961071491241455,#f7b6d2
-0.4603902,6.6227684,neighbor,3488815,Foveation-based Mechanisms Alleviate Adversarial Examples,0.06973922252655029,#f7b6d2
2.2254126,4.58977,neighbor,3488815,Adversarial Diversity and Hard Positive Generation,0.06987178325653076,#f7b6d2
-7.39826,-7.350623,neighbor,3488815,Adversarial learning,0.070487380027771,#f7b6d2
4.487457,-5.77733,neighbor,3488815,The Space of Transferable Adversarial Examples,0.07055336236953735,#f7b6d2
2.008978,-9.753705,neighbor,3488815,Understanding Black-box Predictions via Influence Functions,0.07068043947219849,#f7b6d2
0.41878566,9.131285,neighbor,3488815,Universal Adversarial Perturbations Against Semantic Image Segmentation,0.07092911005020142,#f7b6d2
6.6058693,5.648215,query,3626819,Deep Contextualized Word Representations,0.0,#f7b6d2
-3.656409,-8.328132,neighbor,3626819,context2vec: Learning Generic Context Embedding with Bidirectional LSTM,0.0438421368598938,#f7b6d2
-3.1097972,-9.389452,neighbor,3626819,Learning to Represent Words in Context with Multilingual Supervision,0.044156432151794434,#f7b6d2
3.5947597,3.151381,neighbor,3626819,Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model,0.04871523380279541,#f7b6d2
2.194792,4.406361,neighbor,3626819,Not All Contexts Are Created Equal: Better Word Representations with Variable Attention,0.049457669258117676,#f7b6d2
-3.8835788,-5.277266,neighbor,3626819,Context encoders as a simple but powerful extension of word2vec,0.050021350383758545,#f7b6d2
-7.630127,2.4634745,neighbor,3626819,Learning to Embed Words in Context for Syntactic Tasks,0.05158340930938721,#f7b6d2
-7.1060996,-8.419041,neighbor,3626819,Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models of Meaning,0.05163830518722534,#f7b6d2
5.646808,-6.0612926,neighbor,3626819,Rotated Word Vector Representations and their Interpretability,0.05183368921279907,#f7b6d2
-13.640065,2.0761905,neighbor,3626819,Learning to Understand Phrases by Embedding the Dictionary,0.05202615261077881,#f7b6d2
9.848407,-3.8499696,neighbor,3626819,Learning to Compute Word Embeddings On the Fly,0.052300989627838135,#f7b6d2
-3.682512,-2.7806299,neighbor,3626819,The Role of Context Types and Dimensionality in Learning Word Embeddings,0.05318903923034668,#f7b6d2
-3.0545423,-4.646871,neighbor,3626819,Learning Context-Specific Word/Character Embeddings,0.05324786901473999,#f7b6d2
-10.994696,-11.490174,neighbor,3626819,sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings,0.05448085069656372,#f7b6d2
4.131219,12.083782,neighbor,3626819,On the Effective Use of Pretraining for Natural Language Inference,0.05448395013809204,#f7b6d2
-2.5269885,2.8922205,neighbor,3626819,Multilingual Distributed Representations without Word Alignment,0.05468195676803589,#f7b6d2
-6.382369,2.4759903,neighbor,3626819,Substitute Based SCODE Word Embeddings in Supervised NLP Tasks,0.05481153726577759,#f7b6d2
-6.573563,-3.3130474,neighbor,3626819,Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings,0.05492454767227173,#f7b6d2
8.48027,8.121212,neighbor,3626819,Refining Raw Sentence Representations for Textual Entailment Recognition via Attention,0.054952383041381836,#f7b6d2
7.461444,11.376961,neighbor,3626819,Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning,0.05497688055038452,#f7b6d2
-8.206616,-8.998514,neighbor,3626819,Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space,0.05542349815368652,#f7b6d2
-4.243027,1.6940149,neighbor,3626819,Polyglot: Distributed Word Representations for Multilingual NLP,0.056008100509643555,#f7b6d2
-6.680899,-10.898075,neighbor,3626819,Breaking Sticks and Ambiguities with Adaptive Skip-gram,0.056716740131378174,#f7b6d2
-0.09897976,4.306098,neighbor,3626819,Learning Topic-Sensitive Word Representations,0.05677640438079834,#f7b6d2
15.151006,8.389961,neighbor,3626819,Refining Word Embeddings for Sentiment Analysis,0.05680364370346069,#f7b6d2
-4.818343,-3.6980727,neighbor,3626819,Dependency-Based Word Embeddings,0.05688542127609253,#f7b6d2
14.361066,12.221876,neighbor,3626819,Deep Unordered Composition Rivals Syntactic Methods for Text Classification,0.057309091091156006,#f7b6d2
5.4176807,-2.0594974,neighbor,3626819,Factored Neural Language Models,0.05734419822692871,#f7b6d2
-6.082874,-5.741485,neighbor,3626819,Symmetric Patterns and Coordinations: Fast and Enhanced Representations of Verbs and Adjectives,0.05743563175201416,#f7b6d2
-4.9778433,-0.34377986,neighbor,3626819,The Expressive Power of Word Embeddings,0.057557761669158936,#f7b6d2
-4.7584715,6.9113865,neighbor,3626819,Unsupervised Cross-Domain Word Representation Learning,0.05796688795089722,#f7b6d2
-7.763498,-2.7821574,neighbor,3626819,K-Embeddings: Learning Conceptual Embeddings for Words using Context,0.05801457166671753,#f7b6d2
15.754614,9.973233,neighbor,3626819,Recursive Autoencoder with HowNet Lexicon for Sentence-Level Sentiment Analysis,0.05846965312957764,#f7b6d2
15.933127,13.625568,neighbor,3626819,The Lifted Matrix-Space Model for Semantic Composition,0.058706820011138916,#f7b6d2
3.436106,-4.1414948,neighbor,3626819,Sparse Overcomplete Word Vector Representations,0.05897629261016846,#f7b6d2
-9.159902,-9.984295,neighbor,3626819,A Simple Approach to Learn Polysemous Word Embeddings,0.05900603532791138,#f7b6d2
9.151984,-8.367275,neighbor,3626819,Data Sets: Word Embeddings Learned from Tweets and General Data,0.059318363666534424,#f7b6d2
-8.270278,-6.5036273,neighbor,3626819,"Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",0.059430718421936035,#f7b6d2
11.539581,11.6104965,neighbor,3626819,A Convolutional Neural Network for Modelling Sentences,0.05954134464263916,#f7b6d2
10.34497,4.2670107,neighbor,3626819,Learning to Predict Denotational Probabilities For Modeling Entailment,0.05958080291748047,#f7b6d2
-8.809274,-11.283603,neighbor,3626819,Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context,0.05983608961105347,#f7b6d2
-2.2568004,-1.2065096,neighbor,3626819,Mimicking Word Embeddings using Subword RNNs,0.060036659240722656,#f7b6d2
3.352756,2.1267123,neighbor,3626819,Exploring phrase-compositionality in skip-gram models,0.06012171506881714,#f7b6d2
3.5170953,0.7456884,neighbor,3626819,"""The Sum of Its Parts"": Joint Learning of Word and Phrase Representations with Autoencoders",0.060244083404541016,#f7b6d2
15.586885,11.358687,neighbor,3626819,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,0.06038600206375122,#f7b6d2
-10.784621,-7.0948486,neighbor,3626819,AutoExtend: Combining Word Embeddings with Semantic Resources,0.06038635969161987,#f7b6d2
-8.558714,-3.4706383,neighbor,3626819,Semantic Information Extraction for Improved Word Embeddings,0.06046247482299805,#f7b6d2
-3.4987042,0.6182595,neighbor,3626819,Investigating Language Universal and Specific Properties in Word Embeddings,0.060487985610961914,#f7b6d2
-10.18132,-10.410494,neighbor,3626819,Embedding Words and Senses Together via Joint Knowledge-Enhanced Training,0.06048929691314697,#f7b6d2
-10.487981,-2.9418998,neighbor,3626819,Improving Lexical Embeddings with Semantic Knowledge,0.060906946659088135,#f7b6d2
9.738593,11.75206,neighbor,3626819,DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding,0.06090950965881348,#f7b6d2
-4.048394,6.4625053,neighbor,3626819,A Simple Regularization-based Algorithm for Learning Cross-Domain Word Embeddings,0.060968637466430664,#f7b6d2
-11.080124,-9.407549,neighbor,3626819,Neural context embeddings for automatic discovery of word senses,0.06107980012893677,#f7b6d2
13.232962,11.743191,neighbor,3626819,The Forest Convolutional Network: Compositional Distributional Semantics with a Neural Chart and without Binarization,0.061128199100494385,#f7b6d2
2.1438346,5.495465,neighbor,3626819,A Classification Approach to Word Prediction,0.06116670370101929,#f7b6d2
5.254549,3.5100853,neighbor,3626819,Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles,0.06123107671737671,#f7b6d2
9.461559,4.6786733,neighbor,3626819,A Context-Enriched Neural Network Method for Recognizing Lexical Entailment,0.061469972133636475,#f7b6d2
1.8663977,-1.594468,neighbor,3626819,Joint Word Representation Learning Using a Corpus and a Semantic Lexicon,0.06160116195678711,#f7b6d2
-10.135046,-4.433818,neighbor,3626819,An Ensemble Method to Produce High-Quality Word Embeddings,0.061624109745025635,#f7b6d2
7.4331636,-4.9685864,neighbor,3626819,Neural-based Noise Filtering from Word Embeddings,0.06162893772125244,#f7b6d2
-1.3738755,-13.61039,neighbor,3626819,Leveraging Monolingual Data for Crosslingual Compositional Word Representations,0.06181687116622925,#f7b6d2
-12.823036,-2.4743261,neighbor,3626819,"TALN at SemEval-2016 Task 11: Modelling Complex Words by Contextual, Lexical and Semantic Features",0.06195676326751709,#f7b6d2
1.9783632,-4.824904,neighbor,3626819,All-but-the-Top: Simple and Effective Postprocessing for Word Representations,0.0620461106300354,#f7b6d2
-13.935729,1.3856909,neighbor,3626819,High-risk learning: acquiring new word vectors from tiny data,0.06256848573684692,#f7b6d2
4.9134374,-3.7626276,neighbor,3626819,Neural Word Representations from Large-Scale Commonsense Knowledge,0.06267595291137695,#f7b6d2
11.822066,9.043868,neighbor,3626819,Dependency Based Embeddings for Sentence Classification Tasks,0.06286126375198364,#f7b6d2
-3.274548,-14.090145,neighbor,3626819,Learning Crosslingual Word Embeddings without Bilingual Corpora,0.0628739595413208,#f7b6d2
-9.917303,0.50236136,neighbor,3626819,Massively Multilingual Word Embeddings,0.06300097703933716,#f7b6d2
7.958771,-1.0688463,neighbor,3626819,Sparsifying Word Representations for Deep Unordered Sentence Modeling,0.06326234340667725,#f7b6d2
9.9010725,-1.2921942,neighbor,3626819,Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition,0.06336164474487305,#f7b6d2
2.387621,-7.84894,neighbor,3626819,Incorporating Both Distributional and Relational Semantics in Word Representations,0.06351524591445923,#f7b6d2
-7.0105324,-1.057426,neighbor,3626819,Semantic Structure and Interpretability of Word Embeddings,0.0635671615600586,#f7b6d2
-0.30406553,-1.370419,neighbor,3626819,Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation,0.06357181072235107,#f7b6d2
6.9559765,0.507958,neighbor,3626819,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,0.06357550621032715,#f7b6d2
-10.02639,-12.326739,neighbor,3626819,A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation,0.0635915994644165,#f7b6d2
-9.420886,-1.2488015,neighbor,3626819,Learning Lexical Embeddings with Syntactic and Lexicographic Knowledge,0.0637657642364502,#f7b6d2
7.0063205,13.112222,neighbor,3626819,Language to Logical Form with Neural Attention,0.06406331062316895,#f7b6d2
8.007931,9.969532,neighbor,3626819,DisSent: Sentence Representation Learning from Explicit Discourse Relations,0.06416231393814087,#f7b6d2
-1.4486303,-3.4333308,neighbor,3626819,Robust Gram Embeddings,0.06421560049057007,#f7b6d2
0.61372596,-5.319328,neighbor,3626819,GloVe: Global Vectors for Word Representation,0.06442338228225708,#f7b6d2
0.31614625,1.7506808,neighbor,3626819,"Good, Better, Best: Choosing Word Embedding Context",0.06460267305374146,#f7b6d2
3.1119068,-6.081452,neighbor,3626819,Learning Word Representations with Hierarchical Sparse Coding,0.06478089094161987,#f7b6d2
10.628564,13.511041,neighbor,3626819,TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency,0.06482827663421631,#f7b6d2
8.076113,-7.0781527,neighbor,3626819,Ultradense Word Embeddings by Orthogonal Transformation,0.06493443250656128,#f7b6d2
18.392452,12.287819,neighbor,3626819,Visualizing and Understanding Neural Models in NLP,0.06497937440872192,#f7b6d2
11.455251,10.308032,neighbor,3626819,Dependency-based Convolutional Neural Networks for Sentence Embedding,0.0649874210357666,#f7b6d2
2.4920914,11.691462,neighbor,3626819,An Exploration of Word Embedding Initialization in Deep-Learning Tasks,0.0650438666343689,#f7b6d2
16.442648,12.6401825,neighbor,3626819,Modeling Compositionality with Multiplicative Recurrent Neural Networks,0.06519269943237305,#f7b6d2
7.049367,-3.2117915,neighbor,3626819,SPINE: SParse Interpretable Neural Embeddings,0.06528723239898682,#f7b6d2
5.9105315,11.695059,neighbor,3626819,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,0.06533372402191162,#f7b6d2
9.055855,-7.2494187,neighbor,3626819,Learning Word Representations from Scarce and Noisy Data with Embedding Subspaces,0.06540751457214355,#f7b6d2
11.058462,2.538782,neighbor,3626819,Deriving Boolean structures from distributional vectors,0.0656237006187439,#f7b6d2
0.6682374,-3.8894017,neighbor,3626819,Enriching Word Vectors with Subword Information,0.06564784049987793,#f7b6d2
-11.91664,-5.3606434,neighbor,3626819,ConceptNet 5.5: An Open Multilingual Graph of General Knowledge,0.06575369834899902,#f7b6d2
4.5604115,13.588246,neighbor,3626819,Recent Trends in Deep Learning Based Natural Language Processing,0.06579828262329102,#f7b6d2
-1.8931521,8.812714,neighbor,3626819,Modelling the Combination of Generic and Target Domain Embeddings in a Convolutional Neural Network for Sentence Classification,0.06588423252105713,#f7b6d2
14.213415,14.800286,neighbor,3626819,Bidirectional Tree-Structured LSTM with Head Lexicalization,0.06603789329528809,#f7b6d2
17.63813,10.675264,neighbor,3626819,Decoding Sentiment from Distributed Representations of Sentences,0.06608915328979492,#f7b6d2
-2.4709156,-14.374974,neighbor,3626819,Bilingual Distributed Word Representations from Document-Aligned Comparable Data,0.06620413064956665,#f7b6d2
2.0744605,-3.5198784,neighbor,3626819,Efficient Estimation of Word Representations in Vector Space,0.06629413366317749,#f7b6d2
4.8032575,7.747317,neighbor,3626819,Contextualized Word Representations for Reading Comprehension,0.06648731231689453,#f7b6d2
-4.1220984,-2.5867524,query,3638670,Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,0.0,#f7b6d2
-1.7681553,-3.2186992,neighbor,3638670,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",0.023630201816558838,#f7b6d2
-3.37008,-2.08186,neighbor,3638670,Rethinking Atrous Convolution for Semantic Image Segmentation,0.023893296718597412,#f7b6d2
-1.9577533,-4.4321456,neighbor,3638670,Fully convolutional networks for semantic segmentation,0.02828967571258545,#f7b6d2
-4.718579,-1.0827745,neighbor,3638670,Fully Convolutional Neural Networks with Full-Scale-Features for Semantic Segmentation,0.028741776943206787,#f7b6d2
-3.0600693,-5.060066,neighbor,3638670,Segmentation-aware convolutional nets,0.02996736764907837,#f7b6d2
-2.147206,2.123318,neighbor,3638670,Stacked Deconvolutional Network for Semantic Segmentation,0.03124934434890747,#f7b6d2
-8.978492,-4.278801,neighbor,3638670,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,0.032306790351867676,#f7b6d2
-4.959748,-4.2836647,neighbor,3638670,Understanding Convolution for Semantic Segmentation,0.03291553258895874,#f7b6d2
-1.6007955,2.2391722,neighbor,3638670,Learning Deconvolution Network for Semantic Segmentation,0.03356403112411499,#f7b6d2
-3.6598818,-7.9750285,neighbor,3638670,ReSeg: A Recurrent Neural Network-Based Model for Semantic Segmentation,0.033588528633117676,#f7b6d2
8.297402,1.8155792,neighbor,3638670,Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade,0.033834636211395264,#f7b6d2
-3.5884042,-4.0472956,neighbor,3638670,Learning Dense Convolutional Embeddings for Semantic Segmentation,0.03405606746673584,#f7b6d2
-0.5836928,-7.7435703,neighbor,3638670,Improving Fully Convolution Network for Semantic Segmentation,0.03411543369293213,#f7b6d2
-3.288017,-7.4633875,neighbor,3638670,Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation,0.03488802909851074,#f7b6d2
3.9264228,-3.1182497,neighbor,3638670,Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation,0.03529834747314453,#f7b6d2
2.7839308,5.1945157,neighbor,3638670,Weakly Supervised Semantic Segmentation Using Superpixel Pooling Network,0.035589754581451416,#f7b6d2
4.666964,1.0971986,neighbor,3638670,Semantic Segmentation with Reverse Attention,0.0357857346534729,#f7b6d2
-6.678203,-3.6951468,neighbor,3638670,Squeeze-SegNet: a new fast deep convolutional neural network for semantic segmentation,0.035846829414367676,#f7b6d2
0.74013937,-6.337749,neighbor,3638670,Progressively Diffused Networks for Semantic Image Segmentation,0.03630572557449341,#f7b6d2
-1.1740383,-1.9453481,neighbor,3638670,A top-down manner-based DCNN architecture for semantic image segmentation,0.036578238010406494,#f7b6d2
4.779075,4.008263,neighbor,3638670,Learning Semantic Segmentation with Diverse Supervision,0.0367279052734375,#f7b6d2
-9.888387,-3.9736145,neighbor,3638670,LinkNet: Exploiting encoder representations for efficient semantic segmentation,0.03816348314285278,#f7b6d2
-3.7013018,1.1631233,neighbor,3638670,Convolutional feature masking for joint object and stuff segmentation,0.03848159313201904,#f7b6d2
-4.7605977,-6.159225,neighbor,3638670,RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation,0.03925132751464844,#f7b6d2
2.950279,6.5105896,neighbor,3638670,Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation,0.04012930393218994,#f7b6d2
-2.664282,-5.588936,neighbor,3638670,Segmentation-Aware Convolutional Networks Using Local Attention Masks,0.040369272232055664,#f7b6d2
-0.60585105,4.260035,neighbor,3638670,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.04054558277130127,#f7b6d2
3.4159527,4.908753,neighbor,3638670,Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation,0.040549635887145996,#f7b6d2
4.746582,5.513173,neighbor,3638670,Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network,0.041229188442230225,#f7b6d2
-0.54800713,1.8130885,neighbor,3638670,Object Boundary Guided Semantic Segmentation,0.04149317741394043,#f7b6d2
-1.4384173,-7.8145714,neighbor,3638670,Multi-Scale Context Aggregation by Dilated Convolutions,0.04186826944351196,#f7b6d2
-3.428864,-1.0578831,neighbor,3638670,Attention to Scale: Scale-Aware Semantic Image Segmentation,0.04188483953475952,#f7b6d2
6.5734096,0.8593671,neighbor,3638670,Fully Convolutional Instance-Aware Semantic Segmentation,0.04195791482925415,#f7b6d2
4.0800357,-3.3332355,neighbor,3638670,Exploring Context with Deep Structured Models for Semantic Segmentation,0.042038917541503906,#f7b6d2
-1.2101656,-3.023558,neighbor,3638670,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.04214465618133545,#f7b6d2
3.2452095,-6.7189393,neighbor,3638670,ParseNet: Looking Wider to See Better,0.04228484630584717,#f7b6d2
2.652748,-4.670633,neighbor,3638670,Region-Based Semantic Segmentation with End-to-End Training,0.0425679087638855,#f7b6d2
-9.541588,-3.2690833,neighbor,3638670,The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,0.04275333881378174,#f7b6d2
-6.883317,3.4059105,neighbor,3638670,CNN-aware binary MAP for general semantic segmentation,0.04310595989227295,#f7b6d2
-0.7737164,-4.3725176,neighbor,3638670,Fully Connected Deep Structured Networks,0.04311448335647583,#f7b6d2
-6.968089,3.4375536,neighbor,3638670,Efficient Convolutional Neural Network with Binary Quantization Layer,0.04321110248565674,#f7b6d2
-5.614696,2.3515856,neighbor,3638670,Improving spatial codification in semantic segmentation,0.0433085560798645,#f7b6d2
5.206214,1.4697796,neighbor,3638670,Combining the Best of Graphical Models and ConvNets for Semantic Segmentation,0.04371124505996704,#f7b6d2
2.5983038,5.9958487,neighbor,3638670,Weakly Supervised Semantic Segmentation with Convolutional Networks,0.04394727945327759,#f7b6d2
6.4497685,-3.612798,neighbor,3638670,Recalling Holistic Information for Semantic Segmentation,0.04420191049575806,#f7b6d2
-1.1104392,-5.9724617,neighbor,3638670,Feedforward semantic segmentation with zoom-out features,0.04429590702056885,#f7b6d2
-2.9212427,5.9444714,neighbor,3638670,Improving the discrimination between foreground and background for semantic segmentation,0.044391512870788574,#f7b6d2
7.981381,0.76321155,neighbor,3638670,Instance-Aware Semantic Segmentation via Multi-task Network Cascades,0.04442799091339111,#f7b6d2
0.4925396,-2.0496304,neighbor,3638670,Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform,0.04487049579620361,#f7b6d2
-8.2067585,-10.155437,neighbor,3638670,Loss Max-Pooling for Semantic Image Segmentation,0.044902801513671875,#f7b6d2
1.0906448,7.6336546,neighbor,3638670,W-Net: A Deep Model for Fully Unsupervised Image Segmentation,0.0450018048286438,#f7b6d2
6.8278537,-4.289933,neighbor,3638670,Joint Object and Part Segmentation Using Deep Learned Potentials,0.04530489444732666,#f7b6d2
2.7231987,7.277316,neighbor,3638670,"Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation",0.04546099901199341,#f7b6d2
-4.8209577,-10.351701,neighbor,3638670,Recurrent Segmentation for Variable Computational Budgets,0.045993804931640625,#f7b6d2
-8.054717,-2.6924129,neighbor,3638670,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,0.04623812437057495,#f7b6d2
-6.219651,-1.7982792,neighbor,3638670,Training Bit Fully Convolutional Network for Fast Semantic Segmentation,0.046761274337768555,#f7b6d2
-1.5234575,-1.2042464,neighbor,3638670,Per-Pixel Feedback for improving Semantic Segmentation,0.04676693677902222,#f7b6d2
2.1649144,2.6036866,neighbor,3638670,Label Refinement Network for Coarse-to-Fine Semantic Segmentation,0.04741060733795166,#f7b6d2
-11.657431,-3.6639419,neighbor,3638670,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,0.04743599891662598,#f7b6d2
-9.403379,-1.9990871,neighbor,3638670,Deep structured features for semantic segmentation,0.04746907949447632,#f7b6d2
4.794517,-1.4188685,neighbor,3638670,Semi-supervised hierarchical semantic object parsing,0.047689974308013916,#f7b6d2
2.2799084,2.6362448,neighbor,3638670,Learning Multi-level Region Consistency with Dense Multi-label Networks for Semantic Segmentation,0.04777306318283081,#f7b6d2
-11.18127,-2.5095778,neighbor,3638670,Residual Conv-Deconv Grid Network for Semantic Segmentation,0.04821521043777466,#f7b6d2
-0.045638632,0.6387553,neighbor,3638670,Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks,0.04846394062042236,#f7b6d2
-0.5859896,9.079413,neighbor,3638670,Learning High-level Prior with Convolutional Neural Networks for Semantic Segmentation,0.04849761724472046,#f7b6d2
6.047781,6.2429442,neighbor,3638670,Weakly Supervised Semantic Segmentation Based on Web Image Co-segmentation,0.048520445823669434,#f7b6d2
0.57179785,2.4593627,neighbor,3638670,Better Image Segmentation by Exploiting Dense Semantic Predictions,0.048554301261901855,#f7b6d2
8.019634,5.2332954,neighbor,3638670,What's the Point: Semantic Segmentation with Point Supervision,0.04867011308670044,#f7b6d2
4.324312,4.988626,neighbor,3638670,Transferable Semi-supervised Semantic Segmentation,0.04887259006500244,#f7b6d2
0.36841768,-7.969283,neighbor,3638670,Beyond Forward Shortcuts: Fully Convolutional Master-Slave Networks (MSNets) with Backward Skip Connections for Semantic Segmentation,0.04897189140319824,#f7b6d2
-10.726771,-5.3243566,neighbor,3638670,Cascaded Feature Network for Semantic Segmentation of RGB-D Images,0.049058616161346436,#f7b6d2
8.443075,-1.6049638,neighbor,3638670,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.049225687980651855,#f7b6d2
4.6286936,7.268373,neighbor,3638670,Weakly-Supervised Semantic Segmentation Using Motion Cues,0.04923814535140991,#f7b6d2
0.42861843,-10.448985,neighbor,3638670,Neither Quick Nor Proper - Evaluation of QuickProp for Learning Deep Neural Networks,0.049638330936431885,#f7b6d2
-8.931218,-6.3354793,neighbor,3638670,Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding,0.050174713134765625,#f7b6d2
-1.4358113,5.9581437,neighbor,3638670,Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation,0.05018329620361328,#f7b6d2
5.505624,9.872976,neighbor,3638670,STC: A Simple to Complex Framework for Weakly-Supervised Semantic Segmentation,0.0505748987197876,#f7b6d2
-0.093168095,-9.556975,neighbor,3638670,Log-DenseNet: How to Sparsify a DenseNet,0.050579726696014404,#f7b6d2
-8.315333,-4.990277,neighbor,3638670,BlitzNet: A Real-Time Deep Network for Scene Understanding,0.05067288875579834,#f7b6d2
-7.833117,0.054643802,neighbor,3638670,Depth-Adaptive Deep Neural Network for Semantic Segmentation,0.050681889057159424,#f7b6d2
-12.879227,-3.400148,neighbor,3638670,Real-time Semantic Image Segmentation via Spatial Sparsity,0.05073058605194092,#f7b6d2
1.2521712,-2.817601,neighbor,3638670,Semantic Segmentation with Boundary Neural Fields,0.050779104232788086,#f7b6d2
-6.8814635,-7.1019626,neighbor,3638670,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling,0.05079132318496704,#f7b6d2
4.9264627,9.579941,neighbor,3638670,Bottom-Up Top-Down Cues for Weakly-Supervised Semantic Segmentation,0.05079472064971924,#f7b6d2
3.041168,-2.736353,neighbor,3638670,Fast Semantic Image Segmentation with High Order Context and Guided Filtering,0.051120102405548096,#f7b6d2
6.5490375,10.859765,neighbor,3638670,Investigating the feature collection for semantic segmentation via single skip connection,0.051134586334228516,#f7b6d2
0.9502396,8.549157,neighbor,3638670,Unsupervised Total Variation Loss for Semi-supervised Deep Learning of Semantic Segmentation,0.05115997791290283,#f7b6d2
-1.7095442,6.0852456,neighbor,3638670,Incorporating Network Built-in Priors in Weakly-Supervised Semantic Segmentation,0.051519036293029785,#f7b6d2
3.0688593,8.756835,neighbor,3638670,Learning to Segment With Image-Level Supervision,0.051574885845184326,#f7b6d2
-4.8071294,-10.420508,neighbor,3638670,Convolutional gated recurrent networks for video segmentation,0.051729559898376465,#f7b6d2
3.147922,-8.612911,neighbor,3638670,Pyramid Scene Parsing Network,0.05280768871307373,#f7b6d2
0.88216335,-3.5164576,neighbor,3638670,Convolutional Random Walk Networks for Semantic Image Segmentation,0.05296754837036133,#f7b6d2
10.043905,-0.61115503,neighbor,3638670,Learning to Refine Object Segments,0.053279340267181396,#f7b6d2
9.475415,0.14507104,neighbor,3638670,Shape-aware Instance Segmentation,0.0536271333694458,#f7b6d2
6.68381,0.16061428,neighbor,3638670,Bridging Category-level and Instance-level Semantic Image Segmentation,0.05397486686706543,#f7b6d2
-1.7600274,-9.673234,neighbor,3638670,Dense CNN Learning with Equivalent Mappings,0.054061293601989746,#f7b6d2
6.1599617,-7.9148817,neighbor,3638670,Memory-Efficient Deep Salient Object Segmentation Networks on Gridized Superpixels,0.05410689115524292,#f7b6d2
0.52364856,5.2184196,neighbor,3638670,ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation,0.05437690019607544,#f7b6d2
3.029254,-0.7732205,neighbor,3638670,Semantic Part Segmentation with Deep Learning,0.0545467734336853,#f7b6d2
6.056621,3.9288645,neighbor,3638670,One-Shot Learning for Semantic Segmentation,0.05471241474151611,#f7b6d2
-0.25823775,-1.973139,query,3641284,UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,0.0,#f7b6d2
0.1703789,0.3899223,neighbor,3641284,A geometric viewpoint of manifold learning,0.05397099256515503,#f7b6d2
0.13972013,5.5877323,neighbor,3641284,Maximum Entropy Linear Manifold for Learning Discriminative Low-Dimensional Representation,0.05630338191986084,#f7b6d2
-5.884513,3.172917,neighbor,3641284,A Generalised Solution to the Out-of-Sample Extension Problem in Manifold Learning,0.057717204093933105,#f7b6d2
-1.3700686,0.5055425,neighbor,3641284,Manifold clustering,0.05788034200668335,#f7b6d2
-3.5093296,-2.4074066,neighbor,3641284,Multiscale Manifold Learning,0.05888086557388306,#f7b6d2
-0.2596358,-3.1341462,neighbor,3641284,Interactive Learning Using Manifold Geometry,0.0591464638710022,#f7b6d2
1.5942401,-0.084691875,neighbor,3641284,An Information Geometric Framework for Dimensionality Reduction,0.06122714281082153,#f7b6d2
-1.3972385,-0.7896846,neighbor,3641284,Subspace learning using consensus on the grassmannian manifold,0.06130540370941162,#f7b6d2
-6.392439,-8.182153,neighbor,3641284,Efficient Inter-Geodesic Distance Computation and Fast Classical Scaling,0.0615350604057312,#f7b6d2
-6.4444637,-8.067299,neighbor,3641284,Fast Classical Scaling,0.06274282932281494,#f7b6d2
-4.260701,-3.1193473,neighbor,3641284,A scale-based approach to finding effective dimensionality in manifold learning,0.06281304359436035,#f7b6d2
6.4908295,-11.916224,neighbor,3641284,Approximated and User Steerable tSNE for Progressive Visual Analytics,0.06331312656402588,#f7b6d2
6.1721234,0.31592292,neighbor,3641284,"Linear dimensionality reduction: survey, insights, and generalizations",0.06331539154052734,#f7b6d2
1.0751597,-1.1146153,neighbor,3641284,FINE: Fisher Information Nonparametric Embedding,0.06358939409255981,#f7b6d2
3.901612,2.6020896,neighbor,3641284,Manifold elastic net: a unified framework for sparse dimension reduction,0.0637175440788269,#f7b6d2
4.8742733,-0.46486774,neighbor,3641284,Dimension Reduction Using Active Manifolds,0.06400704383850098,#f7b6d2
0.80201447,1.7154893,neighbor,3641284,Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment,0.06402641534805298,#f7b6d2
-1.4264332,6.051303,neighbor,3641284,Learning Isometric Separation Maps,0.06409269571304321,#f7b6d2
-1.294936,2.2777493,neighbor,3641284,"Manifold learning, a promised land or work in progress?",0.06445109844207764,#f7b6d2
9.846899,9.798505,neighbor,3641284,Dimensionality Reduction on SPD Manifolds: The Emergence of Geometry-Aware Methods,0.0647616982460022,#f7b6d2
6.304823,0.02156201,neighbor,3641284,Unifying linear dimensionality reduction,0.06485611200332642,#f7b6d2
-1.8302722,1.2134552,neighbor,3641284,Learning the Nonlinear Geometry of High-Dimensional Data: Models and Algorithms,0.0650862455368042,#f7b6d2
5.2938657,-6.404127,neighbor,3641284,Metric Learning in Dimensionality Reduction,0.06527483463287354,#f7b6d2
10.493024,2.3420305,neighbor,3641284,ManifoldOptim: An R Interface to the ROPTLIB Library for Riemannian Manifold Optimization,0.06537985801696777,#f7b6d2
2.3864431,9.19534,neighbor,3641284,Sketched Subspace Clustering,0.06546497344970703,#f7b6d2
7.7127314,-6.258545,neighbor,3641284,Trace Ratio Optimization-Based Semi-Supervised Nonlinear Dimensionality Reduction for Marginal Manifold Visualization,0.06611090898513794,#f7b6d2
-7.8731565,-4.8639846,neighbor,3641284,Intrinsic dimension of a dataset: what properties does one expect?,0.06613963842391968,#f7b6d2
8.116446,-11.716001,neighbor,3641284,Optimal Neighborhood Preserving Visualization by Maximum Satisfiability,0.0661664605140686,#f7b6d2
6.5114183,-11.172419,neighbor,3641284,Visualizing and Exploring Dynamic High-Dimensional Datasets with LION-tSNE,0.06645321846008301,#f7b6d2
4.663302,-10.749078,neighbor,3641284,"Constructing Interactive Visual Classification, Clustering and Dimension Reduction Models for n-D Data",0.06671983003616333,#f7b6d2
2.9052563,-14.357404,neighbor,3641284,Probabilistic Dimensionality Reduction via Structure Learning,0.06716412305831909,#f7b6d2
5.7226825,-12.569184,neighbor,3641284,Visualizing Large-scale and High-dimensional Data,0.06754302978515625,#f7b6d2
4.4848704,-6.8233643,neighbor,3641284,A Computational Framework for Nonlinear Dimensionality Reduction of Large Data Sets: The Exploratory Inspection Machine (XIM),0.06772631406784058,#f7b6d2
5.217632,6.203975,neighbor,3641284,Signed Laplacian Embedding for Supervised Dimension Reduction,0.06803792715072632,#f7b6d2
4.27826,-11.611888,neighbor,3641284,DimReader: Axis lines that explain non-linear projections,0.06833130121231079,#f7b6d2
-0.39034706,3.0749483,neighbor,3641284,Manifold Learning: The Price of Normalization,0.06840986013412476,#f7b6d2
4.849104,2.1273677,neighbor,3641284,Regularizers versus Losses for Nonlinear Dimensionality Reduction: A Factored View with New Convex Relaxations,0.06854802370071411,#f7b6d2
8.766774,-0.61880004,neighbor,3641284,A Unifying Probabilistic Perspective for Spectral Dimensionality Reduction: Insights and New Models,0.06898510456085205,#f7b6d2
-0.31438547,4.098755,neighbor,3641284,Bayesian Manifold Learning: Locally Linear Latent Variable Model (LL-LVM): poster,0.06922507286071777,#f7b6d2
3.7737913,-4.7384367,neighbor,3641284,Nonlinear Dimensionality Reduction via Path-Based Isometric Mapping,0.0692870020866394,#f7b6d2
8.256131,-6.63594,neighbor,3641284,Visualization of Non-vectorial Data Using Twin Kernel Embedding,0.06937909126281738,#f7b6d2
4.9018517,7.4571953,neighbor,3641284,Dimensionality Reduction Using Similarity-Induced Embeddings,0.06989729404449463,#f7b6d2
13.113399,5.0377965,neighbor,3641284,Geometric Mean Metric Learning,0.07003957033157349,#f7b6d2
-5.8261123,0.6278286,neighbor,3641284,Testing the Manifold Hypothesis,0.07004106044769287,#f7b6d2
3.2916188,-11.029594,neighbor,3641284,Visualizing Dimensionality Reduction Artifacts: An Evaluation,0.07038015127182007,#f7b6d2
4.0765595,-12.777343,neighbor,3641284,Exploring High‐Dimensional Structure via Axis‐Aligned Decomposition of Linear Projections,0.07077372074127197,#f7b6d2
3.276274,-7.8381643,neighbor,3641284,IT-map: an Effective Nonlinear Dimensionality Reduction Method for Interactive Clustering,0.07089132070541382,#f7b6d2
4.5401516,5.4103775,neighbor,3641284,Manifold Partition Discriminant Analysis,0.07103782892227173,#f7b6d2
-3.0512588,5.3297777,neighbor,3641284,Learning with Cross-Kernels and Ideal PCA,0.07126212120056152,#f7b6d2
3.8448665,6.953504,neighbor,3641284,Multilinear Maximum Distance Embedding Via L1-Norm Optimization,0.07130587100982666,#f7b6d2
-2.9597757,4.5214925,neighbor,3641284,Two-Manifold Problems,0.07138693332672119,#f7b6d2
-8.526544,-0.009585594,neighbor,3641284,Tractable Clustering of Data on the Curve Manifold,0.0714496374130249,#f7b6d2
1.2622089,6.0773435,neighbor,3641284,Representation Optimization with Feature Selection and Manifold Learning in a Holistic Classification Framework,0.07148468494415283,#f7b6d2
2.5764537,3.9080517,neighbor,3641284,Local high-order regularization on data manifolds,0.07161784172058105,#f7b6d2
10.164501,-11.367033,neighbor,3641284,Crossing Minimization within Graph Embeddings,0.07195019721984863,#f7b6d2
9.958676,11.537921,neighbor,3641284,A Riemannian Network for SPD Matrix Learning,0.07199537754058838,#f7b6d2
-7.064058,-9.070599,neighbor,3641284,Multidimensional Scaling in the Poincaré Disk,0.07207125425338745,#f7b6d2
9.271386,10.221503,neighbor,3641284,From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices,0.07212722301483154,#f7b6d2
-8.705661,-0.12665786,neighbor,3641284,Low-Rank Representation over the Manifold of Curves,0.07216686010360718,#f7b6d2
7.483692,4.6466765,neighbor,3641284,"Spectral Sparse Representation for Clustering: Evolved from PCA, K-means, Laplacian Eigenmap, and Ratio Cut",0.07240444421768188,#f7b6d2
-6.407462,2.9634733,neighbor,3641284,Local and Global Regressive Mapping for Manifold Learning with Out-of-Sample Extrapolation,0.07246053218841553,#f7b6d2
9.54654,-7.122361,neighbor,3641284,End-to-End Data Visualization by Metric Learning and Coordinate Transformation,0.07249289751052856,#f7b6d2
12.660273,6.1408415,neighbor,3641284,Stochastic Dykstra Algorithms for Metric Learning on Positive Semi-Definite Cone,0.07260358333587646,#f7b6d2
3.046666,10.946107,neighbor,3641284,Dimensionality Reduction with Subspace Structure Preservation,0.07277143001556396,#f7b6d2
5.471735,-15.595054,neighbor,3641284,Automatic Selection of t-SNE Perplexity,0.0729641318321228,#f7b6d2
2.8984292,-11.9533615,neighbor,3641284,Projections as visual aids for classification system design,0.07304555177688599,#f7b6d2
10.394187,10.4865055,neighbor,3641284,Geometry-Aware Similarity Learning on SPD Manifolds for Visual Recognition,0.07306897640228271,#f7b6d2
2.6511238,2.9013767,neighbor,3641284,LDMNet: Low Dimensional Manifold Regularized Neural Networks,0.0731130838394165,#f7b6d2
-6.413132,-1.9455632,neighbor,3641284,Optimal bandwidth estimation for a fast manifold learning algorithm to detect circular structure in high-dimensional data,0.07327210903167725,#f7b6d2
-3.213876,-0.7810428,neighbor,3641284,Multimodal diffusion geometry by joint diagonalization of Laplacians,0.07348573207855225,#f7b6d2
2.3999696,-15.3889265,neighbor,3641284,Manifold Learning for Jointly Modeling Topic and Visualization,0.07369238138198853,#f7b6d2
-5.192134,-2.6088164,neighbor,3641284,Kernel Scaling for Manifold Learning and Classification,0.07412207126617432,#f7b6d2
2.772735,-0.5781769,neighbor,3641284,A Metric Multidimensional Scaling-Based Nonlinear Manifold Learning Approach for Unsupervised Data Reduction,0.07428896427154541,#f7b6d2
-2.6942453,-4.161217,neighbor,3641284,Manifold Learning Method for Large Scale Dataset Based on Gradient Descent,0.07435023784637451,#f7b6d2
5.5101147,-2.4710484,neighbor,3641284,Nonlinear dimensionality reduction on graphs,0.07438594102859497,#f7b6d2
12.346528,4.8108506,neighbor,3641284,Learning Riemannian Metrics,0.07438784837722778,#f7b6d2
7.3419986,0.6871765,neighbor,3641284,Geometric Dimensionality Reduction for Subsequent Classification,0.07440030574798584,#f7b6d2
7.3769636,9.228949,neighbor,3641284,"Laplacian Eigenmaps From Sparse, Noisy Similarity Measurements",0.0744103193283081,#f7b6d2
-4.3821,-4.7113533,neighbor,3641284,The Local Dimension of Deep Manifold,0.07467246055603027,#f7b6d2
-6.6606708,5.586001,neighbor,3641284,A Quasi-Isometric Embedding Algorithm,0.0746915340423584,#f7b6d2
-9.493204,2.95205,neighbor,3641284,S-Isomap++: Multi manifold learning from streaming data,0.07470810413360596,#f7b6d2
-5.2460814,-8.802794,neighbor,3641284,HORSESHOES IN MULTIDIMENSIONAL SCALING AND LOCAL KERNEL METHODS,0.0750657320022583,#f7b6d2
6.9897366,-13.470858,neighbor,3641284,"A unified data representation theory for network visualization, ordering and coarse-graining",0.07508623600006104,#f7b6d2
-4.5180798,8.20243,neighbor,3641284,Inductive Hashing on Manifolds,0.07511729001998901,#f7b6d2
10.365529,8.454081,neighbor,3641284,Dimensionality Reduction on Grassmannian via Riemannian Optimization: A Generalized Perspective,0.07511764764785767,#f7b6d2
14.4858,4.405173,neighbor,3641284,Classification with Minimax Distance Measures,0.07516646385192871,#f7b6d2
-1.0144855,7.847735,neighbor,3641284,Semi-supervised learning with explicit relationship regularization,0.07545077800750732,#f7b6d2
7.159809,-1.2542827,neighbor,3641284,"Non-negative Matrix Factorization, Convexity and Isometry",0.07553368806838989,#f7b6d2
1.5174236,-5.233282,neighbor,3641284,Vector diffusion maps and the connection Laplacian,0.07571583986282349,#f7b6d2
14.454196,6.5034623,neighbor,3641284,"Interpolated Discretized Embedding of Single Vectors and Vector Pairs for Classification, Metric Learning and Distance Approximation",0.07574212551116943,#f7b6d2
-9.693817,2.8315372,neighbor,3641284,Error Metrics for Learning Reliable Manifolds from Streaming Data,0.07584774494171143,#f7b6d2
13.240572,3.156387,neighbor,3641284,A scaled Bregman theorem with applications,0.07618272304534912,#f7b6d2
12.324642,7.836947,neighbor,3641284,Metric learning with two-dimensional smoothness for visual analysis,0.07627213001251221,#f7b6d2
2.7412636,11.418017,neighbor,3641284,DUSC: Dimensionality Unbiased Subspace Clustering,0.07633483409881592,#f7b6d2
11.239501,11.254213,neighbor,3641284,Learning Discriminative Alpha-Beta-divergence for Positive Definite Matrices (Extended Version),0.07645225524902344,#f7b6d2
-8.288392,-4.232401,neighbor,3641284,De-Biasing for Intrinsic Dimension Estimation,0.07647621631622314,#f7b6d2
5.556211,-7.9495597,neighbor,3641284,Dimension Reduction with Non-degrading Generalization,0.07654601335525513,#f7b6d2
6.476875,1.9289606,neighbor,3641284,Closed-form supervised dimensionality reduction with generalized linear models,0.07688891887664795,#f7b6d2
-3.4694111,2.42082,neighbor,3641284,Aligning Mixed Manifolds,0.07690566778182983,#f7b6d2
7.5349197,5.1749616,neighbor,3641284,Low Rank Representation on Riemannian Manifold of Square Root Densities,0.07694768905639648,#f7b6d2
-4.1562386,-5.152437,query,3719281,U-Net: Convolutional Networks for Biomedical Image Segmentation,0.0,#f7b6d2
-4.8327885,-4.1353683,neighbor,3719281,Deep Learning for Medical Image Segmentation,0.0565260648727417,#f7b6d2
2.7085054,-2.5819063,neighbor,3719281,Boundary Learning by Optimization with Topological Constraints,0.06837272644042969,#f7b6d2
-3.11971,9.221931,neighbor,3719281,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.06905126571655273,#f7b6d2
-2.935392,-16.025255,neighbor,3719281,Automated Detection of Synapses in Serial Section Transmission Electron Microscopy Image Stacks,0.06999260187149048,#f7b6d2
2.2169359,-10.03107,neighbor,3719281,A General System for Automatic Biomedical Image Segmentation Using Intensity Neighborhoods,0.07009434700012207,#f7b6d2
-4.398529,10.9809885,neighbor,3719281,Fast image scanning with deep max-pooling convolutional neural networks,0.07025766372680664,#f7b6d2
-6.502918,-8.829955,neighbor,3719281,Deep neural networks for anatomical brain segmentation,0.07140159606933594,#f7b6d2
4.6097574,8.787132,neighbor,3719281,Fully Connected Deep Structured Networks,0.07150167226791382,#f7b6d2
3.8038387,10.134816,neighbor,3719281,Fully convolutional networks for semantic segmentation,0.07189160585403442,#f7b6d2
-3.577295,-16.204906,neighbor,3719281,Automated Detection and Segmentation of Synaptic Contacts in Nearly Isotropic Serial Electron Microscopy Images,0.07228583097457886,#f7b6d2
3.1141024,-3.055316,neighbor,3719281,Image Segmentation by Size-Dependent Single Linkage Clustering of a Watershed Basin Graph,0.07237911224365234,#f7b6d2
3.6750815,9.154915,neighbor,3719281,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.07243353128433228,#f7b6d2
-3.876001,-14.7475605,neighbor,3719281,An Automated Images-to-Graphs Pipeline for High Resolution Connectomics,0.07332998514175415,#f7b6d2
0.09828788,-14.578802,neighbor,3719281,Gebiss: an ImageJ plugin for the specification of ground truth and the performance evaluation of 3D segmentation algorithms,0.07358592748641968,#f7b6d2
5.671164,11.379709,neighbor,3719281,Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation,0.07469499111175537,#f7b6d2
4.726141,10.906598,neighbor,3719281,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.07578486204147339,#f7b6d2
-0.018484553,-5.653152,neighbor,3719281,Image Segmentation,0.07579833269119263,#f7b6d2
4.697061,-7.6942186,neighbor,3719281,3D cell nuclei segmentation based on gradient flow tracking,0.07651591300964355,#f7b6d2
-1.3572,0.6542874,neighbor,3719281,HMRF-EM-image: Implementation of the Hidden Markov Random Field Model and its Expectation-Maximization Algorithm,0.0771518349647522,#f7b6d2
4.104791,11.506022,neighbor,3719281,Learning Deconvolution Network for Semantic Segmentation,0.07770389318466187,#f7b6d2
-5.535587,-4.1141925,neighbor,3719281,Deep convolutional networks for pancreas segmentation in CT imaging,0.07926064729690552,#f7b6d2
5.4466677,12.241188,neighbor,3719281,Weakly Supervised Semantic Segmentation with Convolutional Networks,0.07961398363113403,#f7b6d2
5.727826,5.837013,neighbor,3719281,A Deep-Structured Fully Connected Random Field Model for Structured Inference,0.0800776481628418,#f7b6d2
-0.4257583,-6.7606416,neighbor,3719281,Review of Biomedical Image Processing,0.08047676086425781,#f7b6d2
-4.6865153,12.014061,neighbor,3719281,Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification,0.0816463828086853,#f7b6d2
3.323855,-8.32635,neighbor,3719281,An unsupervised ensemble-based Markov Random Field approach to microscope cell image segmentation,0.08195960521697998,#f7b6d2
6.36645,10.544316,neighbor,3719281,Combining the Best of Graphical Models and ConvNets for Semantic Segmentation,0.08334624767303467,#f7b6d2
4.002793,5.303537,neighbor,3719281,Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials,0.08463364839553833,#f7b6d2
3.9387956,7.115826,neighbor,3719281,Conditional Random Fields as Recurrent Neural Networks,0.08547163009643555,#f7b6d2
0.33104792,0.778579,neighbor,3719281,Non-Parametric Probabilistic Image Segmentation,0.0859452486038208,#f7b6d2
-0.40111938,-0.7530365,neighbor,3719281,Active Mean Fields for Probabilistic Image Segmentation: Connections with Chan-Vese and Rudin-Osher-Fatemi Models,0.08616983890533447,#f7b6d2
5.799358,-12.207096,neighbor,3719281,HEp-2 Cell Image Classification With Deep Convolutional Neural Networks,0.08618587255477905,#f7b6d2
-1.6483669,8.35657,neighbor,3719281,Image Segmentation by Cascaded Region Agglomeration,0.08670997619628906,#f7b6d2
2.7219617,-12.11691,neighbor,3719281,An interactive learning approach to histology image segmentation,0.08689343929290771,#f7b6d2
3.7422135,6.157208,neighbor,3719281,Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation,0.08697468042373657,#f7b6d2
-0.38585904,2.2104535,neighbor,3719281,Discriminative Parameter Estimation for Random Walks Segmentation: Technical Report,0.08714932203292847,#f7b6d2
1.1034987,-12.488108,neighbor,3719281,"An open-source, MATLAB based annotation tool for virtual slides",0.08763915300369263,#f7b6d2
2.9402087,11.84201,neighbor,3719281,Convolutional feature masking for joint object and stuff segmentation,0.08769524097442627,#f7b6d2
-4.4656672,14.889282,neighbor,3719281,Going deeper with convolutions,0.08776998519897461,#f7b6d2
-1.1789614,-9.612661,neighbor,3719281,Coercive region-level registration for multi-modal images,0.08779299259185791,#f7b6d2
-4.5392175,16.250288,neighbor,3719281,MatConvNet: Convolutional Neural Networks for MATLAB,0.08820599317550659,#f7b6d2
-2.7716022,13.041564,neighbor,3719281,Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection,0.08820784091949463,#f7b6d2
0.03883299,-4.872139,neighbor,3719281,The watershed concept and its use in segmentation : a brief history,0.08824825286865234,#f7b6d2
-5.3104925,15.16403,neighbor,3719281,Striving for Simplicity: The All Convolutional Net,0.0882953405380249,#f7b6d2
0.5097741,-2.6611636,neighbor,3719281,Seeded watershed cut uncertainty estimators for guided interactive segmentation,0.08829677104949951,#f7b6d2
-3.5840716,-9.757762,neighbor,3719281,Multi-atlas segmentation with joint label fusion and corrective learning—an open source implementation,0.08877748250961304,#f7b6d2
4.0026956,-8.830034,neighbor,3719281,Segmentation of Fluorescence Microscopy Cell Images Using Unsupervised Mining,0.08887732028961182,#f7b6d2
2.5705104,9.774647,neighbor,3719281,Feedforward semantic segmentation with zoom-out features,0.08906924724578857,#f7b6d2
-5.688524,-10.4014635,neighbor,3719281,101 Labeled Brain Images and a Consistent Human Cortical Labeling Protocol,0.08931100368499756,#f7b6d2
0.30698022,11.935366,neighbor,3719281,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.08935964107513428,#f7b6d2
5.0356836,-13.030031,neighbor,3719281,Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification,0.089413583278656,#f7b6d2
4.946334,-11.308076,neighbor,3719281,Classification of mitotic figures with convolutional neural networks and seeded blob features,0.08961588144302368,#f7b6d2
-0.6081928,0.61619526,neighbor,3719281,Variational Bayesian image modelling,0.08987772464752197,#f7b6d2
5.234059,9.797451,neighbor,3719281,Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation,0.09016317129135132,#f7b6d2
3.5596774,-5.485794,neighbor,3719281,"Local Optimization Based Segmentation of Spatially-Recurring, Multi-Region Objects With Part Configuration Constraints",0.09028083086013794,#f7b6d2
-1.3628505,-4.892761,neighbor,3719281,Template-Cut: A Pattern-Based Segmentation Paradigm,0.09030503034591675,#f7b6d2
5.888547,-9.15092,neighbor,3719281,Nonnegative Mixed-Norm Convex Optimization for Mitotic Cell Detection in Phase Contrast Microscopy,0.09040790796279907,#f7b6d2
6.761189,9.04062,neighbor,3719281,Context Tricks for Cheap Semantic Segmentation,0.09101420640945435,#f7b6d2
-4.6517406,13.624912,neighbor,3719281,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.09112519025802612,#f7b6d2
-2.6814609,14.526455,neighbor,3719281,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.0915442705154419,#f7b6d2
3.9178677,-14.232442,neighbor,3719281,Beyond KernelBoost,0.09158706665039062,#f7b6d2
-4.8045487,-13.527505,neighbor,3719281,Correction: A Hybrid CPU-GPU Accelerated Framework for Fast Mapping of High-Resolution Human Brain Connectome,0.09173500537872314,#f7b6d2
4.0427747,-1.2850922,neighbor,3719281,Fast Planar Correlation Clustering for Image Segmentation,0.09175127744674683,#f7b6d2
-7.741609,-7.3722987,neighbor,3719281,3D segmentation of rodent brain structures using Active Volume Model with shape priors,0.09177643060684204,#f7b6d2
4.49347,-4.6064835,neighbor,3719281,Multiphase geometric couplings for the segmentation of neural processes,0.09203016757965088,#f7b6d2
-2.355343,-0.22695293,neighbor,3719281,Variational segmentation model for images with intensity inhomogeneity and Poisson noise,0.09203571081161499,#f7b6d2
-1.1213232,-3.1645565,neighbor,3719281,Elastic body spline based image segmentation,0.09240007400512695,#f7b6d2
-6.7747693,16.492033,neighbor,3719281,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.09246194362640381,#f7b6d2
-7.4825406,-0.6474043,neighbor,3719281,On-line re-training and segmentation with reduction of the training set: Application to the left ventricle detection in ultrasound imaging,0.09267711639404297,#f7b6d2
-6.7113714,-11.133375,neighbor,3719281,A Semi-Automated Pipeline for the Segmentation of Rhesus Macaque Hippocampus: Validation across a Wide Age Range,0.09292024374008179,#f7b6d2
5.191917,-8.124353,neighbor,3719281,Hierarchical Mergence Approach to Cell Detection in Phase Contrast Microscopy Images,0.09297877550125122,#f7b6d2
-3.5921545,0.078207016,neighbor,3719281,Convolutional Virtual Electric Field for Image Segmentation Using Active Contours,0.0932123064994812,#f7b6d2
-0.8198119,13.354409,neighbor,3719281,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.09337645769119263,#f7b6d2
-7.7660723,-11.282242,neighbor,3719281,Automated Segmentation of in Vivo and Ex Vivo Mouse Brain Magnetic Resonance Images,0.09360194206237793,#f7b6d2
3.563113,3.0754888,neighbor,3719281,Image Co-segmentation via Consistent Functional Maps,0.09370994567871094,#f7b6d2
1.9002763,-6.3031716,neighbor,3719281,Graphical processing unit implementation of an integrated shape-based active contour: Application to digital pathology,0.09378635883331299,#f7b6d2
-2.694548,-8.651972,neighbor,3719281,Atlas to Image-with-Tumor Registration Based on Demons and Deformation Inpainting,0.0938078761100769,#f7b6d2
-6.7818055,15.775203,neighbor,3719281,Fractional Max-Pooling,0.09395790100097656,#f7b6d2
2.3004801,5.445659,neighbor,3719281,Harmony potentials for joint classification and segmentation,0.0940084457397461,#f7b6d2
2.8352654,1.2267555,neighbor,3719281,Multiclass Semi-Supervised Learning on Graphs using Ginzburg-Landau Functional Minimization,0.09402215480804443,#f7b6d2
-1.9555684,-13.089765,neighbor,3719281,Large-scale automated image analysis for computational profiling of brain tissue surrounding implanted neuroprosthetic devices using Python,0.0940709114074707,#f7b6d2
3.9999006,13.013739,neighbor,3719281,Semantic Part Segmentation with Deep Learning,0.09424483776092529,#f7b6d2
-0.12115714,10.845803,neighbor,3719281,Layered object detection for multi-class segmentation,0.09431040287017822,#f7b6d2
1.2857482,10.943819,neighbor,3719281,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.094363272190094,#f7b6d2
-4.5726757,-2.8557081,neighbor,3719281,Deep structured learning for mass segmentation from mammograms,0.09445327520370483,#f7b6d2
-7.423467,-0.45815432,neighbor,3719281,A Novel Model-Based 3D ${+}$Time Left Ventricular Segmentation Technique,0.09446609020233154,#f7b6d2
-2.731545,-15.342432,neighbor,3719281,A high-throughput framework to detect synapses in electron microscopy images,0.09453737735748291,#f7b6d2
-1.7715304,-16.328722,neighbor,3719281,Improved synapse detection for mGRASP-assisted brain connectivity mapping,0.09471142292022705,#f7b6d2
-8.711287,-9.655371,neighbor,3719281,Brain extraction using the watershed transform from markers,0.09475439786911011,#f7b6d2
3.1779635,-11.123588,neighbor,3719281,Learning histopathological patterns,0.09499597549438477,#f7b6d2
-6.9655256,-10.071674,neighbor,3719281,Subcortical structure segmentation using probabilistic atlas priors,0.09521394968032837,#f7b6d2
-1.282846,-10.295683,neighbor,3719281,Robust image registration of biological microscopic images,0.09530925750732422,#f7b6d2
-0.46732017,8.629576,neighbor,3719281,Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation,0.09540480375289917,#f7b6d2
0.6949951,12.610916,neighbor,3719281,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.09541624784469604,#f7b6d2
-5.752097,14.107414,neighbor,3719281,Enhanced image classification with a fast-learning shallow convolutional neural network,0.09542739391326904,#f7b6d2
-5.48308,17.259104,neighbor,3719281,Highway Networks,0.09544718265533447,#f7b6d2
-4.1735406,-17.331758,neighbor,3719281,The Digital Bee Brain: Integrating and Managing Neurons in a Common 3D Reference System,0.09554588794708252,#f7b6d2
-5.5339823,-12.141614,neighbor,3719281,Fast parallel image registration on CPU and GPU for diagnostic classification of Alzheimer's disease,0.09571069478988647,#f7b6d2
-7.438459,-9.824937,neighbor,3719281,Fully Automated Segmentation of the Pons and Midbrain Using Human T1 MR Brain Images,0.0958397388458252,#f7b6d2
-7.2770157,12.90811,neighbor,3719281,Clustering Learning for Robotic Vision,0.09588342905044556,#f7b6d2
0.055508427,3.3105547,query,3960646,Visualizing and Understanding Convolutional Networks,0.0,#7f7f7f
-0.20632942,5.1379285,neighbor,3960646,Maxout Networks,0.07269805669784546,#7f7f7f
-2.7868803,0.7096508,neighbor,3960646,Piecewise Linear Multilayer Perceptrons and Dropout,0.07423430681228638,#7f7f7f
2.2020962,2.9778507,neighbor,3960646,An introduction to deep learning,0.07550770044326782,#7f7f7f
1.7071162,-0.8098349,neighbor,3960646,Predicting Parameters in Deep Learning,0.0757296085357666,#7f7f7f
-0.620032,8.292722,neighbor,3960646,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.0764043927192688,#7f7f7f
0.03509705,6.5216193,neighbor,3960646,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.07687133550643921,#7f7f7f
6.1097007,12.584098,neighbor,3960646,Multi-column deep neural networks for image classification,0.07724589109420776,#7f7f7f
0.0228578,-1.0733501,neighbor,3960646,An Analysis of the Connections Between Layers of Deep Neural Networks,0.07904660701751709,#7f7f7f
8.18765,9.822194,neighbor,3960646,Scalable stacking and learning for building deep architectures,0.08140498399734497,#7f7f7f
-2.4627223,13.358084,neighbor,3960646,Differentiable Pooling for Hierarchical Feature Learning,0.08420789241790771,#7f7f7f
-4.8070493,-8.641223,neighbor,3960646,"Learning, Memory, and the Role of Neural Network Architecture",0.0855606198310852,#7f7f7f
-0.20186943,-5.859993,neighbor,3960646,Coloring black boxes: visualization of neural network decisions,0.08584243059158325,#7f7f7f
-9.1627445,7.577666,neighbor,3960646,Quadratic Features and Deep Architectures for Chunking,0.08648568391799927,#7f7f7f
2.4422593,-1.7809824,neighbor,3960646,Practical Recommendations for Gradient-Based Training of Deep Architectures,0.08679503202438354,#7f7f7f
7.5342555,12.270222,neighbor,3960646,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.0870901346206665,#7f7f7f
-4.0190773,9.505072,neighbor,3960646,Two SVDs produce more focal deep learning representations,0.08735388517379761,#7f7f7f
-2.2092788,4.7980275,neighbor,3960646,Improving neural networks by preventing co-adaptation of feature detectors,0.08943295478820801,#7f7f7f
-1.6551352,14.772961,neighbor,3960646,Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models,0.09043443202972412,#7f7f7f
4.306238,0.40618327,neighbor,3960646,Joint Training of Deep Boltzmann Machines,0.09145492315292358,#7f7f7f
2.7652812,16.967907,neighbor,3960646,Fast image scanning with deep max-pooling convolutional neural networks,0.09218627214431763,#7f7f7f
0.44285968,0.9420432,neighbor,3960646,Big Neural Networks Waste Capacity,0.09231293201446533,#7f7f7f
-0.91588974,13.577562,neighbor,3960646,Learnable Pooling Regions for Image Classification,0.09237843751907349,#7f7f7f
9.947161,-1.5053549,neighbor,3960646,Riemannian metrics for neural networks I: feedforward networks,0.0928037166595459,#7f7f7f
9.146543,-1.3889217,neighbor,3960646,Revisiting Natural Gradient for Deep Networks,0.09294569492340088,#7f7f7f
7.134664,13.342312,neighbor,3960646,"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",0.09326738119125366,#7f7f7f
3.3531632,1.485741,neighbor,3960646,Deep belief networks,0.0934029221534729,#7f7f7f
-0.48516348,-5.185795,neighbor,3960646,Efficiently explaining the predictions of a probabilistic radial basis function classification network,0.09382468461990356,#7f7f7f
-4.2437234,-16.498455,neighbor,3960646,Visualization of higher-level receptive fields in a hierarchical model of the visual system,0.09506762027740479,#7f7f7f
-5.0059657,9.017695,neighbor,3960646,Deep Learning of Representations: Looking Forward,0.09532022476196289,#7f7f7f
-2.6685898,-0.9601453,neighbor,3960646,Learning multilayer perceptrons efficiently,0.09545093774795532,#7f7f7f
-0.12483411,-10.115664,neighbor,3960646,Machine learning for neuroscience,0.09546023607254028,#7f7f7f
-8.818779,-2.328744,neighbor,3960646,Recurrent neural networks,0.09573918581008911,#7f7f7f
-5.27328,3.331602,neighbor,3960646,Saturating Auto-Encoders,0.09592849016189575,#7f7f7f
-3.515525,14.083665,neighbor,3960646,Deconvolutional networks,0.09597229957580566,#7f7f7f
-2.320528,16.545769,neighbor,3960646,Inverting and Visualizing Features for Object Detection,0.09647798538208008,#7f7f7f
5.7759027,-3.1767032,neighbor,3960646,Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation,0.09654635190963745,#7f7f7f
-9.761353,0.09680264,neighbor,3960646,Advances in optimizing recurrent networks,0.09734898805618286,#7f7f7f
-3.308165,5.4799485,neighbor,3960646,The Attentive Perceptron,0.09815233945846558,#7f7f7f
-0.11386345,12.339874,neighbor,3960646,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,0.09885507822036743,#7f7f7f
-6.4876337,9.796958,neighbor,3960646,Better Mixing via Deep Representations,0.09903454780578613,#7f7f7f
-6.62955,3.3531852,neighbor,3960646,Switched linear encoding with rectified linear autoencoders,0.09911876916885376,#7f7f7f
-4.368179,-5.4431596,neighbor,3960646,Supervised Learning in Multilayer Spiking Neural Networks,0.09980964660644531,#7f7f7f
-6.0273128,6.9853253,neighbor,3960646,Knowledge Matters: Importance of Prior Information for Optimization,0.10016697645187378,#7f7f7f
-8.600308,-8.914402,neighbor,3960646,Hebbian Learning of Recurrent Connections: A Geometrical Perspective,0.10037893056869507,#7f7f7f
-4.9679327,-9.281166,neighbor,3960646,Architectural constraints on learning and memory function,0.10041564702987671,#7f7f7f
7.155598,6.7632403,neighbor,3960646,Deep Multiple Kernel Learning,0.10071074962615967,#7f7f7f
1.3895276,-12.969624,neighbor,3960646,Demonstrations of Neural Network Computations Involving Students,0.10098683834075928,#7f7f7f
-3.0291882,-14.022975,neighbor,3960646,Correction: On How Network Architecture Determines the Dominant Patterns of Spontaneous Neural Activity,0.10111016035079956,#7f7f7f
5.5353866,1.3650312,neighbor,3960646,Learning Feature Hierarchies with Centered Deep Boltzmann Machines,0.10155081748962402,#7f7f7f
3.884997,-7.028174,neighbor,3960646,Deep Unsupervised Learning on a Desktop PC: A Primer for Cognitive Scientists,0.10202991962432861,#7f7f7f
-9.750856,-2.3610327,neighbor,3960646,Complex Valued Recurrent Neural Network: From Architecture to Training,0.10222816467285156,#7f7f7f
7.5534844,-2.1219847,neighbor,3960646,Training Neural Networks with Stochastic Hessian-Free Optimization,0.10227638483047485,#7f7f7f
-7.915186,3.3455722,neighbor,3960646,Discriminative Recurrent Sparse Auto-Encoders,0.10235744714736938,#7f7f7f
4.934432,0.13406694,neighbor,3960646,On Training Deep Boltzmann Machines,0.1036260724067688,#7f7f7f
8.569743,5.414362,neighbor,3960646,Classifying Network Data with Deep Kernel Machines,0.10363143682479858,#7f7f7f
-8.428181,-0.7511532,neighbor,3960646,On the difficulty of training recurrent neural networks,0.10366815328598022,#7f7f7f
-8.101836,-11.965942,neighbor,3960646,Recurrent Processing during Object Recognition,0.10373616218566895,#7f7f7f
-2.4761164,-15.832338,neighbor,3960646,Correction: How Lateral Connections and Spiking Dynamics May Separate Multiple Objects Moving Together,0.10373729467391968,#7f7f7f
4.1299424,6.067025,neighbor,3960646,The Margitron: A Generalised Perceptron with Margin,0.10391592979431152,#7f7f7f
1.981729,-16.564493,neighbor,3960646,A novel 3D visualization tool for large-scale neural networks,0.10405057668685913,#7f7f7f
-4.7643437,-12.394856,neighbor,3960646,Marginalization in Neural Circuits with Divisive Normalization,0.10420489311218262,#7f7f7f
-2.2748575,-10.220289,neighbor,3960646,Neuroscience and AI Share the Same Elegant Mathematical Trap,0.1042141318321228,#7f7f7f
1.8317641,5.296537,neighbor,3960646,Dropout Training as Adaptive Regularization,0.1042170524597168,#7f7f7f
-1.6725657,-3.1468878,neighbor,3960646,Adaptive Cluster Expansion (ACE): A Multilayer Network for Estimating Probability Density Functions,0.10478055477142334,#7f7f7f
6.2399096,4.630808,neighbor,3960646,An Algorithm for Training Polynomial Networks,0.10479694604873657,#7f7f7f
-5.440481,-3.4119658,neighbor,3960646,Faster Training Using Fusion of Activation Functions for Feed Forward Neural Networks,0.10483860969543457,#7f7f7f
-9.519241,4.097511,neighbor,3960646,Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in DeSTIN,0.10485762357711792,#7f7f7f
-2.2234497,2.3172553,neighbor,3960646,Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary Independent Stochastic Neurons,0.10533773899078369,#7f7f7f
6.5453067,7.8292403,neighbor,3960646,Large-Margin kNN Classification Using a Deep Encoder Network,0.10550236701965332,#7f7f7f
5.129187,10.662853,neighbor,3960646,Horizontal and Vertical Ensemble with Deep Representation for Classification,0.10550636053085327,#7f7f7f
2.0344617,15.927314,neighbor,3960646,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.10568803548812866,#7f7f7f
-10.046953,-4.3053627,neighbor,3960646,Capacity measurement of a recurrent inhibitory neural network,0.1057579517364502,#7f7f7f
6.2698383,-11.74885,neighbor,3960646,Elementary derivative tasks and neural net multiscale analysis of tasks.,0.10603976249694824,#7f7f7f
-5.1718516,-15.264376,neighbor,3960646,A Two-Stage Cascade Model of BOLD Responses in Human Visual Cortex,0.10632801055908203,#7f7f7f
5.124381,-3.2030787,neighbor,3960646,Estimating or Propagating Gradients Through Stochastic Neurons,0.10674124956130981,#7f7f7f
2.5689013,-17.25203,neighbor,3960646,VisNEST — Interactive analysis of neural activity data,0.1072695255279541,#7f7f7f
8.118099,14.283264,neighbor,3960646,Handwritten Digits Recognition using Deep Convolutional Neural Network: An Experimental Study using EBlearn,0.10735362768173218,#7f7f7f
-1.1043775,-12.902775,neighbor,3960646,Brain architecture: a design for natural computation,0.10767978429794312,#7f7f7f
10.972604,13.088801,neighbor,3960646,Ускорение обучения нейронной сети для распознавания изображений с помощью технологии NVIDIA CUDA@@@Neural network training acceleration using NVIDIA CUDA technology for image recognition,0.10782504081726074,#7f7f7f
0.7661368,-11.203587,neighbor,3960646,"The Mind within the Net: Models of Learning, Thinking and Acting",0.10814309120178223,#7f7f7f
-11.477778,-0.31093907,neighbor,3960646,Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences,0.10830742120742798,#7f7f7f
12.176439,8.892504,neighbor,3960646,Interpolating destin features for image classification,0.10832130908966064,#7f7f7f
2.585975,-10.699291,neighbor,3960646,Statistical Physics of Feedforward Neural Networks,0.10841935873031616,#7f7f7f
1.6686825,-9.193111,neighbor,3960646,Neurally Implementable Semantic Networks,0.10842680931091309,#7f7f7f
1.3576093,15.072738,neighbor,3960646,Object Recognition with Multi-Scale Pyramidal Pooling Networks,0.10858434438705444,#7f7f7f
-4.9127684,13.198257,neighbor,3960646,Complexity of Representation and Inference in Compositional Models with Part Sharing,0.10870885848999023,#7f7f7f
0.84768015,-15.971541,neighbor,3960646,Neural Schematics as a unified formal graphical representation of large-scale Neural Network Structures,0.10901683568954468,#7f7f7f
-1.2353027,-14.761448,neighbor,3960646,The Neocortical Column,0.10946083068847656,#7f7f7f
-2.4763591,-12.090651,neighbor,3960646,Explicit Logic Circuits Discriminate Neural States,0.10949969291687012,#7f7f7f
5.6852584,15.33378,neighbor,3960646,Can Neural Networks Recognize Parts,0.10959368944168091,#7f7f7f
12.343933,13.305082,neighbor,3960646,GPU-based simulation of cellular neural networks for image processing,0.10965609550476074,#7f7f7f
-8.308967,-12.953397,neighbor,3960646,Modeling feature sharing between object detection and top-down attention,0.10976427793502808,#7f7f7f
-6.8586254,-3.5210147,neighbor,3960646,Hybrid Neural Network Architecture for On-Line Learning,0.10993540287017822,#7f7f7f
-3.4107666,-11.418338,neighbor,3960646,Neural computation with efficient population codes,0.10994088649749756,#7f7f7f
2.2351277,9.010561,neighbor,3960646,SELECTING NEURAL NETWORK ARCHITECTURE FOR INVESTMENT PROFITABILITY PREDICTIONS,0.11002588272094727,#7f7f7f
-10.567722,1.2744123,neighbor,3960646,Regularization and nonlinearities for neural language models: when are they needed?,0.11003386974334717,#7f7f7f
-9.656992,-5.698694,neighbor,3960646,Recurrent network of perceptrons with three state synapses achieves competitive classification on real inputs,0.11006128787994385,#7f7f7f
4.4710813,4.8532686,neighbor,3960646,Introduction to Machine Learning,0.11009013652801514,#7f7f7f
-4.3384233,-0.8113188,neighbor,3960646,A Global Algorithm for Training Multilayer Neural Networks,0.11031579971313477,#7f7f7f
-4.6419697,16.444767,neighbor,3960646,Feature grouping from spatially constrained multiplicative interaction,0.11046010255813599,#7f7f7f
5.5037746,-4.7490087,query,4555207,MobileNetV2: Inverted Residuals and Linear Bottlenecks,0.0,#7f7f7f
4.110292,-7.15301,neighbor,4555207,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,0.049284398555755615,#7f7f7f
7.2482653,-0.7373664,neighbor,4555207,ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,0.050575852394104004,#7f7f7f
-0.049556233,2.8940072,neighbor,4555207,ParseNet: Looking Wider to See Better,0.05557596683502197,#7f7f7f
-6.05746,-2.7745194,neighbor,4555207,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.06098848581314087,#7f7f7f
4.6622157,-1.6715487,neighbor,4555207,Clockwork Convnets for Video Semantic Segmentation,0.06140333414077759,#7f7f7f
2.9700828,3.814386,neighbor,4555207,Fully convolutional networks for semantic segmentation,0.06256192922592163,#7f7f7f
7.106491,-2.7971733,neighbor,4555207,A multitask deep learning model for real-time deployment in embedded systems,0.06325793266296387,#7f7f7f
0.13816935,1.7506944,neighbor,4555207,Context Tricks for Cheap Semantic Segmentation,0.06358033418655396,#7f7f7f
4.5859027,-1.5312718,neighbor,4555207,Recurrent Segmentation for Variable Computational Budgets,0.06363457441329956,#7f7f7f
7.4112473,2.34466,neighbor,4555207,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,0.0642557144165039,#7f7f7f
-2.8321142,4.421766,neighbor,4555207,Instance-Aware Semantic Segmentation via Multi-task Network Cascades,0.06481814384460449,#7f7f7f
2.3125706,4.573218,neighbor,4555207,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.06521707773208618,#7f7f7f
4.3623157,4.033179,neighbor,4555207,Understanding Convolution for Semantic Segmentation,0.0654059648513794,#7f7f7f
2.9212608,0.3848134,neighbor,4555207,Log-DenseNet: How to Sparsify a DenseNet,0.06578069925308228,#7f7f7f
3.0410478,1.9117191,neighbor,4555207,Beyond Forward Shortcuts: Fully Convolutional Master-Slave Networks (MSNets) with Backward Skip Connections for Semantic Segmentation,0.0658232569694519,#7f7f7f
-2.884281,4.9669213,neighbor,4555207,Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade,0.06587725877761841,#7f7f7f
4.6131706,-8.67994,neighbor,4555207,Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,0.06588137149810791,#7f7f7f
2.4083865,3.2311802,neighbor,4555207,Segmentation-aware convolutional nets,0.06668949127197266,#7f7f7f
0.008102932,-7.265565,neighbor,4555207,ChainerCV: a Library for Deep Learning in Computer Vision,0.06681638956069946,#7f7f7f
2.98897,5.751898,neighbor,4555207,Rethinking Atrous Convolution for Semantic Image Segmentation,0.06694638729095459,#7f7f7f
2.53507,4.988742,neighbor,4555207,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",0.0670090913772583,#7f7f7f
3.2475863,2.6510148,neighbor,4555207,Improving Fully Convolution Network for Semantic Segmentation,0.06722509860992432,#7f7f7f
-7.504328,-1.0486645,neighbor,4555207,Hypercolumns for object segmentation and fine-grained localization,0.06819456815719604,#7f7f7f
0.2904525,5.079938,neighbor,4555207,Combining the Best of Convolutional Layers and Recurrent Layers: A Hybrid Network for Semantic Segmentation,0.06835633516311646,#7f7f7f
-0.50298566,5.0513735,neighbor,4555207,Semantic Segmentation with Reverse Attention,0.06862860918045044,#7f7f7f
3.4064283,4.808502,neighbor,4555207,Progressively Diffused Networks for Semantic Image Segmentation,0.06878304481506348,#7f7f7f
5.422206,-5.8606014,neighbor,4555207,DeepCache: Principled Cache for Mobile Deep Vision,0.06898510456085205,#7f7f7f
1.9830478,3.1586788,neighbor,4555207,Learning Dense Convolutional Embeddings for Semantic Segmentation,0.06947523355484009,#7f7f7f
10.728146,0.75140816,neighbor,4555207,MultiNet: Real-time Joint Semantic Reasoning for Autonomous Driving,0.06969606876373291,#7f7f7f
0.9164729,-9.248739,neighbor,4555207,Deep Residual Learning for Image Recognition,0.0697486400604248,#7f7f7f
2.124194,0.7098439,neighbor,4555207,Neither Quick Nor Proper - Evaluation of QuickProp for Learning Deep Neural Networks,0.06994277238845825,#7f7f7f
-8.270883,0.5982988,neighbor,4555207,"Codemaps - Segment, Classify and Search Objects Locally",0.06995189189910889,#7f7f7f
9.951845,1.366489,neighbor,4555207,BlitzNet: A Real-Time Deep Network for Scene Understanding,0.07006222009658813,#7f7f7f
7.3936086,1.2728144,neighbor,4555207,LinkNet: Exploiting encoder representations for efficient semantic segmentation,0.07008928060531616,#7f7f7f
-5.571513,-7.7533307,neighbor,4555207,"YOLO9000: Better, Faster, Stronger",0.0701107382774353,#7f7f7f
0.63134956,4.0304594,neighbor,4555207,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.07013016939163208,#7f7f7f
0.65539795,-7.607976,neighbor,4555207,Rethinking the Inception Architecture for Computer Vision,0.07018768787384033,#7f7f7f
3.9406815,-8.186106,neighbor,4555207,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,0.07025057077407837,#7f7f7f
-0.8552454,5.116341,neighbor,4555207,Combining the Best of Graphical Models and ConvNets for Semantic Segmentation,0.07056087255477905,#7f7f7f
-4.2120914,-6.2683277,neighbor,4555207,Light-Head R-CNN: In Defense of Two-Stage Object Detector,0.07062947750091553,#7f7f7f
4.0788884,3.0940406,neighbor,4555207,Multi-Scale Context Aggregation by Dilated Convolutions,0.07141882181167603,#7f7f7f
-1.1430274,3.2034612,neighbor,4555207,Recalling Holistic Information for Semantic Segmentation,0.07143199443817139,#7f7f7f
6.2625155,2.9691324,neighbor,4555207,Squeeze-SegNet: a new fast deep convolutional neural network for semantic segmentation,0.0714484453201294,#7f7f7f
4.0140767,-7.848439,neighbor,4555207,Shoot to Know What: An Application of Deep Networks on Mobile Devices,0.07177716493606567,#7f7f7f
9.207299,0.8314439,neighbor,4555207,ApesNet: a pixel-wise efficient segmentation network,0.07192641496658325,#7f7f7f
-5.317502,-5.8670053,neighbor,4555207,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.07207566499710083,#7f7f7f
10.193849,0.99776876,neighbor,4555207,Fast Scene Understanding for Autonomous Driving,0.07213979959487915,#7f7f7f
-6.051698,-4.312526,neighbor,4555207,LocNet: Improving Localization Accuracy for Object Detection,0.07218891382217407,#7f7f7f
-4.936301,-5.311312,neighbor,4555207,Feature Selective Networks for Object Detection,0.07222765684127808,#7f7f7f
-1.9461921,1.9620863,neighbor,4555207,What's the Point: Semantic Segmentation with Point Supervision,0.07233560085296631,#7f7f7f
0.97235626,7.693877,neighbor,4555207,Fast Semantic Image Segmentation with High Order Context and Guided Filtering,0.07248938083648682,#7f7f7f
-6.0244565,-2.955239,neighbor,4555207,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.07260710000991821,#7f7f7f
-7.30481,-3.282569,neighbor,4555207,Single-Shot Object Detection with Enriched Semantics,0.0727154016494751,#7f7f7f
-7.0844994,-6.900572,neighbor,4555207,Mobile Video Object Detection with Temporally-Aware Feature Maps,0.0727265477180481,#7f7f7f
2.5750926,-8.060246,neighbor,4555207,DecomposeMe: Simplifying ConvNets for End-to-End Learning,0.07289987802505493,#7f7f7f
-0.13405229,-9.546378,neighbor,4555207,Spatially Adaptive Computation Time for Residual Networks,0.07293403148651123,#7f7f7f
4.2552648,5.8492475,neighbor,4555207,RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation,0.07331061363220215,#7f7f7f
5.4534073,1.4633373,neighbor,4555207,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling,0.07334482669830322,#7f7f7f
-1.7210926,4.8663707,neighbor,4555207,Fully Convolutional Instance-Aware Semantic Segmentation,0.07345879077911377,#7f7f7f
0.8087343,6.4134684,neighbor,4555207,Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation,0.07356816530227661,#7f7f7f
-3.3651989,-7.919346,neighbor,4555207,Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors,0.07385170459747314,#7f7f7f
-3.4387822,-5.788824,neighbor,4555207,Accurate Single Stage Detector Using Recurrent Rolling Convolution,0.07387107610702515,#7f7f7f
-6.276174,-1.9669371,neighbor,4555207,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.07402616739273071,#7f7f7f
2.8759294,-9.9287405,neighbor,4555207,WRPN: Wide Reduced-Precision Networks,0.07415276765823364,#7f7f7f
2.3163369,-7.1048055,neighbor,4555207,Depthwise Separable Convolutions for Neural Machine Translation,0.07428932189941406,#7f7f7f
10.184215,4.0479784,neighbor,4555207,SBNet: Sparse Blocks Network for Fast Inference,0.07436412572860718,#7f7f7f
-4.425714,-0.31302655,neighbor,4555207,Learning to Refine Object Segments,0.07449173927307129,#7f7f7f
-6.0625286,-6.6033826,neighbor,4555207,R-FCN-3000 at 30fps: Decoupling Detection and Classification,0.07487607002258301,#7f7f7f
-4.740126,-4.586734,neighbor,4555207,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.07505273818969727,#7f7f7f
4.2472568,1.8937663,neighbor,4555207,Feedforward semantic segmentation with zoom-out features,0.07505446672439575,#7f7f7f
1.5938127,2.3105311,neighbor,4555207,Per-Pixel Feedback for improving Semantic Segmentation,0.07513749599456787,#7f7f7f
-4.9333076,2.0500152,neighbor,4555207,Reversible Recursive Instance-Level Object Segmentation,0.0752297043800354,#7f7f7f
8.3993025,2.1657803,neighbor,4555207,Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes,0.07539933919906616,#7f7f7f
6.9185343,6.750884,neighbor,4555207,Semantic Video CNNs Through Representation Warping,0.07567834854125977,#7f7f7f
5.3226466,4.538255,neighbor,4555207,Dense CNN Learning with Equivalent Mappings,0.07569348812103271,#7f7f7f
1.5061873,-8.865409,neighbor,4555207,Going deeper with convolutions,0.07571065425872803,#7f7f7f
-7.0202055,-4.8378587,neighbor,4555207,Mid-level Elements for Object Detection,0.07583194971084595,#7f7f7f
-4.751349,-7.099834,neighbor,4555207,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.07592302560806274,#7f7f7f
-5.4631653,-5.4406505,neighbor,4555207,RON: Reverse Connection with Objectness Prior Networks for Object Detection,0.07620120048522949,#7f7f7f
-3.9678576,3.838719,neighbor,4555207,Pixelwise Instance Segmentation with a Dynamically Instantiated Network,0.07631576061248779,#7f7f7f
7.19504,2.0058157,neighbor,4555207,The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,0.07638055086135864,#7f7f7f
-5.7680044,0.11726761,neighbor,4555207,Mask R-CNN,0.07648992538452148,#7f7f7f
-4.3868704,-4.146239,neighbor,4555207,SCAN: Semantic Context Aware Network for Accurate Small Object Detection,0.07665383815765381,#7f7f7f
-4.3427587,-2.011858,neighbor,4555207,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.0767601728439331,#7f7f7f
1.610541,6.0984235,neighbor,4555207,Region-Based Semantic Segmentation with End-to-End Training,0.07699531316757202,#7f7f7f
3.6234987,7.604043,neighbor,4555207,Convolutional feature masking for joint object and stuff segmentation,0.07712870836257935,#7f7f7f
0.8999429,4.356322,neighbor,4555207,Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation,0.07714051008224487,#7f7f7f
2.241836,6.7940884,neighbor,4555207,Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform,0.07726913690567017,#7f7f7f
-4.0088053,-7.0047464,neighbor,4555207,Fast R-CNN,0.07728344202041626,#7f7f7f
-3.709034,-0.40321675,neighbor,4555207,The Middle Child Problem: Revisiting Parametric Min-Cut and Seeds for Object Proposals,0.07742488384246826,#7f7f7f
-2.4228468,0.90547544,neighbor,4555207,Extreme Clicking for Efficient Object Annotation,0.07750082015991211,#7f7f7f
3.2027626,-8.953298,neighbor,4555207,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.07757574319839478,#7f7f7f
1.1171632,-10.333295,neighbor,4555207,Residual Squeeze VGG16,0.07766097784042358,#7f7f7f
7.3026314,6.789164,neighbor,4555207,ConvNet Architecture Search for Spatiotemporal Feature Learning,0.07790392637252808,#7f7f7f
-2.8324347,6.8868876,neighbor,4555207,Loss Max-Pooling for Semantic Image Segmentation,0.07805973291397095,#7f7f7f
1.8762378,-9.068061,neighbor,4555207,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.07808393239974976,#7f7f7f
-2.9897642,-6.979348,neighbor,4555207,"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",0.07810992002487183,#7f7f7f
-0.5471059,6.800931,neighbor,4555207,Semi-supervised hierarchical semantic object parsing,0.07812273502349854,#7f7f7f
-5.2752333,-1.8349779,neighbor,4555207,Learning to decompose for object detection and instance segmentation,0.07817798852920532,#7f7f7f
7.619417,3.6403158,neighbor,4555207,Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding,0.078457772731781,#7f7f7f
3.3765526,4.272937,query,4714433,YOLOv3: An Incremental Improvement,0.0,#c7c7c7
5.274312,5.038796,neighbor,4714433,"YOLO9000: Better, Faster, Stronger",0.05388444662094116,#c7c7c7
-2.466057,-6.2962074,neighbor,4714433,Going deeper with convolutions,0.05961418151855469,#c7c7c7
5.2014046,4.2261343,neighbor,4714433,Fast YOLO: A Fast You Only Look Once System for Real-time Embedded Object Detection in Video,0.06430184841156006,#c7c7c7
9.913381,4.0315056,neighbor,4714433,Light-Head R-CNN: In Defense of Two-Stage Object Detector,0.0670396089553833,#c7c7c7
-4.4023075,-5.600023,neighbor,4714433,Deep Residual Learning for Image Recognition,0.06770056486129761,#c7c7c7
7.233119,2.168265,neighbor,4714433,Beyond Skip Connections: Top-Down Modulation for Object Detection,0.07034033536911011,#c7c7c7
-5.117211,-6.213025,neighbor,4714433,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",0.07133698463439941,#c7c7c7
9.273325,4.069461,neighbor,4714433,Fast R-CNN,0.07182979583740234,#c7c7c7
3.384123,-1.340816,neighbor,4714433,Spatially Adaptive Computation Time for Residual Networks,0.07280892133712769,#c7c7c7
-0.570493,-5.0438166,neighbor,4714433,Rethinking the Inception Architecture for Computer Vision,0.07491797208786011,#c7c7c7
6.649044,4.8101387,neighbor,4714433,R-FCN-3000 at 30fps: Decoupling Detection and Classification,0.07494908571243286,#c7c7c7
-2.7067766,-1.9682604,neighbor,4714433,Dual Path Networks,0.07515794038772583,#c7c7c7
7.1932,3.3436518,neighbor,4714433,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.07567012310028076,#c7c7c7
-3.930358,-8.399837,neighbor,4714433,Residual Squeeze VGG16,0.07575708627700806,#c7c7c7
-4.1477966,-4.2409267,neighbor,4714433,Aggregated Residual Transformations for Deep Neural Networks,0.07593262195587158,#c7c7c7
-10.493362,0.1287907,neighbor,4714433,SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size,0.07639825344085693,#c7c7c7
6.463072,-2.6120539,neighbor,4714433,ConvNet Architecture Search for Spatiotemporal Feature Learning,0.07643651962280273,#c7c7c7
-0.30654562,-1.1687945,neighbor,4714433,SkipNet: Learning Dynamic Routing in Convolutional Networks,0.07710647583007812,#c7c7c7
8.794578,5.90552,neighbor,4714433,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.07715481519699097,#c7c7c7
8.634628,2.5552528,neighbor,4714433,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.07718372344970703,#c7c7c7
10.6804695,3.4994438,neighbor,4714433,Wide-residual-inception networks for real-time object detection,0.07731074094772339,#c7c7c7
-8.591366,0.03824004,neighbor,4714433,Caffe con Troll: Shallow Ideas to Speed Up Deep Learning,0.07742786407470703,#c7c7c7
-4.4296927,1.2045512,neighbor,4714433,Fd-Mobilenet: Improved Mobilenet with a Fast Downsampling Strategy,0.0778922438621521,#c7c7c7
-0.82248074,6.39388,neighbor,4714433,Improvements to Context Based Self-Supervised Learning,0.07836252450942993,#c7c7c7
-7.817464,-0.663636,neighbor,4714433,FastNet: An Efficient Architecture for Smart Devices,0.07861590385437012,#c7c7c7
-1.5367359,-10.61916,neighbor,4714433,Backpropagation Training for Fisher Vectors within Neural Networks,0.07863980531692505,#c7c7c7
10.73002,6.1071033,neighbor,4714433,Yes-Net: An effective Detector Based on Global Information,0.07864344120025635,#c7c7c7
-7.3262258,-8.520119,neighbor,4714433,Swish: a Self-Gated Activation Function,0.07901144027709961,#c7c7c7
-10.758673,-4.7254167,neighbor,4714433,Incremental Training of Deep Convolutional Neural Networks,0.07911771535873413,#c7c7c7
-2.0517478,-1.2186849,neighbor,4714433,Log-DenseNet: How to Sparsify a DenseNet,0.07913404703140259,#c7c7c7
-2.8738954,-5.4393744,neighbor,4714433,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.07928735017776489,#c7c7c7
-8.654923,1.0264487,neighbor,4714433,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,0.07957041263580322,#c7c7c7
7.7112613,6.0828686,neighbor,4714433,An Implementation of Faster RCNN with Study for Region Sampling,0.07962548732757568,#c7c7c7
0.1093115,-6.526893,neighbor,4714433,Large-Scale Deep Learning on the YFCC100M Dataset,0.07970541715621948,#c7c7c7
-3.2087862,1.3941811,neighbor,4714433,MobileNetV2: Inverted Residuals and Linear Bottlenecks,0.08005863428115845,#c7c7c7
-5.060536,0.7871993,neighbor,4714433,IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image Classification,0.08017623424530029,#c7c7c7
-7.232582,-4.936043,neighbor,4714433,BlockDrop: Dynamic Inference Paths in Residual Networks,0.08020418882369995,#c7c7c7
-9.421761,-7.3812184,neighbor,4714433,Highway Networks,0.0803040862083435,#c7c7c7
-11.042171,-2.8803234,neighbor,4714433,Scale out for large minibatch SGD: Residual network training on ImageNet-1K with improved accuracy and reduced time to train,0.08049684762954712,#c7c7c7
4.7291627,-2.270978,neighbor,4714433,Dynamic Computational Time for Visual Attention,0.08056145906448364,#c7c7c7
-2.4221194,6.086975,neighbor,4714433,Temporal Ensembling for Semi-Supervised Learning,0.08081871271133423,#c7c7c7
10.287878,9.002921,neighbor,4714433,Improving Object Localization with Fitness NMS and Bounded IoU Loss,0.0809665322303772,#c7c7c7
2.466898,-4.7737083,neighbor,4714433,Fine-tuning deep CNN models on specific MS COCO categories,0.08106261491775513,#c7c7c7
1.6610794,-4.3428617,neighbor,4714433,On Pre-Trained Image Features and Synthetic Images for Deep Learning,0.08130228519439697,#c7c7c7
-1.2577683,-7.4703226,neighbor,4714433,Diving deeper into mentee networks,0.08148807287216187,#c7c7c7
-9.367273,-7.2127237,neighbor,4714433,Training Very Deep Networks,0.08155131340026855,#c7c7c7
-1.400342,-5.9956717,neighbor,4714433,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.08159500360488892,#c7c7c7
2.2671173,-0.23551388,neighbor,4714433,SaltiNet: Scan-Path Prediction on 360 Degree Images Using Saliency Volumes,0.0816195011138916,#c7c7c7
-8.302986,-2.5290954,neighbor,4714433,WRPN: Training and Inference using Wide Reduced-Precision Networks,0.08165055513381958,#c7c7c7
-1.5414811,-4.295681,neighbor,4714433,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.08173704147338867,#c7c7c7
-2.788,-3.4934344,neighbor,4714433,Binarized Neural Networks on the ImageNet Classification Task,0.08186191320419312,#c7c7c7
-1.7687888,-4.9082775,neighbor,4714433,Enhanced image classification with a fast-learning shallow convolutional neural network,0.08188897371292114,#c7c7c7
3.3563695,-9.171661,neighbor,4714433,VisualBackProp: visualizing CNNs for autonomous driving,0.08193129301071167,#c7c7c7
8.000466,7.5769057,neighbor,4714433,G-CNN: Object Detection via Grid Convolutional Neural Network,0.0819661021232605,#c7c7c7
-11.664557,-2.7341588,neighbor,4714433,Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15 Minutes,0.08197778463363647,#c7c7c7
-1.0179772,6.3447337,neighbor,4714433,Multi-task Self-Supervised Visual Learning,0.08238428831100464,#c7c7c7
8.413829,-0.6259774,neighbor,4714433,Mobile Video Object Detection with Temporally-Aware Feature Maps,0.0824730396270752,#c7c7c7
12.340797,3.6331654,neighbor,4714433,Revisiting RCNN: On Awakening the Classification Power of Faster RCNN,0.08256536722183228,#c7c7c7
8.090701,8.197057,neighbor,4714433,G-CNN: An Iterative Grid Based Object Detector,0.08265024423599243,#c7c7c7
9.270538,6.4980006,neighbor,4714433,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.0828862190246582,#c7c7c7
-1.979216,-9.960316,neighbor,4714433,Improved Bilinear Pooling with CNNs,0.08301657438278198,#c7c7c7
7.234498,4.1689677,neighbor,4714433,Scalable Object Detection Using Deep Neural Networks,0.08305847644805908,#c7c7c7
8.603251,-3.6495774,neighbor,4714433,Depth-Adaptive Computational Policies for Efficient Visual Tracking,0.08319497108459473,#c7c7c7
10.762928,2.063718,neighbor,4714433,Receptive Field Block Net for Accurate and Fast Object Detection,0.08320450782775879,#c7c7c7
13.40473,6.42066,neighbor,4714433,Self Paced Deep Learning for Weakly Supervised Object Detection,0.08321183919906616,#c7c7c7
2.0369797,-9.228006,neighbor,4714433,Visualizing and Understanding Convolutional Networks,0.08338135480880737,#c7c7c7
-6.1754956,-6.857727,neighbor,4714433,Deep Residual Networks with Exponential Linear Unit,0.08349049091339111,#c7c7c7
-8.308428,-2.4902248,neighbor,4714433,WRPN: Wide Reduced-Precision Networks,0.08351892232894897,#c7c7c7
-2.560217,-7.3074727,neighbor,4714433,Striving for Simplicity: The All Convolutional Net,0.08357739448547363,#c7c7c7
3.938399,0.9844356,neighbor,4714433,Learn To Pay Attention,0.08359032869338989,#c7c7c7
12.254934,4.846594,neighbor,4714433,Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids,0.08369266986846924,#c7c7c7
10.192524,-3.1897633,neighbor,4714433,The Visual Object Tracking VOT2017 Challenge Results,0.08376002311706543,#c7c7c7
8.513188,3.5622303,neighbor,4714433,Focal Loss for Dense Object Detection,0.08414393663406372,#c7c7c7
9.750568,7.669654,neighbor,4714433,LocNet: Improving Localization Accuracy for Object Detection,0.08415329456329346,#c7c7c7
11.740797,7.2491856,neighbor,4714433,End-to-End Training of Object Class Detectors for Mean Average Precision,0.08416396379470825,#c7c7c7
-8.35124,-9.019405,neighbor,4714433,Gradient Normalization & Depth Based Decay For Deep Learning,0.08423101902008057,#c7c7c7
-5.2600975,-4.622119,neighbor,4714433,Improving the Robustness of Deep Neural Networks via Stability Training,0.08429008722305298,#c7c7c7
8.653625,-3.9388173,neighbor,4714433,Learning Policies for Adaptive Tracking with Deep Feature Cascades,0.08429902791976929,#c7c7c7
-11.912025,-3.2653525,neighbor,4714433,"Don't Decay the Learning Rate, Increase the Batch Size",0.0844683051109314,#c7c7c7
-6.7948585,-6.0679893,neighbor,4714433,Wide Residual Networks,0.08460801839828491,#c7c7c7
-5.5884576,-2.0725777,neighbor,4714433,Group Normalization,0.08464241027832031,#c7c7c7
6.3708134,8.950851,neighbor,4714433,Spot the Difference by Object Detection,0.08472520112991333,#c7c7c7
-7.810752,-7.116748,neighbor,4714433,FractalNet: Ultra-Deep Neural Networks without Residuals,0.08478391170501709,#c7c7c7
9.211572,5.137699,neighbor,4714433,RON: Reverse Connection with Objectness Prior Networks for Object Detection,0.08490139245986938,#c7c7c7
-0.75568086,-11.658075,neighbor,4714433,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",0.08491230010986328,#c7c7c7
10.196039,5.16319,neighbor,4714433,PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection,0.08495253324508667,#c7c7c7
6.518396,1.3215555,neighbor,4714433,Improving Object Counting with Heatmap Regulation,0.08495813608169556,#c7c7c7
-3.526814,-6.811552,neighbor,4714433,Xception: Deep Learning with Depthwise Separable Convolutions,0.08505690097808838,#c7c7c7
-6.0503635,-2.377526,neighbor,4714433,ImageNet pre-trained models with batch normalization,0.0850667953491211,#c7c7c7
2.1861167,-9.270494,neighbor,4714433,Picasso: A Modular Framework for Visualizing the Learning Process of Neural Network Image Classifiers,0.08509039878845215,#c7c7c7
-2.1180341,0.7411563,neighbor,4714433,ParseNet: Looking Wider to See Better,0.08510351181030273,#c7c7c7
9.48165,-2.9832826,neighbor,4714433,Lucid Data Dreaming for Multiple Object Tracking,0.08513379096984863,#c7c7c7
3.4785144,0.6727386,neighbor,4714433,Training Deep Networks to be Spatially Sensitive,0.08516228199005127,#c7c7c7
3.2558708,6.6705465,neighbor,4714433,Fine-grained object recognition with Gnostic Fields,0.08517926931381226,#c7c7c7
8.246284,-1.23099,neighbor,4714433,Context Matters: Refining Object Detection in Video with Recurrent Neural Networks,0.08518457412719727,#c7c7c7
-10.101404,-4.8042817,neighbor,4714433,FreezeOut: Accelerate Training by Progressively Freezing Layers,0.08520275354385376,#c7c7c7
-10.192151,0.4459327,neighbor,4714433,SqueezeNext: Hardware-Aware Neural Network Design,0.08529388904571533,#c7c7c7
-5.877489,-8.138126,neighbor,4714433,Parametric Exponential Linear Unit for Deep Convolutional Neural Networks,0.0854983925819397,#c7c7c7
-10.666899,-2.2211144,neighbor,4714433,ImageNet Training in Minutes,0.0856090784072876,#c7c7c7
8.514245,4.8603244,neighbor,4714433,Feature Selective Networks for Object Detection,0.08589029312133789,#c7c7c7
-1.235059,1.0249939,query,4766599,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,0.0,#c7c7c7
-0.5881406,0.94210225,neighbor,4766599,Learning to generate images with perceptual similarity metrics,0.045368731021881104,#c7c7c7
8.833766,13.493595,neighbor,4766599,The differential geometry of perceptual similarity,0.06147503852844238,#c7c7c7
-5.1321945,9.041646,neighbor,4766599,Brain-Inspired Deep Networks for Image Aesthetics Assessment,0.06259554624557495,#c7c7c7
0.70588547,-0.3237253,neighbor,4766599,The Perception-Distortion Tradeoff,0.06277543306350708,#c7c7c7
-0.015982727,-3.452155,neighbor,4766599,Learned perceptual image enhancement,0.06320643424987793,#c7c7c7
-3.9464798,-0.42822847,neighbor,4766599,Eigen-Distortions of Hierarchical Representations,0.06328928470611572,#c7c7c7
-11.326779,-3.5111358,neighbor,4766599,PsyPhy: A Psychophysics Driven Evaluation Framework for Visual Recognition,0.06429082155227661,#c7c7c7
-9.983333,-2.328352,neighbor,4766599,Human perception in computer vision,0.065562903881073,#c7c7c7
10.207635,5.562163,neighbor,4766599,Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual Image Quality Index,0.06610971689224243,#c7c7c7
4.216427,1.0059493,neighbor,4766599,RankIQA: Learning from Rankings for No-Reference Image Quality Assessment,0.06740343570709229,#c7c7c7
3.5082,2.0723045,neighbor,4766599,Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment,0.06880968809127808,#c7c7c7
7.55121,-6.341341,neighbor,4766599,Learning to compare image patches via convolutional neural networks,0.06940406560897827,#c7c7c7
1.3996847,-0.32122993,neighbor,4766599,RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment,0.06998378038406372,#c7c7c7
-12.585759,-1.1546838,neighbor,4766599,Deep Convolutional Neural Networks Outperform Feature-Based But Not Categorical Models in Explaining Object Similarity Judgments,0.07116633653640747,#c7c7c7
7.210319,13.059551,neighbor,4766599,Visual Equivalence: an Object-based Approach to Image Quality,0.07120150327682495,#c7c7c7
8.412298,10.139804,neighbor,4766599,Analysis of the Difference of Gaussians Model in Image Difference Metrics,0.07166630029678345,#c7c7c7
-1.6317979,10.552263,neighbor,4766599,A comparative study of computational aesthetics,0.07254111766815186,#c7c7c7
8.214381,8.726928,neighbor,4766599,Simulating binocular vision for no-reference 3D visual quality measurement.,0.07268315553665161,#c7c7c7
-2.7836874,-1.2964177,neighbor,4766599,Quality Resilient Deep Neural Networks,0.07404696941375732,#c7c7c7
6.033759,10.695027,neighbor,4766599,Predicting Image Differences Based on Image-Difference Features,0.07502681016921997,#c7c7c7
3.7395096,3.0816267,neighbor,4766599,On the use of deep learning for blind image quality assessment,0.07527750730514526,#c7c7c7
-3.0624616,7.264868,neighbor,4766599,NIMA: Neural Image Assessment,0.0757630467414856,#c7c7c7
-1.9244189,-1.7249979,neighbor,4766599,Image Quality Assessment Guided Deep Neural Networks Training,0.07610118389129639,#c7c7c7
4.6776896,11.465756,neighbor,4766599,Maximum differentiation (MAD) competition: a methodology for comparing computational models of perceptual quantities.,0.07624268531799316,#c7c7c7
-9.422745,-8.449186,neighbor,4766599,Towards Metamerism via Foveated Style Transfer,0.0764201283454895,#c7c7c7
7.3886747,6.8997426,neighbor,4766599,"Correction: A No Reference Image Quality Assessment Metric Based on Visual Perception. Algorithms 2016, 9, 87",0.0764317512512207,#c7c7c7
-1.986031,-3.1401641,neighbor,4766599,Loss Functions for Neural Networks for Image Processing,0.07651972770690918,#c7c7c7
6.4318047,13.559592,neighbor,4766599,Perceptually Optimized Image Rendering,0.07669687271118164,#c7c7c7
-2.798416,10.149575,neighbor,4766599,Image Aesthetic Assessment: An experimental survey,0.0768011212348938,#c7c7c7
9.35058,-5.254334,neighbor,4766599,Learning Fine-Grained Image Similarity with Deep Ranking,0.07695996761322021,#c7c7c7
-9.581898,-6.7564077,neighbor,4766599,SideEye: A Generative Neural Network Based Simulator of Human Peripheral Vision,0.07702261209487915,#c7c7c7
8.661819,8.6005745,neighbor,4766599,Modeling the Perceptual Quality of Stereoscopic Images in the Primary Visual Cortex,0.07711923122406006,#c7c7c7
-0.17932552,-11.016864,neighbor,4766599,Guetzli: Perceptually Guided JPEG Encoder,0.07742869853973389,#c7c7c7
-10.24325,2.4410532,neighbor,4766599,Deep Spatial Pyramid: The Devil is Once Again in the Details,0.07781445980072021,#c7c7c7
5.3593817,-6.269547,neighbor,4766599,Patch Correspondences for Interpreting Pixel-level CNNs,0.07784801721572876,#c7c7c7
-12.658369,-2.3108425,neighbor,4766599,Modeling human categorization of natural images using deep feature representations,0.07808220386505127,#c7c7c7
-0.51004523,2.6460495,neighbor,4766599,On the Diversity of Realistic Image Synthesis,0.07828128337860107,#c7c7c7
-1.4284456,7.522597,neighbor,4766599,Learning a Discriminative Model for the Perception of Realism in Composite Images,0.07837599515914917,#c7c7c7
2.393433,2.970741,neighbor,4766599,Image quality assessment using deep convolutional networks,0.07860088348388672,#c7c7c7
5.0146546,8.467023,neighbor,4766599,Supporting visual quality assessment with machine learning,0.07896566390991211,#c7c7c7
7.23028,5.311176,neighbor,4766599,Image Quality Assessment Based on Local Linear Information and Distortion-Specific Compensation,0.07898426055908203,#c7c7c7
-13.567487,-1.3016388,neighbor,4766599,Adapting Deep Network Features to Capture Psychological Representations: An Abridged Report,0.07926708459854126,#c7c7c7
-1.7533668,-5.2467937,neighbor,4766599,One-To-Many Network for Visually Pleasing Compression Artifacts Reduction,0.07985121011734009,#c7c7c7
7.7251863,3.0951416,neighbor,4766599,Saliency-based deep convolutional neural network for no-reference image quality assessment,0.08026319742202759,#c7c7c7
-7.772919,-4.0848103,neighbor,4766599,Perception of suprathreshold naturalistic changes in colored natural images.,0.08046609163284302,#c7c7c7
9.664538,6.1328363,neighbor,4766599,Mean Deviation Similarity Index: Efficient and Reliable Full-Reference Image Quality Evaluator,0.08060067892074585,#c7c7c7
-10.615232,-7.8487787,neighbor,4766599,Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks,0.0807761549949646,#c7c7c7
11.152845,9.586222,neighbor,4766599,Perceptual Relevance Measure for Generic Shape Coding,0.08121705055236816,#c7c7c7
6.995053,7.6262074,neighbor,4766599,"Visual quality assessment: recent developments, coding applications and future trends",0.08131927251815796,#c7c7c7
6.6649246,10.331647,neighbor,4766599,The Impact of Image-Difference Features on Perceived Image Differences,0.08155721426010132,#c7c7c7
-1.6100671,4.9631596,neighbor,4766599,Deep Image Harmonization,0.0816698670387268,#c7c7c7
6.6469526,14.624481,neighbor,4766599,Transformation-aware perceptual image metric,0.08172261714935303,#c7c7c7
6.0405545,-8.134456,neighbor,4766599,Neural Color Transfer between Images,0.08179116249084473,#c7c7c7
-2.6566782,-6.7671227,neighbor,4766599,Semantic Perceptual Image Compression Using Deep Convolution Networks,0.08184927701950073,#c7c7c7
-0.3147223,-9.877375,neighbor,4766599,End-to-end optimization of nonlinear transform codes for perceptual quality,0.0823867917060852,#c7c7c7
0.7031037,-4.848334,neighbor,4766599,Fast Image Processing with Fully-Convolutional Networks,0.08265095949172974,#c7c7c7
-13.424813,-2.0904207,neighbor,4766599,Evaluating (and Improving) the Correspondence Between Deep Neural Networks and Human Representations,0.08269000053405762,#c7c7c7
-4.70166,1.224041,neighbor,4766599,Lie group impression for deep learning,0.08269143104553223,#c7c7c7
0.03683865,-1.2022008,neighbor,4766599,Deep Image Prior,0.08294928073883057,#c7c7c7
-3.714237,-5.175586,neighbor,4766599,Local Blur Mapping: Exploiting High-Level Semantics by Deep Neural Networks,0.08323472738265991,#c7c7c7
5.2787185,2.0343347,neighbor,4766599,dipIQ: Blind Image Quality Assessment by Learning-to-Rank Discriminable Image Pairs,0.08353739976882935,#c7c7c7
-3.4906144,9.197889,neighbor,4766599,Photo Aesthetics Ranking Network with Attributes and Content Adaptation,0.0836939811706543,#c7c7c7
-5.6991253,11.464993,neighbor,4766599,Efficient Deep Aesthetic Image Classification using Connected Local and Global Features,0.08385932445526123,#c7c7c7
4.6993127,3.3410268,neighbor,4766599,A Probabilistic Quality Representation Approach to Deep Blind Image Quality Prediction,0.083884596824646,#c7c7c7
-4.665126,11.903893,neighbor,4766599,Image Aesthetic Evaluation Using Parallel Deep Convolution Neural Network,0.08400565385818481,#c7c7c7
1.8860735,-1.5206158,neighbor,4766599,Image Quality Assessment Techniques Show Improved Training and Evaluation of Autoencoder Generative Adversarial Networks,0.08404314517974854,#c7c7c7
9.60065,11.959825,neighbor,4766599,Structural Similarity Index SSIMplified: Is there really a simpler concept at the heart of image quality measurement?,0.08409833908081055,#c7c7c7
5.249214,4.967331,neighbor,4766599,Blind Image Quality Assessment Bases On Natural Scene Statistics And Deep Learning,0.08416491746902466,#c7c7c7
-11.120256,-7.610214,neighbor,4766599,A parametric texture model based on deep convolutional features closely matches texture appearance for humans,0.08418154716491699,#c7c7c7
11.309156,6.7189837,neighbor,4766599,Hierarchical Gradient Similarity Based Video Quality Assessment Metric,0.08453327417373657,#c7c7c7
-4.179885,9.711473,neighbor,4766599,Learning Photography Aesthetics with Deep CNNs,0.0846521258354187,#c7c7c7
7.934575,-7.822781,neighbor,4766599,Optical Flow Requires Multiple Strategies (but Only One Network),0.08469444513320923,#c7c7c7
9.902266,7.646169,neighbor,4766599,Contrast and Visual Saliency Similarity-Induced Index for Assessing Image Quality,0.08535099029541016,#c7c7c7
7.420818,-3.5115292,neighbor,4766599,Deep Fishing: Gradient Features from Deep Nets,0.08581298589706421,#c7c7c7
-9.125561,-5.747018,neighbor,4766599,Filling in the details: Perceiving from low fidelity images,0.08591383695602417,#c7c7c7
-2.3566017,4.5315466,neighbor,4766599,Laplacian-Steered Neural Style Transfer,0.08602869510650635,#c7c7c7
-10.751118,0.7159302,neighbor,4766599,Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons,0.0860709547996521,#c7c7c7
1.2460049,-5.1007833,neighbor,4766599,Deep bilateral learning for real-time image enhancement,0.08615928888320923,#c7c7c7
5.958001,-8.339707,neighbor,4766599,Progressive Color Transfer With Dense Semantic Correspondences,0.08618664741516113,#c7c7c7
-0.06761408,-11.619859,neighbor,4766599,Users prefer Guetzli JPEG over same-sized libjpeg,0.08621066808700562,#c7c7c7
5.773847,5.577977,neighbor,4766599,Learn to Evaluate Image Perceptual Quality Blindly from Statistics of Self-similarity,0.08628034591674805,#c7c7c7
-7.5409064,-1.3628082,neighbor,4766599,Confusing Deep Convolution Networks by Relabelling,0.08646434545516968,#c7c7c7
8.737479,-5.158841,neighbor,4766599,Image similarity using Deep CNN and Curriculum Learning,0.08659446239471436,#c7c7c7
-11.183661,-1.5890373,neighbor,4766599,"Large-Scale, High-Resolution Comparison of the Core Visual Object Recognition Behavior of Humans, Monkeys, and State-of-the-Art Deep Artificial Neural Networks",0.0866047739982605,#c7c7c7
-9.480085,2.617145,neighbor,4766599,The treasure beneath convolutional layers: Cross-convolutional-layer pooling for image classification,0.08670663833618164,#c7c7c7
-9.755573,0.36708316,neighbor,4766599,Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition,0.0867270827293396,#c7c7c7
-6.608384,1.15385,neighbor,4766599,Towards Distortion-Predictable Embedding of Neural Networks,0.08674275875091553,#c7c7c7
1.1146885,-6.589109,neighbor,4766599,DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks,0.08693408966064453,#c7c7c7
-5.9423523,10.601262,neighbor,4766599,ILGNet: inception modules with connected local and global features for efficient image aesthetic quality classification using domain adaptation,0.08702433109283447,#c7c7c7
-4.757773,10.546507,neighbor,4766599,Visual aesthetic analysis using deep neural network: model and techniques to increase accuracy without transfer learning,0.08703947067260742,#c7c7c7
10.274657,-5.1964045,neighbor,4766599,Deep Metric Learning with Angular Loss,0.08717840909957886,#c7c7c7
-9.768263,3.7514157,neighbor,4766599,Local Color Contrastive Descriptor for Image Classification,0.08720207214355469,#c7c7c7
2.9367704,-1.8228621,neighbor,4766599,Adversarial Diversity and Hard Positive Generation,0.0872153639793396,#c7c7c7
-8.308766,-1.5116894,neighbor,4766599,Deep neural networks are easily fooled: High confidence predictions for unrecognizable images,0.08728581666946411,#c7c7c7
-1.975428,-6.1825075,neighbor,4766599,CAS-CNN: A deep convolutional neural network for image compression artifact suppression,0.08752131462097168,#c7c7c7
5.071034,-1.1073169,neighbor,4766599,Semantic Jitter: Dense Supervision for Visual Comparisons via Synthetic Images,0.08768260478973389,#c7c7c7
-3.8007224,11.196164,neighbor,4766599,A-Lamp: Adaptive Layout-Aware Multi-patch Deep Convolutional Neural Network for Photo Aesthetic Assessment,0.0877191424369812,#c7c7c7
-8.304319,1.3001282,neighbor,4766599,Appearance invariance in convolutional networks with neighborhood similarity,0.0877637267112732,#c7c7c7
8.541534,6.125178,neighbor,4766599,Image Quality Assessment Based on Inter-Patch and Intra-Patch Similarity,0.0878148078918457,#c7c7c7
-4.843413,-2.303926,neighbor,4766599,Hallucination from noon to night images using CNN,0.08781719207763672,#c7c7c7
7.625394,1.3871127,query,49670925,Representation Learning with Contrastive Predictive Coding,0.0,#c7c7c7
2.555539,-1.9071776,neighbor,49670925,Neural Discrete Representation Learning,0.04944568872451782,#c7c7c7
-7.7966456,8.195493,neighbor,49670925,On the Limits of Learning Representations with Label-Based Supervision,0.05198097229003906,#c7c7c7
7.6751866,-6.6318393,neighbor,49670925,Deep Predictive Coding Networks,0.05355215072631836,#c7c7c7
11.71161,-1.7512853,neighbor,49670925,BRUNO: A Deep Recurrent Model for Exchangeable Data,0.05419921875,#c7c7c7
7.9562035,7.52341,neighbor,49670925,Learning with Hierarchical-Deep Models,0.056842803955078125,#c7c7c7
9.772981,-4.083088,neighbor,49670925,Deep Generative Networks For Sequence Prediction,0.05738157033920288,#c7c7c7
3.3073747,-0.5175795,neighbor,49670925,Associative Compression Networks for Representation Learning,0.05880105495452881,#c7c7c7
-9.624591,-2.894532,neighbor,49670925,Scoring and Classifying with Gated Auto-Encoders,0.05938434600830078,#c7c7c7
1.4336714,-2.1141255,neighbor,49670925,Variational Lossy Autoencoder,0.05966782569885254,#c7c7c7
1.8763967,3.6131945,neighbor,49670925,Autoencoders Learn Generative Linear Models,0.059764564037323,#c7c7c7
12.13056,1.2584227,neighbor,49670925,The Variational Homoencoder: Learning to learn high capacity generative models from few examples,0.060050904750823975,#c7c7c7
10.549477,1.0399936,neighbor,49670925,Early Visual Concept Learning with Unsupervised Deep Learning,0.06006890535354614,#c7c7c7
-10.347615,0.7730601,neighbor,49670925,Learning Approximate Stochastic Transition Models,0.06226831674575806,#c7c7c7
0.9425514,-3.3510077,neighbor,49670925,PixelVAE: A Latent Variable Model for Natural Images,0.06266975402832031,#c7c7c7
9.436816,-5.146859,neighbor,49670925,Disentangled Sequential Autoencoder,0.06275880336761475,#c7c7c7
-3.713983,2.181348,neighbor,49670925,Implicit Autoencoders,0.06282109022140503,#c7c7c7
12.99226,1.447769,neighbor,49670925,One-Shot Generalization in Deep Generative Models,0.06321227550506592,#c7c7c7
-10.5868435,8.314744,neighbor,49670925,Unsupervised Feature Learning with Discriminative Encoder,0.06325012445449829,#c7c7c7
-12.160847,6.0491557,neighbor,49670925,Generative Adversarial Positive-Unlabelled Learning,0.0632585883140564,#c7c7c7
5.9134197,1.1091665,neighbor,49670925,On Deep Multi-View Representation Learning,0.06343293190002441,#c7c7c7
9.192619,-1.2398366,neighbor,49670925,Learning to Adapt by Minimizing Discrepancy,0.06344413757324219,#c7c7c7
13.886944,0.4425682,neighbor,49670925,Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions,0.06359559297561646,#c7c7c7
-3.628574,-4.550514,neighbor,49670925,JADE: Joint Autoencoders for Dis-Entanglement,0.06360363960266113,#c7c7c7
12.618,3.909104,neighbor,49670925,Generative Low-Shot Network Expansion,0.06442016363143921,#c7c7c7
-7.2085934,-6.1279488,neighbor,49670925,Sum-Product Autoencoding: Encoding and Decoding Representations Using Sum-Product Networks,0.06543892621994019,#c7c7c7
8.443289,3.1530612,neighbor,49670925,Sequential Cost-Sensitive Feature Acquisition,0.065493643283844,#c7c7c7
7.1681714,7.4590173,neighbor,49670925,Deep learning,0.06556427478790283,#c7c7c7
-8.004553,5.9490943,neighbor,49670925,Bayesian Conditional Generative Adverserial Networks,0.06559914350509644,#c7c7c7
7.7775316,-9.695369,neighbor,49670925,Prediction Under Uncertainty with Error-Encoding Networks,0.06568008661270142,#c7c7c7
12.774713,-1.82673,neighbor,49670925,Discovering Order in Unordered Datasets: Generative Markov Networks,0.06571733951568604,#c7c7c7
2.2105713,2.3354974,neighbor,49670925,Deep AutoRegressive Networks,0.0660049319267273,#c7c7c7
2.225653,8.279485,neighbor,49670925,Meta-Learning Update Rules for Unsupervised Representation Learning,0.06603866815567017,#c7c7c7
-0.66331434,10.142416,neighbor,49670925,Unsupervised Learning by Predicting Noise,0.0660664439201355,#c7c7c7
7.366945,-3.0854673,neighbor,49670925,A Recurrent Latent Variable Model for Supervised Modeling of High-Dimensional Sequential Data,0.0660851001739502,#c7c7c7
-10.360353,1.8149676,neighbor,49670925,GAN Q-learning,0.06620156764984131,#c7c7c7
-0.45362228,-3.5194852,neighbor,49670925,Variational Composite Autoencoders,0.06648218631744385,#c7c7c7
-10.873026,-4.3511305,neighbor,49670925,Conditional Restricted Boltzmann Machines for Structured Output Prediction,0.06648898124694824,#c7c7c7
-3.3739862,9.571902,neighbor,49670925,Unsupervised Representation Adversarial Learning Network: from Reconstruction to Generation,0.06660324335098267,#c7c7c7
-7.591366,9.161135,neighbor,49670925,Invariant Representations without Adversarial Training,0.06667524576187134,#c7c7c7
6.7982306,12.401296,neighbor,49670925,Learning Deep Structured Models,0.06669938564300537,#c7c7c7
-2.419752,0.897119,neighbor,49670925,Flexible and accurate inference and learning for deep generative models,0.06703060865402222,#c7c7c7
3.1072092,-5.8780193,neighbor,49670925,Nonparametric Variational Auto-Encoders for Hierarchical Representation Learning,0.06708282232284546,#c7c7c7
-7.089038,12.0134735,neighbor,49670925,Bayesian representation learning with oracle constraints,0.06712579727172852,#c7c7c7
0.6872665,2.471474,neighbor,49670925,Inverting Supervised Representations with Autoregressive Neural Density Models,0.06721162796020508,#c7c7c7
-2.8977935,5.3610716,neighbor,49670925,Inferencing based on unsupervised learning of disentangled representations,0.06723970174789429,#c7c7c7
-2.0464926,-3.6574206,neighbor,49670925,The Variational Fair Autoencoder,0.0676267147064209,#c7c7c7
13.174171,2.9960704,neighbor,49670925,Generalized Zero-Shot Learning via Synthesized Examples,0.06775182485580444,#c7c7c7
7.3926096,-7.82184,neighbor,49670925,Learning to Linearize Under Uncertainty,0.06777018308639526,#c7c7c7
-0.09490046,-2.2563646,neighbor,49670925,Channel-Recurrent Variational Autoencoders,0.06790333986282349,#c7c7c7
-4.243214,-6.1593885,neighbor,49670925,Auto-Encoding Total Correlation Explanation,0.06794416904449463,#c7c7c7
6.7581115,-2.1736357,neighbor,49670925,Predictive-State Decoders: Encoding the Future into Recurrent Networks,0.06802123785018921,#c7c7c7
8.506335,8.579197,neighbor,49670925,Towards deep compositional networks,0.06813704967498779,#c7c7c7
-6.649139,2.9589078,neighbor,49670925,Continual Learning in Generative Adversarial Nets,0.06816285848617554,#c7c7c7
-9.951121,2.6930816,neighbor,49670925,ACtuAL: Actor-Critic Under Adversarial Learning,0.0683014988899231,#c7c7c7
4.252062,6.698067,neighbor,49670925,Pre-training Attention Mechanisms,0.06833463907241821,#c7c7c7
2.0129924,-0.3804481,neighbor,49670925,Towards Conceptual Compression,0.0683743953704834,#c7c7c7
3.3952796,-3.4658413,neighbor,49670925,Deep Quantization: Encoding Convolutional Activations with Deep Generative Model,0.0684698224067688,#c7c7c7
5.6077204,6.6084557,neighbor,49670925,Discriminative Recurrent Sparse Auto-Encoders,0.06858700513839722,#c7c7c7
-3.679123,-1.64873,neighbor,49670925,Learning Disentangled Representations with Semi-Supervised Deep Generative Models,0.06879293918609619,#c7c7c7
-0.8056806,12.356342,neighbor,49670925,Deep Representation Learning with Target Coding,0.06887704133987427,#c7c7c7
-1.1586207,-0.5474556,neighbor,49670925,"Variational Autoencoder for Deep Learning of Images, Labels and Captions",0.06906348466873169,#c7c7c7
5.546238,8.4634,neighbor,49670925,Natural Neural Networks,0.06913059949874878,#c7c7c7
-5.05532,2.6645885,neighbor,49670925,Learning the Base Distribution in Implicit Generative Models,0.06920874118804932,#c7c7c7
8.740158,-2.1982348,neighbor,49670925,Generative Temporal Models with Memory,0.06941491365432739,#c7c7c7
-3.5286028,-0.63599634,neighbor,49670925,Inverting Variational Autoencoders for Improved Generative Accuracy,0.06947869062423706,#c7c7c7
10.007038,8.903916,neighbor,49670925,Recurrent Ladder Networks,0.06957191228866577,#c7c7c7
-13.828451,1.7437894,neighbor,49670925,Data Augmentation via Levy Processes,0.06967359781265259,#c7c7c7
0.5657492,4.599133,neighbor,49670925,Generative Class-conditional Autoencoders,0.06967610120773315,#c7c7c7
-10.572251,10.467624,neighbor,49670925,Stochastic feature mapping for PAC-Bayes classification,0.06969553232192993,#c7c7c7
-6.8410997,-0.66218793,neighbor,49670925,Wasserstein Auto-Encoders,0.06980758905410767,#c7c7c7
-9.183835,7.672836,neighbor,49670925,Good Semi-supervised Learning That Requires a Bad GAN,0.06992852687835693,#c7c7c7
-11.92765,5.7879496,neighbor,49670925,A generative adversarial framework for positive-unlabeled classification,0.06997936964035034,#c7c7c7
7.924054,5.4586163,neighbor,49670925,Mittens: an Extension of GloVe for Learning Domain-Specialized Representations,0.07013314962387085,#c7c7c7
-0.65484613,-6.0967293,neighbor,49670925,Bottleneck Conditional Density Estimation,0.07022225856781006,#c7c7c7
-7.6923094,5.1425133,neighbor,49670925,Bayesian GAN,0.07023632526397705,#c7c7c7
-6.4763575,5.4920464,neighbor,49670925,Flow-GAN: Bridging implicit and prescribed learning in generative models,0.07038366794586182,#c7c7c7
11.853631,-4.06776,neighbor,49670925,Incremental Sequence Learning,0.07038611173629761,#c7c7c7
1.7653284,5.717156,neighbor,49670925,Mixed generative and supervised learning modes in Deep Predictive Coding Networks,0.07039529085159302,#c7c7c7
-2.5083973,9.95474,neighbor,49670925,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,0.07053196430206299,#c7c7c7
4.2426887,4.373556,neighbor,49670925,Switched linear encoding with rectified linear autoencoders,0.07058656215667725,#c7c7c7
8.349319,-10.140791,neighbor,49670925,Stochastic Adversarial Video Prediction,0.0706183910369873,#c7c7c7
-3.6361594,-6.5620823,neighbor,49670925,InfoCatVAE: Representation Learning with Categorical Variational Autoencoders,0.0707053542137146,#c7c7c7
-10.715587,-1.8203042,neighbor,49670925,Learning Non-deterministic Representations with Energy-based Ensembles,0.07087880373001099,#c7c7c7
-2.1825824,11.328284,neighbor,49670925,Unsupervised Representation Learning with Prior-Free and Adversarial Mechanism Embedded Autoencoders,0.07096147537231445,#c7c7c7
15.080651,4.4958496,neighbor,49670925,"Multimedia Content Understanding by Learning from Very Few Examples: Recent Progress on Unsupervised, Semi-Supervised and Supervised Deep Learning Approaches",0.07097291946411133,#c7c7c7
-4.9566298,5.145706,neighbor,49670925,Tractable Generative Convolutional Arithmetic Circuits,0.07099264860153198,#c7c7c7
-6.6930842,-5.103092,neighbor,49670925,On the Latent Space of Wasserstein Auto-Encoders,0.07100552320480347,#c7c7c7
-5.104112,9.369312,neighbor,49670925,Denoising Adversarial Autoencoders,0.07101267576217651,#c7c7c7
-7.2945647,-0.040838778,neighbor,49670925,Gaussian mixture models with Wasserstein distance,0.07103753089904785,#c7c7c7
-4.412061,-1.6941274,neighbor,49670925,Inducing Interpretable Representations with Variational Autoencoders,0.07117855548858643,#c7c7c7
1.2213281,-4.439326,neighbor,49670925,Discrete Variational Autoencoders,0.07120782136917114,#c7c7c7
-6.2664237,-2.7077456,neighbor,49670925,Towards a Neural Statistician,0.0712975263595581,#c7c7c7
15.691019,5.94445,neighbor,49670925,Deep Multiple Instance Feature Learning via Variational Autoencoder,0.07138252258300781,#c7c7c7
10.576386,-8.040921,neighbor,49670925,Video (language) modeling: a baseline for generative models of natural videos,0.07138854265213013,#c7c7c7
9.589494,-8.210177,neighbor,49670925,Unsupervised Learning of Disentangled Representations from Video,0.07141953706741333,#c7c7c7
-5.8976755,6.253004,neighbor,49670925,NIPS 2016 Tutorial: Generative Adversarial Networks,0.07160484790802002,#c7c7c7
5.732784,3.875199,neighbor,49670925,Learning Less-Overlapping Representations,0.07164782285690308,#c7c7c7
-3.1466527,7.597705,neighbor,49670925,Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks,0.07168430089950562,#c7c7c7
-11.282455,12.527066,neighbor,49670925,Learning Informative Features from Restricted Boltzmann Machines,0.07169139385223389,#c7c7c7
-5.1607714,7.357032,neighbor,49670925,Generative Adversarial Image Synthesis with Decision Tree Latent Controller,0.07170957326889038,#c7c7c7
-3.3076515,0.40260223,query,49867180,CBAM: Convolutional Block Attention Module,0.0,#c7c7c7
-2.3639474,0.3607976,neighbor,49867180,Beyond Skip Connections: Top-Down Modulation for Object Detection,0.04245400428771973,#c7c7c7
3.230873,2.8722246,neighbor,49867180,R-FCN++: Towards Accurate Region-Based Fully Convolutional Networks for Object Detection,0.046058714389801025,#c7c7c7
2.3278053,2.5443292,neighbor,49867180,Object Detection with Mask-based Feature Encoding,0.046727776527404785,#c7c7c7
3.2631085,1.4927826,neighbor,49867180,Object Detection Networks on Convolutional Feature Maps,0.04876565933227539,#c7c7c7
-9.358146,-1.2706068,neighbor,49867180,Residual Attention Network for Image Classification,0.04889577627182007,#c7c7c7
3.4571924,-2.1462603,neighbor,49867180,Fast R-CNN,0.04954349994659424,#c7c7c7
3.133674,-3.0589604,neighbor,49867180,Light-Head R-CNN: In Defense of Two-Stage Object Detector,0.049637019634246826,#c7c7c7
2.1532438,1.8157169,neighbor,49867180,Feature Selective Networks for Object Detection,0.05052453279495239,#c7c7c7
3.8809476,2.449967,neighbor,49867180,R-FCN: Object Detection via Region-based Fully Convolutional Networks,0.05131584405899048,#c7c7c7
-5.0464854,2.4821913,neighbor,49867180,Learn To Pay Attention,0.05214548110961914,#c7c7c7
-5.3449225,2.5598426,neighbor,49867180,STNet: Selective Tuning of Convolutional Networks for Object Localization,0.05304080247879028,#c7c7c7
8.399493,0.6622283,neighbor,49867180,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.05346941947937012,#c7c7c7
4.1864343,-3.8955796,neighbor,49867180,Receptive Field Block Net for Accurate and Fast Object Detection,0.05373436212539673,#c7c7c7
0.8159355,1.8750805,neighbor,49867180,Feature Pyramid Networks for Object Detection,0.054374635219573975,#c7c7c7
0.9181207,-1.0350811,neighbor,49867180,Learning Chained Deep Features and Classifiers for Cascade in Object Detection,0.054902613162994385,#c7c7c7
0.8097063,-1.2031577,neighbor,49867180,Chained Cascade Network for Object Detection,0.05568963289260864,#c7c7c7
-8.912366,-4.6875257,neighbor,49867180,Going deeper with convolutions,0.05601358413696289,#c7c7c7
-8.841528,-3.6040542,neighbor,49867180,Deep Residual Learning for Image Recognition,0.05629783868789673,#c7c7c7
4.832508,0.7516278,neighbor,49867180,RON: Reverse Connection with Objectness Prior Networks for Object Detection,0.05646955966949463,#c7c7c7
3.984186,6.800873,neighbor,49867180,Scalable Object Detection Using Deep Neural Networks,0.05694061517715454,#c7c7c7
5.729569,7.8670087,neighbor,49867180,AttentionNet: Aggregating Weak Directions for Accurate Object Detection,0.05709594488143921,#c7c7c7
3.1219578,3.7846472,neighbor,49867180,CoupleNet: Coupling Global Structure with Local Parts for Object Detection,0.057198405265808105,#c7c7c7
0.5338746,9.218322,neighbor,49867180,Revisiting RCNN: On Awakening the Classification Power of Faster RCNN,0.05727064609527588,#c7c7c7
-11.518529,-5.4064145,neighbor,49867180,Multiactivation Pooling Method in Convolutional Neural Networks for Image Recognition,0.057553768157958984,#c7c7c7
-1.105081,3.9631488,neighbor,49867180,G-CNN: Object Detection via Grid Convolutional Neural Network,0.05782818794250488,#c7c7c7
1.7885423,-0.20318234,neighbor,49867180,Crafting GBD-Net for Object Detection,0.05792170763015747,#c7c7c7
4.998336,4.8682127,neighbor,49867180,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.05806618928909302,#c7c7c7
9.507921,6.9514694,neighbor,49867180,Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection,0.058335959911346436,#c7c7c7
6.653699,2.174737,neighbor,49867180,Cascade R-CNN: Delving Into High Quality Object Detection,0.05839759111404419,#c7c7c7
1.4043468,5.7801795,neighbor,49867180,Attentive Contexts for Object Detection,0.05854219198226929,#c7c7c7
-9.941464,-5.74472,neighbor,49867180,Local Binary Convolutional Neural Networks,0.05888456106185913,#c7c7c7
5.010522,-2.290369,neighbor,49867180,MegDet: A Large Mini-Batch Object Detector,0.059223055839538574,#c7c7c7
1.0062176,5.5983543,neighbor,49867180,Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks,0.059256911277770996,#c7c7c7
-5.228984,3.7088454,neighbor,49867180,Top-Down Neural Attention by Excitation Backprop,0.05929851531982422,#c7c7c7
7.022873,-1.1720934,neighbor,49867180,R-FCN-3000 at 30fps: Decoupling Detection and Classification,0.05933505296707153,#c7c7c7
2.218337,-4.052563,neighbor,49867180,Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors,0.059568941593170166,#c7c7c7
8.528066,5.3354406,neighbor,49867180,Keypoint Density-based Region Proposal for Fine-Grained Object Detection and Classification using Regions with Convolutional Neural Network Features,0.05963021516799927,#c7c7c7
5.709402,-7.6083946,neighbor,49867180,Recurrent Attention for Deep Neural Object Detection,0.05975979566574097,#c7c7c7
6.285626,0.3927551,neighbor,49867180,DSOD: Learning Deeply Supervised Object Detectors from Scratch,0.05982351303100586,#c7c7c7
5.5136414,-4.468234,neighbor,49867180,PVANet: Lightweight Deep Neural Networks for Real-time Object Detection,0.06009751558303833,#c7c7c7
-3.9198139,-4.858676,neighbor,49867180,Quantization Mimic: Towards Very Tiny CNN for Object Detection,0.06013953685760498,#c7c7c7
5.5530024,-4.4796352,neighbor,49867180,PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection,0.06041818857192993,#c7c7c7
-8.957151,-7.424739,neighbor,49867180,A Non-Technical Survey on Deep Convolutional Neural Network Architectures,0.06084847450256348,#c7c7c7
5.3079348,3.6466584,neighbor,49867180,Weakly Supervised Cascaded Convolutional Networks,0.06136929988861084,#c7c7c7
-8.795571,3.808528,neighbor,49867180,Multi-scale Recognition with DAG-CNNs,0.06144523620605469,#c7c7c7
-7.09532,-4.931809,neighbor,49867180,DecomposeMe: Simplifying ConvNets for End-to-End Learning,0.061545610427856445,#c7c7c7
8.665547,0.22983444,neighbor,49867180,DenseBox: Unifying Landmark Localization with End to End Object Detection,0.06193733215332031,#c7c7c7
-7.614597,5.579329,neighbor,49867180,Part-Stacked CNN for Fine-Grained Visual Categorization,0.06197124719619751,#c7c7c7
4.5156245,5.693486,neighbor,49867180,Boosting Convolutional Features for Robust Object Proposals,0.06208676099777222,#c7c7c7
3.9149024,-0.14781287,neighbor,49867180,DetNet: A Backbone network for Object Detection,0.062219321727752686,#c7c7c7
3.0459373,-5.1289215,neighbor,49867180,Wide-residual-inception networks for real-time object detection,0.062247276306152344,#c7c7c7
5.416519,7.263246,neighbor,49867180,Action-Driven Object Detection with Top-Down Visual Attentions,0.06229192018508911,#c7c7c7
8.331914,4.3912315,neighbor,49867180,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,0.06276041269302368,#c7c7c7
5.4791303,-0.7604111,neighbor,49867180,Focal Loss for Dense Object Detection,0.0628998875617981,#c7c7c7
1.3666675,4.5714974,neighbor,49867180,Auto-Context R-CNN,0.0629088282585144,#c7c7c7
-6.75919,-0.90051043,neighbor,49867180,CNN with coarse-to-fine layer for hierarchical classification,0.0629192590713501,#c7c7c7
9.003847,2.2624793,neighbor,49867180,LocNet: Improving Localization Accuracy for Object Detection,0.0631287693977356,#c7c7c7
-3.029671,4.027635,neighbor,49867180,Learning Deep Features for Discriminative Localization,0.06347793340682983,#c7c7c7
-8.348791,6.027907,neighbor,49867180,Learning Multi-attention Convolutional Neural Network for Fine-Grained Image Recognition,0.06369155645370483,#c7c7c7
6.1326427,0.90748733,neighbor,49867180,Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids,0.06377065181732178,#c7c7c7
-10.789425,-4.6675463,neighbor,49867180,Striving for Simplicity: The All Convolutional Net,0.06381571292877197,#c7c7c7
0.39919475,6.495883,neighbor,49867180,Spatial Memory for Context Reasoning in Object Detection,0.06394791603088379,#c7c7c7
-9.976099,-0.828869,neighbor,49867180,Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition,0.06411111354827881,#c7c7c7
-9.195794,-7.0400033,neighbor,49867180,A Guide to Convolutional Neural Networks for Computer Vision,0.06427556276321411,#c7c7c7
-8.371064,-5.1957655,neighbor,49867180,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.06440466642379761,#c7c7c7
6.0699368,3.8470674,neighbor,49867180,CRAFT Objects from Images,0.06449675559997559,#c7c7c7
2.5004823,-6.6113973,neighbor,49867180,Towards lightweight convolutional neural networks for object detection,0.06450611352920532,#c7c7c7
-9.030152,-6.135311,neighbor,49867180,Enhanced image classification with a fast-learning shallow convolutional neural network,0.064736008644104,#c7c7c7
8.209102,3.6184433,neighbor,49867180,"Scalable, High-Quality Object Detection",0.06475621461868286,#c7c7c7
7.9939885,9.357568,neighbor,49867180,An Implementation of Faster RCNN with Study for Region Sampling,0.06478065252304077,#c7c7c7
-0.108525865,1.1209955,neighbor,49867180,Pooling Pyramid Network for Object Detection,0.06503552198410034,#c7c7c7
-10.272291,-7.1362257,neighbor,49867180,Feature Representation in Convolutional Neural Networks,0.06506609916687012,#c7c7c7
2.8682885,-6.607285,neighbor,49867180,"SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",0.06518882513046265,#c7c7c7
-9.965791,-0.2808476,neighbor,49867180,Recurrent Soft Attention Model for Common Object Recognition,0.0654226541519165,#c7c7c7
-8.3120165,7.476273,neighbor,49867180,Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition,0.06559240818023682,#c7c7c7
6.721167,-4.862506,neighbor,49867180,Fast Learning and Prediction for Object Detection using Whitened CNN Features,0.06588858366012573,#c7c7c7
10.4000845,-1.8819382,neighbor,49867180,Optimizing the Trade-Off between Single-Stage and Two-Stage Deep Object Detectors using Image Difficulty Prediction,0.06590461730957031,#c7c7c7
-7.78785,-6.96312,neighbor,49867180,SdcNet: A Computation-Efficient CNN for Object Recognition,0.0665634274482727,#c7c7c7
-6.697123,-0.93440354,neighbor,49867180,Network of Experts for Large-Scale Image Categorization,0.06660914421081543,#c7c7c7
0.58796346,2.338606,neighbor,49867180,Fully Convolutional Network With Densely Feature Fusion Models for Object Detection,0.0666918158531189,#c7c7c7
4.9015265,-6.6515393,neighbor,49867180,Accurate Single Stage Detector Using Recurrent Rolling Convolution,0.06688845157623291,#c7c7c7
-9.999956,-7.7133827,neighbor,49867180,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.06705868244171143,#c7c7c7
-7.2744827,-8.579905,neighbor,49867180,Deep FisherNet for Object Classification,0.06710028648376465,#c7c7c7
-6.5056963,2.4466324,neighbor,49867180,Global-and-local attention networks for visual recognition,0.06716734170913696,#c7c7c7
8.143224,-1.7292029,neighbor,49867180,"YOLO9000: Better, Faster, Stronger",0.06716758012771606,#c7c7c7
-9.7585745,-3.2276793,neighbor,49867180,Binarized Neural Networks on the ImageNet Classification Task,0.06730896234512329,#c7c7c7
10.384124,1.1224298,neighbor,49867180,Improving object detection with deep convolutional networks via Bayesian optimization and structured prediction,0.06740176677703857,#c7c7c7
-1.4723247,6.162841,neighbor,49867180,Multilevel Context Representation for Improving Object Recognition,0.06746220588684082,#c7c7c7
10.668693,3.191354,neighbor,49867180,Training Region-Based Object Detectors with Online Hard Example Mining,0.06751888990402222,#c7c7c7
-0.9729651,-2.2708368,neighbor,49867180,Convolutional Channel Features,0.06760185956954956,#c7c7c7
6.0652285,5.522785,neighbor,49867180,Mid-level Elements for Object Detection,0.06777071952819824,#c7c7c7
6.920522,-2.2096274,neighbor,49867180,Object detection at 200 Frames Per Second,0.06780332326889038,#c7c7c7
-8.796889,5.809117,neighbor,49867180,Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition,0.06782734394073486,#c7c7c7
10.215052,4.8251815,neighbor,49867180,Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization,0.06783789396286011,#c7c7c7
3.435253,8.32962,neighbor,49867180,Object-centric Sampling for Fine-grained Image Classification,0.06785333156585693,#c7c7c7
-7.2112837,-3.0705268,neighbor,49867180,Learning Transferable Architectures for Scalable Image Recognition,0.06785815954208374,#c7c7c7
3.040973,7.1039066,neighbor,49867180,Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection,0.06787729263305664,#c7c7c7
-8.313219,7.1445885,neighbor,49867180,Weakly-supervised Discriminative Patch Learning via CNN for Fine-grained Recognition,0.06795740127563477,#c7c7c7
8.358832,5.8871713,neighbor,49867180,Relief R-CNN: Utilizing Convolutional Features for Fast Object Detection,0.06798619031906128,#c7c7c7
-5.964724,-5.170821,neighbor,49867180,ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,0.06828218698501587,#c7c7c7
-5.2845554,0.19174461,query,5201925,Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling,0.0,#bcbd22
-5.5139976,0.9498668,neighbor,5201925,How to Construct Deep Recurrent Neural Networks,0.04356294870376587,#bcbd22
-5.2925134,-1.6879807,neighbor,5201925,Advances in optimizing recurrent networks,0.04573941230773926,#bcbd22
-6.1844726,-2.9777372,neighbor,5201925,A Clockwork RNN,0.05018848180770874,#bcbd22
-4.272332,-1.0461785,neighbor,5201925,Learning Stochastic Recurrent Networks,0.051428139209747314,#bcbd22
-6.5849133,1.8852572,neighbor,5201925,Bach in 2014: Music Composition with Recurrent Neural Network,0.05381661653518677,#bcbd22
-4.4390707,5.0684614,neighbor,5201925,A hybrid recurrent neural network for music transcription,0.05476599931716919,#bcbd22
-5.945892,-6.9747767,neighbor,5201925,Recurrent neural networks,0.05585509538650513,#bcbd22
-6.4945607,-0.74802625,neighbor,5201925,Regularizing Recurrent Networks - On Injected Noise and Norm-based Methods,0.057941734790802,#bcbd22
4.324852,-1.0946543,neighbor,5201925,Speech recognition with deep recurrent neural networks,0.05953490734100342,#bcbd22
5.296054,-1.4981037,neighbor,5201925,Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition,0.06314504146575928,#bcbd22
-3.7549908,6.2453585,neighbor,5201925,Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription,0.06369847059249878,#bcbd22
5.4756436,0.1547886,neighbor,5201925,Phone sequence modeling with recurrent neural networks,0.06499040126800537,#bcbd22
-3.2346303,4.184721,neighbor,5201925,High-dimensional sequence transduction,0.06590229272842407,#bcbd22
-2.1651378,2.3611205,neighbor,5201925,Sequence Transduction with Recurrent Neural Networks,0.06643712520599365,#bcbd22
5.673869,-1.2171108,neighbor,5201925,Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition,0.06684505939483643,#bcbd22
0.19312517,-8.461327,neighbor,5201925,Recurrent Neural Network Regularization,0.06799441576004028,#bcbd22
4.522455,0.56396705,neighbor,5201925,Phoneme recognition in TIMIT with BLSTM-CTC,0.06861966848373413,#bcbd22
6.6416173,-0.40960458,neighbor,5201925,End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results,0.06978696584701538,#bcbd22
-8.024624,1.5312753,neighbor,5201925,Studying the Effect of Metre Perception on Rhythm and Melody Modelling with LSTMs,0.07013446092605591,#bcbd22
-4.8605328,-3.1943536,neighbor,5201925,Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences,0.07038635015487671,#bcbd22
10.528549,8.044991,neighbor,5201925,Autoregressive HMMs for speech synthesis,0.07222265005111694,#bcbd22
-7.3647456,2.902114,neighbor,5201925,Improvising Musical Structure with Hierarchical Neural Nets,0.07252770662307739,#bcbd22
-4.9912386,-6.138038,neighbor,5201925,Learning Input and Recurrent Weight Matrices in Echo State Networks,0.07275116443634033,#bcbd22
-6.701689,-7.5890765,neighbor,5201925,Complex Valued Recurrent Neural Network: From Architecture to Training,0.0748363733291626,#bcbd22
7.2628503,-1.4341253,neighbor,5201925,First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs,0.07485449314117432,#bcbd22
2.2079556,0.26631674,neighbor,5201925,Speaker recognition with recurrent neural networks,0.07583379745483398,#bcbd22
-7.640447,-2.2459793,neighbor,5201925,Generating Sequences With Recurrent Neural Networks,0.07609862089157104,#bcbd22
10.23471,10.717343,neighbor,5201925,UCD Blizzard Challenge 2011 Entry,0.07718402147293091,#bcbd22
7.6757097,-2.2227747,neighbor,5201925,Real-time one-pass decoding with recurrent neural network language model for speech recognition,0.07729578018188477,#bcbd22
-8.522648,-4.509024,neighbor,5201925,A mechanism for temporal sequence learning and recognition in neural systems,0.07745945453643799,#bcbd22
3.485788,-2.2509575,neighbor,5201925,Social signal classification using deep blstm recurrent neural networks,0.07807189226150513,#bcbd22
-0.35088134,-5.75803,neighbor,5201925,Discriminative Recurrent Sparse Auto-Encoders,0.07903343439102173,#bcbd22
1.1148596,6.6458855,neighbor,5201925,Discovering hierarchical speech features using convolutional non-negative matrix factorization,0.07923835515975952,#bcbd22
-4.254095,7.297525,neighbor,5201925,Comparing Probabilistic Models for Melodic Sequences,0.07926088571548462,#bcbd22
2.521753,-7.229338,neighbor,5201925,Improving deep neural networks by using sparse dropout strategy,0.07970601320266724,#bcbd22
-6.3374467,-4.3957086,neighbor,5201925,Context-free and context-sensitive dynamics in recurrent neural networks,0.07985442876815796,#bcbd22
-6.7772,-5.8928475,neighbor,5201925,Recognizing recurrent neural networks (rRNN): Bayesian inference for recurrent neural networks,0.07994049787521362,#bcbd22
-1.4671116,-5.725342,neighbor,5201925,"A tutorial survey of architectures, algorithms, and applications for deep learning",0.0802270770072937,#bcbd22
-2.6831439,-3.177259,neighbor,5201925,Deep AutoRegressive Networks,0.0808631181716919,#bcbd22
-12.019978,-2.6077797,neighbor,5201925,Recurrent Continuous Translation Models,0.08111590147018433,#bcbd22
8.558635,9.629865,neighbor,5201925,A hybrid statistical/RNN approach to prosody synthesis for taiwanese TTS,0.08198714256286621,#bcbd22
10.738291,10.04289,neighbor,5201925,The VUB Blizzard Challenge 2009 Entry,0.08216601610183716,#bcbd22
6.811873,11.679441,neighbor,5201925,Data driven intonation modelling of 6 languages,0.08265060186386108,#bcbd22
11.279901,7.584772,neighbor,5201925,The Effect of Using Normalized Models in Statistical Speech Synthesis,0.08294826745986938,#bcbd22
6.7399335,3.640651,neighbor,5201925,Improvements to Deep Convolutional Neural Networks for LVCSR,0.08306258916854858,#bcbd22
-8.444499,-7.8037434,neighbor,5201925,A Comparative Study of Reservoir Computing for Temporal Signal Processing,0.08309835195541382,#bcbd22
10.891748,1.3071918,neighbor,5201925,Learning a better representation of speech soundwaves using restricted boltzmann machines,0.08324158191680908,#bcbd22
6.1479974,0.94383055,neighbor,5201925,Sequence classification using the high-level features extracted from deep neural networks,0.0836142897605896,#bcbd22
-4.274219,8.019017,neighbor,5201925,A Topic Model for Melodic Sequences,0.08370202779769897,#bcbd22
8.610234,8.276247,neighbor,5201925,Prosody-dependent acoustic modeling using variable-parameter hidden Markov models,0.08421182632446289,#bcbd22
15.111534,4.987059,neighbor,5201925,A tandem connectionist model using combination of multi-scale spectro-temporal features for acoustic event detection,0.08451062440872192,#bcbd22
-5.3459606,-7.9957533,neighbor,5201925,A normalized gradient algorithm for an adaptive recurrent perceptron,0.08546537160873413,#bcbd22
-6.6643667,-9.0913515,neighbor,5201925,A bounded exploration approach to constructive algorithms for recurrent neural networks,0.08557915687561035,#bcbd22
1.2433095,5.6749744,neighbor,5201925,Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures,0.08561992645263672,#bcbd22
-5.5699015,3.8763447,neighbor,5201925,Audio Classical Composer Identification by Deep Neural Network,0.0858035683631897,#bcbd22
8.24627,0.12071393,neighbor,5201925,Recurrent neural network-enhanced HMM speech recognition systems,0.08594805002212524,#bcbd22
-0.24313755,-9.006777,neighbor,5201925,Regularization and nonlinearities for neural language models: when are they needed?,0.08599531650543213,#bcbd22
11.056693,9.3881855,neighbor,5201925,The VUB Blizzard Challenge 2010 Entry: Towards Automatic Voice Building,0.08608567714691162,#bcbd22
-1.6941596,-4.580323,neighbor,5201925,Autoencoder Trees,0.0861092209815979,#bcbd22
-2.6506064,6.577006,neighbor,5201925,Bayesian nonparametric music parser,0.08666706085205078,#bcbd22
6.4120727,6.8052044,neighbor,5201925,Discriminative training of hierarchical acoustic models for large vocabulary continuous speech recognition,0.0869302749633789,#bcbd22
6.4792256,-3.7681503,neighbor,5201925,RNN language model with word clustering and class-based output layer,0.08712577819824219,#bcbd22
10.45918,2.1628218,neighbor,5201925,Non-linear predictive vector quantization of speech,0.08733147382736206,#bcbd22
-1.6018051,-10.82262,neighbor,5201925,Training Connectionist Models for the Structured Language Model,0.08757138252258301,#bcbd22
-12.977741,-2.4433181,neighbor,5201925,On the Properties of Neural Machine Translation: Encoder–Decoder Approaches,0.08763551712036133,#bcbd22
10.185851,12.003714,neighbor,5201925,"Audio stream phrase recognition for a national gallery of the spoken word: ""one small step""",0.08844071626663208,#bcbd22
-8.051431,-5.730281,neighbor,5201925,ROBUST TIMING AND MOTOR PATTERNS BY TAMING CHAOS IN RECURRENT NEURAL NETWORKS,0.08854961395263672,#bcbd22
12.048107,8.737342,neighbor,5201925,The Simple4All entry to the Blizzard Challenge 2014,0.08888518810272217,#bcbd22
8.098979,-3.4520643,neighbor,5201925,Empirically combining unnormalized NNLM and back-off N-gram for fast N-best rescoring in speech recognition,0.08899068832397461,#bcbd22
14.942059,1.5825802,neighbor,5201925,Predicting transformed audio descriptors: a system design and evaluation,0.08899670839309692,#bcbd22
11.427821,-0.24965978,neighbor,5201925,Biologically inspired emotion recognition from speech,0.08930599689483643,#bcbd22
5.1742463,2.798194,neighbor,5201925,Extracting deep bottleneck features using stacked auto-encoders,0.08932226896286011,#bcbd22
8.373516,4.0208,neighbor,5201925,"Evaluation of a Feature Compensation Approach Using High-Order Vector Taylor Series Approximation of an Explicit Distortion Modelon Aurora2, Aurora3, and Aurora4 Tasks",0.08933734893798828,#bcbd22
1.6275603,0.20279439,neighbor,5201925,Locally recurrent probabilistic neural network for text-independent speaker verification,0.0894395112991333,#bcbd22
-13.017004,-2.9962108,neighbor,5201925,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,0.0895615816116333,#bcbd22
7.722545,11.619021,neighbor,5201925,Modeling and generation of accentual phrase F0 contours based on discrete HMMs synchronized at mora-unit transitions,0.08957266807556152,#bcbd22
8.449031,7.46326,neighbor,5201925,Speech recognition using fundamental frequency and voicing in acoustic modeling,0.08960169553756714,#bcbd22
-5.641499,6.7819896,neighbor,5201925,A Stochastic Temporal Model of Polyphonic MIDI Performance with Ornaments,0.08960437774658203,#bcbd22
2.0170898,5.6254473,neighbor,5201925,Learning Factored Representations in a Deep Mixture of Experts,0.08962017297744751,#bcbd22
14.473372,1.17292,neighbor,5201925,Shift-invariant sparse coding for audio classification,0.08972036838531494,#bcbd22
-2.319975,7.7249155,neighbor,5201925,Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling,0.0898016095161438,#bcbd22
6.268841,10.358891,neighbor,5201925,Acoustic modeling of sentence stress using differential features between syllables for English rhythm learning system development,0.08999872207641602,#bcbd22
15.985639,4.630101,neighbor,5201925,Exploiting Temporal Feature Integration for Generalized Sound Recognition,0.08999902009963989,#bcbd22
11.759367,4.8846464,neighbor,5201925,Speech recognition under musical environments using kalman filter and iterative MLLR adaptation,0.09008604288101196,#bcbd22
-4.395781,-5.3636084,neighbor,5201925,A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property,0.09030312299728394,#bcbd22
-6.2741103,7.877273,neighbor,5201925,A Comprehensive Trainable Error Model for Sung Music Queries,0.09043073654174805,#bcbd22
17.055166,3.9676454,neighbor,5201925,Recognition of isolated musical patterns using Context Dependent Dynamic Time Warping,0.09048056602478027,#bcbd22
1.4852215,-7.3354464,neighbor,5201925,Improving neural networks by preventing co-adaptation of feature detectors,0.09049010276794434,#bcbd22
5.508935,6.9776735,neighbor,5201925,Hierarchical class n-gram language models: towards better estimation of unseen events in speech recognition,0.09049719572067261,#bcbd22
12.61456,10.566449,neighbor,5201925,The NTNU Concatenative Speech Synthesizer,0.09058809280395508,#bcbd22
-0.040979136,7.1688538,neighbor,5201925,Positive factor networks: A graphical framework for modeling non-negative sequential data,0.09060049057006836,#bcbd22
-9.847162,-8.05344,neighbor,5201925,Training Deep Fourier Neural Networks to Fit Time-Series Data,0.09062141180038452,#bcbd22
-2.0678928,-6.958864,neighbor,5201925,Piecewise Linear Multilayer Perceptrons and Dropout,0.09109485149383545,#bcbd22
16.100386,5.5568204,neighbor,5201925,Multipitch Detection Based on Weighted Summary Correlogram,0.09112900495529175,#bcbd22
6.470526,4.0916224,neighbor,5201925,Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN,0.091175377368927,#bcbd22
-3.6312768,-8.256999,neighbor,5201925,Sparse Neural Networks With Large Learning Diversity,0.09128624200820923,#bcbd22
11.42612,10.793672,neighbor,5201925,Overview of SHRC-Ginkgo speech synthesis system for Blizzard Challenge 2013,0.09151530265808105,#bcbd22
0.21878166,-5.204665,neighbor,5201925,Scheduled denoising autoencoders,0.09156954288482666,#bcbd22
9.877954,5.7002416,neighbor,5201925,"Multistage coarticulation model combining articulatory, formant and cepstral features",0.09164804220199585,#bcbd22
-1.3647047,-1.118969,neighbor,5201925,Continuous vocal imitation with self-organized vowel spaces in Recurrent Neural Network,0.09207135438919067,#bcbd22
-2.7657685,-4.658156,query,54482423,A Style-Based Generator Architecture for Generative Adversarial Networks,0.0,#bcbd22
-3.0817263,-5.1664515,neighbor,54482423,GAGAN: Geometry-Aware Generative Adversarial Networks,0.030012011528015137,#bcbd22
-4.982408,-4.4207635,neighbor,54482423,CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training,0.035531461238861084,#bcbd22
-2.5631378,-11.438776,neighbor,54482423,FaceFeat-GAN: a Two-Stage Approach for Identity-Preserving Face Synthesis,0.03687185049057007,#bcbd22
-7.286026,2.3474917,neighbor,54482423,"Artsy-GAN: A style transfer system with improved quality, diversity and performance",0.03827029466629028,#bcbd22
-3.6002665,6.495048,neighbor,54482423,Generating a Fusion Image: One's Identity and Another's Shape,0.03905665874481201,#bcbd22
-8.781027,3.0767956,neighbor,54482423,Beyond Textures: Learning from Multi-domain Artistic Images for Arbitrary Style Transfer,0.04047971963882446,#bcbd22
-0.9261002,-13.672827,neighbor,54482423,RankGAN: A Maximum Margin Ranking GAN for Generating Faces,0.04106837511062622,#bcbd22
9.201308,10.259817,neighbor,54482423,Generative Adversarial Networks: An Overview,0.04116368293762207,#bcbd22
-8.6608,-2.463328,neighbor,54482423,BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network,0.041291236877441406,#bcbd22
-5.0305443,4.39579,neighbor,54482423,Style and Content Disentanglement in Generative Adversarial Networks,0.041735291481018066,#bcbd22
-0.6329008,5.901592,neighbor,54482423,Modular Generative Adversarial Networks,0.04205060005187988,#bcbd22
-3.6950994,-9.811767,neighbor,54482423,Differential Generative Adversarial Networks: Synthesizing Non-linear Facial Variations with Limited Number of Training Data,0.04249906539916992,#bcbd22
-2.9971104,-14.106523,neighbor,54482423,Pipeline Generative Adversarial Networks for Facial Images Generation with Multiple Attributes,0.04256778955459595,#bcbd22
-6.1061635,4.188983,neighbor,54482423,Generate Novel Image Styles using Weighted Hybrid Generative Adversarial Nets,0.04261296987533569,#bcbd22
-4.608004,-11.504001,neighbor,54482423,Structured GANs,0.04286050796508789,#bcbd22
-4.915556,-5.495828,neighbor,54482423,Adversarial Training of Variational Auto-Encoders for High Fidelity Image Generation,0.043334901332855225,#bcbd22
8.488751,11.983737,neighbor,54482423,MAGAN: Margin Adaptation for Generative Adversarial Networks,0.04399615526199341,#bcbd22
-8.386527,-8.805818,neighbor,54482423,Editable Generative Adversarial Networks: Generating and Editing Faces Simultaneously,0.04403311014175415,#bcbd22
-10.042271,-7.843559,neighbor,54482423,Adversarial Information Factorization,0.04409664869308472,#bcbd22
-0.13111037,-9.658005,neighbor,54482423,Load Balanced GANs for Multi-view Face Image Synthesis,0.044567227363586426,#bcbd22
-3.0229647,-12.823451,neighbor,54482423,Generate Identity-Preserving Faces by Generative Adversarial Networks,0.04522502422332764,#bcbd22
0.8283866,-7.3439226,neighbor,54482423,Triple consistency loss for pairing distributions in GAN-based face synthesis,0.045229434967041016,#bcbd22
-4.138926,-2.9753463,neighbor,54482423,High-Resolution Deep Convolutional Generative Adversarial Networks,0.04540151357650757,#bcbd22
-2.0135312,-10.3646145,neighbor,54482423,Recent Progress of Face Image Synthesis,0.045424818992614746,#bcbd22
-9.2885475,-6.108451,neighbor,54482423,Intra-class Variation Isolation in Conditional GANs,0.04558748006820679,#bcbd22
-2.0129516,-9.179282,neighbor,54482423,Robust Face Sketch Synthesis via Generative Adversarial Fusion of Priors and Parametric Sigmoid,0.04561746120452881,#bcbd22
1.8352329,2.8103738,neighbor,54482423,Large Scale GAN Training for High Fidelity Natural Image Synthesis,0.045661985874176025,#bcbd22
-4.664648,1.6197238,neighbor,54482423,AlphaGAN: Generative adversarial networks for natural image matting,0.04576289653778076,#bcbd22
0.5528119,-9.493972,neighbor,54482423,Towards High-Resolution Face Pose Synthesis,0.04587286710739136,#bcbd22
-6.6200542,2.187911,neighbor,54482423,Unpaired High-Resolution and Scalable Style Transfer Using Generative Adversarial Networks,0.045981764793395996,#bcbd22
3.6926734,5.619961,neighbor,54482423,DeLiGAN: Generative Adversarial Networks for Diverse and Limited Data,0.04612135887145996,#bcbd22
-9.961468,3.744743,neighbor,54482423,Anime Style Space Exploration Using Metric Learning and Generative Adversarial Networks,0.046311020851135254,#bcbd22
8.507637,6.8051524,neighbor,54482423,Triple Generative Adversarial Nets,0.04692673683166504,#bcbd22
4.6568484,9.602404,neighbor,54482423,IVE-GAN: Invariant Encoding Generative Adversarial Networks,0.04694676399230957,#bcbd22
0.35884875,2.7980893,neighbor,54482423,BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled Representation Learning and Image Synthesis,0.04714876413345337,#bcbd22
-10.62279,1.6627752,neighbor,54482423,Chinese Typeface Transformation with Hierarchical Adversarial Network,0.04734140634536743,#bcbd22
1.4596782,-2.159324,neighbor,54482423,Unsupervised Person Image Synthesis in Arbitrary Poses,0.04740262031555176,#bcbd22
-2.3942342,-6.908991,neighbor,54482423,High Quality Facial Surface and Texture Synthesis via Generative Adversarial Networks,0.04746866226196289,#bcbd22
6.832395,7.224038,neighbor,54482423,Generative Multi-Adversarial Networks,0.04753965139389038,#bcbd22
-4.437958,3.6211126,neighbor,54482423,Generative Image Modeling Using Style and Structure Adversarial Networks,0.047572433948516846,#bcbd22
2.789885,7.6897974,neighbor,54482423,Adversarial Autoencoders,0.04761165380477905,#bcbd22
5.177774,6.888259,neighbor,54482423,Stacked Generative Adversarial Networks,0.04771435260772705,#bcbd22
3.1262007,11.717339,neighbor,54482423,Improved Training of Generative Adversarial Networks Using Representative Features,0.047738730907440186,#bcbd22
4.5608153,7.8725824,neighbor,54482423,Auto-encoder generative adversarial networks,0.04808676242828369,#bcbd22
-2.094501,-12.206234,neighbor,54482423,FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis,0.04812300205230713,#bcbd22
-8.940498,-1.0024259,neighbor,54482423,PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup,0.0481717586517334,#bcbd22
-1.297361,4.3683667,neighbor,54482423,Dual Generator Generative Adversarial Networks for Multi-Domain Image-to-Image Translation,0.04834258556365967,#bcbd22
-3.5586202,7.3508415,neighbor,54482423,Shape-conditioned Image Generation by Learning Latent Appearance Representation from Unpaired Data,0.04842501878738403,#bcbd22
5.075313,8.623896,neighbor,54482423,Unregularized Auto-Encoder with Generative Adversarial Networks for Image Generation,0.048546433448791504,#bcbd22
-7.480547,-5.6133766,neighbor,54482423,Invertible Conditional GANs for image editing,0.04856818914413452,#bcbd22
9.203271,7.8147507,neighbor,54482423,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,0.04869121313095093,#bcbd22
0.95078486,-12.466034,neighbor,54482423,Representation Learning by Rotating Your Faces,0.04877054691314697,#bcbd22
7.165578,10.112064,neighbor,54482423,NIPS 2016 Tutorial: Generative Adversarial Networks,0.04882824420928955,#bcbd22
10.173863,6.47068,neighbor,54482423,Multi-agent Diverse Generative Adversarial Networks,0.04885244369506836,#bcbd22
-7.7770133,4.7015743,neighbor,54482423,Copy the Old or Paint Anew? An Adversarial Framework for (non-) Parametric Image Stylization,0.04891568422317505,#bcbd22
-4.9073677,-13.471411,neighbor,54482423,Identity-preserving Conditional Generative Adversarial Network,0.04895704984664917,#bcbd22
2.0724967,8.925404,neighbor,54482423,Robust Conditional Generative Adversarial Networks,0.049120306968688965,#bcbd22
1.5324345,-4.326106,neighbor,54482423,Dense Pose Transfer,0.04917722940444946,#bcbd22
-6.0027714,-7.977756,neighbor,54482423,Neural Face Editing with Intrinsic Image Disentangling,0.0492512583732605,#bcbd22
0.9690739,7.197731,neighbor,54482423,Coupled Generative Adversarial Networks,0.049282968044281006,#bcbd22
2.0330627,3.288673,neighbor,54482423,Pioneer Networks: Progressively Growing Generative Autoencoder,0.04928719997406006,#bcbd22
5.589807,10.08438,neighbor,54482423,Optimizing the Latent Space of Generative Networks,0.04934120178222656,#bcbd22
1.4549054,-1.2064816,neighbor,54482423,MsCGAN: Multi-scale Conditional Generative Adversarial Networks for Person Image Generation,0.04935652017593384,#bcbd22
-3.2308838,1.1519876,neighbor,54482423,Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks,0.049364686012268066,#bcbd22
7.083927,2.9906492,neighbor,54482423,Generating Images Part by Part with Composite Generative Adversarial Networks,0.049427688121795654,#bcbd22
-6.343235,-4.792996,neighbor,54482423,Image Generation and Editing with Variational Info Generative AdversarialNetworks,0.04944115877151489,#bcbd22
0.7556169,-12.653303,neighbor,54482423,Disentangled Representation Learning GAN for Pose-Invariant Face Recognition,0.04961115121841431,#bcbd22
-10.316377,-7.1364765,neighbor,54482423,GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data,0.04974597692489624,#bcbd22
-7.616698,1.2927147,neighbor,54482423,Targeted style transfer using cycle consistent generative adversarial networks with quantitative analysis of different loss functions,0.0498201847076416,#bcbd22
-6.8761635,-9.240677,neighbor,54482423,Mask-aware photorealistic facial attribute manipulation,0.04993385076522827,#bcbd22
-4.815927,-8.529493,neighbor,54482423,Geometry Guided Adversarial Facial Expression Synthesis,0.050191402435302734,#bcbd22
-7.8694997,-9.139352,neighbor,54482423,AttGAN: Facial Attribute Editing by Only Changing What You Want,0.050318121910095215,#bcbd22
5.401001,4.560384,neighbor,54482423,MEGAN: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation,0.05033397674560547,#bcbd22
-6.472456,5.867638,neighbor,54482423,Doodle Master: A Doodle Beautification System Based on Auto-encoding Generative Adversarial Networks,0.05033743381500244,#bcbd22
8.247886,9.947167,neighbor,54482423,Comparison on Generative Adversarial Networks –A Study,0.05045819282531738,#bcbd22
-5.0618935,-9.696769,neighbor,54482423,Generative Adversarial Talking Head: Bringing Portraits to Life with a Weakly Supervised Neural Network,0.05051743984222412,#bcbd22
3.986652,9.671112,neighbor,54482423,Inverting the Generator of a Generative Adversarial Network,0.05057281255722046,#bcbd22
8.1617985,4.270562,neighbor,54482423,Comparing Generative Adversarial Network Techniques for Image Creation and Modification,0.05059570074081421,#bcbd22
5.024026,11.351925,neighbor,54482423,Generative Adversarial Parallelization,0.050601959228515625,#bcbd22
6.285351,9.028948,neighbor,54482423,Generative adversarial networks,0.05074107646942139,#bcbd22
-1.6433421,4.118521,neighbor,54482423,Image Translation by Domain-Adversarial Training,0.050753235816955566,#bcbd22
7.3816566,8.285734,neighbor,54482423,Generative Adversarial Network Training is a Continual Learning Problem,0.05116838216781616,#bcbd22
9.371741,4.181946,neighbor,54482423,A General Framework for Adversarial Examples with Objectives,0.051310837268829346,#bcbd22
3.2149632,2.9849055,neighbor,54482423,Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks,0.05143791437149048,#bcbd22
6.1163917,6.075179,neighbor,54482423,Generative Warfare Nets: Ensemble via Adversaries and Collaborators,0.05156838893890381,#bcbd22
-5.8381453,7.610131,neighbor,54482423,ArtGAN: Artwork synthesis with conditional categorical GANs,0.05156952142715454,#bcbd22
0.3353696,10.739403,neighbor,54482423,EmojiGAN: learning emojis distributions with a generative model,0.051642656326293945,#bcbd22
1.8702823,-11.576795,neighbor,54482423,Pose-Guided Photorealistic Face Rotation,0.05177927017211914,#bcbd22
-5.89607,-0.046938326,neighbor,54482423,"CAN: Creative Adversarial Networks, Generating ""Art"" by Learning About Styles and Deviating from Style Norms",0.05178624391555786,#bcbd22
-0.19577196,-11.033239,neighbor,54482423,Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis,0.05182367563247681,#bcbd22
-8.511789,5.3521924,neighbor,54482423,CartoonGAN: Generative Adversarial Networks for Photo Cartoonization,0.051884591579437256,#bcbd22
6.4310365,12.926086,neighbor,54482423,Improved Training with Curriculum GANs,0.052339792251586914,#bcbd22
7.471632,11.405051,neighbor,54482423,Training Generative Adversarial Networks With Weights,0.05238080024719238,#bcbd22
2.1212761,-13.357315,neighbor,54482423,GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks,0.05252981185913086,#bcbd22
1.944177,-2.1108937,neighbor,54482423,Deformable GANs for Pose-Based Human Image Generation,0.05262094736099243,#bcbd22
-3.3821406,8.18973,neighbor,54482423,A Variational U-Net for Conditional Appearance and Shape Generation,0.05264008045196533,#bcbd22
-7.4352093,-12.47764,neighbor,54482423,Generative Adversarial Style Transfer Networks for Face Aging,0.052776992321014404,#bcbd22
-9.069891,1.3991178,neighbor,54482423,Style Separation and Synthesis via Generative Adversarial Networks,0.05298084020614624,#bcbd22
12.0105,8.794758,neighbor,54482423,Geometry Score: A Method For Comparing Generative Adversarial Networks,0.053005218505859375,#bcbd22
-2.1784346,-15.171434,neighbor,54482423,DeepFace: Face Generation using Deep Learning,0.053102076053619385,#bcbd22
-8.244096,3.977298,query,5590763,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,0.0,#dbdb8d
-7.462869,4.9371557,neighbor,5590763,Recurrent Continuous Translation Models,0.054015517234802246,#dbdb8d
-8.878537,1.7830291,neighbor,5590763,Semantic Vector Machines,0.056078314781188965,#dbdb8d
-14.158298,7.8002977,neighbor,5590763,Improved Statistical Machine Translation Using Monolingual Paraphrases,0.07093733549118042,#dbdb8d
-4.0432262,9.424022,neighbor,5590763,Sequence Transduction with Recurrent Neural Networks,0.07259374856948853,#dbdb8d
-13.985864,-4.2352796,neighbor,5590763,Exploiting Similarities among Languages for Machine Translation,0.07373851537704468,#dbdb8d
-1.2111269,12.626176,neighbor,5590763,Regularization and nonlinearities for neural language models: when are they needed?,0.07439661026000977,#dbdb8d
4.8902836,-5.685278,neighbor,5590763,Joint Space Neural Probabilistic Language Model for Statistical Machine Translation,0.07454115152359009,#dbdb8d
-7.5438614,-3.0773962,neighbor,5590763,Factored Neural Language Models,0.07767921686172485,#dbdb8d
-8.553474,-5.9888144,neighbor,5590763,Improving the Lexical Function Composition Model with Pathwise Optimized Elastic-Net Regression,0.07888448238372803,#dbdb8d
4.2968407,-5.255864,neighbor,5590763,Continuous Space Language Models for Statistical Machine Translation,0.07930713891983032,#dbdb8d
2.0096996,-0.48866528,neighbor,5590763,A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser Ney Smoothing,0.07964318990707397,#dbdb8d
-11.35827,-5.852529,neighbor,5590763,Distributed Word Representation Learning for Cross-Lingual Dependency Parsing,0.08062952756881714,#dbdb8d
6.650679,7.6191645,neighbor,5590763,Discriminative training on language model,0.08153247833251953,#dbdb8d
-6.600126,-2.3748846,neighbor,5590763,Factorial Hidden Markov Models for Learning Representations of Natural Language,0.082114577293396,#dbdb8d
4.699187,7.020546,neighbor,5590763,RNN language model with word clustering and class-based output layer,0.08216077089309692,#dbdb8d
10.674725,-1.386992,neighbor,5590763,A Study on Richer Syntactic Dependencies for Structured Language Modeling,0.08247965574264526,#dbdb8d
6.47672,-13.260153,neighbor,5590763,Machine Translation Model based on Non-parallel Corpus and Semi-supervised Transductive Learning,0.08303183317184448,#dbdb8d
-2.367081,-5.852758,neighbor,5590763,Natural Language Processing,0.08316963911056519,#dbdb8d
12.804332,5.2440834,neighbor,5590763,Statistical Transformation of Language and Pronunciation Models for Spontaneous Speech Recognition,0.0833330750465393,#dbdb8d
-12.052596,-4.275344,neighbor,5590763,Learning Multilingual Word Representations using a Bag-of-Words Autoencoder,0.08350551128387451,#dbdb8d
-0.39104983,-6.9666553,neighbor,5590763,Hierarchical feature-based translation for scalable natural language understanding,0.08363783359527588,#dbdb8d
-6.914437,0.99409026,neighbor,5590763,A Convolutional Neural Network for Modelling Sentences,0.08370703458786011,#dbdb8d
13.299019,-6.3314757,neighbor,5590763,Does Syntactic Knowledge help English-Hindi SMT?,0.08415055274963379,#dbdb8d
-6.9099975,-7.4837403,neighbor,5590763,Simple Customization of Recursive Neural Networks for Semantic Relation Classification,0.08435523509979248,#dbdb8d
4.3642554,1.1549433,neighbor,5590763,Improve latent semantic analysis based language model by integrating multiple level knowledge,0.08474493026733398,#dbdb8d
-0.8867954,3.0796118,neighbor,5590763,An Empirical Study on Multiple LVCSR Model Combination by Machine Learning,0.0849413275718689,#dbdb8d
10.415997,-0.1240382,neighbor,5590763,Combining semantic and syntactic structure for language modeling,0.08513545989990234,#dbdb8d
12.252859,0.7240209,neighbor,5590763,Exploiting Syntactic Structure for Natural Language Modeling,0.08538568019866943,#dbdb8d
1.8518882,-2.1021106,neighbor,5590763,A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes,0.08548593521118164,#dbdb8d
13.29809,-0.83370876,neighbor,5590763,"Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling",0.08555072546005249,#dbdb8d
9.622137,-8.392207,neighbor,5590763,Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach,0.08578646183013916,#dbdb8d
9.8845215,-1.3445896,neighbor,5590763,Richer syntactic dependencies for structured language modeling,0.08584076166152954,#dbdb8d
-11.447933,0.8749181,neighbor,5590763,Grounded Compositional Semantics for Finding and Describing Images with Sentences,0.08590102195739746,#dbdb8d
13.917767,-6.2953987,neighbor,5590763,Reordering rules for English-Hindi SMT,0.08617645502090454,#dbdb8d
-16.330404,7.1959248,neighbor,5590763,Hybrid Simplification using Deep Semantics and Machine Translation,0.08625799417495728,#dbdb8d
9.425352,4.2167635,neighbor,5590763,Improvement of a Whole Sentence Maximum Entropy Language Model Using Grammatical Features,0.086417555809021,#dbdb8d
-6.306261,2.0420904,neighbor,5590763,Recurrent Convolutional Neural Networks for Discourse Compositionality,0.08655864000320435,#dbdb8d
7.30546,2.0163562,neighbor,5590763,Statistical language model based on a hierarchical approach: MCnv,0.08674865961074829,#dbdb8d
1.790944,4.428876,neighbor,5590763,Quantization-based language model compression,0.08682560920715332,#dbdb8d
10.782691,1.1697391,neighbor,5590763,Probabilistic Top-Down Parsing and Language Modeling,0.08700525760650635,#dbdb8d
3.2103152,7.6912246,neighbor,5590763,Implementing vocal tract length normalization in the MLLR framework,0.08707320690155029,#dbdb8d
-13.445519,4.1879725,neighbor,5590763,An Exploration of Embeddings for Generalized Phrases,0.08736735582351685,#dbdb8d
-10.732769,-4.3830786,neighbor,5590763,Multilingual Distributed Representations without Word Alignment,0.08766144514083862,#dbdb8d
7.455919,-12.824662,neighbor,5590763,Correction of Errors in a Modality Corpus Used for Machine Translation by Using Machine-learning Method,0.08770805597305298,#dbdb8d
-14.573275,3.4080787,neighbor,5590763,Random Walks for Text Semantic Similarity,0.08779287338256836,#dbdb8d
-8.589847,-3.3216386,neighbor,5590763,Efficient Estimation of Word Representations in Vector Space,0.08779376745223999,#dbdb8d
-4.186881,11.779386,neighbor,5590763,Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences,0.08863240480422974,#dbdb8d
-12.926205,7.9056754,neighbor,5590763,Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment,0.08884119987487793,#dbdb8d
-12.353099,10.723565,neighbor,5590763,Accurately extracting coherent relevant passages using hidden Markov models,0.08903837203979492,#dbdb8d
-0.06072049,-0.39927155,neighbor,5590763,Refining Generative Language Models using Discriminative Learning,0.08926063776016235,#dbdb8d
-12.083977,0.3026826,neighbor,5590763,"Discriminative Training: Learning to Describe Video with Sentences, from Video Described with Sentences",0.08936917781829834,#dbdb8d
-15.808098,0.23970139,neighbor,5590763,Exploring the acquisition and production of grammatical constructions through human-robot interaction with echo state networks,0.08941495418548584,#dbdb8d
-8.83993,0.15449777,neighbor,5590763,Distributed Representations of Sentences and Documents,0.0895039439201355,#dbdb8d
-3.4019113,-11.296237,neighbor,5590763,Effective Bilingual Constraints for Semi-Supervised Learning of Named Entity Recognizers,0.08965623378753662,#dbdb8d
0.76228774,6.305808,neighbor,5590763,Training Neural Network Language Models on Very Large Corpora,0.0897032618522644,#dbdb8d
-12.926873,-4.8131795,neighbor,5590763,An Autoencoder Approach to Learning Bilingual Word Representations,0.08973002433776855,#dbdb8d
0.9598436,-11.483503,neighbor,5590763,Approximation Lasso Methods for Language Modeling,0.08976352214813232,#dbdb8d
12.114144,3.6757388,neighbor,5590763,A New Bigram-PLSA Language Model for Speech Recognition,0.0897713303565979,#dbdb8d
-2.2936366,-4.176257,neighbor,5590763,Language without words: A pointillist model for natural language processing,0.08985096216201782,#dbdb8d
1.1190954,-0.14589132,neighbor,5590763,A bit of progress in language modeling,0.09008210897445679,#dbdb8d
1.8751677,6.0238876,neighbor,5590763,Optimization of Neural Network Language Models for keyword search,0.09015774726867676,#dbdb8d
7.322514,4.634455,neighbor,5590763,Language modeling by stochastic dependency grammar for Japanese speech recognition,0.0903017520904541,#dbdb8d
-17.99121,6.621698,neighbor,5590763,"Syntax, Parsing and Production of Natural Language in a Framework of Information Compression by Multiple Alignment, Unification and Search",0.09049081802368164,#dbdb8d
14.621093,5.364069,neighbor,5590763,Error recovery and sentence verification using statistical partial pattern tree for conversational speech,0.09064310789108276,#dbdb8d
-3.4077415,-7.0349045,neighbor,5590763,Natural Language Processing (Almost) from Scratch,0.09068793058395386,#dbdb8d
8.653866,-13.112369,neighbor,5590763,"Using a Support-Vector Machine for Japanese-to-English Translation of Tense, Aspect, and Modality",0.0907168984413147,#dbdb8d
-14.196908,6.258042,neighbor,5590763,Mutaphrase: Paraphrasing with FrameNet,0.09073573350906372,#dbdb8d
-3.0074565,13.345606,neighbor,5590763,Context-free and context-sensitive dynamics in recurrent neural networks,0.09077465534210205,#dbdb8d
11.920103,6.664033,neighbor,5590763,Speech summarization using weighted finite-state transducers,0.09099197387695312,#dbdb8d
2.6293218,9.630969,neighbor,5590763,Real-time one-pass decoding with recurrent neural network language model for speech recognition,0.09101223945617676,#dbdb8d
-2.013882,11.620189,neighbor,5590763,A Clockwork RNN,0.0910612940788269,#dbdb8d
7.8902206,-0.5438688,neighbor,5590763,Improvement of a structured language model: arbori-context tree,0.09108966588973999,#dbdb8d
9.371347,-9.115317,neighbor,5590763,Modality and Negation in SIMT Use of Modality and Negation in Semantically-Informed Syntactic MT,0.09111607074737549,#dbdb8d
-6.6123614,-4.7711043,neighbor,5590763,Compositional Morphology for Word Representations and Language Modelling,0.09148764610290527,#dbdb8d
-6.0385137,0.17268975,neighbor,5590763,Bidirectional Recursive Neural Networks for Token-Level Labeling with Structure,0.09161579608917236,#dbdb8d
9.775126,-3.090295,neighbor,5590763,Training Connectionist Models for the Structured Language Model,0.09179306030273438,#dbdb8d
6.3319316,0.32323885,neighbor,5590763,Modeling of Long Distance Context Dependency in Chinese,0.09179520606994629,#dbdb8d
-0.48022532,-4.4634237,neighbor,5590763,Incorporating linguistic structure into statistical language models,0.09211719036102295,#dbdb8d
-13.734695,9.340353,neighbor,5590763,Looking up phrase rephrasings via a pivot language,0.092215895652771,#dbdb8d
5.7182617,5.7592263,neighbor,5590763,A new lexicon optimization method for LVCSR based on linguistic and acoustic characteristics of words,0.09256917238235474,#dbdb8d
0.19574307,10.225873,neighbor,5590763,Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition,0.09258681535720825,#dbdb8d
-5.0647707,9.304469,neighbor,5590763,High-dimensional sequence transduction,0.09260118007659912,#dbdb8d
-4.453091,-11.125196,neighbor,5590763,Maximum Entropy Models for Named Entity Recognition,0.09284031391143799,#dbdb8d
8.338997,2.6596367,neighbor,5590763,Combination of N-Grams and Stochastic Context-Free Grammars for Language Modeling,0.09304529428482056,#dbdb8d
4.7285957,9.55297,neighbor,5590763,Recasting the discriminative n-gram model as a pseudo-conventional n-gram model for LVCSR,0.09321081638336182,#dbdb8d
-2.6968427,10.834155,neighbor,5590763,Advances in optimizing recurrent networks,0.09326612949371338,#dbdb8d
5.3370724,-1.8718717,neighbor,5590763,Vector space representation of language probabilities through SVD of n-gram matrix,0.09342336654663086,#dbdb8d
-0.009884491,-11.571298,neighbor,5590763,Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty,0.09346616268157959,#dbdb8d
3.1345816,0.8362974,neighbor,5590763,Methods to Integrate a Language Model with Semantic Information for a Word Prediction Component,0.09351164102554321,#dbdb8d
10.22426,2.1860492,neighbor,5590763,Integration of supra-lexical linguistic models with speech recognition using shallow parsing and finite state transducers,0.0936809778213501,#dbdb8d
-0.49231118,1.9993596,neighbor,5590763,A Dynamic Language Model Based on Individual Word Domains,0.09382772445678711,#dbdb8d
4.5792503,-7.621104,neighbor,5590763,A Probabilistic Model of Machine Translation,0.0939221978187561,#dbdb8d
-2.355463,-11.727143,neighbor,5590763,Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning,0.09397637844085693,#dbdb8d
13.072703,1.7720753,neighbor,5590763,Structured Language Modeling for Speech Recognition,0.09402072429656982,#dbdb8d
-0.9827364,10.435895,neighbor,5590763,Speech recognition with deep recurrent neural networks,0.09403687715530396,#dbdb8d
-1.9365424,-1.5046846,neighbor,5590763,Temporal Analysis of Language through Neural Language Models,0.09407085180282593,#dbdb8d
-9.7859535,-1.4779856,neighbor,5590763,Reading to Learn: Constructing Features from Semantic Abstracts,0.0941314697265625,#dbdb8d
-11.0419855,8.236292,neighbor,5590763,Learning to Say It Well: Reranking Realizations by Predicted Synthesis Quality,0.09417790174484253,#dbdb8d
9.453857,9.282777,neighbor,5590763,Comparison of Modified Kneser-Ney and Witten-Bell smoothing techniques in statistical language model of Bahasa Indonesia,0.09417992830276489,#dbdb8d
-10.167606,4.083051,neighbor,5590763,Word or Phrase? Learning Which Unit to Stress for Information Retrieval,0.09460127353668213,#dbdb8d
-1.7706643,-3.9809518,query,5808102,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,0.0,#dbdb8d
-2.0211525,-5.1087465,neighbor,5808102,Committees of Deep Feedforward Networks Trained with Few Data,0.054557979106903076,#dbdb8d
-1.9054774,-0.80256253,neighbor,5808102,Stochastic Pooling for Regularization of Deep Convolutional Neural Networks,0.055301129817962646,#dbdb8d
0.10971781,3.3391426,neighbor,5808102,Efficient batchwise dropout training using submatrices,0.05615878105163574,#dbdb8d
-1.4748286,0.12107657,neighbor,5808102,Improving Deep Neural Networks with Probabilistic Maxout Units,0.05660653114318848,#dbdb8d
-4.622072,-5.475878,neighbor,5808102,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.05743128061294556,#dbdb8d
-1.1274561,1.074888,neighbor,5808102,Maxout Networks,0.05776846408843994,#dbdb8d
-2.1258543,-1.7693973,neighbor,5808102,Fractional Max-Pooling,0.05826848745346069,#dbdb8d
8.444374,-3.2349532,neighbor,5808102,Big Neural Networks Waste Capacity,0.060594022274017334,#dbdb8d
-0.5604161,2.2711616,neighbor,5808102,Learning Compact Convolutional Neural Networks with Nested Dropout,0.06126821041107178,#dbdb8d
-10.692187,-1.5662798,neighbor,5808102,Deep Fried Convnets,0.0634412169456482,#dbdb8d
-5.254722,-7.9068365,neighbor,5808102,How transferable are features in deep neural networks?,0.06409192085266113,#dbdb8d
-3.653637,5.5170317,neighbor,5808102,An introduction to deep learning,0.0657958984375,#dbdb8d
-6.395793,-6.485953,neighbor,5808102,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.06611406803131104,#dbdb8d
7.589006,-1.2385308,neighbor,5808102,Exact solutions to the nonlinear dynamics of learning in deep linear neural networks,0.0664440393447876,#dbdb8d
0.84550065,-5.4863253,neighbor,5808102,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.06689703464508057,#dbdb8d
2.4452004,5.606513,neighbor,5808102,Improving neural networks by preventing co-adaptation of feature detectors,0.06716316938400269,#dbdb8d
0.43012294,5.7358456,neighbor,5808102,Dropout Training as Adaptive Regularization,0.06820136308670044,#dbdb8d
0.9424683,2.5409007,neighbor,5808102,Neural Network Regularization via Robust Weight Factorization,0.06826364994049072,#dbdb8d
4.9038587,-3.1824913,neighbor,5808102,Predicting Parameters in Deep Learning,0.06849408149719238,#dbdb8d
-7.168068,-4.881964,neighbor,5808102,Going deeper with convolutions,0.06863069534301758,#dbdb8d
-8.01084,-1.637067,neighbor,5808102,Deep Epitomic Convolutional Neural Networks,0.06884443759918213,#dbdb8d
3.2201262,3.9581344,neighbor,5808102,An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks,0.06888478994369507,#dbdb8d
-0.6484832,-2.0095758,neighbor,5808102,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.06909924745559692,#dbdb8d
-4.29102,6.2469254,neighbor,5808102,Gradual training of deep denoising auto encoders,0.06914079189300537,#dbdb8d
-7.1910276,-0.84287626,neighbor,5808102,Network In Network,0.06957894563674927,#dbdb8d
-6.507359,-4.6768765,neighbor,5808102,Striving for Simplicity: The All Convolutional Net,0.06995171308517456,#dbdb8d
-6.8746524,-2.4410625,neighbor,5808102,SimNets: A Generalization of Convolutional Networks,0.07009768486022949,#dbdb8d
2.7853472,0.42985332,neighbor,5808102,Dropout Rademacher complexity of deep neural networks,0.07036608457565308,#dbdb8d
-3.4589124,3.1671827,neighbor,5808102,Deeply-Supervised Nets,0.07043284177780151,#dbdb8d
-4.461392,2.027034,neighbor,5808102,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.07054603099822998,#dbdb8d
-9.612699,0.67472607,neighbor,5808102,Scalable stacking and learning for building deep architectures,0.07061588764190674,#dbdb8d
0.33676505,0.91273046,neighbor,5808102,Piecewise Linear Multilayer Perceptrons and Dropout,0.07078129053115845,#dbdb8d
-4.5920067,-2.2722402,neighbor,5808102,Understanding Deep Architectures using a Recursive Convolutional Network,0.07159560918807983,#dbdb8d
-11.951863,-1.0258815,neighbor,5808102,Low precision storage for deep learning,0.07161736488342285,#dbdb8d
-4.5880675,-0.46918356,neighbor,5808102,Memory Bounded Deep Convolutional Networks,0.07206207513809204,#dbdb8d
2.2061212,9.767232,neighbor,5808102,Zero-bias autoencoders and the benefits of co-adapting features,0.07210737466812134,#dbdb8d
6.1113014,-1.8190454,neighbor,5808102,Practical Recommendations for Gradient-Based Training of Deep Architectures,0.07255679368972778,#dbdb8d
5.679387,9.241724,neighbor,5808102,Difference Target Propagation,0.07291758060455322,#dbdb8d
-1.3572098,8.557642,neighbor,5808102,Learning from Noisy Labels with Deep Neural Networks,0.07342928647994995,#dbdb8d
8.418107,6.9567323,neighbor,5808102,Joint Training of Deep Boltzmann Machines,0.07439374923706055,#dbdb8d
-0.10637366,5.6708484,neighbor,5808102,Dropout Training for Support Vector Machines,0.07479983568191528,#dbdb8d
2.3417068,2.4700518,neighbor,5808102,An empirical analysis of dropout in piecewise linear networks,0.07495218515396118,#dbdb8d
4.8861256,-5.1689715,neighbor,5808102,Hyper-parameter optimization of deep convolutional networks for object recognition,0.07533663511276245,#dbdb8d
-7.906424,-12.649558,neighbor,5808102,Deformation Models for Image Recognition,0.07562965154647827,#dbdb8d
-4.834839,5.7574587,neighbor,5808102,Scheduled denoising autoencoders,0.07600575685501099,#dbdb8d
-7.762547,-5.5699215,neighbor,5808102,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.07603400945663452,#dbdb8d
-1.9257693,3.437994,neighbor,5808102,Unsupervised Pretraining Encourages Moderate-Sparseness,0.07603627443313599,#dbdb8d
-5.7141547,-3.8269496,neighbor,5808102,Visualizing and Understanding Convolutional Networks,0.07606494426727295,#dbdb8d
-9.830171,-11.183395,neighbor,5808102,Locally Scale-Invariant Convolutional Neural Networks,0.0761297345161438,#dbdb8d
7.6986957,-1.8573809,neighbor,5808102,Qualitatively characterizing neural network optimization problems,0.07652002573013306,#dbdb8d
-8.17323,-2.939438,neighbor,5808102,MatConvNet: Convolutional Neural Networks for MATLAB,0.07652097940444946,#dbdb8d
7.0072975,-0.56775767,neighbor,5808102,Random Walk Initialization for Training Very Deep Feedforward Networks,0.07677757740020752,#dbdb8d
-6.6682973,0.37992772,neighbor,5808102,An Analysis of the Connections Between Layers of Deep Neural Networks,0.07684433460235596,#dbdb8d
-11.195712,-0.24462448,neighbor,5808102,Compressing Deep Convolutional Networks using Vector Quantization,0.0770224928855896,#dbdb8d
7.5922174,2.8072696,neighbor,5808102,"Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients",0.0770343542098999,#dbdb8d
-2.9363346,-6.9328103,neighbor,5808102,Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs,0.07719540596008301,#dbdb8d
6.4699974,1.0773017,neighbor,5808102,Training Neural Networks with Stochastic Hessian-Free Optimization,0.07797342538833618,#dbdb8d
6.8467207,3.1943145,neighbor,5808102,Parallel training of Deep Neural Networks with Natural Gradient and Parameter Averaging,0.07839125394821167,#dbdb8d
-8.770657,3.8376958,neighbor,5808102,One-Shot Adaptation of Supervised Deep Convolutional Models,0.07911747694015503,#dbdb8d
-10.365706,-3.094697,neighbor,5808102,Fast Training of Convolutional Networks through FFTs,0.07975637912750244,#dbdb8d
-1.8265712,-8.928616,neighbor,5808102,One weird trick for parallelizing convolutional neural networks,0.07991796731948853,#dbdb8d
-7.72946,-6.4132767,neighbor,5808102,Analyzing the Performance of Multilayer Neural Networks for Object Recognition,0.08016365766525269,#dbdb8d
4.5042977,-3.2735147,neighbor,5808102,Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning,0.08017832040786743,#dbdb8d
-12.8041,-4.5890207,neighbor,5808102,Efficient and accurate approximations of nonlinear convolutional networks,0.080191969871521,#dbdb8d
0.655898,-0.13115719,neighbor,5808102,Do Deep Nets Really Need to be Deep?,0.08037722110748291,#dbdb8d
-7.4834557,-8.92782,neighbor,5808102,Learnable Pooling Regions for Image Classification,0.08043640851974487,#dbdb8d
1.8338734,6.444497,neighbor,5808102,Recurrent Neural Network Regularization,0.08085650205612183,#dbdb8d
0.92028856,-5.97101,neighbor,5808102,Unsupervised feature learning by augmenting single images,0.0809207558631897,#dbdb8d
5.590764,9.274356,neighbor,5808102,How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation,0.08133584260940552,#dbdb8d
2.1777515,9.958944,neighbor,5808102,Saturating Auto-Encoders,0.08152621984481812,#dbdb8d
-12.484551,3.2446134,neighbor,5808102,Large-Margin kNN Classification Using a Deep Encoder Network,0.08171600103378296,#dbdb8d
9.797744,-0.58871275,neighbor,5808102,Avoiding pathologies in very deep networks,0.08177322149276733,#dbdb8d
-1.3404905,-9.655733,neighbor,5808102,Multi-GPU Training of ConvNets,0.08184540271759033,#dbdb8d
-6.3822327,-9.5385275,neighbor,5808102,Differentiable Pooling for Hierarchical Feature Learning,0.08186787366867065,#dbdb8d
-2.9805324,1.5210807,neighbor,5808102,FitNets: Hints for Thin Deep Nets,0.0820389986038208,#dbdb8d
-5.2104383,-12.459215,neighbor,5808102,Deep Domain Confusion: Maximizing for Domain Invariance,0.08212149143218994,#dbdb8d
-10.042665,-5.5757833,neighbor,5808102,Convolutional neural networks at constrained time cost,0.08234673738479614,#dbdb8d
-11.926024,4.052857,neighbor,5808102,Learning Invariant Representations with Local Transformations,0.0828675627708435,#dbdb8d
8.631747,7.1700497,neighbor,5808102,On Training Deep Boltzmann Machines,0.08301937580108643,#dbdb8d
-9.023508,-4.314401,neighbor,5808102,Caffe: Convolutional Architecture for Fast Feature Embedding,0.08307129144668579,#dbdb8d
1.3816532,-2.2223067,neighbor,5808102,On the Number of Linear Regions of Deep Neural Networks,0.08309638500213623,#dbdb8d
-8.881359,-11.620733,neighbor,5808102,Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection,0.08362096548080444,#dbdb8d
3.5902855,-0.21259722,neighbor,5808102,On the Computational Efficiency of Training Neural Networks,0.08372986316680908,#dbdb8d
-4.4458213,7.761282,neighbor,5808102,Towards Deep Neural Network Architectures Robust to Adversarial Examples,0.08393591642379761,#dbdb8d
-0.83493805,-9.2379,neighbor,5808102,GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training,0.08401793241500854,#dbdb8d
8.825008,2.8334353,neighbor,5808102,Statistically adaptive learning for a general class of cost functions (SA L-BFGS),0.08404821157455444,#dbdb8d
-12.010578,-3.9077513,neighbor,5808102,Flattened Convolutional Neural Networks for Feedforward Acceleration,0.084195077419281,#dbdb8d
-9.289222,-8.089439,neighbor,5808102,Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification,0.08429521322250366,#dbdb8d
-13.125972,-7.9170775,neighbor,5808102,"Fast, simple and accurate handwritten digit classification using extreme learning machines with shaped input-weights",0.0843161940574646,#dbdb8d
-5.8714695,-12.895823,neighbor,5808102,Domain Adaptive Neural Networks for Object Recognition,0.08440953493118286,#dbdb8d
-11.638543,-4.5617976,neighbor,5808102,Speeding up Convolutional Neural Networks with Low Rank Expansions,0.08485591411590576,#dbdb8d
-1.3155404,8.610356,neighbor,5808102,Training Deep Neural Networks on Noisy Labels with Bootstrapping,0.08487749099731445,#dbdb8d
9.643976,7.31918,neighbor,5808102,Learning Feature Hierarchies with Centered Deep Boltzmann Machines,0.08501207828521729,#dbdb8d
-7.983734,-8.024179,neighbor,5808102,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.08515030145645142,#dbdb8d
-8.505668,6.1205997,neighbor,5808102,Domain-Adversarial Neural Networks,0.08571171760559082,#dbdb8d
-9.963906,-11.405454,neighbor,5808102,Scale-Invariant Convolutional Neural Networks,0.08571517467498779,#dbdb8d
-5.7263103,6.1892304,neighbor,5808102,Denoising autoencoder with modulated lateral connections learns invariant representations of natural images,0.08573949337005615,#dbdb8d
9.135358,8.342125,neighbor,5808102,How to Center Binary Deep Boltzmann Machines,0.08581608533859253,#dbdb8d
8.768862,-4.409675,neighbor,5808102,In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning,0.08582991361618042,#dbdb8d
0.41802475,4.5091023,neighbor,5808102,A Bayesian encourages dropout,0.08596980571746826,#dbdb8d
-5.9669676,-1.0232968,query,5959482,Efficient Estimation of Word Representations in Vector Space,0.0,#dbdb8d
-7.0862093,-0.14559773,neighbor,5959482,Factored Neural Language Models,0.0498659610748291,#dbdb8d
-4.3558702,-2.141626,neighbor,5959482,The Expressive Power of Word Embeddings,0.05829143524169922,#dbdb8d
0.93360174,-4.636648,neighbor,5959482,From Frequency to Meaning: Vector Space Models of Semantics,0.0606917142868042,#dbdb8d
-11.086699,4.101382,neighbor,5959482,A Classification Approach to Word Prediction,0.06223928928375244,#dbdb8d
-5.6390777,-2.76099,neighbor,5959482,Semantic Vector Machines,0.06460297107696533,#dbdb8d
11.162269,-5.7407355,neighbor,5959482,Frequency Estimates for Statistical Word Similarity Measures,0.06696683168411255,#dbdb8d
-0.37399232,-1.2422032,neighbor,5959482,Distributional Memory: A General Framework for Corpus-Based Semantics,0.0670309066772461,#dbdb8d
10.318942,-8.429611,neighbor,5959482,A General Framework for Distributional Similarity,0.07179504632949829,#dbdb8d
6.961448,-4.164296,neighbor,5959482,Word Vectors and Two Kinds of Similarity,0.07380431890487671,#dbdb8d
-17.657469,5.1802607,neighbor,5959482,A multi-class approach for modelling out-of-vocabulary words,0.07401823997497559,#dbdb8d
2.417339,-2.6019003,neighbor,5959482,Random Walks for Text Semantic Similarity,0.07614481449127197,#dbdb8d
8.959867,-8.194419,neighbor,5959482,A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches,0.07756060361862183,#dbdb8d
3.0154953,-5.0240164,neighbor,5959482,A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness,0.07798242568969727,#dbdb8d
-20.354424,4.420068,neighbor,5959482,An Empirical Study on Multiple LVCSR Model Combination by Machine Learning,0.07809603214263916,#dbdb8d
7.441678,-10.075182,neighbor,5959482,Distributional Similarity for Chinese: Exploiting Characters and Radicals,0.07862329483032227,#dbdb8d
8.687824,-4.8111777,neighbor,5959482,A Relational Model of Semantic Similarity between Words using Automatically Extracted Lexical Pattern Clusters from the Web,0.07864874601364136,#dbdb8d
14.876172,-6.0950937,neighbor,5959482,Web-Scale Distributional Similarity and Entity Set Expansion,0.07891243696212769,#dbdb8d
0.48271376,4.92795,neighbor,5959482,Learning Structured Embeddings of Knowledge Bases,0.07915544509887695,#dbdb8d
-5.3707395,5.909455,neighbor,5959482,Sequential Document Representations and Simplicial Curves,0.0793188214302063,#dbdb8d
-19.023933,0.66041905,neighbor,5959482,A Dynamic Language Model Based on Individual Word Domains,0.07983744144439697,#dbdb8d
13.716101,-7.5697465,neighbor,5959482,Large-scale Computation of Distributional Similarities for Queries,0.0801740288734436,#dbdb8d
-21.459816,-1.1964204,neighbor,5959482,Vector space representation of language probabilities through SVD of n-gram matrix,0.08044207096099854,#dbdb8d
-13.169255,-0.56154114,neighbor,5959482,Variable word rate N-grams,0.08051896095275879,#dbdb8d
-20.730371,-0.41862813,neighbor,5959482,Nonlocal Language Modeling based on Context Co-occurrence Vectors,0.08085364103317261,#dbdb8d
5.759908,-7.838156,neighbor,5959482,The Google Similarity Distance,0.080899178981781,#dbdb8d
-6.1067586,3.542621,neighbor,5959482,Using CCA to improve CCA: A new spectral method for estimating vector models of words,0.08123266696929932,#dbdb8d
-13.570647,4.4266105,neighbor,5959482,Methods to Integrate a Language Model with Semantic Information for a Word Prediction Component,0.08144211769104004,#dbdb8d
-18.568472,4.85826,neighbor,5959482,Learning units for domain-independent out-of- vocabulary word modelling,0.08152258396148682,#dbdb8d
11.108972,-7.6802597,neighbor,5959482,Co-occurrence Retrieval: A Flexible Framework for Lexical Distributional Similarity,0.08175474405288696,#dbdb8d
-4.377062,4.8188004,neighbor,5959482,Fine: Information embedding for document classification,0.0818096399307251,#dbdb8d
-1.7547747,-11.648062,neighbor,5959482,Syntactic and Semantic Kernels for Short Text Pair Categorization,0.08246797323226929,#dbdb8d
6.0452123,5.894443,neighbor,5959482,A Computational Model to Disentangle Semantic Information Embedded in Word Association Norms,0.0827144980430603,#dbdb8d
11.852484,-10.733905,neighbor,5959482,Feature Weighting for Co-occurrence-based Classification of Words,0.08272618055343628,#dbdb8d
8.803111,-6.65404,neighbor,5959482,Distributional measures of concept-distance: A task-oriented evaluation,0.08315223455429077,#dbdb8d
-6.0239267,-5.8592534,neighbor,5959482,Learning Word Meanings and Descriptive Parameter Spaces from Music,0.08330291509628296,#dbdb8d
-12.1799135,3.301843,neighbor,5959482,Adaptive Language Modeling for Word Prediction,0.08339762687683105,#dbdb8d
-19.943237,6.7166023,neighbor,5959482,Improved word confidence estimation using long range features,0.0837753415107727,#dbdb8d
4.7338057,-8.214896,neighbor,5959482,Google distance between words,0.08384722471237183,#dbdb8d
17.216425,-7.8152246,neighbor,5959482,Lossy Conservative Update (LCU) Sketch: Succinct Approximate Count Storage,0.0842244029045105,#dbdb8d
8.692319,8.294983,neighbor,5959482,Infer the Semantic Orientation of Words by Optimizing Modularity,0.08431762456893921,#dbdb8d
7.8970513,1.1884171,neighbor,5959482,Expressing Implicit Semantic Relations without Supervision,0.08433032035827637,#dbdb8d
15.774009,-10.79295,neighbor,5959482,Learning Term-weighting Functions for Similarity Measures,0.08445888757705688,#dbdb8d
-1.4197437,-12.13981,neighbor,5959482,Efficient kernels for sentence pair classification,0.08460891246795654,#dbdb8d
-8.580822,6.3394346,neighbor,5959482,Fast logistic regression for text categorization with variable-length n-grams,0.08490598201751709,#dbdb8d
18.49861,-10.583712,neighbor,5959482,Learning Preferences with Millions of Parameters by Enforcing Sparsity,0.08492690324783325,#dbdb8d
13.167267,-2.243321,neighbor,5959482,Contribution to topic identification by using word similarity,0.08505505323410034,#dbdb8d
-14.13338,6.439457,neighbor,5959482,Testing the Efficacy of Part-of-Speech Information in Word Completion,0.08525329828262329,#dbdb8d
9.6964445,2.0040376,neighbor,5959482,N Semantic Classes are Harder than Two,0.08534771203994751,#dbdb8d
-16.476398,-3.694004,neighbor,5959482,Training Neural Network Language Models on Very Large Corpora,0.08535891771316528,#dbdb8d
-0.06440189,-0.35749993,neighbor,5959482,"One Distributional Memory, Many Semantic Spaces",0.08542835712432861,#dbdb8d
-16.234211,-0.12458758,neighbor,5959482,Language model size reduction by pruning and clustering,0.08600622415542603,#dbdb8d
-0.69413143,-4.3690014,neighbor,5959482,A Context-Theoretic Framework for Compositionality in Distributional Semantics,0.08645933866500854,#dbdb8d
12.419581,2.7868028,neighbor,5959482,Discriminative training for call classification and routing,0.08650678396224976,#dbdb8d
14.413237,-10.8409815,neighbor,5959482,Part of Speech Based Term Weighting for Information Retrieval,0.08660000562667847,#dbdb8d
-12.762747,5.531496,neighbor,5959482,Evaluating Word Prediction: Framing Keystroke Savings,0.08676594495773315,#dbdb8d
6.495612,-0.92902046,neighbor,5959482,Measuring Semantic Similarity by Latent Relational Analysis,0.08676612377166748,#dbdb8d
6.338158,5.063303,neighbor,5959482,Using Curvature and Markov Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination,0.08705097436904907,#dbdb8d
0.61567175,1.0165913,neighbor,5959482,ISA meets Lara: An incremental word space model for cognitively plausible simulations of semantic learning,0.08711540699005127,#dbdb8d
-8.75263,7.8860335,neighbor,5959482,A robust approach to sequence classification,0.08805334568023682,#dbdb8d
17.544342,-9.749701,neighbor,5959482,Adaptive Regularization for Similarity Measures,0.08818185329437256,#dbdb8d
-3.2739046,0.74905497,neighbor,5959482,Language without words: A pointillist model for natural language processing,0.08821749687194824,#dbdb8d
-16.95441,-1.7371447,neighbor,5959482,Quantization-based language model compression,0.08830243349075317,#dbdb8d
-6.1618085,6.405474,neighbor,5959482,An Improved Hierarchical Bayesian Model of Language for Document Classification,0.08831286430358887,#dbdb8d
-1.5308694,-3.4767857,neighbor,5959482,Concrete Sentence Spaces for Compositional Distributional Models of Meaning,0.08841514587402344,#dbdb8d
5.2796693,-9.45849,neighbor,5959482,Normalized Web Distance and Word Similarity,0.08853262662887573,#dbdb8d
10.007923,0.09783643,neighbor,5959482,Inferring Knowledge from a Large Semantic Network,0.08862632513046265,#dbdb8d
10.954513,-10.431167,neighbor,5959482,Linguistic Preprocessing for Distributional Classification of Words,0.08874225616455078,#dbdb8d
11.107244,6.6299033,neighbor,5959482,Sense Sentiment Similarity: An Analysis,0.08894574642181396,#dbdb8d
-12.209023,-0.7452447,neighbor,5959482,Maximum entropy good-turing estimator for language modeling,0.08896517753601074,#dbdb8d
-8.327158,0.7825476,neighbor,5959482,Training Restricted Boltzmann Machines on Word Observations,0.08921664953231812,#dbdb8d
-18.339851,2.2828307,neighbor,5959482,Statistical language model based on a hierarchical approach: MCnv,0.08922386169433594,#dbdb8d
-18.344055,6.6328316,neighbor,5959482,Automatically incorporating unknown words in JUPITER,0.08923691511154175,#dbdb8d
-11.322561,-7.7631454,neighbor,5959482,Direct and latent modeling techniques for computing spoken document similarity,0.08933556079864502,#dbdb8d
5.716128,0.34656543,neighbor,5959482,Analogy perception applied to seven tests of word comprehension,0.08965939283370972,#dbdb8d
-15.2760105,-0.79791284,neighbor,5959482,A bit of progress in language modeling,0.08999419212341309,#dbdb8d
-11.758638,-6.9073324,neighbor,5959482,Estimating document frequencies in a speech corpus,0.09059566259384155,#dbdb8d
10.117203,7.3537807,neighbor,5959482,Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus,0.0906299352645874,#dbdb8d
9.786637,-4.7708073,neighbor,5959482,A Web Search Engine-Based Approach to Measure Semantic Similarity between Words,0.09065699577331543,#dbdb8d
15.497257,-4.5587697,neighbor,5959482,Text Categorization from Category Name via Lexical Reference,0.09113055467605591,#dbdb8d
1.9913247,-6.3580894,neighbor,5959482,An Introduction to Random Indexing,0.09114348888397217,#dbdb8d
-14.656326,-2.3480647,neighbor,5959482,Tied-Mixture Language Modeling in Continuous Space,0.09115737676620483,#dbdb8d
3.993171,-2.3280594,neighbor,5959482,Measuring semantic relatedness with vector space models and random walks,0.09119546413421631,#dbdb8d
-2.0533733,4.7760363,neighbor,5959482,Reading to Learn: Constructing Features from Semantic Abstracts,0.09126138687133789,#dbdb8d
-9.422924,-3.4288776,neighbor,5959482,Statistical mechanics of letters in words.,0.0913051962852478,#dbdb8d
7.4085155,3.2814598,neighbor,5959482,Learning vocabulary in lexical sets: dangers and guidelines,0.09157061576843262,#dbdb8d
9.371245,9.082203,neighbor,5959482,Extracting Semantic Orientations of Words using Spin Model,0.09178203344345093,#dbdb8d
-22.872053,-0.1586627,neighbor,5959482,Interpolated distanced bigram language models for robust word clustering,0.09221279621124268,#dbdb8d
3.9478872,2.9325998,neighbor,5959482,Detection of New Word Senses by the Outlier Detection Method,0.09232926368713379,#dbdb8d
4.8283024,-4.949332,neighbor,5959482,Using Bag-of-Concepts to Improve the Performance of Support Vector Machines in Text Categorization,0.09259122610092163,#dbdb8d
-15.548347,-4.6644855,neighbor,5959482,Continuous Space Language Models for Statistical Machine Translation,0.09263789653778076,#dbdb8d
7.704294,-7.5326366,neighbor,5959482,Distributional Measures of Semantic Distance: A Survey,0.09275758266448975,#dbdb8d
-10.335337,-0.10965697,neighbor,5959482,Classes for fast maximum entropy training,0.09304499626159668,#dbdb8d
1.8335118,5.259898,neighbor,5959482,A semantic matching energy function for learning with multi-relational data,0.09307283163070679,#dbdb8d
-13.256748,1.6786605,neighbor,5959482,N-gram distribution based language model adaptation,0.09314841032028198,#dbdb8d
-2.5345175,-6.0156593,neighbor,5959482,SVO triple based Latent Semantic Analysis for recognising textual entailment,0.09319579601287842,#dbdb8d
10.025826,-2.7924109,neighbor,5959482,Estimating Semantic Distance Using Soft Semantic Constraints in Knowledge-Source – Corpus Hybrid Models,0.093234121799469,#dbdb8d
-4.3518596,7.6190047,neighbor,5959482,Sparse Topical Coding,0.09336727857589722,#dbdb8d
6.3912725,-1.7717776,neighbor,5959482,Similarity of Semantic Relations,0.0933799147605896,#dbdb8d
-17.225622,2.7129123,neighbor,5959482,Hierarchical class n-gram language models: towards better estimation of unseen events in speech recognition,0.09338027238845825,#dbdb8d
-20.847965,3.5331254,neighbor,5959482,N-best list generation using word and phoneme recognition fusion,0.09346169233322144,#dbdb8d
-5.6240625,-1.621551,query,604334,Intriguing properties of neural networks,0.0,#dbdb8d
-9.585953,-1.7048689,neighbor,604334,Piecewise Linear Multilayer Perceptrons and Dropout,0.06843405961990356,#dbdb8d
-5.386772,2.0727925,neighbor,604334,Switched linear encoding with rectified linear autoencoders,0.07047545909881592,#dbdb8d
-5.0302205,-3.5653832,neighbor,604334,An introduction to deep learning,0.07137250900268555,#dbdb8d
0.18252233,-7.260912,neighbor,604334,Expressive Power and Approximation Errors of Restricted Boltzmann Machines,0.07260805368423462,#dbdb8d
-10.650925,-1.3861483,neighbor,604334,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.07289403676986694,#dbdb8d
-4.618032,0.4043547,neighbor,604334,Saturating Auto-Encoders,0.07394057512283325,#dbdb8d
-9.266335,-3.0286605,neighbor,604334,Maxout Networks,0.07430464029312134,#dbdb8d
-6.8574224,-6.106966,neighbor,604334,Big Neural Networks Waste Capacity,0.07633811235427856,#dbdb8d
-3.1452734,-2.726458,neighbor,604334,Deep Learning of Representations: Looking Forward,0.07731157541275024,#dbdb8d
-13.151693,-2.7233846,neighbor,604334,Visualizing and Understanding Convolutional Networks,0.07862979173660278,#dbdb8d
8.865379,8.2986765,neighbor,604334,Quantifying the complexity of neural network output using entropy measures,0.07939088344573975,#dbdb8d
-7.906987,-1.6694918,neighbor,604334,Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary Independent Stochastic Neurons,0.0802658200263977,#dbdb8d
3.434189,-3.773099,neighbor,604334,The Margitron: A Generalised Perceptron with Margin,0.08160990476608276,#dbdb8d
-5.8403893,4.0440693,neighbor,604334,Recklessly Approximate Sparse Coding,0.08245790004730225,#dbdb8d
-0.17724432,-6.9706936,neighbor,604334,Refinements of Universal Approximation Results for Deep Belief Networks and Restricted Boltzmann Machines,0.08255273103713989,#dbdb8d
-3.4485402,5.888623,neighbor,604334,Robust exponential binary pattern storage in Little-Hopfield networks,0.08367085456848145,#dbdb8d
-4.7146993,4.7549186,neighbor,604334,Sparse Neural Networks With Large Learning Diversity,0.084156334400177,#dbdb8d
-11.8036,-2.7069514,neighbor,604334,Network In Network,0.08450651168823242,#dbdb8d
-10.598937,-9.733834,neighbor,604334,On the difficulty of training recurrent neural networks,0.08455008268356323,#dbdb8d
3.6856134,2.341345,neighbor,604334,Machine learning for neuroscience,0.08473670482635498,#dbdb8d
7.373621,9.494838,neighbor,604334,Encoding auditory scenes with a population of Hodgkin-Huxley neurons,0.0848037600517273,#dbdb8d
-8.030739,-4.315975,neighbor,604334,Improving neural networks by preventing co-adaptation of feature detectors,0.08491259813308716,#dbdb8d
-5.2987204,-5.9348345,neighbor,604334,Predicting Parameters in Deep Learning,0.0849531888961792,#dbdb8d
1.21614,4.383509,neighbor,604334,"Learning, Memory, and the Role of Neural Network Architecture",0.08501410484313965,#dbdb8d
-0.45265827,-0.6223,neighbor,604334,Learning Invariant Representations with Local Transformations,0.08517199754714966,#dbdb8d
-3.5957685,-4.93242,neighbor,604334,Deep belief networks,0.08530747890472412,#dbdb8d
-1.5661439,-4.7533746,neighbor,604334,Sparse Group Restricted Boltzmann Machines,0.0854870080947876,#dbdb8d
-4.2382007,-8.587618,neighbor,604334,Estimating or Propagating Gradients Through Stochastic Neurons,0.0856621265411377,#dbdb8d
-0.29567927,7.630942,neighbor,604334,Encoding Binary Neural Codes in Networks of Threshold-Linear Neurons,0.08594483137130737,#dbdb8d
-6.4753594,0.096008055,neighbor,604334,Unsupervised Pretraining Encourages Moderate-Sparseness,0.08596312999725342,#dbdb8d
-7.188163,-3.6104991,neighbor,604334,Dropout Training as Adaptive Regularization,0.08662223815917969,#dbdb8d
-3.2388997,-2.164631,neighbor,604334,Two SVDs produce more focal deep learning representations,0.0866767168045044,#dbdb8d
2.096553,2.3455718,neighbor,604334,Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning?,0.08668220043182373,#dbdb8d
-4.262197,-8.698169,neighbor,604334,Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation,0.08674675226211548,#dbdb8d
-3.7860775,0.8604406,neighbor,604334,Contractive De-noising Auto-Encoder,0.08694076538085938,#dbdb8d
-6.174409,-12.283389,neighbor,604334,Riemannian metrics for neural networks I: feedforward networks,0.08716690540313721,#dbdb8d
-5.0761642,6.20397,neighbor,604334,Multi-level error-resilient neural networks,0.08719426393508911,#dbdb8d
-0.23881182,0.40718094,neighbor,604334,Learning Stable Group Invariant Representations with Convolutional Networks,0.08765929937362671,#dbdb8d
2.0400233,9.647791,neighbor,604334,Classification images with uncertainty.,0.0880928635597229,#dbdb8d
8.135762,4.937539,neighbor,604334,A talkative Potts attractor neural network welcomes BLISS words,0.08840173482894897,#dbdb8d
-8.58734,-10.405648,neighbor,604334,Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences,0.08840322494506836,#dbdb8d
-0.8419876,7.288604,neighbor,604334,A No-Go Theorem for One-Layer Feedforward Networks,0.08866614103317261,#dbdb8d
3.7152112,-6.8643827,neighbor,604334,A Note on Sample Complexity of Learning Binary Output Neural Networks under Fixed Input Distributions,0.08898764848709106,#dbdb8d
5.2343245,-4.834743,neighbor,604334,Testing Hypotheses by Regularized Maximum Mean Discrepancy,0.08908605575561523,#dbdb8d
-7.2045493,4.214391,neighbor,604334,Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint,0.08908909559249878,#dbdb8d
1.9829164,6.932823,neighbor,604334,Role of Homeostasis in Learning Sparse Representations,0.08918237686157227,#dbdb8d
3.680647,13.703668,neighbor,604334,Elementary derivative tasks and neural net multiscale analysis of tasks.,0.08935749530792236,#dbdb8d
2.9217374,-3.949834,neighbor,604334,The Perceptron with Dynamic Margin,0.08965480327606201,#dbdb8d
5.156384,10.730598,neighbor,604334,Information Loss Associated with Imperfect Observation and Mismatched Decoding,0.08967214822769165,#dbdb8d
-3.8644037,12.800733,neighbor,604334,Stochastic Binary Neural Networks for Qualitatively Robust Predictive Model Mapping,0.09013396501541138,#dbdb8d
-1.5887281,9.547895,neighbor,604334,Representation and Measure of Structural Information,0.09045171737670898,#dbdb8d
-2.512349,-6.2759366,neighbor,604334,On Training Deep Boltzmann Machines,0.0905197262763977,#dbdb8d
-10.765114,2.0371082,neighbor,604334,"Image denoising with multi-layer perceptrons, part 2: training trade-offs and analysis of their mechanisms",0.0907554030418396,#dbdb8d
-14.769485,-1.9585867,neighbor,604334,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.09084075689315796,#dbdb8d
-1.2535458,4.474564,neighbor,604334,Supervised Learning in Multilayer Spiking Neural Networks,0.09087371826171875,#dbdb8d
-6.9779797,1.4156948,neighbor,604334,Discriminative Recurrent Sparse Auto-Encoders,0.0909850001335144,#dbdb8d
1.5478097,-9.175104,neighbor,604334,Neurally Implementable Semantic Networks,0.09101307392120361,#dbdb8d
-11.982981,-0.07808335,neighbor,604334,Differentiable Pooling for Hierarchical Feature Learning,0.09101748466491699,#dbdb8d
3.305359,4.637316,neighbor,604334,Change-Based Inference in Attractor Nets: Linear Analysis,0.09102398157119751,#dbdb8d
-0.22162734,8.850805,neighbor,604334,Combinatorial Neural Codes from a Mathematical Coding Theory Perspective,0.09125351905822754,#dbdb8d
6.3870807,1.6193607,neighbor,604334,Discovering and Characterizing Hidden Variables Using a Novel Neural Network Architecture: LO-Net,0.09126102924346924,#dbdb8d
2.7805111,4.0662284,neighbor,604334,Neuroscience and AI Share the Same Elegant Mathematical Trap,0.09133422374725342,#dbdb8d
6.5735717,5.263159,neighbor,604334,Homeostasis causes hallucinations in a hierarchical generative model of the visual cortex: the Charles Bonnet Syndrome,0.09134984016418457,#dbdb8d
4.635807,6.263984,neighbor,604334,Predictive Coding of Dynamical Variables in Balanced Spiking Networks,0.09139633178710938,#dbdb8d
2.5796316,-1.9983572,neighbor,604334,Introduction to Machine Learning,0.09151065349578857,#dbdb8d
0.21371119,2.6356242,neighbor,604334,Knowledge Matters: Importance of Prior Information for Optimization,0.09153705835342407,#dbdb8d
-6.065494,1.5030485,neighbor,604334,k-Sparse Autoencoders,0.09213781356811523,#dbdb8d
-5.8891463,-11.834066,neighbor,604334,Revisiting Natural Gradient for Deep Networks,0.0921475887298584,#dbdb8d
10.164937,3.3463454,neighbor,604334,Optimal capacity of the Blume-Emery-Griffiths perceptron.,0.09222555160522461,#dbdb8d
2.8640616,-5.237383,neighbor,604334,Perceptron Mistake Bounds,0.09247773885726929,#dbdb8d
6.666528,8.250441,neighbor,604334,Sparse coding of natural communication signals in midbrain neurons,0.09251594543457031,#dbdb8d
-12.69114,0.4967346,neighbor,604334,Deconvolutional networks,0.09254348278045654,#dbdb8d
-16.163942,-0.3509618,neighbor,604334,Deep Attribute Networks,0.09261929988861084,#dbdb8d
-10.308618,5.2656503,neighbor,604334,Generating High-Order Threshold Functions with Multiple Thresholds,0.09268677234649658,#dbdb8d
-5.743445,-13.257608,neighbor,604334,Manifold Learning: The Price of Normalization,0.09275603294372559,#dbdb8d
4.0270286,0.9197584,neighbor,604334,Statistical Physics of Feedforward Neural Networks,0.09279727935791016,#dbdb8d
7.1862855,-1.1126386,neighbor,604334,A probabilistic study of neural complexity,0.09279811382293701,#dbdb8d
-10.180812,-3.643443,neighbor,604334,From Maxout to Channel-Out: Encoding Information on Sparse Pathways,0.09281361103057861,#dbdb8d
-9.540935,-9.315462,neighbor,604334,Advances in optimizing recurrent networks,0.09283888339996338,#dbdb8d
-9.566037,0.019042961,neighbor,604334,A Global Algorithm for Training Multilayer Neural Networks,0.09295284748077393,#dbdb8d
-15.605577,-2.616369,neighbor,604334,Multi-column deep neural networks for image classification,0.09309834241867065,#dbdb8d
5.632733,3.08984,neighbor,604334,Hierarchical Models in the Brain,0.09314119815826416,#dbdb8d
-12.136884,-4.863793,neighbor,604334,Quadratic Features and Deep Architectures for Chunking,0.09315276145935059,#dbdb8d
-15.89012,-3.1007955,neighbor,604334,"Deep, Big, Simple Neural Nets for Handwritten Digit Recognition",0.09348088502883911,#dbdb8d
3.008956,9.763803,neighbor,604334,Optimal stimulus encoders for natural tasks.,0.0938035249710083,#dbdb8d
4.317015,7.938911,neighbor,604334,Function Identification in Neuron Populations via Information Bottleneck,0.09384036064147949,#dbdb8d
1.612008,7.271862,neighbor,604334,Group sparse coding with a collection of winner-take-all networks,0.09392493963241577,#dbdb8d
-9.839729,-5.5237164,neighbor,604334,Learning multilayer perceptrons efficiently,0.09396904706954956,#dbdb8d
-2.5549357,-5.881949,neighbor,604334,Joint Training of Deep Boltzmann Machines,0.09397172927856445,#dbdb8d
0.11819823,-2.2526593,neighbor,604334,Large-Margin kNN Classification Using a Deep Encoder Network,0.09402555227279663,#dbdb8d
3.684926,6.657822,neighbor,604334,Neural computation with efficient population codes,0.09407007694244385,#dbdb8d
6.2617316,6.67249,neighbor,604334,Review: The Noisy Brain: Stochastic Dynamics as a Principle of Brain Function,0.09407031536102295,#dbdb8d
-2.9264283,11.390309,neighbor,604334,Stochastic pooling networks,0.09409123659133911,#dbdb8d
7.488492,7.129308,neighbor,604334,Correction: On How Network Architecture Determines the Dominant Patterns of Spontaneous Neural Activity,0.09413033723831177,#dbdb8d
5.579214,10.31021,neighbor,604334,Information Loss in an Optimal Maximum Likelihood Decoding,0.09419071674346924,#dbdb8d
5.005159,-2.920777,neighbor,604334,Statistical mechanics of learning with soft margin classifiers.,0.09423846006393433,#dbdb8d
0.4417808,11.818373,neighbor,604334,Symmetry breaking in soft clustering decoding of neural codes,0.09427720308303833,#dbdb8d
-9.015536,-8.723959,neighbor,604334,Regularization and nonlinearities for neural language models: when are they needed?,0.09433656930923462,#dbdb8d
-3.5643573,10.371195,neighbor,604334,Finite-State Dimension and Lossy Decompressors,0.09437382221221924,#dbdb8d
-1.3287295,-7.89571,neighbor,604334,Understanding Boltzmann Machine and Deep Learning via A Confident Information First Principle,0.09438151121139526,#dbdb8d
-1.6829702,5.662179,query,60814714,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,0.0,#17becf
-2.9862883,4.9238305,neighbor,60814714,Fully convolutional networks for semantic segmentation,0.026295900344848633,#17becf
-2.4501925,4.6319146,neighbor,60814714,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,0.03468841314315796,#17becf
-4.8275886,3.6998296,neighbor,60814714,BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation,0.03978598117828369,#17becf
-2.070826,6.7109046,neighbor,60814714,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling,0.04015737771987915,#17becf
-4.4286857,2.5587976,neighbor,60814714,Learning Deconvolution Network for Semantic Segmentation,0.04025834798812866,#17becf
-3.0954514,3.1742957,neighbor,60814714,Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation,0.04107266664505005,#17becf
-3.434856,4.265411,neighbor,60814714,Fully Connected Deep Structured Networks,0.04118824005126953,#17becf
-3.1050575,6.544899,neighbor,60814714,Feedforward semantic segmentation with zoom-out features,0.041767656803131104,#17becf
-3.3950682,1.6208669,neighbor,60814714,Convolutional feature masking for joint object and stuff segmentation,0.043506503105163574,#17becf
-6.3643017,2.8509026,neighbor,60814714,Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation,0.04374927282333374,#17becf
-5.8978586,2.7464998,neighbor,60814714,Weakly Supervised Semantic Segmentation with Convolutional Networks,0.04760468006134033,#17becf
1.5157782,7.4410644,neighbor,60814714,Deep Deconvolutional Networks for Scene Parsing,0.0476374626159668,#17becf
-4.4968104,5.7209597,neighbor,60814714,ParseNet: Looking Wider to See Better,0.04870861768722534,#17becf
3.496828,-0.05808781,neighbor,60814714,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,0.05117887258529663,#17becf
-5.1203184,4.3641977,neighbor,60814714,Combining the Best of Graphical Models and ConvNets for Semantic Segmentation,0.05242466926574707,#17becf
-5.7663465,1.9461334,neighbor,60814714,Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation,0.0531996488571167,#17becf
0.5368541,7.0647593,neighbor,60814714,Deep hierarchical parsing for semantic segmentation,0.05321776866912842,#17becf
2.0063426,12.062807,neighbor,60814714,Constrained Parametric Proposals and Pooling Methods for Semantic Segmentation in RGB-D Images,0.05338054895401001,#17becf
-1.748133,0.73673344,neighbor,60814714,Semantic Part Segmentation with Deep Learning,0.05455780029296875,#17becf
-0.36532575,12.532083,neighbor,60814714,Layered Interpretation of Street View Images,0.05460023880004883,#17becf
-5.177066,0.6233828,neighbor,60814714,STC: A Simple to Complex Framework for Weakly-Supervised Semantic Segmentation,0.05511009693145752,#17becf
-5.6488776,6.217446,neighbor,60814714,Context Tricks for Cheap Semantic Segmentation,0.055268168449401855,#17becf
3.976218,-0.9813637,neighbor,60814714,segDeepM: Exploiting segmentation and context in deep neural networks for object detection,0.055615007877349854,#17becf
-2.1080549,1.9760904,neighbor,60814714,Joint Object and Part Segmentation Using Deep Learned Potentials,0.055833399295806885,#17becf
-8.770217,-2.6166337,neighbor,60814714,Conditional Random Fields as Recurrent Neural Networks,0.055882811546325684,#17becf
2.1381931,11.341301,neighbor,60814714,Learning Rich Features from RGB-D Images for Object Detection and Segmentation,0.05598551034927368,#17becf
2.846292,12.272997,neighbor,60814714,Indoor Semantic Segmentation using depth information,0.05614572763442993,#17becf
-6.771645,4.5535803,neighbor,60814714,What's the Point: Semantic Segmentation with Point Supervision,0.056276917457580566,#17becf
4.6544614,7.011338,neighbor,60814714,Places205-VGGNet Models for Scene Recognition,0.05695122480392456,#17becf
-8.840868,-3.366509,neighbor,60814714,Closed-Form Training of Conditional Random Fields for Large Scale Image Segmentation,0.05761849880218506,#17becf
1.3136431,7.2188926,neighbor,60814714,Recurrent Convolutional Neural Networks for Scene Parsing,0.057850182056427,#17becf
-0.53950346,2.735131,neighbor,60814714,Improving spatial codification in semantic segmentation,0.058004677295684814,#17becf
-4.456997,-2.5652404,neighbor,60814714,Scene Segmentation with Low-Dimensional Semantic Representations and Conditional Random Fields,0.059386610984802246,#17becf
2.4051206,-2.777882,neighbor,60814714,Proposal-Free Network for Instance-Level Object Segmentation,0.06003439426422119,#17becf
-7.1620293,2.4080136,neighbor,60814714,Constrained Convolutional Neural Networks for Weakly Supervised Segmentation,0.06028485298156738,#17becf
3.4346,-0.19616468,neighbor,60814714,Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model,0.060669541358947754,#17becf
3.275002,-6.5011153,neighbor,60814714,High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision,0.060694217681884766,#17becf
1.9894372,-1.1419113,neighbor,60814714,Layered object detection for multi-class segmentation,0.06169170141220093,#17becf
3.4306726,2.105324,neighbor,60814714,Hypercolumns for object segmentation and fine-grained localization,0.06199979782104492,#17becf
4.062359,11.521074,neighbor,60814714,SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes,0.06303739547729492,#17becf
-8.071523,4.577539,neighbor,60814714,Joint Calibration for Semantic Segmentation,0.06310349702835083,#17becf
-5.340093,7.653916,neighbor,60814714,Mapping Auto-context Decision Forests to Deep ConvNets for Semantic Segmentation,0.06380003690719604,#17becf
5.0621567,9.639512,neighbor,60814714,Understand scene categories by objects: A semantic regularized scene classifier using Convolutional Neural Networks,0.06406670808792114,#17becf
0.12774184,-2.628691,neighbor,60814714,Semantic segmentation using regions and parts,0.06412327289581299,#17becf
-1.9457321,-6.5792236,neighbor,60814714,A fast learning algorithm for image segmentation with max-pooling convolutional networks,0.06617218255996704,#17becf
-10.843064,-0.5841896,neighbor,60814714,Semantic video segmentation: Exploring inference efficiency,0.06686276197433472,#17becf
1.6265863,10.382364,neighbor,60814714,Monocular Object Instance Segmentation and Depth Ordering with CNNs,0.06788128614425659,#17becf
9.752534,3.2617607,neighbor,60814714,Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification,0.06888270378112793,#17becf
-1.5260539,13.836411,neighbor,60814714,Sensor fusion for semantic segmentation of urban scenes,0.06925594806671143,#17becf
-4.9289603,-8.449181,neighbor,60814714,Image Segmentation by Cascaded Region Agglomeration,0.06969869136810303,#17becf
0.27826333,-4.5550413,neighbor,60814714,Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation,0.07000339031219482,#17becf
6.795704,6.6445165,neighbor,60814714,Scene understanding based on Multi-Scale Pooling of deep learning features,0.0706779956817627,#17becf
5.107157,12.632841,neighbor,60814714,"Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture",0.07092773914337158,#17becf
5.525311,-0.7433719,neighbor,60814714,"End-to-end integration of a Convolutional Network, Deformable Parts Model and non-maximum suppression",0.07102042436599731,#17becf
1.916775,-7.5370255,neighbor,60814714,Semantic Amodal Segmentation,0.07183682918548584,#17becf
-8.682476,-5.999211,neighbor,60814714,Multiclass Image Segmentation Based on Pixel and Segment Level,0.07193225622177124,#17becf
-5.033788,-2.4225543,neighbor,60814714,Dense Semantic Image Segmentation with Objects and Attributes,0.07220447063446045,#17becf
-8.484394,-4.801556,neighbor,60814714,Semantic Segmentation with Same Topic Constraints,0.07239621877670288,#17becf
-1.6187712,13.738283,neighbor,60814714,Fusion Based Holistic Road Scene Understanding,0.072520911693573,#17becf
-8.51561,-8.834829,neighbor,60814714,A fast method for inferring high-quality simply-connected superpixels,0.07307678461074829,#17becf
3.5349138,-1.8730766,neighbor,60814714,Learning to Segment Object Candidates,0.07312136888504028,#17becf
5.2547245,-2.644198,neighbor,60814714,Deep Joint Task Learning for Generic Object Extraction,0.07327812910079956,#17becf
-9.520894,-3.5315006,neighbor,60814714,Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials,0.07345759868621826,#17becf
2.0593915,0.7098555,neighbor,60814714,Pedestrian detection aided by deep learning semantic tasks,0.07375222444534302,#17becf
-1.3609163,-0.8884603,neighbor,60814714,Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency,0.07383811473846436,#17becf
-1.9145122,-3.610215,neighbor,60814714,Context by region ancestry,0.07442116737365723,#17becf
8.274777,4.5791397,neighbor,60814714,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,0.0748710036277771,#17becf
-5.774145,-8.446089,neighbor,60814714,Image Segmentation Using Hierarchical Merge Tree,0.07510411739349365,#17becf
-8.202484,1.864934,neighbor,60814714,Learning from Weak and Noisy Labels for Semantic Segmentation,0.07542818784713745,#17becf
5.035799,1.4276274,neighbor,60814714,Scalable Object Detection Using Deep Neural Networks,0.07551223039627075,#17becf
4.5798807,-6.096699,neighbor,60814714,DeepEdge: A multi-scale bifurcated deep network for top-down contour detection,0.07565414905548096,#17becf
-2.5325317,-9.325012,neighbor,60814714,A benchmark for semantic image segmentation,0.0759589672088623,#17becf
2.602373,-6.1975455,neighbor,60814714,Situational object boundary detection,0.07653826475143433,#17becf
2.6910853,-5.489919,neighbor,60814714,Semantic contours from inverse detectors,0.07658588886260986,#17becf
6.582856,-0.6337587,neighbor,60814714,Deformable part models are convolutional neural networks,0.07669508457183838,#17becf
5.976115,2.1401713,neighbor,60814714,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",0.0767713189125061,#17becf
6.0351577,7.9996614,neighbor,60814714,Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification,0.0771569013595581,#17becf
-0.14134884,-2.213363,neighbor,60814714,Semantic part segmentation using compositional model combining shape and appearance,0.0772165060043335,#17becf
2.1784625,12.924343,neighbor,60814714,Semantic Instance Labeling Leveraging Hierarchical Segmentation,0.07739627361297607,#17becf
1.4676226,3.0857244,neighbor,60814714,"Codemaps - Segment, Classify and Search Objects Locally",0.07746142148971558,#17becf
4.293296,-7.2039514,neighbor,60814714,Holistically-Nested Edge Detection,0.07770669460296631,#17becf
7.288219,4.302132,neighbor,60814714,Very Deep Convolutional Networks for Large-Scale Image Recognition,0.07812851667404175,#17becf
-10.81303,-0.8737697,neighbor,60814714,Beyond Semantic Image Segmentation : Exploring Efficient Inference in Video,0.07834118604660034,#17becf
-11.136785,1.580406,neighbor,60814714,A Novel Knowledge-Compatibility Benchmarker for Semantic Segmentation,0.07834553718566895,#17becf
-5.3579082,-6.0292664,neighbor,60814714,Loosecut: Interactive image segmentation with loosely bounded boxes,0.07943904399871826,#17becf
9.3995285,2.151197,neighbor,60814714,Fast image scanning with deep max-pooling convolutional neural networks,0.07948219776153564,#17becf
-2.3025584,8.896307,neighbor,60814714,Training Deep Networks with Structured Layers by Matrix Backpropagation,0.07973825931549072,#17becf
-3.8563535,-8.148542,neighbor,60814714,Hierarchical Image Segmentation Using a Combined Geometrical and Feature Based Approach,0.07986027002334595,#17becf
-3.957278,-9.695024,neighbor,60814714,Interactive multiclass segmentation using superpixel classification,0.07998621463775635,#17becf
9.8755455,3.629204,neighbor,60814714,Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems,0.08004730939865112,#17becf
6.792357,5.313939,neighbor,60814714,Multi-scale Recognition with DAG-CNNs,0.0801350474357605,#17becf
-7.3584642,-3.6885169,neighbor,60814714,Harmony potentials for joint classification and segmentation,0.08041715621948242,#17becf
-9.815994,-5.509982,neighbor,60814714,Enhanced Hierarchical Conditional Random Field model for semantic image segmentation,0.08056718111038208,#17becf
5.624104,8.492624,neighbor,60814714,Object Detectors Emerge in Deep Scene CNNs,0.08063197135925293,#17becf
-3.756621,-0.015066264,neighbor,60814714,Fully Convolutional Neural Networks for Crowd Segmentation,0.08065205812454224,#17becf
7.6104927,3.4196963,neighbor,60814714,Going deeper with convolutions,0.08090955018997192,#17becf
-6.599551,-8.538693,neighbor,60814714,Data-driven tree-structured Bayesian network for image segmentation,0.08100420236587524,#17becf
-6.594176,-6.469782,neighbor,60814714,Non-Parametric Probabilistic Image Segmentation,0.08117038011550903,#17becf
-4.2718635,10.005114,neighbor,60814714,Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians,0.08160591125488281,#17becf
-5.8418612,-6.2973228,neighbor,60814714,Tree-Cut for Probabilistic Image Segmentation,0.08171242475509644,#17becf
-1.936413,2.9605315,query,6200260,Image-to-Image Translation with Conditional Adversarial Networks,0.0,#17becf
4.187487,2.8083034,neighbor,6200260,Neural Photo Editing with Introspective Adversarial Networks,0.045805931091308594,#17becf
2.2244503,1.6648265,neighbor,6200260,Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks,0.055589497089385986,#17becf
-10.530805,-1.2845271,neighbor,6200260,Pixel-Level Domain Transfer,0.055737435817718506,#17becf
5.495383,1.8254243,neighbor,6200260,Learning What and Where to Draw,0.058402180671691895,#17becf
6.04889,2.1281588,neighbor,6200260,Generative Adversarial Text to Image Synthesis,0.05996119976043701,#17becf
-0.6178851,7.1656694,neighbor,6200260,Invertible Conditional GANs for image editing,0.060427188873291016,#17becf
3.4455779,2.5577095,neighbor,6200260,Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks,0.06049704551696777,#17becf
3.0919385,4.333956,neighbor,6200260,Energy-based Generative Adversarial Network,0.06055563688278198,#17becf
-9.953147,7.1905756,neighbor,6200260,Automatic Photo Adjustment Using Deep Learning,0.0625467300415039,#17becf
5.2811766,0.6961509,neighbor,6200260,Generative Image Modeling Using Style and Structure Adversarial Networks,0.06325101852416992,#17becf
2.1598446,2.8316784,neighbor,6200260,Improved Techniques for Training GANs,0.06534945964813232,#17becf
0.9725481,6.26569,neighbor,6200260,Coupled Generative Adversarial Networks,0.06581968069076538,#17becf
2.2460873,6.1935086,neighbor,6200260,Inverting the Generator of a Generative Adversarial Network,0.06818610429763794,#17becf
6.809528,3.678305,neighbor,6200260,Generative Visual Manipulation on the Natural Image Manifold,0.06818920373916626,#17becf
3.1235468,5.4832754,neighbor,6200260,Generative Adversarial Parallelization,0.06866675615310669,#17becf
12.019101,4.89477,neighbor,6200260,Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks,0.06876367330551147,#17becf
4.956656,3.8459663,neighbor,6200260,Generating images with recurrent adversarial networks,0.0690232515335083,#17becf
4.454003,1.0214454,neighbor,6200260,LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation,0.06969296932220459,#17becf
1.2321955,6.992749,neighbor,6200260,Adversarial Autoencoders,0.06976974010467529,#17becf
1.9263889,4.5418653,neighbor,6200260,Least Squares Generative Adversarial Networks,0.07044905424118042,#17becf
1.9618556,-8.479392,neighbor,6200260,Colorful Image Colorization,0.07106930017471313,#17becf
-2.1273777,-0.010311131,neighbor,6200260,Adversarial Manipulation of Deep Representations,0.07148802280426025,#17becf
-2.136528,6.8811736,neighbor,6200260,Autoencoding beyond pixels using a learned similarity metric,0.07159721851348877,#17becf
-3.8083496,-4.8282423,neighbor,6200260,Some Improvements on Deep Convolutional Neural Network Based Image Classification,0.07332658767700195,#17becf
1.221093,-0.9987466,neighbor,6200260,Conditional Image Generation with PixelCNN Decoders,0.07355177402496338,#17becf
-5.593201,7.722581,neighbor,6200260,Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation,0.07359415292739868,#17becf
12.141006,3.560511,neighbor,6200260,Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer,0.07362884283065796,#17becf
-3.0097625,5.8592267,neighbor,6200260,Inverting face embeddings with convolutional neural networks,0.07396483421325684,#17becf
-10.105593,-1.6621771,neighbor,6200260,Unsupervised Cross-Domain Image Generation,0.07433396577835083,#17becf
-11.542614,6.1798606,neighbor,6200260,Image restoration using online photo collections,0.07466036081314087,#17becf
5.8425174,-3.2423775,neighbor,6200260,NICE: Non-linear Independent Components Estimation,0.07515609264373779,#17becf
-3.936434,-3.0246756,neighbor,6200260,Confusing Deep Convolution Networks by Relabelling,0.07536602020263672,#17becf
10.003987,1.8016663,neighbor,6200260,Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks,0.07537800073623657,#17becf
-10.584806,-11.480024,neighbor,6200260,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",0.0754934549331665,#17becf
0.7614992,2.00248,neighbor,6200260,RenderGAN: Generating Realistic Labeled Data,0.07571744918823242,#17becf
-2.5283458,-0.5971411,neighbor,6200260,The Artificial Mind's Eye: Resisting Adversarials for Convolutional Neural Networks using Internal Projection,0.07584506273269653,#17becf
-1.3263319,0.79072154,neighbor,6200260,Adversarial Diversity and Hard Positive Generation,0.0759115219116211,#17becf
0.94109714,8.203881,neighbor,6200260,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,0.07591992616653442,#17becf
5.4337597,7.6853776,neighbor,6200260,"How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?",0.07650691270828247,#17becf
9.849721,-1.7043889,neighbor,6200260,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,0.07715469598770142,#17becf
-8.345454,7.294293,neighbor,6200260,Deep Feature Interpolation for Image Content Changes,0.07725244760513306,#17becf
4.026606,5.915218,neighbor,6200260,Generative Multi-Adversarial Networks,0.07765001058578491,#17becf
2.4454298,7.878752,neighbor,6200260,Conditional Generative Adversarial Nets,0.0777503252029419,#17becf
-6.7516804,7.417712,neighbor,6200260,What Happened to My Dog in That Network: Unraveling Top-down Generators in Convolutional Neural Networks,0.07816940546035767,#17becf
-1.3991781,8.572352,neighbor,6200260,Towards Automatic Image Editing: Learning to See another You,0.07821589708328247,#17becf
-4.1593847,4.417145,neighbor,6200260,Deep Lambertian Networks,0.07828855514526367,#17becf
-5.674141,7.0647736,neighbor,6200260,Warped Convolutions: Efficient Invariance to Spatial Transformations,0.07868015766143799,#17becf
-7.3171606,-0.260352,neighbor,6200260,The Cross-Depiction Problem: Computer Vision Algorithms for Recognising Objects in Artwork and in Photographs,0.0791933536529541,#17becf
5.3193736,-4.829342,neighbor,6200260,The Patch Transform,0.07941651344299316,#17becf
-10.577306,7.3994737,neighbor,6200260,Example-based image color and tone style enhancement,0.08012181520462036,#17becf
-8.62588,2.601612,neighbor,6200260,Learning a Discriminative Model for the Perception of Realism in Composite Images,0.08055317401885986,#17becf
0.3262055,-1.4334581,neighbor,6200260,"Variational Autoencoder for Deep Learning of Images, Labels and Captions",0.08113229274749756,#17becf
9.509689,1.3934065,neighbor,6200260,Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis,0.08113449811935425,#17becf
0.6225034,-8.975861,neighbor,6200260,Image Colorization Using a Deep Convolutional Neural Network,0.08120512962341309,#17becf
5.190964,5.6270013,neighbor,6200260,Generating Images Part by Part with Composite Generative Adversarial Networks,0.0812598466873169,#17becf
2.6460001,0.39520952,neighbor,6200260,Context Encoders: Feature Learning by Inpainting,0.08130079507827759,#17becf
10.55993,5.088719,neighbor,6200260,Instance Normalization: The Missing Ingredient for Fast Stylization,0.08132517337799072,#17becf
-9.147514,-9.116806,neighbor,6200260,Boosting Image Captioning with Attributes,0.08148884773254395,#17becf
-8.893573,-10.402565,neighbor,6200260,Cross-Lingual Image Caption Generation,0.08198201656341553,#17becf
-7.106313,2.6063077,neighbor,6200260,Understanding Deep Features with Computer-Generated Imagery,0.08207440376281738,#17becf
-10.348017,-10.711485,neighbor,6200260,Guiding Long-Short Term Memory for Image Caption Generation,0.08220511674880981,#17becf
-9.512634,-11.508571,neighbor,6200260,Show and tell: A neural image caption generator,0.0822264552116394,#17becf
0.39876124,3.9901063,neighbor,6200260,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,0.08235418796539307,#17becf
-8.966645,-2.884985,neighbor,6200260,Domain Separation Networks,0.08259743452072144,#17becf
11.1232605,3.9099653,neighbor,6200260,From A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators,0.08261531591415405,#17becf
-9.170948,-11.104025,neighbor,6200260,Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge,0.08266931772232056,#17becf
-10.999761,4.9510427,neighbor,6200260,FrameBreak: Dramatic Image Extrapolation by Guided Shift-Maps,0.08267080783843994,#17becf
-6.6821437,1.6076345,neighbor,6200260,Exploring Invariances in Deep Convolutional Neural Networks Using Synthetic Images,0.08268511295318604,#17becf
-10.379947,-9.277514,neighbor,6200260,Technical Report: Image Captioning with Semantically Similar Images,0.08279949426651001,#17becf
10.937266,2.694743,neighbor,6200260,Texture Networks: Feed-forward Synthesis of Textures and Stylized Images,0.08304142951965332,#17becf
6.9704633,-3.8655243,neighbor,6200260,Mixtures of Conditional Gaussian Scale Mixtures Applied to Multiscale Image Representations,0.08321607112884521,#17becf
-4.5013876,-2.8325403,neighbor,6200260,Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach,0.08336621522903442,#17becf
8.8659115,-1.4792271,neighbor,6200260,Learning to generate images with perceptual similarity metrics,0.08343654870986938,#17becf
1.8350115,-9.64971,neighbor,6200260,Epitome for Automatic Image Colorization,0.08360612392425537,#17becf
13.336496,3.4051194,neighbor,6200260,Content Aware Neural Style Transfer,0.08368098735809326,#17becf
-5.6210427,3.476779,neighbor,6200260,Deep Convolutional Inverse Graphics Network,0.08376294374465942,#17becf
-9.135017,2.3752484,neighbor,6200260,How Useful Is Photo-Realistic Rendering for Visual Learning?,0.08392167091369629,#17becf
3.799301,6.67885,neighbor,6200260,Generative adversarial networks,0.08403563499450684,#17becf
-11.4356,4.2293854,neighbor,6200260,Data-driven hallucination of different times of day from a single outdoor photo,0.08438140153884888,#17becf
-5.2334614,2.4895818,neighbor,6200260,Learning to generate chairs with convolutional neural networks,0.08474868535995483,#17becf
9.972932,-2.2837853,neighbor,6200260,Amortised MAP Inference for Image Super-resolution,0.0847826600074768,#17becf
-7.03464,4.0696597,neighbor,6200260,Deep Stereo: Learning to Predict New Views from the World's Imagery,0.08481347560882568,#17becf
6.689611,-4.519166,neighbor,6200260,Modeling Images using Transformed Indian Buffet Processes,0.08484119176864624,#17becf
-4.6030803,-5.1266546,neighbor,6200260,Master's Thesis : Deep Learning for Visual Recognition,0.08498591184616089,#17becf
-8.641952,-2.445573,neighbor,6200260,Adapting Models to Signal Degradation using Distillation,0.08508831262588501,#17becf
-9.817365,-9.954253,neighbor,6200260,Image Representations and New Domains in Neural Image Captioning,0.08532160520553589,#17becf
-0.9874461,-8.129543,neighbor,6200260,Color Space Transformation Network,0.08536076545715332,#17becf
2.427303,-2.8337598,neighbor,6200260,Learning to Linearize Under Uncertainty,0.08545732498168945,#17becf
3.2146015,8.942652,neighbor,6200260,Adversarially Learned Inference,0.0856550931930542,#17becf
0.6706385,-8.283028,neighbor,6200260,Deep Colorization,0.08569455146789551,#17becf
5.434147,-2.2215838,neighbor,6200260,Efficient inference in occlusion-aware generative models of images,0.08597958087921143,#17becf
2.4476993,-8.768531,neighbor,6200260,Colorization for Image Compression,0.08613896369934082,#17becf
3.6273959,7.464804,neighbor,6200260,Generative Adversarial Nets,0.08628857135772705,#17becf
-3.86051,-5.293102,neighbor,6200260,Rethinking the Inception Architecture for Computer Vision,0.08634448051452637,#17becf
2.7680817,-1.9132286,neighbor,6200260,Unsupervised Learning of Visual Structure using Predictive Generative Networks,0.08665478229522705,#17becf
12.214084,2.951344,neighbor,6200260,Style Transfer Via Texture Synthesis,0.08668166399002075,#17becf
0.2883662,-6.6571484,neighbor,6200260,Can fully convolutional networks perform well for general image restoration problems?,0.08683514595031738,#17becf
-3.5422366,8.102143,neighbor,6200260,Learning invariance through imitation,0.08695220947265625,#17becf
-6.601211,-4.625787,neighbor,6200260,Information-theoretical label embeddings for large-scale image classification,0.08701425790786743,#17becf
-7.7658944,-4.5849876,neighbor,6200260,Associative Embeddings for Large-Scale Knowledge Transfer with Self-Assessment,0.08702260255813599,#17becf
6.7736254,0.940611,query,6447277,Identity Mappings in Deep Residual Networks,0.0,#17becf
7.82734,-0.6452475,neighbor,6447277,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),0.045044898986816406,#17becf
6.7455626,2.6697,neighbor,6447277,Resnet in Resnet: Generalizing Residual Architectures,0.04770082235336304,#17becf
3.0818975,5.733453,neighbor,6447277,Deep Residual Learning for Image Recognition,0.05028802156448364,#17becf
9.808488,0.21209514,neighbor,6447277,Empirical Evaluation of Rectified Activations in Convolutional Network,0.05325949192047119,#17becf
3.043258,4.918175,neighbor,6447277,"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",0.0533907413482666,#17becf
8.974014,0.74469686,neighbor,6447277,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,0.05404549837112427,#17becf
-8.972006,-6.3441668,neighbor,6447277,Natural Neural Networks,0.0588533878326416,#17becf
8.651214,-1.2228853,neighbor,6447277,Expressiveness of Rectifier Networks,0.05961430072784424,#17becf
-0.9675295,7.533939,neighbor,6447277,How far can we go without convolution: Improving fully-connected networks,0.061077117919921875,#17becf
4.9418874,4.115284,neighbor,6447277,Network In Network,0.062174975872039795,#17becf
-4.4944844,2.6300004,neighbor,6447277,Maxout Networks,0.06252360343933105,#17becf
-2.6177983,-2.9707065,neighbor,6447277,All you need is a good init,0.06341999769210815,#17becf
-4.7961664,10.029405,neighbor,6447277,BinaryConnect: Training Deep Neural Networks with binary weights during propagations,0.06373375654220581,#17becf
-5.8335786,7.790048,neighbor,6447277,Adjustable Bounded Rectifiers: Towards Deep Binary Representations,0.06420302391052246,#17becf
-7.042079,3.7049637,neighbor,6447277,Improving Back-Propagation by Adding an Adversarial Gradient,0.06436276435852051,#17becf
-4.478115,3.5986128,neighbor,6447277,Improving Deep Neural Networks with Probabilistic Maxout Units,0.06493586301803589,#17becf
3.822996,-5.345964,neighbor,6447277,Representation Benefits of Deep Feedforward Networks,0.06511622667312622,#17becf
7.4610906,8.697757,neighbor,6447277,SimNets: A Generalization of Convolutional Networks,0.06515657901763916,#17becf
10.842164,-1.0361974,neighbor,6447277,Taming the ReLU with Parallel Dither in a Deep Neural Network,0.06522071361541748,#17becf
-10.810622,2.0685189,neighbor,6447277,Scheduled denoising autoencoders,0.06554263830184937,#17becf
6.2351713,5.524866,neighbor,6447277,Deep Epitomic Convolutional Neural Networks,0.06734079122543335,#17becf
-3.0795143,-6.5460305,neighbor,6447277,Exact solutions to the nonlinear dynamics of learning in deep linear neural networks,0.0675501823425293,#17becf
0.68673855,-2.50619,neighbor,6447277,Training Very Deep Networks,0.06775534152984619,#17becf
0.2037127,-2.7295027,neighbor,6447277,Highway Networks,0.06790512800216675,#17becf
-10.216198,-0.52926487,neighbor,6447277,Switched linear encoding with rectified linear autoencoders,0.0680968165397644,#17becf
-7.1499195,-2.1036751,neighbor,6447277,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,0.06852966547012329,#17becf
-8.020546,-2.692817,neighbor,6447277,Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks,0.0689241886138916,#17becf
-4.1000333,-7.770692,neighbor,6447277,Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity,0.06939303874969482,#17becf
7.612691,-5.155193,neighbor,6447277,On the importance of normalisation layers in deep learning with piecewise linear activation units,0.06947249174118042,#17becf
-4.4863505,0.9338123,neighbor,6447277,Fundamental differences between Dropout and Weight Decay in Deep Networks,0.06955039501190186,#17becf
-4.2330174,9.093133,neighbor,6447277,Binarized Neural Networks,0.06997889280319214,#17becf
1.650184,-5.1461315,neighbor,6447277,Learning Neural Network Architectures using Backpropagation,0.0700412392616272,#17becf
-11.593488,-6.1331725,neighbor,6447277,Knowledge Transfer in Deep Block-Modular Neural Networks,0.07025223970413208,#17becf
-0.15185389,2.0700984,neighbor,6447277,Deeply-Supervised Nets,0.07035493850708008,#17becf
-7.6395955,-4.22098,neighbor,6447277,Understanding symmetries in deep networks,0.07058972120285034,#17becf
0.7155798,12.614847,neighbor,6447277,Training CNNs with Low-Rank Filters for Efficient Image Classification,0.07086819410324097,#17becf
5.7649136,-1.9591348,neighbor,6447277,Intriguing properties of neural networks,0.07090318202972412,#17becf
0.6372023,2.2860076,neighbor,6447277,Training Deeper Convolutional Networks with Deep Supervision,0.07091730833053589,#17becf
-7.299433,-11.471794,neighbor,6447277,Difference Target Propagation,0.07101577520370483,#17becf
9.310366,5.2732496,neighbor,6447277,Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks,0.07105886936187744,#17becf
-7.8042917,-11.390848,neighbor,6447277,How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation,0.07183241844177246,#17becf
-9.912194,0.75213474,neighbor,6447277,A Winner-Take-All Method for Training Sparse Convolutional Autoencoders,0.0721292495727539,#17becf
1.9144806,-3.110585,neighbor,6447277,FitNets: Hints for Thin Deep Nets,0.07221978902816772,#17becf
-3.682929,0.21655604,neighbor,6447277,Learning Compact Convolutional Neural Networks with Nested Dropout,0.0722915530204773,#17becf
2.0379093,12.577341,neighbor,6447277,Efficient and accurate approximations of nonlinear convolutional networks,0.07263636589050293,#17becf
-11.818826,-2.2469802,neighbor,6447277,Saturating Auto-Encoders,0.0728117823600769,#17becf
11.436144,7.037837,neighbor,6447277,Network Morphism,0.07293128967285156,#17becf
-5.4133186,1.4695759,neighbor,6447277,Shakeout: A New Regularized Deep Neural Network Training Scheme,0.07360684871673584,#17becf
-12.191521,1.8897593,neighbor,6447277,Gradual Training Method for Denoising Auto Encoders,0.07366722822189331,#17becf
0.021189181,-0.6851144,neighbor,6447277,My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013,0.07374948263168335,#17becf
1.2746202,-14.081504,neighbor,6447277,Low-rank passthrough neural networks,0.07393860816955566,#17becf
0.045316547,-5.0772386,neighbor,6447277,GradNets: Dynamic Interpolation Between Neural Architectures,0.07399773597717285,#17becf
8.060239,6.4172783,neighbor,6447277,Fractional Max-Pooling,0.07406151294708252,#17becf
4.5806794,7.2972193,neighbor,6447277,Do Convnets Learn Correspondence?,0.07436788082122803,#17becf
2.485815,7.765249,neighbor,6447277,Striving for Simplicity: The All Convolutional Net,0.07440561056137085,#17becf
-15.329381,-3.5154932,neighbor,6447277,From Maxout to Channel-Out: Encoding Information on Sparse Pathways,0.0745241641998291,#17becf
9.101093,6.3656588,neighbor,6447277,"Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree",0.07454806566238403,#17becf
3.9944165,2.3519475,neighbor,6447277,An Analysis of the Connections Between Layers of Deep Neural Networks,0.07457053661346436,#17becf
-4.6845775,-10.994045,neighbor,6447277,Equilibrium Propagation: Bridging the Gap between Energy-Based Models and Backpropagation,0.07457250356674194,#17becf
-12.118701,2.332762,neighbor,6447277,Gradual training of deep denoising auto encoders,0.07470434904098511,#17becf
-13.176767,0.36337793,neighbor,6447277,"Why are deep nets reversible: A simple theory, with implications for training",0.07491326332092285,#17becf
-3.6477764,-5.2999253,neighbor,6447277,Random Walk Initialization for Training Very Deep Feedforward Networks,0.07498574256896973,#17becf
6.578662,-5.5976796,neighbor,6447277,On the Number of Linear Regions of Deep Neural Networks,0.07500791549682617,#17becf
-4.3760962,-3.2035952,neighbor,6447277,RandomOut: Using a convolutional gradient norm to rescue convolutional filters,0.07504332065582275,#17becf
-5.0513334,-0.15732676,neighbor,6447277,Efficient batchwise dropout training using submatrices,0.07505691051483154,#17becf
7.2209625,9.118262,neighbor,6447277,Deep SimNets,0.07511389255523682,#17becf
-0.9815382,-8.217022,neighbor,6447277,StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity,0.07522499561309814,#17becf
4.5721097,-3.4885716,neighbor,6447277,Piecewise Linear Multilayer Perceptrons and Dropout,0.07539522647857666,#17becf
7.888363,-5.79552,neighbor,6447277,Piecewise Linear Activation Functions For More Efficient Deep Networks,0.07555210590362549,#17becf
-7.198759,-7.8443775,neighbor,6447277,A Kronecker-factored approximate Fisher matrix for convolution layers,0.0756879448890686,#17becf
2.362941,-12.497168,neighbor,6447277,Recurrent Neural Network Regularization,0.0758521556854248,#17becf
-8.365244,-1.3968744,neighbor,6447277,Why Regularized Auto-Encoders learn Sparse Representation?,0.07587873935699463,#17becf
0.7926434,13.31566,neighbor,6447277,Convolutional neural networks with low-rank regularization,0.07610344886779785,#17becf
5.942503,-11.1047535,neighbor,6447277,Capacity measurement of a recurrent inhibitory neural network,0.07627391815185547,#17becf
-10.26208,-10.425195,neighbor,6447277,The Multiverse Loss for Robust Transfer Learning,0.07641464471817017,#17becf
1.2777711,6.9160643,neighbor,6447277,Visualizing and Understanding Convolutional Networks,0.07657212018966675,#17becf
2.5874646,-10.878538,neighbor,6447277,Benchmarking of LSTM Networks,0.07662707567214966,#17becf
2.1740003,8.705473,neighbor,6447277,Going deeper with convolutions,0.07667911052703857,#17becf
-9.803411,-7.521539,neighbor,6447277,Representational Distance Learning for Deep Neural Networks,0.0768057107925415,#17becf
-5.5021887,9.927733,neighbor,6447277,Neural Networks with Few Multiplications,0.07716327905654907,#17becf
4.867589,-11.443006,neighbor,6447277,Recurrent neural networks,0.07719868421554565,#17becf
1.2960949,10.052787,neighbor,6447277,Accelerating Very Deep Convolutional Networks for Classification and Detection,0.07725930213928223,#17becf
-3.1308956,11.285548,neighbor,6447277,Memory Bounded Deep Convolutional Networks,0.07736384868621826,#17becf
2.8360949,-12.849151,neighbor,6447277,A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,0.07745307683944702,#17becf
-1.930594,4.2894154,neighbor,6447277,An Analysis of Unsupervised Pre-training in Light of Recent Advances,0.07752150297164917,#17becf
9.370397,9.231228,neighbor,6447277,Group Equivariant Convolutional Networks,0.0776553750038147,#17becf
9.998702,-4.909182,neighbor,6447277,"A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks",0.07765799760818481,#17becf
-0.96673995,10.545474,neighbor,6447277,Learning separable fixed-point kernels for deep convolutional neural networks,0.07774221897125244,#17becf
10.675535,-6.142784,neighbor,6447277,Stochastic Neural Networks with Monotonic Activation Functions,0.07778632640838623,#17becf
-1.6087065,-6.2193065,neighbor,6447277,Cyclical Learning Rates for Training Neural Networks,0.07786351442337036,#17becf
-1.3603796,13.555643,neighbor,6447277,Fast ConvNets Using Group-Wise Brain Damage,0.07789921760559082,#17becf
-1.9892143,9.341619,neighbor,6447277,Deep Fried Convnets,0.0780676007270813,#17becf
-2.7609336,1.0871255,neighbor,6447277,Dropout Rademacher complexity of deep neural networks,0.07813620567321777,#17becf
0.34283468,4.0946684,neighbor,6447277,Mediated experts for deep convolutional networks,0.07815450429916382,#17becf
-7.9180374,-5.577074,neighbor,6447277,Practical Riemannian Neural Networks,0.07828378677368164,#17becf
1.9945409,-6.411585,neighbor,6447277,Understanding Deep Architectures using a Recursive Convolutional Network,0.0783424973487854,#17becf
1.8206347,0.9687636,neighbor,6447277,Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks,0.07845574617385864,#17becf
3.1806068,-1.2377983,neighbor,6447277,Do Deep Nets Really Need to be Deep?,0.07848566770553589,#17becf
4.190481,10.455592,neighbor,6447277,Understanding learned CNN features through Filter Decoding with Substitution,0.0785251259803772,#17becf
11.168022,1.0886661,neighbor,6447277,Deep Learning with S-Shaped Rectified Linear Activation Units,0.07864850759506226,#17becf
