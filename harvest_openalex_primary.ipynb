{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e014e2f",
   "metadata": {},
   "source": [
    "## OpenAlex Topic-Based Preprint Harvesting Pipeline\n",
    "\n",
    "This notebook collects and stores large-scale database from the OpenAlex API by:\n",
    "- Harvesting works for a predefined set of OpenAlex research topics, as given by topic_ids in excel sheets  \n",
    "- Fetching records in parallel with retry logic and rate-limit handling  \n",
    "- Extracting work metadata, topic assignments, etc.\n",
    "\n",
    "**Input:**\n",
    "- OpenAlex API (`/works`, `/topics`)\n",
    "- Topic IDs excel (e.g., Psychology-related OpenAlex topics)\n",
    "- Publication date range (`start_date`, `end_date`)\n",
    "\n",
    "**Output Database:**\n",
    "Field-based OpenAlex database, eg: `psychology_AI.db` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44ea2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "# Create a lock to control database access\n",
    "db_lock = threading.Lock()\n",
    "# Set number of concurrent threads (5 is usually safe for OpenAlex without hitting limits too hard)\n",
    "MAX_WORKERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database at dbs/openAlex_psychology.db\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import duckdb\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta, date\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Configs\n",
    "OPENALEX_BASE = \"https://api.openalex.org/works\"\n",
    "TOPIC_BASE = \"https://api.openalex.org/topics\"\n",
    "PAGE_SIZE = 200\n",
    "COUNT_LIMIT = 5000 \n",
    "DB_DIR = './data/dbs'\n",
    "if not os.path.exists(DB_DIR):\n",
    "    os.makedirs(DB_DIR)\n",
    "DB_FILENAME = 'openAlex_psychology.db'\n",
    "DB_PATH = os.path.join(DB_DIR, DB_FILENAME)\n",
    "\n",
    "#Safety Settings\n",
    "MAX_RETRIES_PER_INTERVAL = 3   # Try each interval 3 times before giving up\n",
    "MAX_CONSECUTIVE_FAILURES = 10  # If 10 intervals fail in a row, STOP the script.\n",
    "consecutive_failure_count = 0  # Global counter (don't edit this manually)\n",
    "failure_lock = threading.Lock() # Lock to update the counter safely\n",
    "\n",
    "# Connect to DuckDB\n",
    "conn = duckdb.connect(DB_PATH)\n",
    "print(f\"Connected to database at {DB_PATH}\")\n",
    "\n",
    "# Request Wrapper\n",
    "def request_with_retry(url, params=None):\n",
    "    \"\"\"\n",
    "    Makes a GET request. If it fails due to network issues or \n",
    "    server errors (500, 502, 503, 429), it waits and retries indefinitely.\n",
    "    Raises exception only for client errors (400, 401, 404, etc).\n",
    "    \"\"\"\n",
    "    delay = 2  # Start with 2 seconds delay\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            # If successful, return response\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            \n",
    "            # If server error or rate limit, print and retry\n",
    "            if r.status_code in [429, 500, 502, 503, 504]:\n",
    "                print(f\"Status {r.status_code} received. Retrying in {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "                delay = min(delay * 2, 60) # Exponential backoff up to 60s\n",
    "                continue\n",
    "            \n",
    "            # If it's a client error (e.g. 404), raise it immediately\n",
    "            r.raise_for_status()\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Network error: {e}. Retrying in {delay}s...\")\n",
    "            time.sleep(delay)\n",
    "            delay = min(delay * 2, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c3dd59-eeba-4366-ad4a-a1bb2beececd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema initialized with MAG ID.\n"
     ]
    }
   ],
   "source": [
    "def init_schema(conn):\n",
    "    # conn.execute(\"DROP TABLE IF EXISTS works\") \n",
    "    \n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS works (\n",
    "            id VARCHAR PRIMARY KEY,\n",
    "            title VARCHAR,\n",
    "            doi VARCHAR,\n",
    "            publication_date DATE,\n",
    "            primary_topic VARCHAR,\n",
    "            version VARCHAR,\n",
    "            fwci DOUBLE,\n",
    "            citation_count INTEGER,\n",
    "            mag_id BIGINT             -- New Column for MAG ID\n",
    "        )\n",
    "    \"\"\")\n",
    "    # ... rest of the schema tables (topics, work_topics, harvest_intervals) remain the same ...\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS topics (\n",
    "            topic_id VARCHAR PRIMARY KEY,\n",
    "            topic_name VARCHAR UNIQUE\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS work_topics (\n",
    "            work_id VARCHAR,\n",
    "            topic_id VARCHAR,\n",
    "            score DOUBLE,\n",
    "            PRIMARY KEY (work_id, topic_id),\n",
    "            FOREIGN KEY (work_id) REFERENCES works(id),\n",
    "            FOREIGN KEY (topic_id) REFERENCES topics(topic_id)\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS harvest_intervals (\n",
    "            topic_id VARCHAR,\n",
    "            start_date DATE,\n",
    "            end_date DATE,\n",
    "            last_token VARCHAR,\n",
    "            fetched_count INTEGER DEFAULT 0,\n",
    "            status VARCHAR,\n",
    "            last_updated TIMESTAMP,\n",
    "            PRIMARY KEY (topic_id, start_date, end_date)\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"Schema initialized with MAG ID.\")\n",
    "\n",
    "init_schema(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba10a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(topic_id, start_date, end_date):\n",
    "    params = {\n",
    "        \"filter\": f\"primary_topic.id:{topic_id},from_publication_date:{start_date},to_publication_date:{end_date},type:preprint|article\",\n",
    "        \"per-page\": 1\n",
    "    }\n",
    "    # Uses the retry wrapper\n",
    "    r = request_with_retry(OPENALEX_BASE, params=params)\n",
    "    return int(r.json()[\"meta\"][\"count\"])\n",
    "\n",
    "def get_topic_name(topic_id):\n",
    "    # Uses the retry wrapper\n",
    "    r = request_with_retry(f\"{TOPIC_BASE}/{topic_id}\")\n",
    "    data = r.json()\n",
    "    return data.get(\"display_name\") or data.get(\"name\")\n",
    "\n",
    "def fetch_page(topic_id, start_date, end_date, cursor=\"*\"):\n",
    "    params = {\n",
    "        \"filter\": f\"primary_topic.id:{topic_id},from_publication_date:{start_date},to_publication_date:{end_date},type:preprint|article\",\n",
    "        \"per-page\": PAGE_SIZE,\n",
    "        \"cursor\": cursor\n",
    "    }\n",
    "    # Uses the retry wrapper\n",
    "    r = request_with_retry(OPENALEX_BASE, params=params)\n",
    "    data = r.json()\n",
    "    results = data.get(\"results\", [])\n",
    "    next_cursor = data[\"meta\"].get(\"next_cursor\")\n",
    "    return results, next_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a1db57-c97c-4d4a-a41c-09b7ec475c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_intervals(conn, topic_id, intervals):\n",
    "    for start, end in intervals:\n",
    "        conn.execute(\"\"\"\n",
    "            INSERT OR IGNORE INTO harvest_intervals\n",
    "            (topic_id, start_date, end_date, status, fetched_count, last_updated)\n",
    "            VALUES (?, ?, ?, 'pending', 0, NOW())\n",
    "        \"\"\", [topic_id, start, end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8d759-c9a8-4af3-b4ea-e9c9acc69110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_works_bulk(conn, works_list):\n",
    "    \"\"\"\n",
    "    Inserts a large list of works efficiently.\n",
    "    \"\"\"\n",
    "    if not works_list:\n",
    "        return\n",
    "    \n",
    "    # specific optimization: Convert date objects to string to ensure DuckDB compatibility\n",
    "    for w in works_list:\n",
    "        if isinstance(w.get('publication_date'), (date, datetime)):\n",
    "            w['publication_date'] = str(w['publication_date'])\n",
    "\n",
    "    df = pd.DataFrame(works_list)\n",
    "    \n",
    "    # We use the lock only for the split second of writing\n",
    "    with db_lock:\n",
    "        conn.register(\"batch_works\", df)\n",
    "        # Bulk insert handling duplicates automatically\n",
    "        conn.execute(\"INSERT OR IGNORE INTO works SELECT * FROM batch_works\")\n",
    "        conn.unregister(\"batch_works\")\n",
    "\n",
    "def insert_topics_bulk(conn, results_list):\n",
    "    \"\"\"\n",
    "    Extracts all topics and relations from the results list and bulk inserts them.\n",
    "    Drastically faster than row-by-row checks.\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        return\n",
    "\n",
    "    # 1. Prepare Data in Memory (No Lock needed yet)\n",
    "    unique_topics = {}\n",
    "    work_topic_relations = []\n",
    "\n",
    "    for w in results_list:\n",
    "        work_id = w.get(\"id\").split('/')[-1]\n",
    "        all_topics = w.get(\"topics\") or []\n",
    "        \n",
    "        for t in all_topics:\n",
    "            topic_id = t.get(\"id\").split('/')[-1]\n",
    "            topic_name = t.get(\"display_name\") or t.get(\"name\")\n",
    "            score = t.get(\"score\") or 0.0\n",
    "            \n",
    "            if topic_id and topic_name:\n",
    "                unique_topics[topic_id] = topic_name\n",
    "                work_topic_relations.append({\n",
    "                    \"work_id\": work_id,\n",
    "                    \"topic_id\": topic_id,\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "\n",
    "    if not unique_topics:\n",
    "        return\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    topics_df = pd.DataFrame(list(unique_topics.items()), columns=[\"topic_id\", \"topic_name\"])\n",
    "    relations_df = pd.DataFrame(work_topic_relations)\n",
    "\n",
    "    # 2. Bulk Write (Lock needed now)\n",
    "    with db_lock:\n",
    "        # A. Insert missing Topics\n",
    "        # We assume existing topics are relatively static, but let's just use INSERT OR IGNORE\n",
    "        conn.register(\"batch_topics\", topics_df)\n",
    "        conn.execute(\"INSERT OR IGNORE INTO topics SELECT * FROM batch_topics\")\n",
    "        conn.unregister(\"batch_topics\")\n",
    "\n",
    "        # B. Insert Work-Topic Relations\n",
    "        if not relations_df.empty:\n",
    "            conn.register(\"batch_relations\", relations_df)\n",
    "            conn.execute(\"INSERT OR IGNORE INTO work_topics SELECT * FROM batch_relations\")\n",
    "            conn.unregister(\"batch_relations\")\n",
    "\n",
    "def mark_interval_complete(conn, topic_id, start_date, end_date, total_count):\n",
    "    \"\"\"\n",
    "    Updates the interval status to 'done' in one go.\n",
    "    \"\"\"\n",
    "    with db_lock:\n",
    "        conn.execute(\"\"\"\n",
    "            UPDATE harvest_intervals\n",
    "            SET status='done', fetched_count=?, last_updated=now()\n",
    "            WHERE topic_id=? AND start_date=? AND end_date=?\n",
    "        \"\"\", [total_count, topic_id, start_date, end_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f426cc18-e9f0-400c-b82d-ac63d5aa59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_interval_atomic(conn, topic_id, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches the ENTIRE interval into memory first, then dumps to DB.\n",
    "    Eliminates database locking during the download phase.\n",
    "    \"\"\"\n",
    "    #Memory Buffers\n",
    "    all_works = []\n",
    "    all_raw_results = [] # We need raw results for topic parsing\n",
    "    \n",
    "    cursor = \"*\"\n",
    "    finished = False\n",
    "    \n",
    "    # Download Loop\n",
    "    while not finished:\n",
    "        # If an interval was halfway done, we restart it to ensure consistency.\n",
    "        \n",
    "        results, next_cursor = fetch_page(topic_id, start_date, end_date, cursor)\n",
    "        \n",
    "        # Buffer raw results for topic extraction later\n",
    "        all_raw_results.extend(results)\n",
    "\n",
    "        # drop unused JSON fields\n",
    "        for w in results:\n",
    "            primary_topic = w.get(\"primary_topic\", {}).get(\"display_name\") if w.get(\"primary_topic\") else None\n",
    "            mag_id = w.get(\"ids\", {}).get(\"mag\")\n",
    "            \n",
    "            all_works.append({\n",
    "                \"id\": w.get(\"id\").split('/')[-1],\n",
    "                \"title\": w.get(\"title\"),\n",
    "                \"doi\": w.get(\"doi\"),\n",
    "                \"publication_date\": w.get(\"publication_date\"),\n",
    "                \"primary_topic\": primary_topic,\n",
    "                \"version\": w.get(\"primary_location\", {}).get(\"version\", {}),\n",
    "                \"fwci\": w.get(\"fwci\"),\n",
    "                \"citation_count\": w.get(\"cited_by_count\"),\n",
    "                \"mag_id\": mag_id\n",
    "            })\n",
    "\n",
    "        if next_cursor is None:\n",
    "            finished = True\n",
    "        else:\n",
    "            cursor = next_cursor\n",
    "\n",
    "   \n",
    "    if all_works:\n",
    "        insert_works_bulk(conn, all_works)\n",
    "        insert_topics_bulk(conn, all_raw_results)\n",
    "        mark_interval_complete(conn, topic_id, start_date, end_date, len(all_works))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa13d45-af3a-470e-a835-0612d380ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_range_safely(topic_id, start_date, end_date, limit=COUNT_LIMIT):\n",
    "    \"\"\"\n",
    "    Recursively splits date ranges until the count is below the limit.\n",
    "    \"\"\"\n",
    "    cnt = get_count(topic_id, start_date, end_date)\n",
    "    print(f\"   Checking range {start_date} -> {end_date}: count = {cnt}\")\n",
    "    \n",
    "    if cnt <= limit:\n",
    "        return [(start_date, end_date)]\n",
    "    \n",
    "    days = (end_date - start_date).days\n",
    "    if days <= 1:\n",
    "        # If we are down to 1 day and still over limit, we must accept it (or handle pagination)\n",
    "        return [(start_date, end_date)]\n",
    "        \n",
    "    mid = start_date + timedelta(days=days // 2)\n",
    "    left = split_range_safely(topic_id, start_date, mid, limit)\n",
    "    right = split_range_safely(topic_id, mid + timedelta(days=1), end_date, limit)\n",
    "    return left + right\n",
    "\n",
    "def process_interval_wrapper(args):\n",
    "    \"\"\"\n",
    "    Wrapper that retries the interval if it fails, and triggers a stop \n",
    "    if too many failures happen globally.\n",
    "    \"\"\"\n",
    "    conn, topic_id, start_date, end_date = args\n",
    "    global consecutive_failure_count\n",
    "\n",
    "    # Check Circuit Breaker\n",
    "    if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "        return (start_date, end_date, False)\n",
    "\n",
    "    # Retry Loop for this specific interval\n",
    "    for attempt in range(1, MAX_RETRIES_PER_INTERVAL + 1):\n",
    "        try:\n",
    "            # Try to fetch\n",
    "            fetch_interval_atomic(conn, topic_id, start_date, end_date)\n",
    "            \n",
    "            # If successful:\n",
    "            with failure_lock:\n",
    "                consecutive_failure_count = 0 # Reset global failure count on success\n",
    "            return (start_date, end_date, True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error in interval {start_date}->{end_date} (Attempt {attempt}/{MAX_RETRIES_PER_INTERVAL}): {e}\")\n",
    "            time.sleep(2 * attempt) # Wait a bit before retrying (2s, 4s, 6s)\n",
    "    \n",
    "    # If we failed 3 times, mark as a failure\n",
    "    print(f\" GIVING UP on interval {start_date}->{end_date} after {MAX_RETRIES_PER_INTERVAL} attempts.\")\n",
    "    with failure_lock:\n",
    "        consecutive_failure_count += 1\n",
    "        if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "            print(f\"\\n {consecutive_failure_count} consecutive failures detected. Stopping script to prevent skipping all intervals.\\n\")\n",
    "            \n",
    "    return (start_date, end_date, False)\n",
    "\n",
    "def harvest_topics(conn, topic_ids, start_date, end_date):\n",
    "    global consecutive_failure_count\n",
    "    \n",
    "    for topic_id in topic_ids:\n",
    "        if not str(topic_id).startswith('t'):\n",
    "             topic_id_fmt = 't' + str(topic_id)\n",
    "        else:\n",
    "             topic_id_fmt = str(topic_id)\n",
    "             \n",
    "        try:\n",
    "            topic_name = get_topic_name(topic_id_fmt)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch name for {topic_id_fmt}, skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Harvesting topic {topic_id_fmt} ({topic_name}) ===\")\n",
    "\n",
    "        with db_lock:\n",
    "            existing_intervals = conn.execute(\"\"\"\n",
    "                SELECT start_date, end_date \n",
    "                FROM harvest_intervals \n",
    "                WHERE topic_id = ? \n",
    "                ORDER BY start_date\n",
    "            \"\"\", [topic_id_fmt]).fetchall()\n",
    "\n",
    "        if existing_intervals:\n",
    "            print(f\" Found {len(existing_intervals)} existing intervals.\")\n",
    "            intervals = existing_intervals\n",
    "        else:\n",
    "            print(f\" Calculating splits via API...\")\n",
    "            intervals = split_range_safely(topic_id_fmt, start_date, end_date)\n",
    "            with db_lock:\n",
    "                register_intervals(conn, topic_id_fmt, intervals)\n",
    "\n",
    "        # Identify Pending Work\n",
    "        tasks = []\n",
    "        for safe_start, safe_end in intervals:\n",
    "            # (Date fixing code same as before...)\n",
    "            if hasattr(safe_start, 'date'): safe_start = safe_start.date()\n",
    "            if hasattr(safe_end, 'date'): safe_end = safe_end.date()\n",
    "            if isinstance(safe_start, str): safe_start = datetime.strptime(safe_start, '%Y-%m-%d').date()\n",
    "            if isinstance(safe_end, str): safe_end = datetime.strptime(safe_end, '%Y-%m-%d').date()\n",
    "\n",
    "            with db_lock:\n",
    "                row = conn.execute(\"\"\"\n",
    "                    SELECT status FROM harvest_intervals\n",
    "                    WHERE topic_id=? AND start_date=? AND end_date=?\n",
    "                \"\"\", [topic_id_fmt, safe_start, safe_end]).fetchone()\n",
    "            \n",
    "            if row and row[0] == \"done\":\n",
    "                continue\n",
    "                \n",
    "            tasks.append((conn, topic_id_fmt, safe_start, safe_end))\n",
    "\n",
    "        print(f\" Starting {len(tasks)} pending intervals with {MAX_WORKERS} threads...\")\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            # Submit tasks\n",
    "            futures = []\n",
    "            for t in tasks:\n",
    "                # Check stop signal BEFORE submitting\n",
    "                if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "                    print(\" Circuit breaker active. Cancelling remaining tasks.\")\n",
    "                    break\n",
    "                futures.append(executor.submit(process_interval_wrapper, t))\n",
    "            \n",
    "            # Monitor results\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "                start, end, success = future.result()\n",
    "                if not success and consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "                    print(\" Stopping execution loop due to critical failures.\")\n",
    "                    executor.shutdown(wait=False) # Force stop\n",
    "                    return # Exit the function completely\n",
    "\n",
    "        if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "            print(\" Execution halted.\")\n",
    "            break\n",
    "\n",
    "        print(f\"=== Completed harvesting topic {topic_id_fmt} ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd61320e-5d5f-4103-bbb8-5266f489d369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Harvesting topic t11084 (Workplace Health and Well-being) ===\n",
      " Calculating splits via API...\n",
      "   Checking range 2000-01-01 -> 2026-01-01: count = 710\n",
      " Starting 1 pending intervals with 5 threads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mharvest_topics\u001b[39m\u001b[34m(conn, topic_ids, start_date, end_date)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Monitor results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuccess\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/DataLit/.venv/lib/python3.14/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/concurrent/futures/_base.py:237\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    233\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    234\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    235\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/threading.py:669\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/threading.py:369\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m start_date = date(\u001b[32m2000\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     12\u001b[39m end_date = date(\u001b[32m2026\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mharvest_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mharvest_topics\u001b[39m\u001b[34m(conn, topic_ids, start_date, end_date)\u001b[39m\n\u001b[32m    109\u001b[39m     tasks.append((conn, topic_id_fmt, safe_start, safe_end))\n\u001b[32m    111\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Starting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pending intervals with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_WORKERS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m threads...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Submit tasks\u001b[39;00m\n\u001b[32m    115\u001b[39m     futures = []\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tasks:\n\u001b[32m    117\u001b[39m         \u001b[38;5;66;03m# Check stop signal BEFORE submitting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/concurrent/futures/_base.py:667\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/concurrent/futures/thread.py:273\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0/Frameworks/Python.framework/Versions/3.14/lib/python3.14/threading.py:1132\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1130\u001b[39m     timeout = \u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_os_thread_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Read topic IDs from an Excel file (must contain a 'topic_id' column)\n",
    "topics_df = pd.read_excel(\"./data/topic_sheets/Psychology.xlsx\")\n",
    "\n",
    "# Extract topic IDs as a list (strip leading 't' if present)\n",
    "topic_ids = (\n",
    "    topics_df[\"topic_id\"]\n",
    "    .astype(str)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "start_date = date(2000, 1, 1)\n",
    "end_date = date(2026, 1, 1)\n",
    "\n",
    "harvest_topics(conn, topic_ids, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097dad60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
