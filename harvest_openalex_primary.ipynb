{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7207cc",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "This script is a robust data harvesting tool designed to fetch academic works from the OpenAlex API and store them in a local DuckDB database. It is specifically optimized for high-volume data retrieval by using multi-threading, automatic retries for network failures, and efficient bulk database writes.\n",
    "\n",
    "The script targets specific \"**Topics**\" from topic list file and retrieves all associated articles within a specified date range (2000â€“2026)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import duckdb\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta, date\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import concurrent.futures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fda29",
   "metadata": {},
   "source": [
    "### **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OPENALEX_BASE = \"https://api.openalex.org/works\"\n",
    "TOPIC_BASE = \"https://api.openalex.org/topics\"\n",
    "PAGE_SIZE = 200\n",
    "COUNT_LIMIT = 5000 \n",
    "DB_DIR = 'db'\n",
    "DB_FILENAME = 'openAlex.db'\n",
    "DB_PATH = os.path.join(DB_DIR, DB_FILENAME)\n",
    "MAX_WORKERS = 5\n",
    "db_lock = threading.Lock()\n",
    "\n",
    "MAX_RETRIES_PER_INTERVAL = 3   # Try each interval 3 times before giving up\n",
    "MAX_CONSECUTIVE_FAILURES = 10  # If 10 intervals fail in a row, STOP the script.\n",
    "consecutive_failure_count = 0  # Global counter (don't edit this manually)\n",
    "thread_lock = threading.Lock() # Lock to update the counter safely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26044da",
   "metadata": {},
   "source": [
    "### **Setup and Database Connections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26088044-6cf4-43a7-a01f-f5a4a285d3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database at db\\openAlex.db\n"
     ]
    }
   ],
   "source": [
    "DB_DIR = 'db'\n",
    "if not os.path.exists(DB_DIR):\n",
    "    os.makedirs(DB_DIR)\n",
    "DB_FILENAME = 'openAlex.db'\n",
    "DB_PATH = os.path.join(DB_DIR, DB_FILENAME)\n",
    "conn = duckdb.connect(DB_PATH)\n",
    "print(f\"Connected to database at {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c3dd59-eeba-4366-ad4a-a1bb2beececd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema initialized with MAG ID.\n"
     ]
    }
   ],
   "source": [
    "def init_schema(conn):\n",
    "    # conn.execute(\"DROP TABLE IF EXISTS works\") \n",
    "    \n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS works (\n",
    "            id VARCHAR PRIMARY KEY,\n",
    "            title VARCHAR,\n",
    "            doi VARCHAR,\n",
    "            publication_date DATE,\n",
    "            primary_topic VARCHAR,\n",
    "            version VARCHAR,\n",
    "            fwci DOUBLE,\n",
    "            citation_count INTEGER,\n",
    "            mag_id BIGINT             -- New Column for MAG ID\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS topics (\n",
    "            topic_id VARCHAR PRIMARY KEY,\n",
    "            topic_name VARCHAR UNIQUE\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS work_topics (\n",
    "            work_id VARCHAR,\n",
    "            topic_id VARCHAR,\n",
    "            score DOUBLE,\n",
    "            PRIMARY KEY (work_id, topic_id),\n",
    "            FOREIGN KEY (work_id) REFERENCES works(id),\n",
    "            FOREIGN KEY (topic_id) REFERENCES topics(topic_id)\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS harvest_intervals (\n",
    "            topic_id VARCHAR,\n",
    "            start_date DATE,\n",
    "            end_date DATE,\n",
    "            last_token VARCHAR,\n",
    "            fetched_count INTEGER DEFAULT 0,\n",
    "            status VARCHAR,\n",
    "            last_updated TIMESTAMP,\n",
    "            PRIMARY KEY (topic_id, start_date, end_date)\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"Schema initialized with MAG ID.\")\n",
    "\n",
    "init_schema(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e148ce",
   "metadata": {},
   "source": [
    "### **Request Wraper**\n",
    "Makes a GET request. If it fails due to network issues or server errors (500, 502, 503, 429), it waits and retries indefinitely.Raises exception only for client errors (400, 401, 404, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_with_retry(url, params=None):\n",
    "    \"\"\"\n",
    "    Makes a GET request. If it fails due to network issues or \n",
    "    server errors (500, 502, 503, 429), it waits and retries indefinitely.\n",
    "    Raises exception only for client errors (400, 401, 404, etc).\n",
    "    \"\"\"\n",
    "    delay = 2  # Start with 2 seconds delay\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            # If successful, return response\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            \n",
    "            # If server error or rate limit, print and retry\n",
    "            if r.status_code in [429, 500, 502, 503, 504]:\n",
    "                print(f\"Status {r.status_code} received. Retrying in {delay}s...\")\n",
    "                time.sleep(delay)\n",
    "                delay = min(delay * 2, 60) # Exponential backoff up to 60s\n",
    "                continue\n",
    "            \n",
    "            # If it's a client error (e.g. 404), raise it immediately\n",
    "            r.raise_for_status()\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Network error: {e}. Retrying in {delay}s...\")\n",
    "            time.sleep(delay)\n",
    "            delay = min(delay * 2, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3443f",
   "metadata": {},
   "source": [
    "### **Creating Manageable Intervals for each Topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba10a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_name(topic_id):\n",
    "    # Uses the retry wrapper\n",
    "    r = request_with_retry(f\"{TOPIC_BASE}/{topic_id}\")\n",
    "    data = r.json()\n",
    "    return data.get(\"display_name\") or data.get(\"name\")\n",
    "\n",
    "# to get count of works for a topic in a date range\n",
    "def get_count(topic_id, start_date, end_date):\n",
    "    params = {\n",
    "        \"filter\": f\"primary_topic.id:{topic_id},from_publication_date:{start_date},to_publication_date:{end_date},type:article\",\n",
    "        \"per-page\": 1\n",
    "    }\n",
    "    # Uses the retry wrapper\n",
    "    r = request_with_retry(OPENALEX_BASE, params=params)\n",
    "    return int(r.json()[\"meta\"][\"count\"])\n",
    "\n",
    "# to split date ranges to paper counts in  intervals below limit so if the  inverval fail we have to retry for small number of papers\n",
    "def split_range_safely(topic_id, start_date, end_date, limit=COUNT_LIMIT):\n",
    "    \"\"\"\n",
    "    Recursively splits date ranges until the count is below the limit.\n",
    "    \"\"\"\n",
    "    cnt = get_count(topic_id, start_date, end_date)\n",
    "    print(f\"   Checking range {start_date} -> {end_date}: count = {cnt}\")\n",
    "    \n",
    "    if cnt <= limit:\n",
    "        return [(start_date, end_date)]\n",
    "    \n",
    "    days = (end_date - start_date).days\n",
    "    if days <= 1:\n",
    "        # If we are down to 1 day and still over limit, we must accept it (or handle pagination)\n",
    "        return [(start_date, end_date)]\n",
    "        \n",
    "    mid = start_date + timedelta(days=days // 2)\n",
    "    left = split_range_safely(topic_id, start_date, mid, limit)\n",
    "    right = split_range_safely(topic_id, mid + timedelta(days=1), end_date, limit)\n",
    "    return left + right\n",
    "\n",
    "# to register intervals of each topic in the database\n",
    "def register_intervals(conn, topic_id, intervals):\n",
    "    for start, end in intervals:\n",
    "        conn.execute(\"\"\"\n",
    "            INSERT OR IGNORE INTO harvest_intervals\n",
    "            (topic_id, start_date, end_date, status, fetched_count, last_updated)\n",
    "            VALUES (?, ?, ?, 'pending', 0, NOW())\n",
    "        \"\"\", [topic_id, start, end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e366778b",
   "metadata": {},
   "source": [
    "### **Inserting Data into Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8d759-c9a8-4af3-b4ea-e9c9acc69110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def insert_works_bulk(conn, works_list):\n",
    "    \"\"\"\n",
    "    Inserts a large list of works efficiently.\n",
    "    \"\"\"\n",
    "    if not works_list:\n",
    "        return\n",
    "    \n",
    "\n",
    "    for w in works_list:\n",
    "        if isinstance(w.get('publication_date'), (date, datetime)):\n",
    "            w['publication_date'] = str(w['publication_date'])\n",
    "\n",
    "    df = pd.DataFrame(works_list)\n",
    "    \n",
    "    with db_lock:\n",
    "        conn.register(\"batch_works\", df)\n",
    "        conn.execute(\"INSERT OR IGNORE INTO works SELECT * FROM batch_works\")\n",
    "        conn.unregister(\"batch_works\")\n",
    "\n",
    "def insert_topics_bulk(conn, results_list):\n",
    "    \"\"\"\n",
    "    Extracts all topics and relations from the results list and bulk inserts them.\n",
    "    Drastically faster than row-by-row checks.\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        return\n",
    "    # 1. Prepare Data in Memory (No Lock needed yet)\n",
    "    unique_topics = {}\n",
    "    work_topic_relations = []\n",
    "\n",
    "    for w in results_list:\n",
    "        work_id = w.get(\"id\").split('/')[-1]\n",
    "        all_topics = w.get(\"topics\") or []\n",
    "        \n",
    "        for t in all_topics:\n",
    "            topic_id = t.get(\"id\").split('/')[-1]\n",
    "            topic_name = t.get(\"display_name\") or t.get(\"name\")\n",
    "            score = t.get(\"score\") or 0.0\n",
    "            \n",
    "            if topic_id and topic_name:\n",
    "                unique_topics[topic_id] = topic_name\n",
    "                work_topic_relations.append({\n",
    "                    \"work_id\": work_id,\n",
    "                    \"topic_id\": topic_id,\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "\n",
    "    if not unique_topics:\n",
    "        return\n",
    "\n",
    "    topics_df = pd.DataFrame(list(unique_topics.items()), columns=[\"topic_id\", \"topic_name\"])\n",
    "    relations_df = pd.DataFrame(work_topic_relations)\n",
    "\n",
    "    # 2. Bulk Write (Lock needed now)\n",
    "    with db_lock:\n",
    "        conn.register(\"batch_topics\", topics_df)\n",
    "        conn.execute(\"INSERT OR IGNORE INTO topics SELECT * FROM batch_topics\")\n",
    "        conn.unregister(\"batch_topics\")\n",
    "\n",
    "        if not relations_df.empty:\n",
    "            conn.register(\"batch_relations\", relations_df)\n",
    "            conn.execute(\"INSERT OR IGNORE INTO work_topics SELECT * FROM batch_relations\")\n",
    "            conn.unregister(\"batch_relations\")\n",
    "\n",
    "def mark_interval_complete(conn, topic_id, start_date, end_date, total_count):\n",
    "    \"\"\"\n",
    "    Updates the interval status to 'done' in one go.\n",
    "    \"\"\"\n",
    "    with db_lock:\n",
    "        conn.execute(\"\"\"\n",
    "            UPDATE harvest_intervals\n",
    "            SET status='done', fetched_count=?, last_updated=now()\n",
    "            WHERE topic_id=? AND start_date=? AND end_date=?\n",
    "        \"\"\", [total_count, topic_id, start_date, end_date])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d59eb3",
   "metadata": {},
   "source": [
    "### **Data Fetching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db52e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fetch a single page of works for a topic in a date range\n",
    "def fetch_page(topic_id, start_date, end_date, cursor=\"*\"):\n",
    "    params = {\n",
    "        \"filter\": f\"primary_topic.id:{topic_id},from_publication_date:{start_date},to_publication_date:{end_date},type:article\",\n",
    "        \"per-page\": PAGE_SIZE,\n",
    "        \"cursor\": cursor\n",
    "    }\n",
    "    # Uses the retry wrapper\n",
    "    r = request_with_retry(OPENALEX_BASE, params=params)\n",
    "    data = r.json()\n",
    "    results = data.get(\"results\", [])\n",
    "    next_cursor = data[\"meta\"].get(\"next_cursor\")\n",
    "    return results, next_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f426cc18-e9f0-400c-b82d-ac63d5aa59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_interval_atomic(conn, topic_id, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches the ENTIRE interval into memory first, then dumps to DB.\n",
    "    Eliminates database locking during the download phase.\n",
    "    \"\"\"\n",
    "    all_works = []\n",
    "    all_raw_results = [] \n",
    "    \n",
    "    cursor = \"*\"\n",
    "    finished = False\n",
    "    \n",
    "\n",
    "    while not finished:\n",
    "        results, next_cursor = fetch_page(topic_id, start_date, end_date, cursor)\n",
    "        all_raw_results.extend(results)\n",
    "\n",
    "        for w in results:\n",
    "            primary_topic = w.get(\"primary_topic\", {}).get(\"display_name\") if w.get(\"primary_topic\") else None\n",
    "            mag_id = w.get(\"ids\", {}).get(\"mag\")\n",
    "            \n",
    "            all_works.append({\n",
    "                \"id\": w.get(\"id\").split('/')[-1],\n",
    "                \"title\": w.get(\"title\"),\n",
    "                \"doi\": w.get(\"doi\"),\n",
    "                \"publication_date\": w.get(\"publication_date\"),\n",
    "                \"primary_topic\": primary_topic,\n",
    "                \"version\": w.get(\"primary_location\", {}).get(\"version\", {}),\n",
    "                \"fwci\": w.get(\"fwci\"),\n",
    "                \"citation_count\": w.get(\"cited_by_count\"),\n",
    "                \"mag_id\": mag_id\n",
    "            })\n",
    "\n",
    "        if next_cursor is None:\n",
    "            finished = True\n",
    "        else:\n",
    "            cursor = next_cursor\n",
    "\n",
    "\n",
    "    if all_works:\n",
    "        insert_works_bulk(conn, all_works)\n",
    "        insert_topics_bulk(conn, all_raw_results)\n",
    "        mark_interval_complete(conn, topic_id, start_date, end_date, len(all_works))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa13d45-af3a-470e-a835-0612d380ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_interval_wrapper(args):\n",
    "    \"\"\"\n",
    "    Wrapper that retries the interval if it fails, and triggers a stop \n",
    "    if too many failures happen globally.\n",
    "    \"\"\"\n",
    "    conn, topic_id, start_date, end_date = args\n",
    "    global consecutive_failure_count\n",
    "\n",
    "    # 1. Check Circuit Breaker\n",
    "    if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "        return (start_date, end_date, False)\n",
    "\n",
    "    # 2. Retry Loop for this specific interval\n",
    "    for attempt in range(1, MAX_RETRIES_PER_INTERVAL + 1):\n",
    "        try:\n",
    "            fetch_interval_atomic(conn, topic_id, start_date, end_date)\n",
    "            with thread_lock:\n",
    "                consecutive_failure_count = 0 # Reset global failure count on success\n",
    "            return (start_date, end_date, True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in interval {start_date}->{end_date} (Attempt {attempt}/{MAX_RETRIES_PER_INTERVAL}): {e}\")\n",
    "            time.sleep(2 * attempt) # Wait a bit before retrying (2s, 4s, 6s)\n",
    "    \n",
    "    # 3. If we failed 3 times, mark as a global failure\n",
    "    print(f\"GIVING UP on interval {start_date}->{end_date} after {MAX_RETRIES_PER_INTERVAL} attempts.\")\n",
    "    with thread_lock:\n",
    "        consecutive_failure_count += 1\n",
    "        if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "            print(f\"\\nCRITICAL: {consecutive_failure_count} consecutive failures detected. Stopping script to prevent skipping all intervals.\\n\")\n",
    "            \n",
    "    return (start_date, end_date, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_topics(conn, topic_ids, start_date, end_date):\n",
    "    global consecutive_failure_count\n",
    "    \n",
    "    for topic_id in topic_ids:\n",
    "        if not str(topic_id).startswith('t'):\n",
    "             topic_id_fmt = 't' + str(topic_id)\n",
    "        else:\n",
    "             topic_id_fmt = str(topic_id)\n",
    "             \n",
    "        try:\n",
    "            topic_name = get_topic_name(topic_id_fmt)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch name for {topic_id_fmt}, skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Harvesting topic {topic_id_fmt} ({topic_name}) ===\")\n",
    "\n",
    "        with db_lock:\n",
    "            existing_intervals = conn.execute(\"\"\"\n",
    "                SELECT start_date, end_date \n",
    "                FROM harvest_intervals \n",
    "                WHERE topic_id = ? \n",
    "                ORDER BY start_date\n",
    "            \"\"\", [topic_id_fmt]).fetchall()\n",
    "\n",
    "        if existing_intervals:\n",
    "            print(f\"Found {len(existing_intervals)} existing intervals.\")\n",
    "            intervals = existing_intervals\n",
    "        else:\n",
    "            print(f\"Calculating splits via API...\")\n",
    "            intervals = split_range_safely(topic_id_fmt, start_date, end_date)\n",
    "            with db_lock:\n",
    "                register_intervals(conn, topic_id_fmt, intervals)\n",
    "\n",
    "        # Identify Pending Work\n",
    "        tasks = []\n",
    "        for safe_start, safe_end in intervals:\n",
    "    \n",
    "            if hasattr(safe_start, 'date'): safe_start = safe_start.date()\n",
    "            if hasattr(safe_end, 'date'): safe_end = safe_end.date()\n",
    "            if isinstance(safe_start, str): safe_start = datetime.strptime(safe_start, '%Y-%m-%d').date()\n",
    "            if isinstance(safe_end, str): safe_end = datetime.strptime(safe_end, '%Y-%m-%d').date()\n",
    "\n",
    "            with db_lock:\n",
    "                row = conn.execute(\"\"\"\n",
    "                    SELECT status FROM harvest_intervals\n",
    "                    WHERE topic_id=? AND start_date=? AND end_date=?\n",
    "                \"\"\", [topic_id_fmt, safe_start, safe_end]).fetchone()\n",
    "            \n",
    "            if row and row[0] == \"done\":\n",
    "                continue\n",
    "                \n",
    "            tasks.append((conn, topic_id_fmt, safe_start, safe_end))\n",
    "\n",
    "        print(f\"Starting {len(tasks)} pending intervals with {MAX_WORKERS} threads...\")\n",
    "\n",
    "        # --- EXECUTOR LOOP WITH STOP CHECK ---\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            futures = []\n",
    "            for t in tasks:\n",
    "                if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "                    print(\"Circuit breaker active. Cancelling remaining tasks.\")\n",
    "                    break\n",
    "                futures.append(executor.submit(process_interval_wrapper, t))\n",
    "\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "                start, end, success = future.result()\n",
    "                if not success and consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "                    print(\"Stopping execution loop due to critical failures.\")\n",
    "                    executor.shutdown(wait=False)\n",
    "                    return \n",
    "\n",
    "        if consecutive_failure_count >= MAX_CONSECUTIVE_FAILURES:\n",
    "            print(\"Execution halted.\")\n",
    "            break\n",
    "\n",
    "        print(f\"=== Completed harvesting topic {topic_id_fmt} ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a3293",
   "metadata": {},
   "source": [
    "### **Start of Fetching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dab69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Harvesting topic t10466 (Meteorological Phenomena and Simulations) ===\n",
      "Found 57 existing intervals.\n",
      "Starting 35 pending intervals with 5 threads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:09<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_ids = pd.read_excel('Physics.xlsx')['topic_id']\n",
    "start_date = date(2000, 1, 1)\n",
    "end_date = date(2026, 1, 1)\n",
    "\n",
    "harvest_topics(conn, topic_ids, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7439c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openalex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
