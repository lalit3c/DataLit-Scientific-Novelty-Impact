{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1b87e4",
   "metadata": {},
   "source": [
    "# ANN-based Novelty Computation Pipeline\n",
    "\n",
    "This notebook constructs a temporally consistent **approximate nearest neighbor (ANN)**\n",
    "pipeline to compute **novelty metrics for scientific papers** based on embedding similarity\n",
    "to prior work.\n",
    "\n",
    "It performs the following steps:\n",
    "1. Downloads paper metadata and embedding files from Hugging Face Hub\n",
    "2. Merges multiple metadata databases into a unified view\n",
    "3. Joins metadata with embeddings to create an ANN-ready table\n",
    "4. Sorts papers by publication date for forward-in-time indexing\n",
    "5. Builds an incremental FAISS HNSW index over paper embeddings\n",
    "6. Computes multiple novelty metrics using nearest-neighbor distances from papers indexed so far\n",
    "7. Stores and uploads the resulting metrics database to Hugging Face Hub\n",
    "\n",
    "**Input Databases:**\n",
    "- `S2_papers_cleaned.db` – Cleaned Semantic Scholar metadata\n",
    "- `S2_papers_cleaned_additional_papers.db` – Additional cleaned papers\n",
    "- `embeddings/embeddings_*.db` – Sharded embedding databases\n",
    "\n",
    "**Output:**\n",
    "- `ann_pipeline.db` – ANN-ready papers with embeddings sorted by publication date\n",
    "- `metrics.db` – Per-paper novelty metrics computed via ANN search\n",
    "- Hugging Face Dataset:  \n",
    "  [`lalit3c/S2_CS_PHY_PYSCH_papers`](https://huggingface.co/datasets/lalit3c/S2_CS_PHY_PYSCH_papers)\n",
    "\n",
    "FAISS documentation:\n",
    "- https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexHNSW.html\n",
    "- https://github.com/facebookresearch/faiss/wiki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcaa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import faiss\n",
    "from datetime import date\n",
    "import glob\n",
    "from huggingface_hub import snapshot_download, upload_file\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904b7c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanchanpoudel/uni/DataLit/.venv/lib/python3.14/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dbf6a0bd6440a5ad17ef1aa5acd3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 32 files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/kanchanpoudel/uni/DataLit/DataLit-Scientific-Novelty-Impact/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "snapshot_download(\n",
    "    repo_id=\"lalit3c/S2_CS_PHY_PYSCH_papers\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=\"data\",\n",
    "    local_dir_use_symlinks=False,\n",
    "    allow_patterns=[\n",
    "        \"S2_papers_cleaned.db\",\n",
    "        \"S2_papers_cleaned_additional_papers.db\",\n",
    "        \"embeddings/**\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36e4a3",
   "metadata": {},
   "source": [
    "Merge Metadata db created from semantic scholar with embedding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb831c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embedding file 1/30: ./data/embeddings/embeddings_1.db\n",
      "Processing embedding file 2/30: ./data/embeddings/embeddings_10.db\n",
      "Processing embedding file 3/30: ./data/embeddings/embeddings_11.db\n",
      "Processing embedding file 4/30: ./data/embeddings/embeddings_12.db\n",
      "Processing embedding file 5/30: ./data/embeddings/embeddings_13.db\n",
      "Processing embedding file 6/30: ./data/embeddings/embeddings_14.db\n",
      "Processing embedding file 7/30: ./data/embeddings/embeddings_15.db\n",
      "Processing embedding file 8/30: ./data/embeddings/embeddings_16.db\n",
      "Processing embedding file 9/30: ./data/embeddings/embeddings_17.db\n",
      "Processing embedding file 10/30: ./data/embeddings/embeddings_18.db\n",
      "Processing embedding file 11/30: ./data/embeddings/embeddings_19.db\n",
      "Processing embedding file 12/30: ./data/embeddings/embeddings_2.db\n",
      "Processing embedding file 13/30: ./data/embeddings/embeddings_20.db\n",
      "Processing embedding file 14/30: ./data/embeddings/embeddings_21.db\n",
      "Processing embedding file 15/30: ./data/embeddings/embeddings_22.db\n",
      "Processing embedding file 16/30: ./data/embeddings/embeddings_23.db\n",
      "Processing embedding file 17/30: ./data/embeddings/embeddings_24.db\n",
      "Processing embedding file 18/30: ./data/embeddings/embeddings_25.db\n",
      "Processing embedding file 19/30: ./data/embeddings/embeddings_26.db\n",
      "Processing embedding file 20/30: ./data/embeddings/embeddings_27.db\n",
      "Processing embedding file 21/30: ./data/embeddings/embeddings_28.db\n",
      "Processing embedding file 22/30: ./data/embeddings/embeddings_29.db\n",
      "Processing embedding file 23/30: ./data/embeddings/embeddings_3.db\n",
      "Processing embedding file 24/30: ./data/embeddings/embeddings_30.db\n",
      "Processing embedding file 25/30: ./data/embeddings/embeddings_4.db\n",
      "Processing embedding file 26/30: ./data/embeddings/embeddings_5.db\n",
      "Processing embedding file 27/30: ./data/embeddings/embeddings_6.db\n",
      "Processing embedding file 28/30: ./data/embeddings/embeddings_7.db\n",
      "Processing embedding file 29/30: ./data/embeddings/embeddings_8.db\n",
      "Processing embedding file 30/30: ./data/embeddings/embeddings_9.db\n",
      "All embedding files merged.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d74f77199d4f82a976d3a8b56116a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ANN-ready DB created: ./data/ann_pipeline.db with table 'papers_ann_sorted'\n"
     ]
    }
   ],
   "source": [
    "# CONFIGS\n",
    "META_DBS = [\n",
    "    \"./data/S2_papers_cleaned.db\",\n",
    "    \"./data/S2_papers_cleaned_additional_papers.db\"\n",
    "]\n",
    "\n",
    "META_TABLE = \"papers_with_abstracts\"\n",
    "\n",
    "EMBED_FILES_PATTERN = \"./data/embeddings/embeddings_*.db\"\n",
    "EMBED_TABLE = \"embeddings\"\n",
    "\n",
    "OUTPUT_DB = \"./data/ann_pipeline.db\"\n",
    "ANN_READY_TABLE = \"papers_ann_ready\"\n",
    "ANN_SORTED_TABLE = \"papers_ann_sorted\"\n",
    "\n",
    "# Columns to merge from metadata\n",
    "MERGE_COLUMNS = [\"corpusid\", \"title\", \"publication_date\", \"citation_count\"]\n",
    "MERGE_COLS_STR = \", \".join(MERGE_COLUMNS)\n",
    "\n",
    "# Connect to output DuckDB\n",
    "con = duckdb.connect(OUTPUT_DB)\n",
    "\n",
    "# Attach metadata DBs\n",
    "for idx, db_path in enumerate(META_DBS, start=1):\n",
    "    con.execute(f\"ATTACH DATABASE '{db_path}' AS meta{idx};\")\n",
    "\n",
    "# Create a unified metadata VIEW\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE VIEW meta_all AS\n",
    "SELECT {MERGE_COLS_STR} FROM meta1.{META_TABLE}\n",
    "UNION ALL\n",
    "SELECT {MERGE_COLS_STR} FROM meta2.{META_TABLE};\n",
    "\"\"\")\n",
    "\n",
    "# Create empty ANN-ready table\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {ANN_READY_TABLE} (\n",
    "    corpus_id BIGINT,\n",
    "    pub_date DATE,\n",
    "    embedding FLOAT[]\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Process each embeddings file\n",
    "embedding_files = sorted(glob.glob(EMBED_FILES_PATTERN))\n",
    "\n",
    "for idx, emb_file in enumerate(embedding_files, 1):\n",
    "    print(f\"Processing embedding file {idx}/{len(embedding_files)}: {emb_file}\")\n",
    "\n",
    "    con.execute(f\"ATTACH DATABASE '{emb_file}' AS emb;\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        INSERT INTO {ANN_READY_TABLE}\n",
    "        SELECT\n",
    "            m.corpusid,\n",
    "            m.publication_date,\n",
    "            e.embedding\n",
    "        FROM meta_all m\n",
    "        JOIN emb.{EMBED_TABLE} e\n",
    "        USING (corpusid);\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(\"DETACH DATABASE emb;\")\n",
    "\n",
    "print(\"All embedding files merged.\")\n",
    "\n",
    "# Sort final table\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE {ANN_SORTED_TABLE} AS\n",
    "SELECT *\n",
    "FROM {ANN_READY_TABLE}\n",
    "ORDER BY pub_date, corpus_id;\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(f\"Final ANN-ready DB created: {OUTPUT_DB} with table '{ANN_SORTED_TABLE}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ecc9813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e01147a62e4b98ac0745950e788426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   corpus_id   pub_date                                          embedding\n",
      "0      60282 2000-01-01  [-0.015204819, 0.7857948, -1.3321525, 0.037485...\n",
      "1     755730 2000-01-01  [-0.32926, 0.587181, -0.51643664, 0.7690042, -...\n",
      "2     757185 2000-01-01  [0.35842383, -0.14089696, 0.1665931, -0.651995...\n",
      "3     858325 2000-01-01  [1.0989523, -0.2769418, -0.35953367, 0.3855169...\n",
      "4    1119702 2000-01-01  [0.56514174, 0.09447052, 0.1302544, 0.22057152...\n",
      "Shape: (2564613, 3)\n",
      "Columns: Index(['corpus_id', 'pub_date', 'embedding'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Sanity checking\n",
    "\n",
    "con = duckdb.connect(\"./data/ann_pipeline.db\", read_only=True)\n",
    "\n",
    "# Load the table into a pandas DataFrame\n",
    "df = con.execute(\"SELECT * FROM papers_ann_sorted\").fetch_df()\n",
    "\n",
    "# View the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Checking the shape and column names\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "\n",
    "# Path to the DuckDB database containing the paper metadata and embeddings\n",
    "DUCKDB_PATH = \"./data/ann_pipeline.db\"\n",
    "TABLE_NAME = \"papers_ann_sorted\"\n",
    "\n",
    "# Dimensionality of the embedding vector\n",
    "EMBED_DIM = 768\n",
    "\n",
    "# Number of nearest neighbors to retrieve per query\n",
    "K = 100\n",
    "\n",
    "# HNSW parameters:\n",
    "INDEX_M = 32 # Maximum number of connections per node (higher = better recall, more memory)\n",
    "\n",
    "EF_CONSTRUCTION = 200 # Controls accuracy vs. indexing time during index build\n",
    "\n",
    "EF_SEARCH = 50 # Controls recall vs. query-time latency during ANN search\n",
    "\n",
    "# Beginning publication date considered in experiments\n",
    "START_DATE = date(2000, 1, 1)\n",
    "\n",
    "# Last publication date considered in experiments\n",
    "END_DATE   = date(2026, 1, 1)\n",
    "\n",
    "# Read-only connection for the source paper data\n",
    "source_con = duckdb.connect(DUCKDB_PATH, read_only=True)\n",
    "\n",
    "# Writable connection for storing computed metrics\n",
    "neighbors_con = duckdb.connect(\"./data/metrics.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57bb62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('meta_all',), ('papers_ann_ready',), ('papers_ann_sorted',)]\n"
     ]
    }
   ],
   "source": [
    "tables = source_con.execute(\"SHOW TABLES\").fetchall()\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45848ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNSW-based FAISS index with ID mapping for efficient approximate nearest-neighbor search on embeddings\n",
    "base_index = faiss.IndexHNSWFlat(EMBED_DIM, INDEX_M)\n",
    "base_index.hnsw.efConstruction = EF_CONSTRUCTION\n",
    "base_index.hnsw.efSearch = EF_SEARCH\n",
    "index = faiss.IndexIDMap2(base_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37fd027d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_duckdb.DuckDBPyConnection at 0x15b8432f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table to store per-paper novelty metrics computed from nearest-neighbor distances and citation weightings\n",
    "neighbors_con.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS paper_neighbors_metrics (\n",
    "    corpus_id BIGINT,\n",
    "    novelty_mean FLOAT,\n",
    "    novelty_max FLOAT,\n",
    "    novelty_min FLOAT,\n",
    "    novelty_median FLOAT,\n",
    "    novelty_harmonic FLOAT\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b733de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for iteration, Returns the first day of the next calendar month for a given date\n",
    "\n",
    "def add_month(d):\n",
    "    if d.month == 12:\n",
    "        return date(d.year + 1, 1, 1)\n",
    "    else:\n",
    "        return date(d.year, d.month + 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fec97",
   "metadata": {},
   "source": [
    "**Monthly batch processing strategy**\n",
    "\n",
    "- Iterates over papers in **monthly batches**, yielding corpus IDs and their corresponding embeddings.\n",
    "- All papers published within the **same calendar month** are processed together.\n",
    "- Novelty metrics for a batch are computed **only against papers from earlier months** (past batches), not against papers within the same batch for computational efficiency.\n",
    "- The approach relies on the assumption that papers published within the **same month of the same year** are unlikely to substantially influence one another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a577e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def monthly_batch_iterator(con, table, start_date, end_date):\n",
    "    current = start_date\n",
    "    while current < end_date:\n",
    "        next_month = add_month(current)\n",
    "        df = con.execute(f\"\"\"\n",
    "            SELECT corpus_id, embedding\n",
    "            FROM {table}\n",
    "            WHERE pub_date >= ?\n",
    "              AND pub_date < ?\n",
    "            ORDER BY pub_date\n",
    "        \"\"\", [current, next_month]).fetch_df()\n",
    "        if not df.empty:\n",
    "            yield (\n",
    "                df[\"corpus_id\"].to_numpy(),\n",
    "                np.vstack(df[\"embedding\"].to_numpy())\n",
    "            )\n",
    "        current = next_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab30da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1,211 papers\n",
      "Indexed 2,397 papers\n",
      "Indexed 3,744 papers\n",
      "Indexed 4,925 papers\n",
      "Indexed 6,228 papers\n",
      "Indexed 7,591 papers\n",
      "Indexed 8,954 papers\n",
      "Indexed 10,153 papers\n",
      "Indexed 11,517 papers\n",
      "Indexed 13,638 papers\n",
      "Indexed 15,028 papers\n",
      "Indexed 16,583 papers\n",
      "Indexed 17,898 papers\n",
      "Indexed 19,160 papers\n",
      "Indexed 20,709 papers\n",
      "Indexed 22,043 papers\n",
      "Indexed 23,577 papers\n",
      "Indexed 25,160 papers\n",
      "Indexed 26,684 papers\n",
      "Indexed 28,016 papers\n",
      "Indexed 30,037 papers\n",
      "Indexed 31,626 papers\n",
      "Indexed 33,168 papers\n",
      "Indexed 34,740 papers\n",
      "Indexed 36,290 papers\n",
      "Indexed 37,560 papers\n",
      "Indexed 39,088 papers\n",
      "Indexed 40,602 papers\n",
      "Indexed 42,145 papers\n",
      "Indexed 43,692 papers\n",
      "Indexed 45,414 papers\n",
      "Indexed 47,072 papers\n",
      "Indexed 49,214 papers\n",
      "Indexed 50,852 papers\n",
      "Indexed 52,504 papers\n",
      "Indexed 54,309 papers\n",
      "Indexed 55,843 papers\n",
      "Indexed 57,360 papers\n",
      "Indexed 59,000 papers\n",
      "Indexed 60,649 papers\n",
      "Indexed 62,380 papers\n",
      "Indexed 64,202 papers\n",
      "Indexed 66,114 papers\n",
      "Indexed 67,539 papers\n",
      "Indexed 69,998 papers\n",
      "Indexed 71,993 papers\n",
      "Indexed 73,634 papers\n",
      "Indexed 75,667 papers\n",
      "Indexed 77,336 papers\n",
      "Indexed 78,979 papers\n",
      "Indexed 81,083 papers\n",
      "Indexed 82,889 papers\n",
      "Indexed 84,744 papers\n",
      "Indexed 86,811 papers\n",
      "Indexed 88,843 papers\n",
      "Indexed 90,611 papers\n",
      "Indexed 92,629 papers\n",
      "Indexed 94,814 papers\n",
      "Indexed 96,875 papers\n",
      "Indexed 99,109 papers\n",
      "Indexed 101,038 papers\n",
      "Indexed 102,719 papers\n",
      "Indexed 104,994 papers\n",
      "Indexed 107,201 papers\n",
      "Indexed 109,213 papers\n",
      "Indexed 111,676 papers\n",
      "Indexed 113,797 papers\n",
      "Indexed 115,798 papers\n",
      "Indexed 118,350 papers\n",
      "Indexed 120,748 papers\n",
      "Indexed 123,016 papers\n",
      "Indexed 125,462 papers\n",
      "Indexed 127,616 papers\n",
      "Indexed 129,601 papers\n",
      "Indexed 131,925 papers\n",
      "Indexed 134,095 papers\n",
      "Indexed 136,441 papers\n",
      "Indexed 139,049 papers\n",
      "Indexed 141,581 papers\n",
      "Indexed 143,933 papers\n",
      "Indexed 146,422 papers\n",
      "Indexed 149,291 papers\n",
      "Indexed 151,814 papers\n",
      "Indexed 154,538 papers\n",
      "Indexed 157,183 papers\n",
      "Indexed 159,328 papers\n",
      "Indexed 161,926 papers\n",
      "Indexed 164,415 papers\n",
      "Indexed 166,987 papers\n",
      "Indexed 169,998 papers\n",
      "Indexed 172,911 papers\n",
      "Indexed 175,426 papers\n",
      "Indexed 178,226 papers\n",
      "Indexed 181,460 papers\n",
      "Indexed 184,137 papers\n",
      "Indexed 187,079 papers\n",
      "Indexed 189,856 papers\n",
      "Indexed 192,333 papers\n",
      "Indexed 195,213 papers\n",
      "Indexed 198,170 papers\n",
      "Indexed 201,087 papers\n",
      "Indexed 204,254 papers\n",
      "Indexed 207,363 papers\n",
      "Indexed 209,875 papers\n",
      "Indexed 213,003 papers\n",
      "Indexed 216,332 papers\n",
      "Indexed 219,008 papers\n",
      "Indexed 222,390 papers\n",
      "Indexed 225,154 papers\n",
      "Indexed 227,941 papers\n",
      "Indexed 231,277 papers\n",
      "Indexed 234,191 papers\n",
      "Indexed 237,197 papers\n",
      "Indexed 240,842 papers\n",
      "Indexed 244,400 papers\n",
      "Indexed 247,532 papers\n",
      "Indexed 251,113 papers\n",
      "Indexed 254,539 papers\n",
      "Indexed 257,707 papers\n",
      "Indexed 261,357 papers\n",
      "Indexed 264,403 papers\n",
      "Indexed 267,426 papers\n",
      "Indexed 271,533 papers\n",
      "Indexed 275,106 papers\n",
      "Indexed 278,550 papers\n",
      "Indexed 282,616 papers\n",
      "Indexed 286,580 papers\n",
      "Indexed 290,004 papers\n",
      "Indexed 293,799 papers\n",
      "Indexed 297,616 papers\n",
      "Indexed 301,359 papers\n",
      "Indexed 305,648 papers\n",
      "Indexed 309,207 papers\n",
      "Indexed 312,561 papers\n",
      "Indexed 316,704 papers\n",
      "Indexed 320,244 papers\n",
      "Indexed 324,314 papers\n",
      "Indexed 328,646 papers\n",
      "Indexed 333,200 papers\n",
      "Indexed 337,357 papers\n",
      "Indexed 341,762 papers\n",
      "Indexed 346,204 papers\n",
      "Indexed 350,498 papers\n",
      "Indexed 355,037 papers\n",
      "Indexed 359,075 papers\n",
      "Indexed 363,085 papers\n",
      "Indexed 367,609 papers\n",
      "Indexed 371,782 papers\n",
      "Indexed 376,482 papers\n",
      "Indexed 381,436 papers\n",
      "Indexed 386,512 papers\n",
      "Indexed 391,116 papers\n",
      "Indexed 395,845 papers\n",
      "Indexed 400,994 papers\n",
      "Indexed 405,708 papers\n",
      "Indexed 410,759 papers\n",
      "Indexed 415,307 papers\n",
      "Indexed 419,847 papers\n",
      "Indexed 425,342 papers\n",
      "Indexed 430,436 papers\n",
      "Indexed 435,552 papers\n",
      "Indexed 441,264 papers\n",
      "Indexed 447,295 papers\n",
      "Indexed 452,429 papers\n",
      "Indexed 457,850 papers\n",
      "Indexed 463,709 papers\n",
      "Indexed 469,342 papers\n",
      "Indexed 475,621 papers\n",
      "Indexed 480,955 papers\n",
      "Indexed 485,779 papers\n",
      "Indexed 492,637 papers\n",
      "Indexed 499,103 papers\n",
      "Indexed 504,981 papers\n",
      "Indexed 511,890 papers\n",
      "Indexed 518,235 papers\n",
      "Indexed 523,777 papers\n",
      "Indexed 529,809 papers\n",
      "Indexed 535,747 papers\n",
      "Indexed 541,550 papers\n",
      "Indexed 548,284 papers\n",
      "Indexed 554,099 papers\n",
      "Indexed 559,529 papers\n",
      "Indexed 566,314 papers\n",
      "Indexed 573,012 papers\n",
      "Indexed 579,346 papers\n",
      "Indexed 586,730 papers\n",
      "Indexed 593,722 papers\n",
      "Indexed 599,566 papers\n",
      "Indexed 608,084 papers\n",
      "Indexed 615,031 papers\n",
      "Indexed 621,884 papers\n",
      "Indexed 629,742 papers\n",
      "Indexed 635,954 papers\n",
      "Indexed 642,579 papers\n",
      "Indexed 650,504 papers\n",
      "Indexed 657,326 papers\n",
      "Indexed 664,577 papers\n",
      "Indexed 672,899 papers\n",
      "Indexed 680,311 papers\n",
      "Indexed 687,599 papers\n",
      "Indexed 695,169 papers\n",
      "Indexed 702,534 papers\n",
      "Indexed 710,639 papers\n",
      "Indexed 718,507 papers\n",
      "Indexed 725,422 papers\n",
      "Indexed 732,609 papers\n",
      "Indexed 741,079 papers\n",
      "Indexed 748,497 papers\n",
      "Indexed 756,569 papers\n",
      "Indexed 765,017 papers\n",
      "Indexed 773,721 papers\n",
      "Indexed 781,996 papers\n",
      "Indexed 790,905 papers\n",
      "Indexed 799,434 papers\n",
      "Indexed 807,952 papers\n",
      "Indexed 816,789 papers\n",
      "Indexed 825,239 papers\n",
      "Indexed 833,491 papers\n",
      "Indexed 842,623 papers\n",
      "Indexed 852,764 papers\n",
      "Indexed 863,724 papers\n",
      "Indexed 884,171 papers\n",
      "Indexed 908,072 papers\n",
      "Indexed 926,439 papers\n",
      "Indexed 947,441 papers\n",
      "Indexed 970,571 papers\n",
      "Indexed 990,834 papers\n",
      "Indexed 1,010,525 papers\n",
      "Indexed 1,028,952 papers\n",
      "Indexed 1,045,161 papers\n",
      "Indexed 1,066,407 papers\n",
      "Indexed 1,087,015 papers\n",
      "Indexed 1,109,901 papers\n",
      "Indexed 1,133,318 papers\n",
      "Indexed 1,159,163 papers\n",
      "Indexed 1,178,772 papers\n",
      "Indexed 1,202,998 papers\n",
      "Indexed 1,227,667 papers\n",
      "Indexed 1,253,180 papers\n",
      "Indexed 1,277,320 papers\n",
      "Indexed 1,299,940 papers\n",
      "Indexed 1,320,441 papers\n",
      "Indexed 1,344,852 papers\n",
      "Indexed 1,369,266 papers\n",
      "Indexed 1,394,007 papers\n",
      "Indexed 1,421,354 papers\n",
      "Indexed 1,450,584 papers\n",
      "Indexed 1,473,823 papers\n",
      "Indexed 1,499,878 papers\n",
      "Indexed 1,529,152 papers\n",
      "Indexed 1,553,940 papers\n",
      "Indexed 1,581,895 papers\n",
      "Indexed 1,602,516 papers\n",
      "Indexed 1,621,684 papers\n",
      "Indexed 1,645,769 papers\n",
      "Indexed 1,667,233 papers\n",
      "Indexed 1,689,206 papers\n",
      "Indexed 1,713,819 papers\n",
      "Indexed 1,735,899 papers\n",
      "Indexed 1,755,414 papers\n",
      "Indexed 1,774,659 papers\n",
      "Indexed 1,794,682 papers\n",
      "Indexed 1,810,481 papers\n",
      "Indexed 1,827,151 papers\n",
      "Indexed 1,841,004 papers\n",
      "Indexed 1,853,108 papers\n",
      "Indexed 1,868,789 papers\n",
      "Indexed 1,883,168 papers\n",
      "Indexed 1,898,870 papers\n",
      "Indexed 1,915,755 papers\n",
      "Indexed 1,932,661 papers\n",
      "Indexed 1,948,450 papers\n",
      "Indexed 1,964,559 papers\n",
      "Indexed 1,984,470 papers\n",
      "Indexed 2,001,799 papers\n",
      "Indexed 2,019,006 papers\n",
      "Indexed 2,033,332 papers\n",
      "Indexed 2,047,013 papers\n",
      "Indexed 2,064,269 papers\n",
      "Indexed 2,079,273 papers\n",
      "Indexed 2,097,114 papers\n",
      "Indexed 2,116,830 papers\n",
      "Indexed 2,134,332 papers\n",
      "Indexed 2,150,308 papers\n",
      "Indexed 2,166,914 papers\n",
      "Indexed 2,189,285 papers\n",
      "Indexed 2,205,965 papers\n",
      "Indexed 2,223,158 papers\n",
      "Indexed 2,238,624 papers\n",
      "Indexed 2,252,832 papers\n",
      "Indexed 2,269,460 papers\n",
      "Indexed 2,285,292 papers\n",
      "Indexed 2,303,576 papers\n",
      "Indexed 2,322,350 papers\n",
      "Indexed 2,340,297 papers\n",
      "Indexed 2,355,854 papers\n",
      "Indexed 2,371,499 papers\n",
      "Indexed 2,391,686 papers\n",
      "Indexed 2,407,183 papers\n",
      "Indexed 2,424,448 papers\n",
      "Indexed 2,438,723 papers\n",
      "Indexed 2,452,933 papers\n",
      "Indexed 2,467,262 papers\n",
      "Indexed 2,482,555 papers\n",
      "Indexed 2,497,499 papers\n",
      "Indexed 2,511,545 papers\n",
      "Indexed 2,523,801 papers\n",
      "Indexed 2,533,674 papers\n",
      "Indexed 2,544,812 papers\n",
      "Indexed 2,552,942 papers\n",
      "Indexed 2,564,520 papers\n",
      "Indexed 2,564,612 papers\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-6\n",
    "\n",
    "for corpus_ids, embeddings in monthly_batch_iterator(\n",
    "        source_con, TABLE_NAME, START_DATE, END_DATE\n",
    "    ):\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = embeddings.astype(\"float32\")\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    embeddings = embeddings / np.clip(norms, 1e-10, None)\n",
    "\n",
    "    if index.ntotal > 0:\n",
    "        # Search neighbors (IDs now returned because of IndexIDMap2)\n",
    "        distances, neighbors = index.search(embeddings, K)\n",
    "        neighbors = neighbors.astype(int)\n",
    "\n",
    "        # Cosine similarity\n",
    "        cosine_sim = np.clip(1 - distances / 2, 0.0, 1.0)\n",
    "\n",
    "        # Novelty metrics\n",
    "        novelty_mean = 1 - cosine_sim.mean(axis=1)\n",
    "        novelty_max = 1 - cosine_sim.max(axis=1)\n",
    "        novelty_min = 1 - cosine_sim.min(axis=1)\n",
    "        novelty_median = 1 - np.median(cosine_sim, axis=1)\n",
    "        novelty_harmonic = K / np.sum(1 / np.maximum(1 - cosine_sim, epsilon), axis=1)\n",
    "        \n",
    "\n",
    "        # Prepare rows for DuckDB insert\n",
    "        rows = [\n",
    "            (\n",
    "                int(corpus_ids[i]),\n",
    "                float(novelty_mean[i]),\n",
    "                float(novelty_max[i]),\n",
    "                float(novelty_min[i]),\n",
    "                float(novelty_median[i]),\n",
    "                float(novelty_harmonic[i])\n",
    "                \n",
    "            )\n",
    "            for i in range(len(corpus_ids))\n",
    "        ]\n",
    "        \n",
    "        neighbors_con.executemany(\"\"\"\n",
    "            INSERT INTO paper_neighbors_metrics (\n",
    "            corpus_id,                    \n",
    "            novelty_mean,                               \n",
    "            novelty_max,\n",
    "            novelty_min,\n",
    "            novelty_median,\n",
    "            novelty_harmonic\n",
    "            )\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", rows)\n",
    "        \n",
    "        # FREE MEMORY\n",
    "        import gc\n",
    "        del neighbors,rows\n",
    "        gc.collect()\n",
    "\n",
    "    # Add current batch to FAISS index\n",
    "    index.add_with_ids(\n",
    "        embeddings,\n",
    "        corpus_ids.astype(\"int64\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Indexed {index.ntotal:,} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6177f782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>novelty_mean</th>\n",
       "      <th>novelty_max</th>\n",
       "      <th>novelty_min</th>\n",
       "      <th>novelty_median</th>\n",
       "      <th>novelty_harmonic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6331294</td>\n",
       "      <td>0.212569</td>\n",
       "      <td>0.154055</td>\n",
       "      <td>0.237483</td>\n",
       "      <td>0.217403</td>\n",
       "      <td>0.210313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>11291405</td>\n",
       "      <td>0.211614</td>\n",
       "      <td>0.134824</td>\n",
       "      <td>0.232631</td>\n",
       "      <td>0.213135</td>\n",
       "      <td>0.210220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>58975810</td>\n",
       "      <td>0.209327</td>\n",
       "      <td>0.141295</td>\n",
       "      <td>0.230281</td>\n",
       "      <td>0.216912</td>\n",
       "      <td>0.207048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>46002453</td>\n",
       "      <td>0.206089</td>\n",
       "      <td>0.146562</td>\n",
       "      <td>0.244218</td>\n",
       "      <td>0.206014</td>\n",
       "      <td>0.203224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>144667741</td>\n",
       "      <td>0.203484</td>\n",
       "      <td>0.140061</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>0.209135</td>\n",
       "      <td>0.201741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097918</th>\n",
       "      <td>259283111</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098106</th>\n",
       "      <td>259328480</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097114</th>\n",
       "      <td>259107395</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116443</th>\n",
       "      <td>259830204</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116498</th>\n",
       "      <td>259856095</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2563401 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus_id  novelty_mean  novelty_max  novelty_min  novelty_median  \\\n",
       "26         6331294      0.212569     0.154055     0.237483        0.217403   \n",
       "52        11291405      0.211614     0.134824     0.232631        0.213135   \n",
       "143       58975810      0.209327     0.141295     0.230281        0.216912   \n",
       "133       46002453      0.206089     0.146562     0.244218        0.206014   \n",
       "1387     144667741      0.203484     0.140061     0.223022        0.209135   \n",
       "...            ...           ...          ...          ...             ...   \n",
       "2097918  259283111      0.000414     0.000004     0.002197        0.000077   \n",
       "2098106  259328480      0.000407     0.000011     0.002016        0.000235   \n",
       "2097114  259107395      0.000406     0.000005     0.002167        0.000079   \n",
       "2116443  259830204      0.000380     0.000015     0.002526        0.000065   \n",
       "2116498  259856095      0.000378     0.000009     0.002512        0.000051   \n",
       "\n",
       "         novelty_harmonic  \n",
       "26               0.210313  \n",
       "52               0.210220  \n",
       "143              0.207048  \n",
       "133              0.203224  \n",
       "1387             0.201741  \n",
       "...                   ...  \n",
       "2097918          0.000020  \n",
       "2098106          0.000212  \n",
       "2097114          0.000022  \n",
       "2116443          0.000048  \n",
       "2116498          0.000028  \n",
       "\n",
       "[2563401 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the output\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"./data/metrics.db\", read_only=True)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    COPY paper_neighbors_metrics\n",
    "    TO 'papers.csv'\n",
    "    (FORMAT CSV, HEADER)\n",
    "\"\"\")\n",
    "df = pd.read_csv('papers.csv')\n",
    "df.sort_values('novelty_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload ./data/metrics.db to hugginface\n",
    "upload_file(\n",
    "    path_or_fileobj=\"./data/metrics.db\",\n",
    "    path_in_repo=\"metrics.db\",\n",
    "    repo_id=\"lalit3c/S2_CS_PHY_PYSCH_papers\",\n",
    "    repo_type=\"dataset\",\n",
    "    commit_message=\"Upload computed metrics database\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
